{
    "MovieScenes": {
        "Unique Dataset Identifier": "moviescenes",
        "Collection": "moviescenes",
        "Collection URL": "https://arxiv.org/abs/2004.02678",
        "Dataset Name": "MovieScenes",
        "Paper Title": "MovieScenes",
        "Paper URL": "https://arxiv.org/abs/2004.02678",
        "GitHub URL": "https://github.com/AnyiRao/SceneSeg",
        "Hugging Face URL": "",
        "Papers with Code URL": "https://paperswithcode.com/paper/a-local-to-global-approach-to-multi-modal",
        "ArXiv URL": "https://arxiv.org/abs/2004.02678",
        "Semantic Scholar Corpus ID": 214802984,
        "Year Released": 2020,
        "Text Sources": [
            "movies"
        ],
        "Licenses": [
            {
                "License": "Unspecified",
                "License URL": ""
            }
        ],
        "Creators": [
            "The Chinese University of Hong Kong",
            "UC Berkeley"
        ],
        "Countries": [
            "China",
            "United States of America"
        ],
        "License Verified By": "Vivek Sharma",
        "Video Hours": 250.0,
        "Taken Down": "False",
        "Video Sources": [
            "movies"
        ],
        "Inferred Metadata": {
            "GitHub License": "",
            "GitHub Stars (June 2024)": 213,
            "GitHub Topics": [
                "boundary-detection",
                "scene",
                "segmentation",
                "video-analysis"
            ],
            "Github Date": "",
            "HF Config": "",
            "HF Config License": "",
            "HF Dataset": "",
            "HF Date": "",
            "HF Downloads (June 2024)": "",
            "HF Likes (June 2024)": "",
            "HF Yaml License": "",
            "PwC Date": "",
            "PwC Description": "4 code implementations in PyTorch and TensorFlow. Scene, as the crucial unit of storytelling in movies, contains complex activities of actors and their interactions in a physical environment. Identifying the composition of scenes serves as a critical step towards semantic understanding of movies. This is very challenging -- compared to the videos studied in conventional vision problems, e.g. action recognition, as scenes in movies usually contain much richer temporal structures and more complex semantic information. Towards this goal, we scale up the scene segmentation task by building a large-scale video dataset MovieScenes, which contains 21K annotated scene segments from 150 movies. We further propose a local-to-global scene segmentation framework, which integrates multi-modal information across three levels, i.e. clip, segment, and movie. This framework is able to distill complex semantics from hierarchical temporal structures over a long movie, providing top-down guidance for scene segmentation. Our experiments show that the proposed network is able to segment a movie into scenes with high accuracy, consistently outperforming previous methods. We also found that pretraining on our MovieScenes can bring significant improvements to the existing approaches.",
            "PwC License Name": "",
            "PwC License URL": "",
            "S2 Citation Count (June 2024)": "",
            "S2 Date": ""
        }
    }
}
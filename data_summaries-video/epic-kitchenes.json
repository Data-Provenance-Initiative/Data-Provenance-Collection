{
    "EPIC-KITCHENS": {
        "Unique Dataset Identifier": "epic-kitchenes",
        "Collection": "epic-kitchenes",
        "Collection URL": "https://arxiv.org/abs/1804.02748",
        "Dataset Name": "EPIC-KITCHENS",
        "Paper Title": "EPIC-KITCHENS",
        "Paper URL": "https://arxiv.org/abs/1804.02748",
        "GitHub URL": "https://github.com/epic-kitchens",
        "Hugging Face URL": "",
        "Papers with Code URL": "https://paperswithcode.com/dataset/epic-kitchens-100",
        "ArXiv URL": "https://arxiv.org/abs/1804.02748",
        "Semantic Scholar Corpus ID": 4710439,
        "Year Released": 2018,
        "Text Sources": [
            "human"
        ],
        "Licenses": [
            {
                "License": "CC BY-NC 4.0",
                "License URL": "https://epic-kitchens.github.io/2024"
            }
        ],
        "Creators": [
            "University of Toronto",
            "University of Bristol",
            "University of Catania"
        ],
        "Countries": [
            "United Kingdom",
            "Canada",
            "Spain"
        ],
        "License Verified By": "Vivek Sharma",
        "Video Hours": 100.0,
        "Taken Down": "False",
        "Video Sources": [
            "human"
        ],
        "Inferred Metadata": {
            "GitHub License": "",
            "GitHub Stars (June 2024)": 0,
            "GitHub Topics": [
                ""
            ],
            "Github Date": "",
            "HF Config": "",
            "HF Config License": "",
            "HF Dataset": "",
            "HF Date": "",
            "HF Downloads (June 2024)": "",
            "HF Likes (June 2024)": "",
            "HF Yaml License": "",
            "PwC Date": "2020-06-23",
            "PwC Description": "This paper introduces the pipeline to scale the largest dataset in egocentric vision EPIC-KITCHENS. The effort culminates in EPIC-KITCHENS-100, a collection of 100 hours, 20M frames, 90K actions in 700 variable-length videos, capturing long-term unscripted activities in 45 environments, using head-mounted cameras. Compared to its previous version (EPIC-KITCHENS-55), EPIC-KITCHENS-100 has been annotated using a novel pipeline that allows denser (54% more actions per minute) and more complete annotations of fine-grained actions (+128% more action segments). This collection also enables evaluating the \"test of time\" - i.e. whether models trained on data collected in 2018 can generalise to new footage collected under the same hypotheses albeit \"two years on\".\nThe dataset is aligned with 6 challenges: action recognition (full and weak supervision), action detection, action anticipation, cross-modal retrieval (from captions), as well as unsupervised domain adaptation for action recognition. For each challenge, we define the task, provide baselines and evaluation metrics.",
            "PwC License Name": "CC BY-NC 4.0",
            "PwC License URL": "https://epic-kitchens.github.io/2021",
            "S2 Citation Count (June 2024)": "",
            "S2 Date": ""
        },
        "Bibtex": "@Article{Damen2018ScalingEV,\n author = {D. Damen and Hazel Doughty and G. Farinella and S. Fidler and Antonino Furnari and E. Kazakos and D. Moltisanti and Jonathan Munro and Toby Perrett and Will Price and Michael Wray},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {Scaling Egocentric Vision: The EPIC-KITCHENS Dataset},\n volume = {abs/1804.02748},\n year = {2018}\n}\n"
    }
}
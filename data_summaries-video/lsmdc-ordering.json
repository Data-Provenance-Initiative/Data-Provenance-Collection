{
    "LSMDC Ordering": {
        "Unique Dataset Identifier": "lsmdc-ordering",
        "Collection": "lsmdc-ordering",
        "Collection URL": "https://arxiv.org/pdf/2004.02205",
        "Dataset Name": "LSMDC Ordering",
        "Paper Title": "LSMDC Ordering",
        "Paper URL": "https://arxiv.org/pdf/2004.02205",
        "GitHub URL": "https://github.com/vivoutlaw/TCBP",
        "Hugging Face URL": "",
        "Papers with Code URL": "https://paperswithcode.com/paper/deep-multimodal-feature-encoding-for-video",
        "ArXiv URL": "https://arxiv.org/pdf/2004.02205",
        "Semantic Scholar Corpus ID": 214802821,
        "Year Released": 2020,
        "Text Sources": [
            "movies"
        ],
        "Licenses": [
            {
                "License": "MIT License",
                "License URL": "https://github.com/vivoutlaw/tcbp/blob/master/LICENSE"
            }
        ],
        "Creators": [
            "University of Toronto",
            "Karlsruhe Institute of Technology",
            "Inria",
            "Massachusetts Institute of Technology"
        ],
        "Countries": [
            "Germany",
            "Canada",
            "United States of America",
            "France"
        ],
        "License Verified By": "Vivek Sharma",
        "Video Hours": 158.0,
        "Taken Down": "False",
        "Video Sources": [
            "movies"
        ],
        "Inferred Metadata": {
            "GitHub License": "MIT License",
            "GitHub Stars (June 2024)": 11,
            "GitHub Topics": [],
            "Github Date": "",
            "HF Config": "",
            "HF Config License": "",
            "HF Dataset": "",
            "HF Date": "",
            "HF Downloads (June 2024)": "",
            "HF Likes (June 2024)": "",
            "HF Yaml License": "",
            "PwC Date": "",
            "PwC Description": "1 code implementation in PyTorch. True understanding of videos comes from a joint analysis of all its modalities: the video frames, the audio track, and any accompanying text such as closed captions. We present a way to learn a compact multimodal feature representation that encodes all these modalities. Our model parameters are learned through a proxy task of inferring the temporal ordering of a set of unordered videos in a timeline. To this end, we create a new multimodal dataset for temporal ordering that consists of approximately 30K scenes (2-6 clips per scene) based on the \"Large Scale Movie Description Challenge\". We analyze and evaluate the individual and joint modalities on three challenging tasks: (i) inferring the temporal ordering of a set of videos; and (ii) action recognition. We demonstrate empirically that multimodal representations are indeed complementary, and can play a key role in improving the performance of many applications.",
            "PwC License Name": "",
            "PwC License URL": "",
            "S2 Citation Count (June 2024)": "",
            "S2 Date": ""
        }
    }
}
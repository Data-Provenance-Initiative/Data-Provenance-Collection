{
    "VIOLIN": {
        "Unique Dataset Identifier": "violin",
        "Collection": "violin",
        "Collection URL": "https://arxiv.org/abs/2003.11618",
        "Dataset Name": "VIOLIN",
        "Paper Title": "VIOLIN",
        "Paper URL": "https://arxiv.org/abs/2003.11618",
        "GitHub URL": "https://github.com/jimmy646/violin",
        "Hugging Face URL": "",
        "Papers with Code URL": "https://paperswithcode.com/dataset/violin",
        "ArXiv URL": "https://arxiv.org/abs/2003.11618",
        "Semantic Scholar Corpus ID": 214668012,
        "Year Released": 2020,
        "Text Sources": [
            "youtube"
        ],
        "Licenses": [
            {
                "License": "Unspecified",
                "License URL": ""
            }
        ],
        "Creators": [
            "Microsoft"
        ],
        "Countries": [
            "United States of America"
        ],
        "License Verified By": "Vivek Sharma",
        "Video Hours": 582.0,
        "Taken Down": "False",
        "Video Sources": [
            "youtube"
        ],
        "Inferred Metadata": {
            "GitHub License": "MIT License",
            "GitHub Stars (June 2024)": 156,
            "GitHub Topics": [],
            "Github Date": "",
            "HF Config": "",
            "HF Config License": "",
            "HF Dataset": "",
            "HF Date": "",
            "HF Downloads (June 2024)": "",
            "HF Likes (June 2024)": "",
            "HF Yaml License": "",
            "PwC Date": "",
            "PwC Description": "Video-and-Language Inference is the task of joint multimodal understanding of video and text. Given a video clip with aligned subtitles as premise, paired with a natural language hypothesis based on the video content, a model needs to infer whether the hypothesis is entailed or contradicted by the given video clip. The Violin dataset is a dataset for this task which consists of 95,322 video-hypothesis pairs from 15,887 video clips, spanning over 582 hours of video. These video clips contain rich content with diverse temporal dynamics, event shifts, and people interactions, collected from two sources: (i) popular TV shows, and (ii) movie clips from YouTube channels.",
            "PwC License Name": "Unspecified",
            "PwC License URL": null,
            "S2 Citation Count (June 2024)": "",
            "S2 Date": ""
        },
        "Bibtex": "@Article{Liu2020ViolinAL,\n author = {J. Liu and Wenhu Chen and Yu Cheng and Zhe Gan and Licheng Yu and Yiming Yang and Jingjing Liu},\n booktitle = {Computer Vision and Pattern Recognition},\n journal = {2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},\n pages = {10897-10907},\n title = {Violin: A Large-Scale Dataset for Video-and-Language Inference},\n year = {2020}\n}\n"
    }
}
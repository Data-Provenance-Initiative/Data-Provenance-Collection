{
    "LEMMA: A Multi-view Dataset for LEarning Multi-agent Multi-task Activities (ECCV 2020)": {
        "Unique Dataset Identifier": "lemma-dataset",
        "Collection": "lemma-dataset",
        "Collection URL": "https://arxiv.org/abs/2007.15781",
        "Dataset Name": "LEMMA: A Multi-view Dataset for LEarning Multi-agent Multi-task Activities (ECCV 2020)",
        "Paper Title": "LEMMA: A Multi-view Dataset for LEarning Multi-agent Multi-task Activities (ECCV 2020)",
        "Paper URL": "https://arxiv.org/abs/2007.15781",
        "GitHub URL": "https://github.com/Buzz-Beater/LEMMA",
        "Hugging Face URL": "",
        "Papers with Code URL": "https://paperswithcode.com/dataset/lemma",
        "ArXiv URL": "https://arxiv.org/abs/2007.15781",
        "Semantic Scholar Corpus ID": 220634784,
        "Year Released": 2020,
        "Text Sources": [
            "crowdsourced",
            "human"
        ],
        "Licenses": [
            {
                "License": "Unspecified",
                "License URL": ""
            }
        ],
        "Creators": [
            "UCLA"
        ],
        "Countries": [
            "United States of America"
        ],
        "License Verified By": "Vivek Sharma",
        "Video Hours": 10.8,
        "Taken Down": "False",
        "Video Sources": [
            "crowdsourced",
            "human"
        ],
        "Task Categories": [
            "Video Classification"
        ],
        "Inferred Metadata": {
            "GitHub License": "",
            "GitHub Stars (June 2024)": 27,
            "GitHub Topics": [],
            "Github Date": "",
            "HF Config": "",
            "HF Config License": "",
            "HF Dataset": "",
            "HF Date": "",
            "HF Downloads (June 2024)": "",
            "HF Likes (June 2024)": "",
            "HF Yaml License": "",
            "PwC Date": "",
            "PwC Description": "The LEMMA dataset aims to explore the essence of complex human activities in a goal-directed, multi-agent, multi-task setting with ground-truth labels of compositional atomic-actions and their associated tasks. By quantifying the scenarios to up to two multi-step tasks with two agents, the authors strive to address human multi-task and multi-agent interactions in four scenarios: single-agent single-task (1 x 1), single-agent multi-task (1 x 2), multi-agent single-task (2 x 1), and multi-agent multi-task (2 x 2). Task instructions are only given to one agent in the 2 x 1 setting to resemble the robot-helping scenario, hoping that the learned perception models could be applied in robotic tasks (especially in HRI) in the near future.\n\nBoth the third-person views (TPVs) and the first-person views (FPVs) were recorded to account for different perspectives of the same activities. The authors densely annotate atomic-actions (in the form of compositional verb-noun pairs) and tasks of each atomic-action, as well as the spatial location of each participating agent (bounding boxes) to facilitate the learning of multi-agent multi-task task scheduling and assignment.",
            "PwC License Name": "Unspecified",
            "PwC License URL": null,
            "S2 Citation Count (June 2024)": "",
            "S2 Date": ""
        },
        "Bibtex": "@Article{Jia2020LEMMAAM,\n author = {Baoxiong Jia and Yixin Chen and Siyuan Huang and Yixin Zhu and Song-Chun Zhu},\n booktitle = {European Conference on Computer Vision},\n journal = {ArXiv},\n title = {LEMMA: A Multi-view Dataset for Learning Multi-agent Multi-task Activities},\n volume = {abs/2007.15781},\n year = {2020}\n}\n"
    }
}
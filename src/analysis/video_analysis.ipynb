{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.2.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "\u001b[31mERROR: Could not find a version that satisfies the requirement install (from versions: none)\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[31mERROR: No matching distribution found for install\u001b[0m\u001b[31m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.2.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Collecting vl-convert-python==1.6.0\n",
      "  Obtaining dependency information for vl-convert-python==1.6.0 from https://files.pythonhosted.org/packages/90/16/405caeb736f870439e153319346b9544f3364efa30c8cd8f07efd8f44600/vl_convert_python-1.6.0-cp37-abi3-macosx_11_0_arm64.whl.metadata\n",
      "  Downloading vl_convert_python-1.6.0-cp37-abi3-macosx_11_0_arm64.whl.metadata (5.2 kB)\n",
      "Downloading vl_convert_python-1.6.0-cp37-abi3-macosx_11_0_arm64.whl (26.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m26.3/26.3 MB\u001b[0m \u001b[31m15.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: vl-convert-python\n",
      "  Attempting uninstall: vl-convert-python\n",
      "    Found existing installation: vl-convert-python 1.6.1\n",
      "    Uninstalling vl-convert-python-1.6.1:\n",
      "      Successfully uninstalled vl-convert-python-1.6.1\n",
      "Successfully installed vl-convert-python-1.6.0\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.2.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip3 install -q -U pandas altair langcodes\n",
    "!pip3 install -q -U install semanticscholar\n",
    "!pip install vl-convert-python==1.6.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "# Append system path\n",
    "sys.path = [p for p in sys.path if not p.endswith(\"../..\")]  # Cleans duplicated '../..'\n",
    "sys.path.insert(0, \"../\")  # This adds `src` to the path\n",
    "import os\n",
    "import numpy as np\n",
    "import typing\n",
    "import pandas as pd\n",
    "import altair as alt\n",
    "alt.data_transformers.disable_max_rows() # Allow using more than 5000 rows, for now\n",
    "import langcodes\n",
    "from collections import defaultdict\n",
    "from helpers import io, filters\n",
    "from typing import Any\n",
    "\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Functions\n",
    "\n",
    "Utility functions to process and transform data summaries:\n",
    "\n",
    "---\n",
    "```python\n",
    "def invert_dict_of_lists(\n",
    "  d: dict[str, list[str]]\n",
    ") -> dict[str, str]\n",
    "```\n",
    "- Inverts a dictionary of lists for easier mapping of constants.\n",
    "---\n",
    "```python\n",
    "def remap_licenses_with_paraphrases(\n",
    "  summaries: list[dict[str, Any]],\n",
    "  paraphrases: dict[str, str]\n",
    ") -> dict[str, Any]\n",
    "``` \n",
    "- Standardizes inconsistent license names in data summaries using predefined paraphrases.\n",
    "---\n",
    "```python\n",
    "def map_license_criteria_multimodal(\n",
    "  data_summary: list[dict[str, Any]],\n",
    "  all_constants: dict[str, dict[str, list[str]]]\n",
    ") -> list[dict[str, Any]]\n",
    "```\n",
    "- Maps license criteria for multimodal datasets, resolving them according to predefined constants.\n",
    "---\n",
    "```python\n",
    "def get_country(x: str) -> list[int]\n",
    "```\n",
    "- Takes a country name as input and returns a list of ISO3166 codes (mostly, of length 1). It handles a special case that appears in some text annotations (\"African Continent\" -> list of ISO codes) and logs a warning for any countries not found in the mapping.\n",
    "---\n",
    "```python\n",
    "def gini(array: np.ndarray) -> float:\n",
    "```\n",
    "- Takes an array of values and computes the Gini coefficient.\n",
    "---\n",
    "```python\n",
    "def factor_year(\n",
    "  df: pd.DataFrame,\n",
    "  column: str = \"Year Released\",\n",
    "  min_year: int = 2013\n",
    ") -> pd.DataFrame:\n",
    "```\n",
    "- Converts the year column into a categorical variable (with years before a given value grouped together).\n",
    "---\n",
    "```python\n",
    "def order_by_grouped_permisiveness(\n",
    "        df: pd.DataFrame,\n",
    "        group_column: str,\n",
    "        licensetype_column: str = \"License Type\",\n",
    "        permissive_licensetypes: list[str] = [\"Commercial\"]\n",
    ") -> pd.Series:\n",
    "```\n",
    "- Computes permisiveness (proportion of license types in a given set, by default only those marked `Commercial`) by a given grouping factor and returns an order for that factor.\n",
    "---\n",
    "```python\n",
    "def reduce_categories_to_topk(\n",
    "    df: pd.DataFrame,\n",
    "    column: str,\n",
    "    k: int = 6\n",
    ") -> pd.DataFrame:\n",
    "```\n",
    "- Reduces the number of categories in a column to `k`, with the rest grouped under `Other`. So returns a `DataFrame` with a version of that column with `k + 1` total categories.\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def invert_dict_of_lists(d: typing.Dict[str, typing.List[str]]) -> typing.Dict[str, str]:\n",
    "    \"\"\"Useful for mapping constants, paraphrases, etc.\n",
    "    These are normally in the form:\n",
    "        { \"Category\": [\"item1\", \"item2\", … ] }\n",
    "    Whereas we want to invert it to:\n",
    "        { \"item1\": \"Category\", \"item2\": \"Category\", … }\n",
    "    \"\"\"\n",
    "    inverted = {}\n",
    "    for k, v in d.items():\n",
    "        for item in v:\n",
    "            inverted[item] = k\n",
    "    return inverted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remap_licenses_with_paraphrases(\n",
    "        summaries: typing.List[typing.Dict[str, Any]],\n",
    "        paraphrases: typing.Dict[str, str]\n",
    "    ) -> typing.Dict[str, Any]:\n",
    "    \"\"\"Map inconsistent license names to shared paraphrases using the constants.\n",
    "    E.g. \"CC-BY-SA 4.0\", \"CC BY SA 4.0\" -> \"CC BY-SA 4.0\"\n",
    "    \"\"\"\n",
    "\n",
    "    for i, summary in enumerate(summaries):\n",
    "        for j, license in enumerate(summary[\"Licenses\"]):\n",
    "            license = license[\"License\"]\n",
    "            summaries[i][\"Licenses\"][j][\"License\"] = paraphrases.get(\n",
    "                license,\n",
    "                license\n",
    "            )\n",
    "    return summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_and_resolve_licenses(\n",
    "    license_infos: typing.List[typing.Tuple[str, str]],\n",
    "    all_constants: typing.Dict[str, typing.Dict[str, typing.List[str]]]\n",
    ") -> typing.List[str]:\n",
    "    \"\"\"Function taken from `text_ft_plots.ipynb`\"\"\"\n",
    "    classified_licenses = []\n",
    "    for (license_name, license_url) in license_infos:\n",
    "        # Classify an individual license\n",
    "        classifications = filters.classify_license(license_name, license_url, all_constants)\n",
    "        classified_licenses.append(classifications)\n",
    "\n",
    "    # By default, multiple licenses yield to the most restrictive one\n",
    "    resolved_criteria = filters.resolve_multiple_licenses(classified_licenses)\n",
    "    return resolved_criteria\n",
    "\n",
    "\n",
    "def add_license_classes_to_summaries(\n",
    "    data_summary: typing.List[typing.Dict[str, Any]],\n",
    "    resolved_classes: typing.Dict[str, typing.List[str]],\n",
    "    aggregator: str\n",
    "):\n",
    "    \"\"\"Function taken from `text_ft_plots.ipynb`\"\"\"\n",
    "    # Update DataFrame with columns for use, attribution, share_alike\n",
    "    for row in data_summary:\n",
    "        row[f\"License Use ({aggregator})\"] = resolved_classes[row[\"Unique Dataset Identifier\"]][0]\n",
    "        row[f\"License Attribution ({aggregator})\"] = resolved_classes[row[\"Unique Dataset Identifier\"]][1]\n",
    "        row[f\"License Share Alike ({aggregator})\"] = resolved_classes[row[\"Unique Dataset Identifier\"]][2]\n",
    "    return data_summary\n",
    "\n",
    "\n",
    "def map_license_criteria_multimodal(\n",
    "    data_summary: typing.List[typing.Dict[str, Any]],\n",
    "    all_constants: typing.Dict[str, typing.Dict[str, typing.List[str]]]\n",
    ") -> typing.List[typing.Dict[str, Any]]:\n",
    "    \"\"\"Variant of `map_license_criteria` that works with multimodal datasets.\n",
    "    Simplified to only include `Licenses` (not HF, etc.).\n",
    "\n",
    "    Function adapted from `text_ft_plots.ipynb`.\n",
    "    \"\"\"\n",
    "\n",
    "    # Unpack licenses for each dataset. {uid --> (license_name, license_url)}\n",
    "    our_uid_to_license_infos = defaultdict(list)\n",
    "\n",
    "    # Same as ours, but excludes OpenAI Terms:\n",
    "    our_uid_to_license_infos_no_openai = defaultdict(list)\n",
    "\n",
    "    for row in data_summary:\n",
    "        uid = row[\"Unique Dataset Identifier\"]\n",
    "        for license_info in row[\"Licenses\"]:\n",
    "            license_name = license_info[\"License\"]\n",
    "            license_url = license_info.get(\"License URL\", None) # FOR NOW\n",
    "            our_uid_to_license_infos[uid].append((license_name, license_url))\n",
    "            if license_info[\"License\"] != \"OpenAI\":\n",
    "                our_uid_to_license_infos_no_openai[uid].append((license_name, license_url))\n",
    "\n",
    "        # If OpenAI was the only license, we add Unspecified so there isn't nothing there.\n",
    "        if len(our_uid_to_license_infos_no_openai[uid]) == 0:\n",
    "            our_uid_to_license_infos_no_openai[uid].append((\"Unspecified\", None))\n",
    "\n",
    "\n",
    "    # classify and resolve licenses for each dataset and each aggregator\n",
    "    ours_resolved, ours_openai_resolved = {}, {}\n",
    "    for uid in our_uid_to_license_infos.keys():\n",
    "        ours_resolved[uid] = classify_and_resolve_licenses(our_uid_to_license_infos[uid], all_constants)\n",
    "        ours_openai_resolved[uid] = classify_and_resolve_licenses(our_uid_to_license_infos_no_openai[uid], all_constants)\n",
    "\n",
    "\n",
    "    data_summary = add_license_classes_to_summaries(data_summary, ours_resolved, \"DataProvenance\")\n",
    "    data_summary = add_license_classes_to_summaries(data_summary, ours_openai_resolved, \"DataProvenance IgnoreOpenAI\")\n",
    "\n",
    "    return data_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gini(array: np.ndarray) -> float:\n",
    "    \"\"\"Calculate the Gini coefficient of a numpy array.\n",
    "\n",
    "    Implementation taken from: https://github.com/oliviaguest/gini\n",
    "    \"\"\"\n",
    "    # based on bottom eq:\n",
    "    # http://www.statsdirect.com/help/generatedimages/equations/equation154.svg\n",
    "    # from:\n",
    "    # http://www.statsdirect.com/help/default.htm#nonparametric_methods/gini.htm\n",
    "    # All values are treated equally, arrays must be 1d:\n",
    "    array = array.flatten()\n",
    "    if np.amin(array) < 0:\n",
    "        # Values cannot be negative:\n",
    "        array -= np.amin(array)\n",
    "    # Values cannot be 0:\n",
    "    array = array + 0.0000001\n",
    "    # Values must be sorted:\n",
    "    array = np.sort(array)\n",
    "    # Index per array element:\n",
    "    index = np.arange(1,array.shape[0]+1)\n",
    "    # Number of array elements:\n",
    "    n = array.shape[0]\n",
    "    # Gini coefficient:\n",
    "    return ((np.sum((2 * index - n  - 1) * array)) / (n * np.sum(array)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bootstrap_cis_for_gini(\n",
    "    data: np.ndarray,\n",
    "    n_samples: int = 1000,\n",
    "    alpha: float = 0.05\n",
    ") -> typing.Tuple[float, float]:\n",
    "    \"\"\"Calculate the confidence interval for the Gini coefficient using bootstrapping.\n",
    "    \"\"\"\n",
    "\n",
    "    ginis = []\n",
    "    for _ in range(n_samples):\n",
    "        sample = np.random.choice(data, size=len(data), replace=True)\n",
    "        ginis.append(gini(sample))\n",
    "\n",
    "    ginis = np.array(ginis)\n",
    "    lower_bound = np.percentile(ginis, alpha / 2 * 100)\n",
    "    upper_bound = np.percentile(ginis, (1 - alpha / 2) * 100)\n",
    "\n",
    "    return np.mean(ginis), lower_bound, upper_bound"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def factor_year(\n",
    "    df: pd.DataFrame,\n",
    "    column: str = \"Year Released\",\n",
    "    min_year: int = 2004\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"Transform the year column into a categorical column.\n",
    "\n",
    "    Years before `min_year` are grouped into a category, i.e. \"<`min_year`\" (e.g. )\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "\n",
    "    min_yeartext = \"<%d\" % min_year\n",
    "    max_year = df[column].max().astype(int)\n",
    "\n",
    "    df[column] = df[column].map(\n",
    "        lambda x: min_yeartext if (x < min_year) else str(x)\n",
    "    )\n",
    "\n",
    "    order = [min_yeartext, *map(str, range(min_year, max_year + 1))]\n",
    "\n",
    "    df[column] = pd.Categorical(\n",
    "        df[column],\n",
    "        categories=order,\n",
    "        ordered=True\n",
    "    )\n",
    "\n",
    "    return df, order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def order_by_grouped_permisiveness(\n",
    "        df: pd.DataFrame,\n",
    "        group_column: str,\n",
    "        licensetype_column: str = \"License Type\",\n",
    "        permissive_licensetypes: typing.List[str] = [\"Commercial\"]\n",
    ") -> pd.Series:\n",
    "    \"\"\"Given a DataFrame, group it by `group_column` and calculate the permissiveness of each group.\n",
    "\n",
    "    Permisiveness is calculated as the proportion of licenses that are in `permissive_licensetypes`.\n",
    "    \"\"\"\n",
    "    permisiveness = df.groupby(group_column).apply(\n",
    "        lambda x: (x[licensetype_column].isin(permissive_licensetypes)).mean()\n",
    "    ).reset_index(name=\"Permissiveness\")\n",
    "\n",
    "    permisiveness_order = permisiveness.sort_values(by=\"Permissiveness\")[group_column].tolist()\n",
    "\n",
    "    return permisiveness_order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduce_categories_to_topk(\n",
    "    df: pd.DataFrame,\n",
    "    column: str,\n",
    "    k: int = 6\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"Reduce the number of categories in a column to the top `k` categories.\n",
    "\n",
    "    The rest are grouped into an \"Other\" category.\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    topk = df[column].value_counts().head(k).index.tolist()\n",
    "    df[column] = df[column].map(\n",
    "        lambda x: x if x in topk else \"Other\"\n",
    "    )\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read Constants and Summaries\n",
    "\n",
    "Load constants and data summaries from JSON files. Constants provide mappings and criteria for licenses, creator groups, various other categories. Data summaries contain modality-specific information about datasets.\n",
    "\n",
    "- `all_constants`: Dictionary containing all predefined constants.\n",
    "- `video_summaries`: Data summaries for speech."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_constants = io.read_all_constants(\"../../constants/\")\n",
    "video_summaries = io.read_data_summary_json(\"../..//data_summaries-video\")\n",
    "license_paraphrases = invert_dict_of_lists(all_constants[\"LICENSE_PARAPHRASES\"])\n",
    "creator_categories = invert_dict_of_lists(all_constants[\"CREATOR_GROUPS\"])\n",
    "\n",
    "\n",
    "video_summaries = map_license_criteria_multimodal(\n",
    "    remap_licenses_with_paraphrases(\n",
    "        video_summaries,\n",
    "        license_paraphrases\n",
    "    ),\n",
    "    all_constants\n",
    ")\n",
    "\n",
    "df_video = pd.DataFrame(video_summaries)\n",
    "# do some checks on the video dataset\n",
    "# raise if there are any missing values in the license, year released, video sources, tasks\n",
    "assert df_video[\"Licenses\"].apply(lambda x: len(x) == 0).sum() == 0, \"print the rows with missing licenses: \\n\" + str(df_video[df_video[\"Licenses\"].apply(lambda x: len(x) == 0)])\n",
    "assert df_video[\"Year Released\"].isna().sum() == 0, \"print the rows with missing year released: \\n\" + str(df_video[df_video[\"Year Released\"].isna()])\n",
    "assert df_video[\"Video Sources\"].apply(lambda x: len(x) == 0).sum() == 0, \"print the rows with missing video sources: \\n\" + str(df_video[df_video[\"Video Sources\"].apply(lambda x: len(x) == 0)])\n",
    "assert df_video[\"Task Categories\"].apply(lambda x: len(x) == 0).sum() == 0, \"print the rows with missing tasks: \\n\" + str(df_video[df_video[\"Tasks\"].apply(lambda x: len(x) == 0)])\n",
    "\n",
    "df_video, YEARS_ORDER = factor_year(df_video)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9387673958184601"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Overall Gini coefficient (hours by dataset)\n",
    "gini(df_video[\"Video Hours\"].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting constants\n",
    "FONT_SIZE = 20\n",
    "LEGEND_POSITION = \"bottom\"\n",
    "PLOT_TOFILE = False # Whether and where to output plots\n",
    "PLOT_DIR = \"/home/gridsan/ktiwary/src/dpi-ktiwary-fork/dpi-plots/video\"\n",
    "PLOT_PPI = 300\n",
    "MAX_LABELLIMIT = 1000 # Large number to avoid label summarization in plots\n",
    "\n",
    "if PLOT_TOFILE:\n",
    "    PLOT_DIR = os.path.expanduser(PLOT_DIR)\n",
    "    os.makedirs(PLOT_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ThemeRegistry.enable('times_newroman')"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def times_newroman():\n",
    "    font = \"Times New Roman\"\n",
    "\n",
    "    return {\n",
    "          \"config\" : {\n",
    "               \"title\": {\"font\": font},\n",
    "               \"axis\": {\n",
    "               \"labelFont\": font,\n",
    "               \"titleFont\": font\n",
    "          },\n",
    "          \"header\": {\n",
    "               \"labelFont\": font,\n",
    "               \"titleFont\": font\n",
    "          },\n",
    "          \"legend\": {\n",
    "               \"labelFont\": font,\n",
    "               \"titleFont\": font\n",
    "          },\n",
    "          \"text\": {\n",
    "               \"font\": font\n",
    "          }\n",
    "     }\n",
    "}\n",
    "\n",
    "alt.themes.register(\"times_newroman\", times_newroman)\n",
    "alt.themes.enable(\"times_newroman\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## License Use Vs. Source Category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting constants\n",
    "LICENSE_ORDER = [\"Non-Commercial/Academic\", \"Unspecified\", \"Commercial\"]\n",
    "LICENSE_PALETTE = [\"#e04c71\", \"#e0cd92\", \"#82b5cf\"]\n",
    "LICENSE_PLOTW = 600\n",
    "LICENSE_PLOTH = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map to main DPI license types\n",
    "df_video[\"License Type\"] = df_video[\"License Use (DataProvenance)\"].map({\n",
    "    \"academic-only\": \"Non-Commercial/Academic\",\n",
    "    \"non-commercial\": \"Non-Commercial/Academic\",\n",
    "    \"unspecified\": \"Unspecified\",\n",
    "    \"commercial\": \"Commercial\"\n",
    "})\n",
    "\n",
    "df_video[\"License Type\"] = pd.Categorical(\n",
    "    df_video[\"License Type\"],\n",
    "    categories=LICENSE_ORDER,\n",
    "    ordered=True\n",
    ")\n",
    "df_video = df_video.sort_values(by=\"License Type\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Video Sources</th>\n",
       "      <th>License Type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>crowdsourced</td>\n",
       "      <td>Non-Commercial/Academic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>crowdsourced</td>\n",
       "      <td>Commercial</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>crowdsourced</td>\n",
       "      <td>Non-Commercial/Academic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>crowdsourced</td>\n",
       "      <td>Unspecified</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>crowdsourced</td>\n",
       "      <td>Commercial</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Video Sources             License Type\n",
       "32  crowdsourced  Non-Commercial/Academic\n",
       "34  crowdsourced               Commercial\n",
       "42  crowdsourced  Non-Commercial/Academic\n",
       "91  crowdsourced              Unspecified\n",
       "13  crowdsourced               Commercial"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Remap language families for condensed plots\n",
    "INCLUDE_TOP_N_CATEGORIES = 8\n",
    "df_videosourcelicences = df_video.explode(\"Video Sources\")\n",
    "df_videosourcelicences = reduce_categories_to_topk(df_videosourcelicences, \"Video Sources\", INCLUDE_TOP_N_CATEGORIES)\n",
    "\n",
    "\n",
    "# Calculate permissiveness by language family (defined as the proportion of commercial licenses)\n",
    "permisiveness = df_videosourcelicences.groupby(\"Video Sources\").apply(\n",
    "    lambda x: (x[\"License Type\"] == \"Commercial\").mean()\n",
    ").reset_index(name=\"Permissiveness\")\n",
    "\n",
    "# Sort by computed permisiveness\n",
    "videosources_order = permisiveness.sort_values(by=\"Permissiveness\")[\"Video Sources\"].tolist()\n",
    "\n",
    "# Make factor\n",
    "df_videosourcelicences[\"Video Sources\"] = pd.Categorical(\n",
    "    df_videosourcelicences[\"Video Sources\"],\n",
    "    categories=videosources_order,\n",
    "    ordered=True\n",
    ")\n",
    "\n",
    "# Sort by Video Sources\n",
    "df_videosourcelicences = df_videosourcelicences.sort_values(by=\"Video Sources\")\n",
    "df_videosourcelicences.head()[['Video Sources', 'License Type']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "  #altair-viz-7709e444312c4858adbc185df5c15ea4.vega-embed {\n",
       "    width: 100%;\n",
       "    display: flex;\n",
       "  }\n",
       "\n",
       "  #altair-viz-7709e444312c4858adbc185df5c15ea4.vega-embed details,\n",
       "  #altair-viz-7709e444312c4858adbc185df5c15ea4.vega-embed details summary {\n",
       "    position: relative;\n",
       "  }\n",
       "</style>\n",
       "<div id=\"altair-viz-7709e444312c4858adbc185df5c15ea4\"></div>\n",
       "<script type=\"text/javascript\">\n",
       "  var VEGA_DEBUG = (typeof VEGA_DEBUG == \"undefined\") ? {} : VEGA_DEBUG;\n",
       "  (function(spec, embedOpt){\n",
       "    let outputDiv = document.currentScript.previousElementSibling;\n",
       "    if (outputDiv.id !== \"altair-viz-7709e444312c4858adbc185df5c15ea4\") {\n",
       "      outputDiv = document.getElementById(\"altair-viz-7709e444312c4858adbc185df5c15ea4\");\n",
       "    }\n",
       "    const paths = {\n",
       "      \"vega\": \"https://cdn.jsdelivr.net/npm/vega@5?noext\",\n",
       "      \"vega-lib\": \"https://cdn.jsdelivr.net/npm/vega-lib?noext\",\n",
       "      \"vega-lite\": \"https://cdn.jsdelivr.net/npm/vega-lite@5.20.1?noext\",\n",
       "      \"vega-embed\": \"https://cdn.jsdelivr.net/npm/vega-embed@6?noext\",\n",
       "    };\n",
       "\n",
       "    function maybeLoadScript(lib, version) {\n",
       "      var key = `${lib.replace(\"-\", \"\")}_version`;\n",
       "      return (VEGA_DEBUG[key] == version) ?\n",
       "        Promise.resolve(paths[lib]) :\n",
       "        new Promise(function(resolve, reject) {\n",
       "          var s = document.createElement('script');\n",
       "          document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
       "          s.async = true;\n",
       "          s.onload = () => {\n",
       "            VEGA_DEBUG[key] = version;\n",
       "            return resolve(paths[lib]);\n",
       "          };\n",
       "          s.onerror = () => reject(`Error loading script: ${paths[lib]}`);\n",
       "          s.src = paths[lib];\n",
       "        });\n",
       "    }\n",
       "\n",
       "    function showError(err) {\n",
       "      outputDiv.innerHTML = `<div class=\"error\" style=\"color:red;\">${err}</div>`;\n",
       "      throw err;\n",
       "    }\n",
       "\n",
       "    function displayChart(vegaEmbed) {\n",
       "      vegaEmbed(outputDiv, spec, embedOpt)\n",
       "        .catch(err => showError(`Javascript Error: ${err.message}<br>This usually means there's a typo in your chart specification. See the javascript console for the full traceback.`));\n",
       "    }\n",
       "\n",
       "    if(typeof define === \"function\" && define.amd) {\n",
       "      requirejs.config({paths});\n",
       "      require([\"vega-embed\"], displayChart, err => showError(`Error loading script: ${err.message}`));\n",
       "    } else {\n",
       "      maybeLoadScript(\"vega\", \"5\")\n",
       "        .then(() => maybeLoadScript(\"vega-lite\", \"5.20.1\"))\n",
       "        .then(() => maybeLoadScript(\"vega-embed\", \"6\"))\n",
       "        .catch(showError)\n",
       "        .then(() => displayChart(vegaEmbed));\n",
       "    }\n",
       "  })({\"config\": {\"title\": {\"font\": \"Times New Roman\"}, \"axis\": {\"labelFont\": \"Times New Roman\", \"titleFont\": \"Times New Roman\", \"labelFontSize\": 20, \"titleFontSize\": 20}, \"header\": {\"labelFont\": \"Times New Roman\", \"titleFont\": \"Times New Roman\"}, \"legend\": {\"labelFont\": \"Times New Roman\", \"titleFont\": \"Times New Roman\", \"labelFontSize\": 20, \"labelLimit\": 1000, \"orient\": \"bottom\", \"titleFontSize\": 20}, \"text\": {\"font\": \"Times New Roman\"}}, \"layer\": [{\"mark\": {\"type\": \"bar\"}, \"encoding\": {\"color\": {\"field\": \"License Type\", \"scale\": {\"domain\": [\"Non-Commercial/Academic\", \"Unspecified\", \"Commercial\"], \"range\": [\"#e04c71\", \"#e0cd92\", \"#82b5cf\"]}, \"title\": \"License Type\", \"type\": \"nominal\"}, \"x\": {\"axis\": {\"labelAngle\": -30}, \"field\": \"Video Sources\", \"sort\": [\"crowdsourced\", \"human\", \"undisclosed web\", \"movies\", \"youtube\", \"Other\", \"getty-images\", \"flickr\", \"google videos\"], \"title\": \"Video Sources\", \"type\": \"nominal\"}, \"y\": {\"aggregate\": \"count\", \"axis\": {\"format\": \"%\"}, \"stack\": \"normalize\", \"title\": \"Pct. Datasets\", \"type\": \"quantitative\"}}}, {\"mark\": {\"type\": \"text\", \"align\": \"center\", \"baseline\": \"top\", \"dy\": -68, \"fontSize\": 12}, \"encoding\": {\"text\": {\"aggregate\": \"count\", \"type\": \"quantitative\"}, \"x\": {\"field\": \"Video Sources\", \"sort\": [\"crowdsourced\", \"human\", \"undisclosed web\", \"movies\", \"youtube\", \"Other\", \"getty-images\", \"flickr\", \"google videos\"], \"title\": \"Video Sources\", \"type\": \"nominal\"}}}], \"data\": {\"name\": \"data-2b7aa37b3e32b2c2c1a4dda40beecf0a\"}, \"height\": 100, \"width\": 600, \"$schema\": \"https://vega.github.io/schema/vega-lite/v5.20.1.json\", \"datasets\": {\"data-2b7aa37b3e32b2c2c1a4dda40beecf0a\": [{\"Unique Dataset Identifier\": \"ntu-rgbd\", \"Collection\": \"ntu-rgbd\", \"Collection URL\": \"https://arxiv.org/pdf/1604.02808.pdf\", \"Dataset Name\": \"NTU RGB+D: A Large Scale Dataset for 3D Human Activity Analysis (CVPR 2016, TPAMI 2019)\", \"Paper Title\": \"NTU RGB+D: A Large Scale Dataset for 3D Human Activity Analysis (CVPR 2016, TPAMI 2019)\", \"Paper URL\": \"https://arxiv.org/pdf/1604.02808.pdf\", \"GitHub URL\": \"https://github.com/shahroudy/NTURGB-D\", \"Hugging Face URL\": \"\", \"Papers with Code URL\": \"https://paperswithcode.com/paper/ntu-rgbd-a-large-scale-dataset-for-3d-human\", \"ArXiv URL\": \"https://arxiv.org/pdf/1604.02808.pdf\", \"Semantic Scholar Corpus ID\": 15928602, \"Year Released\": \"2016\", \"Text Sources\": [\"crowdsourced\"], \"Licenses\": [{\"License\": \"Custom\", \"License URL\": \"https://rose1.ntu.edu.sg/dataset/actionRecognition/\"}], \"Creators\": [\"Nanyang Technological University\"], \"Countries\": [\"Singapore\"], \"License Verified By\": \"Vivek Sharma\", \"Video Hours\": 74.1, \"Taken Down\": \"False\", \"Video Sources\": \"crowdsourced\", \"Task Categories\": [\"Video Classification\"], \"Inferred Metadata\": {\"GitHub License\": \"\", \"GitHub Stars (June 2024)\": 715, \"GitHub Topics\": [], \"Github Date\": \"\", \"HF Config\": \"\", \"HF Config License\": \"\", \"HF Dataset\": \"\", \"HF Date\": \"\", \"HF Downloads (June 2024)\": \"\", \"HF Likes (June 2024)\": \"\", \"HF Yaml License\": \"\", \"PwC Date\": \"\", \"PwC Description\": \"#4 best model for Skeleton Based Action Recognition on Varying-view RGB-D Action-Skeleton (Accuracy (CS) metric)\", \"PwC License Name\": \"\", \"PwC License URL\": \"\", \"S2 Citation Count (June 2024)\": \"\", \"S2 Date\": \"\"}, \"Bibtex\": \"@Article{Shahroudy2016NTURA,\\n author = {Amir Shahroudy and Jun Liu and T. Ng and G. Wang},\\n booktitle = {Computer Vision and Pattern Recognition},\\n journal = {2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},\\n pages = {1010-1019},\\n title = {NTU RGB+D: A Large Scale Dataset for 3D Human Activity Analysis},\\n year = {2016}\\n}\\n\", \"License Use (DataProvenance)\": \"academic-only\", \"License Attribution (DataProvenance)\": 1, \"License Share Alike (DataProvenance)\": 0, \"License Use (DataProvenance IgnoreOpenAI)\": \"academic-only\", \"License Attribution (DataProvenance IgnoreOpenAI)\": 1, \"License Share Alike (DataProvenance IgnoreOpenAI)\": 0, \"License Type\": \"Non-Commercial/Academic\"}, {\"Unique Dataset Identifier\": \"20bn-something\", \"Collection\": \"20bn-something\", \"Collection URL\": \"https://arxiv.org/abs/1706.04261\", \"Dataset Name\": \"20BN-SOMETHING-SOMETHING: The \\\"something something\\\" video database for learning and evaluating visual common sense\", \"Paper Title\": \"20BN-SOMETHING-SOMETHING: The \\\"something something\\\" video database for learning and evaluating visual common sense\", \"Paper URL\": \"https://arxiv.org/abs/1706.04261\", \"GitHub URL\": \"\", \"Hugging Face URL\": \"\", \"Papers with Code URL\": \"https://paperswithcode.com/dataset/something-something-v2\", \"ArXiv URL\": \"https://arxiv.org/abs/1706.04261\", \"Semantic Scholar Corpus ID\": 834612, \"Year Released\": \"2017\", \"Text Sources\": [\"crowdsourced\"], \"Licenses\": [{\"License\": \"Custom\", \"License URL\": \"https://developer.qualcomm.com/software/ai-datasets/something-something\"}], \"Creators\": [\"Twenty Billion Neurons GmbH\"], \"Countries\": [\"Germany\"], \"License Verified By\": \"Vivek Sharma\", \"Video Hours\": 121.46, \"Taken Down\": \"False\", \"Video Sources\": \"crowdsourced\", \"Task Categories\": [\"Video Classification\"], \"Inferred Metadata\": {\"GitHub License\": \"\", \"GitHub Stars (June 2024)\": \"\", \"GitHub Topics\": \"\", \"Github Date\": \"\", \"HF Config\": \"\", \"HF Config License\": \"\", \"HF Dataset\": \"\", \"HF Date\": \"\", \"HF Downloads (June 2024)\": \"\", \"HF Likes (June 2024)\": \"\", \"HF Yaml License\": \"\", \"PwC Date\": \"2017-01-01\", \"PwC Description\": \"The 20BN-SOMETHING-SOMETHING V2 dataset is a large collection of labeled video clips that show humans performing pre-defined basic actions with everyday objects. The dataset was created by a large number of crowd workers. It allows machine learning models to develop fine-grained understanding of basic actions that occur in the physical world. It contains 220,847 videos, with 168,913 in the training set, 24,777 in the validation set and 27,157 in the test set. There are 174 labels.\\n\\nSource\\n\\nImage Source\", \"PwC License Name\": \"Various\", \"PwC License URL\": \"https://20bn.com/licensing/datasets/academic\", \"S2 Citation Count (June 2024)\": \"\", \"S2 Date\": \"\"}, \"Bibtex\": \"@Article{Goyal2017TheS,\\n author = {Raghav Goyal and Samira Ebrahimi Kahou and Vincent Michalski and Joanna Materzynska and S. Westphal and Heuna Kim and V. Haenel and Ingo Fr\\u00fcnd and P. Yianilos and Moritz Mueller-Freitag and F. Hoppe and Christian Thurau and Ingo Bax and R. Memisevic},\\n booktitle = {IEEE International Conference on Computer Vision},\\n journal = {2017 IEEE International Conference on Computer Vision (ICCV)},\\n pages = {5843-5851},\\n title = {The \\u201cSomething Something\\u201d Video Database for Learning and Evaluating Visual Common Sense},\\n year = {2017}\\n}\\n\", \"License Use (DataProvenance)\": \"commercial\", \"License Attribution (DataProvenance)\": 1, \"License Share Alike (DataProvenance)\": 1, \"License Use (DataProvenance IgnoreOpenAI)\": \"commercial\", \"License Attribution (DataProvenance IgnoreOpenAI)\": 1, \"License Share Alike (DataProvenance IgnoreOpenAI)\": 1, \"License Type\": \"Commercial\"}, {\"Unique Dataset Identifier\": \"titan\", \"Collection\": \"titan\", \"Collection URL\": \"https://openaccess.thecvf.com/content_CVPR_2020/papers/Malla_TITAN_Future_Forecast_Using_Action_Priors_CVPR_2020_paper.pdf\", \"Dataset Name\": \"TITAN: Future Forecast using Action Priors (CVPR 2020)\", \"Paper Title\": \"TITAN: Future Forecast using Action Priors (CVPR 2020)\", \"Paper URL\": \"https://openaccess.thecvf.com/content_CVPR_2020/papers/Malla_TITAN_Future_Forecast_Using_Action_Priors_CVPR_2020_paper.pdf\", \"GitHub URL\": \"\", \"Hugging Face URL\": \"\", \"Papers with Code URL\": \"https://paperswithcode.com/paper/titan-future-forecast-using-action-priors\", \"ArXiv URL\": \"https://openaccess.thecvf.com/content_CVPR_2020/papers/Malla_TITAN_Future_Forecast_Using_Action_Priors_CVPR_2020_paper.pdf\", \"Semantic Scholar Corpus ID\": 214727763, \"Year Released\": \"2020\", \"Text Sources\": [\"crowdsourced\"], \"Licenses\": [{\"License\": \"Non Commercial\", \"License URL\": \"https://usa.honda-ri.com/titan\"}], \"Creators\": [\"Honda Research Institute\"], \"Countries\": [\"United States of America\"], \"License Verified By\": \"Vivek Sharma\", \"Video Hours\": 2.91, \"Taken Down\": \"False\", \"Video Sources\": \"crowdsourced\", \"Task Categories\": [\"Video Classification\"], \"Inferred Metadata\": {\"GitHub License\": \"\", \"GitHub Stars (June 2024)\": \"\", \"GitHub Topics\": \"\", \"Github Date\": \"\", \"HF Config\": \"\", \"HF Config License\": \"\", \"HF Dataset\": \"\", \"HF Date\": \"\", \"HF Downloads (June 2024)\": \"\", \"HF Likes (June 2024)\": \"\", \"HF Yaml License\": \"\", \"PwC Date\": \"\", \"PwC Description\": \"We consider the problem of predicting the future trajectory of scene agents from egocentric views obtained from a moving platform. This problem is important in a variety of domains, particularly for autonomous systems making reactive or strategic decisions in navigation. In an attempt to address this problem, we introduce TITAN (Trajectory Inference using Targeted Action priors Network), a new model that incorporates prior positions, actions, and context to forecast future trajectory of agents and future ego-motion. In the absence of an appropriate dataset for this task, we created the TITAN dataset that consists of 700 labeled video-clips (with odometry) captured from a moving vehicle on highly interactive urban traffic scenes in Tokyo. Our dataset includes 50 labels including vehicle states and actions, pedestrian age groups, and targeted pedestrian action attributes that are organized hierarchically corresponding to atomic, simple/complex-contextual, transportive, and communicative actions. To evaluate our model, we conducted extensive experiments on the TITAN dataset, revealing significant performance improvement against baselines and state-of-the-art algorithms. We also report promising results from our Agent Importance Mechanism (AIM), a module which provides insight into assessment of perceived risk by calculating the relative influence of each agent on the future ego-trajectory. The dataset is available at https://usa.honda-ri.com/titan\", \"PwC License Name\": \"\", \"PwC License URL\": \"\", \"S2 Citation Count (June 2024)\": \"\", \"S2 Date\": \"\"}, \"Bibtex\": \"@Article{Malla2020TITANFF,\\n author = {Srikanth Malla and B. Dariush and Chiho Choi},\\n booktitle = {Computer Vision and Pattern Recognition},\\n journal = {2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},\\n pages = {11183-11193},\\n title = {TITAN: Future Forecast Using Action Priors},\\n year = {2020}\\n}\\n\", \"License Use (DataProvenance)\": \"non-commercial\", \"License Attribution (DataProvenance)\": 0, \"License Share Alike (DataProvenance)\": 0, \"License Use (DataProvenance IgnoreOpenAI)\": \"non-commercial\", \"License Attribution (DataProvenance IgnoreOpenAI)\": 0, \"License Share Alike (DataProvenance IgnoreOpenAI)\": 0, \"License Type\": \"Non-Commercial/Academic\"}, {\"Unique Dataset Identifier\": \"lemma-dataset\", \"Collection\": \"lemma-dataset\", \"Collection URL\": \"https://arxiv.org/abs/2007.15781\", \"Dataset Name\": \"LEMMA: A Multi-view Dataset for LEarning Multi-agent Multi-task Activities (ECCV 2020)\", \"Paper Title\": \"LEMMA: A Multi-view Dataset for LEarning Multi-agent Multi-task Activities (ECCV 2020)\", \"Paper URL\": \"https://arxiv.org/abs/2007.15781\", \"GitHub URL\": \"https://github.com/Buzz-Beater/LEMMA\", \"Hugging Face URL\": \"\", \"Papers with Code URL\": \"https://paperswithcode.com/dataset/lemma\", \"ArXiv URL\": \"https://arxiv.org/abs/2007.15781\", \"Semantic Scholar Corpus ID\": 220634784, \"Year Released\": \"2020\", \"Text Sources\": [\"crowdsourced\", \"human\"], \"Licenses\": [{\"License\": \"Unspecified\", \"License URL\": \"\"}], \"Creators\": [\"UCLA\"], \"Countries\": [\"United States of America\"], \"License Verified By\": \"Vivek Sharma\", \"Video Hours\": 10.8, \"Taken Down\": \"False\", \"Video Sources\": \"crowdsourced\", \"Task Categories\": [\"Video Classification\"], \"Inferred Metadata\": {\"GitHub License\": \"\", \"GitHub Stars (June 2024)\": 27, \"GitHub Topics\": [], \"Github Date\": \"\", \"HF Config\": \"\", \"HF Config License\": \"\", \"HF Dataset\": \"\", \"HF Date\": \"\", \"HF Downloads (June 2024)\": \"\", \"HF Likes (June 2024)\": \"\", \"HF Yaml License\": \"\", \"PwC Date\": \"\", \"PwC Description\": \"The LEMMA dataset aims to explore the essence of complex human activities in a goal-directed, multi-agent, multi-task setting with ground-truth labels of compositional atomic-actions and their associated tasks. By quantifying the scenarios to up to two multi-step tasks with two agents, the authors strive to address human multi-task and multi-agent interactions in four scenarios: single-agent single-task (1 x 1), single-agent multi-task (1 x 2), multi-agent single-task (2 x 1), and multi-agent multi-task (2 x 2). Task instructions are only given to one agent in the 2 x 1 setting to resemble the robot-helping scenario, hoping that the learned perception models could be applied in robotic tasks (especially in HRI) in the near future.\\n\\nBoth the third-person views (TPVs) and the first-person views (FPVs) were recorded to account for different perspectives of the same activities. The authors densely annotate atomic-actions (in the form of compositional verb-noun pairs) and tasks of each atomic-action, as well as the spatial location of each participating agent (bounding boxes) to facilitate the learning of multi-agent multi-task task scheduling and assignment.\", \"PwC License Name\": \"Unspecified\", \"PwC License URL\": null, \"S2 Citation Count (June 2024)\": \"\", \"S2 Date\": \"\"}, \"Bibtex\": \"@Article{Jia2020LEMMAAM,\\n author = {Baoxiong Jia and Yixin Chen and Siyuan Huang and Yixin Zhu and Song-Chun Zhu},\\n booktitle = {European Conference on Computer Vision},\\n journal = {ArXiv},\\n title = {LEMMA: A Multi-view Dataset for Learning Multi-agent Multi-task Activities},\\n volume = {abs/2007.15781},\\n year = {2020}\\n}\\n\", \"License Use (DataProvenance)\": \"unspecified\", \"License Attribution (DataProvenance)\": 0, \"License Share Alike (DataProvenance)\": 0, \"License Use (DataProvenance IgnoreOpenAI)\": \"unspecified\", \"License Attribution (DataProvenance IgnoreOpenAI)\": 0, \"License Share Alike (DataProvenance IgnoreOpenAI)\": 0, \"License Type\": \"Unspecified\"}, {\"Unique Dataset Identifier\": \"20BN-jester\", \"Collection\": \"20BN-jester\", \"Collection URL\": \"https://openaccess.thecvf.com/content_ICCVW_2019/papers/HANDS/Materzynska_The_Jester_Dataset_A_Large-Scale_Video_Dataset_of_Human_Gestures_ICCVW_2019_paper.pdf\", \"Dataset Name\": \"20BN-jester: The Jester Dataset: A Large-Scale Video Dataset of Human Gestures (ICCVW 2019)\", \"Paper Title\": \"20BN-jester: The Jester Dataset: A Large-Scale Video Dataset of Human Gestures (ICCVW 2019)\", \"Paper URL\": \"https://openaccess.thecvf.com/content_ICCVW_2019/papers/HANDS/Materzynska_The_Jester_Dataset_A_Large-Scale_Video_Dataset_of_Human_Gestures_ICCVW_2019_paper.pdf\", \"GitHub URL\": \"\", \"Hugging Face URL\": \"\", \"Papers with Code URL\": \"\", \"ArXiv URL\": \"https://openaccess.thecvf.com/content_ICCVW_2019/papers/HANDS/Materzynska_The_Jester_Dataset_A_Large-Scale_Video_Dataset_of_Human_Gestures_ICCVW_2019_paper.pdf\", \"Semantic Scholar Corpus ID\": 208010438, \"Year Released\": \"2019\", \"Text Sources\": [\"crowdsourced\"], \"Licenses\": [{\"License\": \"Custom\", \"License URL\": \"https://developer.qualcomm.com/software/ai-datasets/jester\"}], \"Creators\": [\"Twenty Billion Neurons GmbH\"], \"Countries\": [\"Germany\"], \"License Verified By\": \"Vivek Sharma\", \"Video Hours\": 13.0, \"Taken Down\": \"False\", \"Video Sources\": \"crowdsourced\", \"Task Categories\": [\"Video Classification\"], \"Inferred Metadata\": {\"GitHub License\": \"\", \"GitHub Stars (June 2024)\": \"\", \"GitHub Topics\": \"\", \"Github Date\": \"\", \"HF Config\": \"\", \"HF Config License\": \"\", \"HF Dataset\": \"\", \"HF Date\": \"\", \"HF Downloads (June 2024)\": \"\", \"HF Likes (June 2024)\": \"\", \"HF Yaml License\": \"\", \"PwC Date\": \"\", \"PwC Description\": \"\", \"PwC License Name\": \"\", \"PwC License URL\": \"\", \"S2 Citation Count (June 2024)\": \"\", \"S2 Date\": \"\"}, \"Bibtex\": \"@Article{Materzynska2019TheJD,\\n author = {Joanna Materzynska and Guillaume Berger and Ingo Bax and R. Memisevic},\\n booktitle = {2019 IEEE/CVF International Conference on Computer Vision Workshop (ICCVW)},\\n journal = {2019 IEEE/CVF International Conference on Computer Vision Workshop (ICCVW)},\\n pages = {2874-2882},\\n title = {The Jester Dataset: A Large-Scale Video Dataset of Human Gestures},\\n year = {2019}\\n}\\n\", \"License Use (DataProvenance)\": \"commercial\", \"License Attribution (DataProvenance)\": 1, \"License Share Alike (DataProvenance)\": 1, \"License Use (DataProvenance IgnoreOpenAI)\": \"commercial\", \"License Attribution (DataProvenance IgnoreOpenAI)\": 1, \"License Share Alike (DataProvenance IgnoreOpenAI)\": 1, \"License Type\": \"Commercial\"}, {\"Unique Dataset Identifier\": \"summe\", \"Collection\": \"summe\", \"Collection URL\": \"https://link.springer.com/chapter/10.1007/978-3-319-10584-0_33\", \"Dataset Name\": \"SumMe: Creating Summaries from User Videos (ECCV 2014)\", \"Paper Title\": \"SumMe: Creating Summaries from User Videos (ECCV 2014)\", \"Paper URL\": \"https://link.springer.com/chapter/10.1007/978-3-319-10584-0_33\", \"GitHub URL\": \"\", \"Hugging Face URL\": \"\", \"Papers with Code URL\": \"https://paperswithcode.com/dataset/summe\", \"ArXiv URL\": \"https://link.springer.com/chapter/10.1007/978-3-319-10584-0_33\", \"Semantic Scholar Corpus ID\": 2111093, \"Year Released\": \"2014\", \"Text Sources\": [\"crowdsourced\"], \"Licenses\": [{\"License\": \"Unspecified\", \"License URL\": \"\"}], \"Creators\": [\"ETH Z\\u00fcrich\", \"KU Leuven\", \"upicto GmbH\"], \"Countries\": [\"Belgium\", \"Switzerland\"], \"License Verified By\": \"Vivek Sharma\", \"Video Hours\": 1.11, \"Taken Down\": \"False\", \"Video Sources\": \"crowdsourced\", \"Task Categories\": [\"Video Summarization\"], \"Inferred Metadata\": {\"GitHub License\": \"\", \"GitHub Stars (June 2024)\": \"\", \"GitHub Topics\": \"\", \"Github Date\": \"\", \"HF Config\": \"\", \"HF Config License\": \"\", \"HF Dataset\": \"\", \"HF Date\": \"\", \"HF Downloads (June 2024)\": \"\", \"HF Likes (June 2024)\": \"\", \"HF Yaml License\": \"\", \"PwC Date\": \"2014-01-01\", \"PwC Description\": \"The SumMe dataset is a video summarization dataset consisting of 25 videos, each annotated with at least 15 human summaries (390 in total).\", \"PwC License Name\": \"Academic Research Purposes Only\", \"PwC License URL\": \"https://gyglim.github.io/me/vsum/index.html#benchmark\", \"S2 Citation Count (June 2024)\": \"\", \"S2 Date\": \"\"}, \"Bibtex\": \"@Article{Gygli2014CreatingSF,\\n author = {Michael Gygli and H. Grabner and Hayko Riemenschneider and L. Gool},\\n booktitle = {European Conference on Computer Vision},\\n pages = {505-520},\\n title = {Creating Summaries from User Videos},\\n year = {2014}\\n}\\n\", \"License Use (DataProvenance)\": \"unspecified\", \"License Attribution (DataProvenance)\": 0, \"License Share Alike (DataProvenance)\": 0, \"License Use (DataProvenance IgnoreOpenAI)\": \"unspecified\", \"License Attribution (DataProvenance IgnoreOpenAI)\": 0, \"License Share Alike (DataProvenance IgnoreOpenAI)\": 0, \"License Type\": \"Unspecified\"}, {\"Unique Dataset Identifier\": \"pku-mmd-dataset\", \"Collection\": \"pku-mmd-dataset\", \"Collection URL\": \"https://arxiv.org/abs/1703.07475\", \"Dataset Name\": \"PKU-MMD: A Large Scale Benchmark for Continuous Multi-Modal Human Action Understanding (ACM Multimedia Workshop)\", \"Paper Title\": \"PKU-MMD: A Large Scale Benchmark for Continuous Multi-Modal Human Action Understanding (ACM Multimedia Workshop)\", \"Paper URL\": \"https://arxiv.org/abs/1703.07475\", \"GitHub URL\": \"https://struct002.github.io/PKUMMD/\", \"Hugging Face URL\": \"\", \"Papers with Code URL\": \"https://paperswithcode.com/paper/pku-mmd-a-large-scale-benchmark-for\", \"ArXiv URL\": \"https://arxiv.org/abs/1703.07475\", \"Semantic Scholar Corpus ID\": 1904265, \"Year Released\": \"2017\", \"Text Sources\": [\"crowdsourced\"], \"Licenses\": [{\"License\": \"Unspecified\", \"License URL\": \"\"}], \"Creators\": [\"Microsoft Research\", \"Peking University\"], \"Countries\": [\"China\"], \"License Verified By\": \"Vivek Sharma\", \"Video Hours\": 50.0, \"Taken Down\": \"False\", \"Video Sources\": \"crowdsourced\", \"Task Categories\": [\"Video Classification\"], \"Inferred Metadata\": {\"GitHub License\": \"\", \"GitHub Stars (June 2024)\": \"\", \"GitHub Topics\": \"\", \"Github Date\": \"\", \"HF Config\": \"\", \"HF Config License\": \"\", \"HF Dataset\": \"\", \"HF Date\": \"\", \"HF Downloads (June 2024)\": \"\", \"HF Likes (June 2024)\": \"\", \"HF Yaml License\": \"\", \"PwC Date\": \"\", \"PwC Description\": \"No code available yet.\", \"PwC License Name\": \"\", \"PwC License URL\": \"\", \"S2 Citation Count (June 2024)\": \"\", \"S2 Date\": \"\"}, \"Bibtex\": \"@Article{Liu2017PKUMMDAL,\\n author = {Chunhui Liu and Yueyu Hu and Yanghao Li and Sijie Song and Jiaying Liu},\\n booktitle = {arXiv.org},\\n journal = {ArXiv},\\n title = {PKU-MMD: A Large Scale Benchmark for Continuous Multi-Modal Human Action Understanding},\\n volume = {abs/1703.07475},\\n year = {2017}\\n}\\n\", \"License Use (DataProvenance)\": \"unspecified\", \"License Attribution (DataProvenance)\": 0, \"License Share Alike (DataProvenance)\": 0, \"License Use (DataProvenance IgnoreOpenAI)\": \"unspecified\", \"License Attribution (DataProvenance IgnoreOpenAI)\": 0, \"License Share Alike (DataProvenance IgnoreOpenAI)\": 0, \"License Type\": \"Unspecified\"}, {\"Unique Dataset Identifier\": \"tiny-virat\", \"Collection\": \"tiny-virat\", \"Collection URL\": \"https://arxiv.org/abs/2007.07355\", \"Dataset Name\": \"TinyVIRAT: Low-resolution Video Action Recognition\", \"Paper Title\": \"TinyVIRAT: Low-resolution Video Action Recognition\", \"Paper URL\": \"https://arxiv.org/abs/2007.07355\", \"GitHub URL\": \"https://github.com/UgurDemir/Tiny-VIRAT\", \"Hugging Face URL\": \"\", \"Papers with Code URL\": \"https://paperswithcode.com/dataset/tinyvirat\", \"ArXiv URL\": \"https://arxiv.org/abs/2007.07355\", \"Semantic Scholar Corpus ID\": 220525685, \"Year Released\": \"2020\", \"Text Sources\": [\"crowdsourced\"], \"Licenses\": [{\"License\": \"Unspecified\", \"License URL\": \"\"}], \"Creators\": [\"University of Central Florida\"], \"Countries\": [\"United States of America\"], \"License Verified By\": \"Vivek Sharma\", \"Video Hours\": 10.83, \"Taken Down\": \"False\", \"Video Sources\": \"crowdsourced\", \"Task Categories\": [\"Video Classification\"], \"Inferred Metadata\": {\"GitHub License\": \"\", \"GitHub Stars (June 2024)\": 16, \"GitHub Topics\": [], \"Github Date\": \"\", \"HF Config\": \"\", \"HF Config License\": \"\", \"HF Dataset\": \"\", \"HF Date\": \"\", \"HF Downloads (June 2024)\": \"\", \"HF Likes (June 2024)\": \"\", \"HF Yaml License\": \"\", \"PwC Date\": \"\", \"PwC Description\": \"TinyVIRAT contains natural low-resolution activities. The actions in TinyVIRAT videos have multiple labels and they are extracted from surveillance videos which makes them realistic and more challenging.\", \"PwC License Name\": \"Unspecified\", \"PwC License URL\": null, \"S2 Citation Count (June 2024)\": \"\", \"S2 Date\": \"\"}, \"Bibtex\": \"@Article{Demir2020TinyVIRATLV,\\n author = {Ugur Demir and Y. Rawat and M. Shah},\\n booktitle = {International Conference on Pattern Recognition},\\n journal = {2020 25th International Conference on Pattern Recognition (ICPR)},\\n pages = {7387-7394},\\n title = {TinyVIRAT: Low-resolution Video Action Recognition},\\n year = {2020}\\n}\\n\", \"License Use (DataProvenance)\": \"unspecified\", \"License Attribution (DataProvenance)\": 0, \"License Share Alike (DataProvenance)\": 0, \"License Use (DataProvenance IgnoreOpenAI)\": \"unspecified\", \"License Attribution (DataProvenance IgnoreOpenAI)\": 0, \"License Share Alike (DataProvenance IgnoreOpenAI)\": 0, \"License Type\": \"Unspecified\"}, {\"Unique Dataset Identifier\": \"multi-moments-in-time-dataset\", \"Collection\": \"multi-moments-in-time-dataset\", \"Collection URL\": \"https://arxiv.org/pdf/1911.00232.pdf\", \"Dataset Name\": \"Multi-Moments in Time: Learning and Interpreting Models for Multi-Action Video Understanding\", \"Paper Title\": \"Multi-Moments in Time: Learning and Interpreting Models for Multi-Action Video Understanding\", \"Paper URL\": \"https://arxiv.org/pdf/1911.00232.pdf\", \"GitHub URL\": \"\", \"Hugging Face URL\": \"\", \"Papers with Code URL\": \"\", \"ArXiv URL\": \"https://arxiv.org/pdf/1911.00232.pdf\", \"Semantic Scholar Corpus ID\": 207780280, \"Year Released\": \"2021\", \"Text Sources\": [\"crowdsourced\", \"undisclosed web\"], \"Licenses\": [{\"License\": \"Unspecified\", \"License URL\": \"\"}], \"Creators\": [\"Massachusetts Institute of Technology\"], \"Countries\": [\"United States of America\"], \"License Verified By\": \"Vivek Sharma\", \"Video Hours\": 833.0, \"Taken Down\": \"False\", \"Video Sources\": \"crowdsourced\", \"Task Categories\": [\"Video Classification\"], \"Inferred Metadata\": {\"GitHub License\": \"\", \"GitHub Stars (June 2024)\": \"\", \"GitHub Topics\": \"\", \"Github Date\": \"\", \"HF Config\": \"\", \"HF Config License\": \"\", \"HF Dataset\": \"\", \"HF Date\": \"\", \"HF Downloads (June 2024)\": \"\", \"HF Likes (June 2024)\": \"\", \"HF Yaml License\": \"\", \"PwC Date\": \"\", \"PwC Description\": \"\", \"PwC License Name\": \"\", \"PwC License URL\": \"\", \"S2 Citation Count (June 2024)\": \"\", \"S2 Date\": \"\"}, \"Bibtex\": \"@Article{Monfort2019MultiMomentsIT,\\n author = {Mathew Monfort and K. Ramakrishnan and A. Andonian and Barry A. McNamara and A. Lascelles and Bowen Pan and Quanfu Fan and Dan Gutfreund and R. Feris and A. Oliva},\\n booktitle = {IEEE Transactions on Pattern Analysis and Machine Intelligence},\\n journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},\\n pages = {1-1},\\n title = {Multi-Moments in Time: Learning and Interpreting Models for Multi-Action Video Understanding},\\n volume = {PP},\\n year = {2019}\\n}\\n\", \"License Use (DataProvenance)\": \"unspecified\", \"License Attribution (DataProvenance)\": 0, \"License Share Alike (DataProvenance)\": 0, \"License Use (DataProvenance IgnoreOpenAI)\": \"unspecified\", \"License Attribution (DataProvenance IgnoreOpenAI)\": 0, \"License Share Alike (DataProvenance IgnoreOpenAI)\": 0, \"License Type\": \"Unspecified\"}, {\"Unique Dataset Identifier\": \"qfvs\", \"Collection\": \"qfvs\", \"Collection URL\": \"https://arxiv.org/abs/1707.04960\", \"Dataset Name\": \"QFVS: Query-Focused Video Summarization: Dataset, Evaluation, and A Memory Network Based Approach (CVPR 2017)\", \"Paper Title\": \"QFVS: Query-Focused Video Summarization: Dataset, Evaluation, and A Memory Network Based Approach (CVPR 2017)\", \"Paper URL\": \"https://arxiv.org/abs/1707.04960\", \"GitHub URL\": \"\", \"Hugging Face URL\": \"\", \"Papers with Code URL\": \"https://paperswithcode.com/dataset/query-focused-video-summarization-dataset\", \"ArXiv URL\": \"https://arxiv.org/abs/1707.04960\", \"Semantic Scholar Corpus ID\": 2774608, \"Year Released\": \"2017\", \"Text Sources\": [\"crowdsourced\"], \"Licenses\": [{\"License\": \"Unspecified\", \"License URL\": \"\"}], \"Creators\": [\"University of Central Florida\", \"University of Alabama\"], \"Countries\": [\"United States of America\"], \"License Verified By\": \"Vivek Sharma\", \"Video Hours\": 20.0, \"Taken Down\": \"False\", \"Video Sources\": \"crowdsourced\", \"Task Categories\": [\"Video Summarization\"], \"Inferred Metadata\": {\"GitHub License\": \"\", \"GitHub Stars (June 2024)\": \"\", \"GitHub Topics\": \"\", \"Github Date\": \"\", \"HF Config\": \"\", \"HF Config License\": \"\", \"HF Dataset\": \"\", \"HF Date\": \"\", \"HF Downloads (June 2024)\": \"\", \"HF Likes (June 2024)\": \"\", \"HF Yaml License\": \"\", \"PwC Date\": \"\", \"PwC Description\": \"Collects dense per-video-shot concept annotations.\", \"PwC License Name\": \"Unspecified\", \"PwC License URL\": null, \"S2 Citation Count (June 2024)\": \"\", \"S2 Date\": \"\"}, \"Bibtex\": \"@Article{Sharghi2017QueryFocusedVS,\\n author = {Aidean Sharghi and Jacob S. Laurel and Boqing Gong},\\n booktitle = {Computer Vision and Pattern Recognition},\\n journal = {2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},\\n pages = {2127-2136},\\n title = {Query-Focused Video Summarization: Dataset, Evaluation, and a Memory Network Based Approach},\\n year = {2017}\\n}\\n\", \"License Use (DataProvenance)\": \"unspecified\", \"License Attribution (DataProvenance)\": 0, \"License Share Alike (DataProvenance)\": 0, \"License Use (DataProvenance IgnoreOpenAI)\": \"unspecified\", \"License Attribution (DataProvenance IgnoreOpenAI)\": 0, \"License Share Alike (DataProvenance IgnoreOpenAI)\": 0, \"License Type\": \"Unspecified\"}, {\"Unique Dataset Identifier\": \"cacd\", \"Collection\": \"cacd\", \"Collection URL\": \"https://openaccess.thecvf.com/content/CVPR2022W/L3D-IVU/papers/Xiang_CDAD_A_Common_Daily_Action_Dataset_With_Collected_Hard_Negative_CVPRW_2022_paper.pdf\", \"Dataset Name\": \"CDAD: A Common Daily Action Dataset with Collected Hard Negative Samples (CVPR 2022)\", \"Paper Title\": \"CDAD: A Common Daily Action Dataset with Collected Hard Negative Samples (CVPR 2022)\", \"Paper URL\": \"https://openaccess.thecvf.com/content/CVPR2022W/L3D-IVU/papers/Xiang_CDAD_A_Common_Daily_Action_Dataset_With_Collected_Hard_Negative_CVPRW_2022_paper.pdf\", \"GitHub URL\": \"https://github.com/MartinXM/CDAD\", \"Hugging Face URL\": \"\", \"Papers with Code URL\": \"\", \"ArXiv URL\": \"https://openaccess.thecvf.com/content/CVPR2022W/L3D-IVU/papers/Xiang_CDAD_A_Common_Daily_Action_Dataset_With_Collected_Hard_Negative_CVPRW_2022_paper.pdf\", \"Semantic Scholar Corpus ID\": 251035434, \"Year Released\": \"2022\", \"Text Sources\": [\"crowdsourced\"], \"Licenses\": [{\"License\": \"Unspecified\", \"License URL\": \"\"}], \"Creators\": [\"The Hong Kong Polytechnic University\", \"Alibaba Group\"], \"Countries\": [\"China\"], \"License Verified By\": \"Vivek Sharma\", \"Video Hours\": 215.0, \"Taken Down\": \"False\", \"Video Sources\": \"crowdsourced\", \"Task Categories\": [\"Video Classification\"], \"Inferred Metadata\": {\"GitHub License\": \"\", \"GitHub Stars (June 2024)\": 6, \"GitHub Topics\": [], \"Github Date\": \"\", \"HF Config\": \"\", \"HF Config License\": \"\", \"HF Dataset\": \"\", \"HF Date\": \"\", \"HF Downloads (June 2024)\": \"\", \"HF Likes (June 2024)\": \"\", \"HF Yaml License\": \"\", \"PwC Date\": \"\", \"PwC Description\": \"\", \"PwC License Name\": \"\", \"PwC License URL\": \"\", \"S2 Citation Count (June 2024)\": \"\", \"S2 Date\": \"\"}, \"Bibtex\": \"@Article{Xiang2022CDADAC,\\n author = {Wangmeng Xiang and C. Li and Ke Li and Biao Wang and Xiangpei Hua and Lei Zhang},\\n booktitle = {2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)},\\n journal = {2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)},\\n pages = {3920-3929},\\n title = {CDAD: A Common Daily Action Dataset with Collected Hard Negative Samples},\\n year = {2022}\\n}\\n\", \"License Use (DataProvenance)\": \"unspecified\", \"License Attribution (DataProvenance)\": 0, \"License Share Alike (DataProvenance)\": 0, \"License Use (DataProvenance IgnoreOpenAI)\": \"unspecified\", \"License Attribution (DataProvenance IgnoreOpenAI)\": 0, \"License Share Alike (DataProvenance IgnoreOpenAI)\": 0, \"License Type\": \"Unspecified\"}, {\"Unique Dataset Identifier\": \"charades-ego\", \"Collection\": \"charades-ego\", \"Collection URL\": \"https://arxiv.org/abs/1804.09627\", \"Dataset Name\": \"Charades-Ego: Actor and Observer: Joint Modeling of First and Third-Person Videos (CVPR 2018)\", \"Paper Title\": \"Charades-Ego: Actor and Observer: Joint Modeling of First and Third-Person Videos (CVPR 2018)\", \"Paper URL\": \"https://arxiv.org/abs/1804.09627\", \"GitHub URL\": \"https://github.com/gsig/actor-observer\", \"Hugging Face URL\": \"\", \"Papers with Code URL\": \"https://paperswithcode.com/search?q_meta=&q_type=&q=Actor+and+Observer%3A+Joint+Modeling+of+First+and+Third-Person+Videos\", \"ArXiv URL\": \"https://arxiv.org/abs/1804.09627\", \"Semantic Scholar Corpus ID\": 4562167, \"Year Released\": \"2018\", \"Text Sources\": [\"human\"], \"Licenses\": [{\"License\": \"Custom\", \"License URL\": \"https://prior.allenai.org/projects/data/charades-ego/license.txt\"}], \"Creators\": [\"Allen Institute for AI\"], \"Countries\": [\"United States of America\"], \"License Verified By\": \"Vivek Sharma\", \"Video Hours\": 69.33, \"Taken Down\": \"False\", \"Video Sources\": \"human\", \"Task Categories\": [\"Video Classification\"], \"Inferred Metadata\": {\"GitHub License\": \"GNU General Public License v3.0\", \"GitHub Stars (June 2024)\": 75, \"GitHub Topics\": [], \"Github Date\": \"\", \"HF Config\": \"\", \"HF Config License\": \"\", \"HF Dataset\": \"\", \"HF Date\": \"\", \"HF Downloads (June 2024)\": \"\", \"HF Likes (June 2024)\": \"\", \"HF Yaml License\": \"\", \"PwC Date\": \"\", \"PwC Description\": \"\", \"PwC License Name\": \"\", \"PwC License URL\": \"\", \"S2 Citation Count (June 2024)\": \"\", \"S2 Date\": \"\"}, \"Bibtex\": \"@Book{Sigurdsson2018ActorAO,\\n author = {Gunnar A. Sigurdsson and A. Gupta and C. Schmid and Ali Farhadi and Alahari Karteek},\\n booktitle = {2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition},\\n journal = {2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition},\\n pages = {7396-7404},\\n title = {Actor and Observer: Joint Modeling of First and Third-Person Videos},\\n year = {2018}\\n}\\n\", \"License Use (DataProvenance)\": \"academic-only\", \"License Attribution (DataProvenance)\": 1, \"License Share Alike (DataProvenance)\": 0, \"License Use (DataProvenance IgnoreOpenAI)\": \"academic-only\", \"License Attribution (DataProvenance IgnoreOpenAI)\": 1, \"License Share Alike (DataProvenance IgnoreOpenAI)\": 0, \"License Type\": \"Non-Commercial/Academic\"}, {\"Unique Dataset Identifier\": \"mars\", \"Collection\": \"mars\", \"Collection URL\": \"https://link.springer.com/content/pdf/10.1007/978-3-319-46466-4_52.pdf\", \"Dataset Name\": \"Mars\", \"Paper Title\": \"Mars\", \"Paper URL\": \"https://link.springer.com/content/pdf/10.1007/978-3-319-46466-4_52.pdf\", \"GitHub URL\": \"\", \"Hugging Face URL\": \"\", \"Papers with Code URL\": \"https://paperswithcode.com/dataset/mars\", \"ArXiv URL\": \"https://link.springer.com/content/pdf/10.1007/978-3-319-46466-4_52.pdf\", \"Semantic Scholar Corpus ID\": 2214158, \"Year Released\": \"2016\", \"Text Sources\": [\"human\"], \"Licenses\": [{\"License\": \"Unspecified\", \"License URL\": \"\"}], \"Creators\": [\"Microsoft\", \"Tsinghua University\", \"The University of Texas at San Antonio\", \"Peking University\"], \"Countries\": [\"China\"], \"License Verified By\": \"Vivek Sharma\", \"Video Hours\": 0.24, \"Taken Down\": \"False\", \"Video Sources\": \"human\", \"Task Categories\": [\"Video Segmentation\", \"Video Classification\"], \"Inferred Metadata\": {\"GitHub License\": \"\", \"GitHub Stars (June 2024)\": \"\", \"GitHub Topics\": \"\", \"Github Date\": \"\", \"HF Config\": \"\", \"HF Config License\": \"\", \"HF Dataset\": \"\", \"HF Date\": \"\", \"HF Downloads (June 2024)\": \"\", \"HF Likes (June 2024)\": \"\", \"HF Yaml License\": \"\", \"PwC Date\": \"2016-01-01\", \"PwC Description\": \"MARS (Motion Analysis and Re-identification Set) is a large scale video based person reidentification dataset, an extension of the Market-1501 dataset. It has been collected from six near-synchronized cameras. It consists of 1,261 different pedestrians, who are captured by at least 2 cameras. The variations in poses, colors and illuminations of pedestrians, as well as the poor image quality, make it very difficult to yield high matching accuracy. Moreover, the dataset contains 3,248 distractors in order to make it more realistic. Deformable Part Model and GMMCP tracker were used to automatically generate the tracklets (mostly 25-50 frames long).\", \"PwC License Name\": \"Unspecified\", \"PwC License URL\": null, \"S2 Citation Count (June 2024)\": \"\", \"S2 Date\": \"\"}, \"Bibtex\": \"@Article{Zheng2016MARSAV,\\n author = {Liang Zheng and Zhi Bie and Yifan Sun and Jingdong Wang and Chi Su and Shengjin Wang and Q. Tian},\\n booktitle = {European Conference on Computer Vision},\\n pages = {868-884},\\n title = {MARS: A Video Benchmark for Large-Scale Person Re-Identification},\\n year = {2016}\\n}\\n\", \"License Use (DataProvenance)\": \"unspecified\", \"License Attribution (DataProvenance)\": 0, \"License Share Alike (DataProvenance)\": 0, \"License Use (DataProvenance IgnoreOpenAI)\": \"unspecified\", \"License Attribution (DataProvenance IgnoreOpenAI)\": 0, \"License Share Alike (DataProvenance IgnoreOpenAI)\": 0, \"License Type\": \"Unspecified\"}, {\"Unique Dataset Identifier\": \"homage\", \"Collection\": \"homage\", \"Collection URL\": \"https://arxiv.org/abs/2105.05226\", \"Dataset Name\": \"HOMAGE: Home Action Genome: Cooperative Compositional Action Understanding (CVPR 2021)\", \"Paper Title\": \"HOMAGE: Home Action Genome: Cooperative Compositional Action Understanding (CVPR 2021)\", \"Paper URL\": \"https://arxiv.org/abs/2105.05226\", \"GitHub URL\": \"https://github.com/nishantrai18/homage\", \"Hugging Face URL\": \"\", \"Papers with Code URL\": \"https://paperswithcode.com/paper/home-action-genome-cooperative-compositional\", \"ArXiv URL\": \"https://arxiv.org/abs/2105.05226\", \"Semantic Scholar Corpus ID\": 234357543, \"Year Released\": \"2021\", \"Text Sources\": [\"human\"], \"Licenses\": [{\"License\": \"Unspecified\", \"License URL\": \"\"}], \"Creators\": [\"Stanford University\", \"Panasonic Corporation\"], \"Countries\": [\"United States of America\"], \"License Verified By\": \"Vivek Sharma\", \"Video Hours\": 30.0, \"Taken Down\": \"False\", \"Video Sources\": \"human\", \"Task Categories\": [\"Video Classification\", \"Temporal Action Segmentation\"], \"Inferred Metadata\": {\"GitHub License\": \"MIT License\", \"GitHub Stars (June 2024)\": 17, \"GitHub Topics\": [], \"Github Date\": \"\", \"HF Config\": \"\", \"HF Config License\": \"\", \"HF Dataset\": \"\", \"HF Date\": \"\", \"HF Downloads (June 2024)\": \"\", \"HF Likes (June 2024)\": \"\", \"HF Yaml License\": \"\", \"PwC Date\": \"\", \"PwC Description\": \"\\ud83c\\udfc6 SOTA for Video Classification on Home Action Genome (Accuracy (%) metric)\", \"PwC License Name\": \"\", \"PwC License URL\": \"\", \"S2 Citation Count (June 2024)\": \"\", \"S2 Date\": \"\"}, \"Bibtex\": \"@Article{Rai2021HomeAG,\\n author = {Nishant Rai and Haofeng Chen and Jingwei Ji and Rishi Desai and K. Kozuka and Shun Ishizaka and E. Adeli and Juan Carlos Niebles},\\n booktitle = {Computer Vision and Pattern Recognition},\\n journal = {2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},\\n pages = {11179-11188},\\n title = {Home Action Genome: Cooperative Compositional Action Understanding},\\n year = {2021}\\n}\\n\", \"License Use (DataProvenance)\": \"unspecified\", \"License Attribution (DataProvenance)\": 0, \"License Share Alike (DataProvenance)\": 0, \"License Use (DataProvenance IgnoreOpenAI)\": \"unspecified\", \"License Attribution (DataProvenance IgnoreOpenAI)\": 0, \"License Share Alike (DataProvenance IgnoreOpenAI)\": 0, \"License Type\": \"Unspecified\"}, {\"Unique Dataset Identifier\": \"lemma-dataset\", \"Collection\": \"lemma-dataset\", \"Collection URL\": \"https://arxiv.org/abs/2007.15781\", \"Dataset Name\": \"LEMMA: A Multi-view Dataset for LEarning Multi-agent Multi-task Activities (ECCV 2020)\", \"Paper Title\": \"LEMMA: A Multi-view Dataset for LEarning Multi-agent Multi-task Activities (ECCV 2020)\", \"Paper URL\": \"https://arxiv.org/abs/2007.15781\", \"GitHub URL\": \"https://github.com/Buzz-Beater/LEMMA\", \"Hugging Face URL\": \"\", \"Papers with Code URL\": \"https://paperswithcode.com/dataset/lemma\", \"ArXiv URL\": \"https://arxiv.org/abs/2007.15781\", \"Semantic Scholar Corpus ID\": 220634784, \"Year Released\": \"2020\", \"Text Sources\": [\"crowdsourced\", \"human\"], \"Licenses\": [{\"License\": \"Unspecified\", \"License URL\": \"\"}], \"Creators\": [\"UCLA\"], \"Countries\": [\"United States of America\"], \"License Verified By\": \"Vivek Sharma\", \"Video Hours\": 10.8, \"Taken Down\": \"False\", \"Video Sources\": \"human\", \"Task Categories\": [\"Video Classification\"], \"Inferred Metadata\": {\"GitHub License\": \"\", \"GitHub Stars (June 2024)\": 27, \"GitHub Topics\": [], \"Github Date\": \"\", \"HF Config\": \"\", \"HF Config License\": \"\", \"HF Dataset\": \"\", \"HF Date\": \"\", \"HF Downloads (June 2024)\": \"\", \"HF Likes (June 2024)\": \"\", \"HF Yaml License\": \"\", \"PwC Date\": \"\", \"PwC Description\": \"The LEMMA dataset aims to explore the essence of complex human activities in a goal-directed, multi-agent, multi-task setting with ground-truth labels of compositional atomic-actions and their associated tasks. By quantifying the scenarios to up to two multi-step tasks with two agents, the authors strive to address human multi-task and multi-agent interactions in four scenarios: single-agent single-task (1 x 1), single-agent multi-task (1 x 2), multi-agent single-task (2 x 1), and multi-agent multi-task (2 x 2). Task instructions are only given to one agent in the 2 x 1 setting to resemble the robot-helping scenario, hoping that the learned perception models could be applied in robotic tasks (especially in HRI) in the near future.\\n\\nBoth the third-person views (TPVs) and the first-person views (FPVs) were recorded to account for different perspectives of the same activities. The authors densely annotate atomic-actions (in the form of compositional verb-noun pairs) and tasks of each atomic-action, as well as the spatial location of each participating agent (bounding boxes) to facilitate the learning of multi-agent multi-task task scheduling and assignment.\", \"PwC License Name\": \"Unspecified\", \"PwC License URL\": null, \"S2 Citation Count (June 2024)\": \"\", \"S2 Date\": \"\"}, \"Bibtex\": \"@Article{Jia2020LEMMAAM,\\n author = {Baoxiong Jia and Yixin Chen and Siyuan Huang and Yixin Zhu and Song-Chun Zhu},\\n booktitle = {European Conference on Computer Vision},\\n journal = {ArXiv},\\n title = {LEMMA: A Multi-view Dataset for Learning Multi-agent Multi-task Activities},\\n volume = {abs/2007.15781},\\n year = {2020}\\n}\\n\", \"License Use (DataProvenance)\": \"unspecified\", \"License Attribution (DataProvenance)\": 0, \"License Share Alike (DataProvenance)\": 0, \"License Use (DataProvenance IgnoreOpenAI)\": \"unspecified\", \"License Attribution (DataProvenance IgnoreOpenAI)\": 0, \"License Share Alike (DataProvenance IgnoreOpenAI)\": 0, \"License Type\": \"Unspecified\"}, {\"Unique Dataset Identifier\": \"collective\", \"Collection\": \"collective\", \"Collection URL\": \"https://cvgl.stanford.edu/papers/Wongun_CollectiveActivityRecognition09.pdf\", \"Dataset Name\": \"Collective: What are they doing? : Collective activity classification using spatio-temporal relationship among people\", \"Paper Title\": \"Collective: What are they doing? : Collective activity classification using spatio-temporal relationship among people\", \"Paper URL\": \"https://cvgl.stanford.edu/papers/Wongun_CollectiveActivityRecognition09.pdf\", \"GitHub URL\": \"\", \"Hugging Face URL\": \"\", \"Papers with Code URL\": \"\", \"ArXiv URL\": \"https://cvgl.stanford.edu/papers/Wongun_CollectiveActivityRecognition09.pdf\", \"Semantic Scholar Corpus ID\": 5925915, \"Year Released\": \"2009\", \"Text Sources\": [\"human\"], \"Licenses\": [{\"License\": \"Unspecified\", \"License URL\": \"\"}], \"Creators\": [\"University of Michigan\"], \"Countries\": [\"United States of America\"], \"License Verified By\": \"Vivek Sharma\", \"Video Hours\": 0.1, \"Taken Down\": \"False\", \"Video Sources\": \"human\", \"Task Categories\": [\"Group Activity Recognition\"], \"Inferred Metadata\": {\"GitHub License\": \"\", \"GitHub Stars (June 2024)\": \"\", \"GitHub Topics\": \"\", \"Github Date\": \"\", \"HF Config\": \"\", \"HF Config License\": \"\", \"HF Dataset\": \"\", \"HF Date\": \"\", \"HF Downloads (June 2024)\": \"\", \"HF Likes (June 2024)\": \"\", \"HF Yaml License\": \"\", \"PwC Date\": \"\", \"PwC Description\": \"\", \"PwC License Name\": \"\", \"PwC License URL\": \"\", \"S2 Citation Count (June 2024)\": \"\", \"S2 Date\": \"\"}, \"Bibtex\": \"@Article{Choi2009WhatAT,\\n author = {Wongun Choi and Khuram Shahid and S. Savarese},\\n booktitle = {2009 IEEE 12th International Conference on Computer Vision Workshops, ICCV Workshops},\\n journal = {2009 IEEE 12th International Conference on Computer Vision Workshops, ICCV Workshops},\\n pages = {1282-1289},\\n title = {What are they doing? : Collective activity classification using spatio-temporal relationship among people},\\n year = {2009}\\n}\\n\", \"License Use (DataProvenance)\": \"unspecified\", \"License Attribution (DataProvenance)\": 0, \"License Share Alike (DataProvenance)\": 0, \"License Use (DataProvenance IgnoreOpenAI)\": \"unspecified\", \"License Attribution (DataProvenance IgnoreOpenAI)\": 0, \"License Share Alike (DataProvenance IgnoreOpenAI)\": 0, \"License Type\": \"Unspecified\"}, {\"Unique Dataset Identifier\": \"50salads\", \"Collection\": \"50salads\", \"Collection URL\": \"https://citeseerx.ist.psu.edu/document?repid=rep1&type=pdf&doi=b9410401cec076baef045e83953f3ff24f25d149\", \"Dataset Name\": \"50Salads\", \"Paper Title\": \"50Salads\", \"Paper URL\": \"https://citeseerx.ist.psu.edu/document?repid=rep1&type=pdf&doi=b9410401cec076baef045e83953f3ff24f25d149\", \"GitHub URL\": \"\", \"Hugging Face URL\": \"\", \"Papers with Code URL\": \"https://paperswithcode.com/dataset/50-salads\", \"ArXiv URL\": \"https://citeseerx.ist.psu.edu/document?repid=rep1&type=pdf&doi=b9410401cec076baef045e83953f3ff24f25d149\", \"Semantic Scholar Corpus ID\": 2333743, \"Year Released\": \"2013\", \"Text Sources\": [\"human\"], \"Licenses\": [{\"License\": \"CC BY-NC-SA 4.0\", \"License URL\": \"https://cvip.computing.dundee.ac.uk/datasets/foodpreparation/50salads/\"}], \"Creators\": [\"University of Dundee\"], \"Countries\": [\"United Kingdom\"], \"License Verified By\": \"Vivek Sharma\", \"Video Hours\": 40.0, \"Taken Down\": \"False\", \"Video Sources\": \"human\", \"Task Categories\": [\"Action Segmentation\", \"Action Localization\"], \"Inferred Metadata\": {\"GitHub License\": \"\", \"GitHub Stars (June 2024)\": \"\", \"GitHub Topics\": \"\", \"Github Date\": \"\", \"HF Config\": \"\", \"HF Config License\": \"\", \"HF Dataset\": \"\", \"HF Date\": \"\", \"HF Downloads (June 2024)\": \"\", \"HF Likes (June 2024)\": \"\", \"HF Yaml License\": \"\", \"PwC Date\": \"2013-09-08\", \"PwC Description\": \"Activity recognition research has shifted focus from distinguishing full-body motion patterns to recognizing complex interactions of multiple entities. Manipulative gestures \\u2013 characterized by interactions between hands, tools, and manipulable objects \\u2013 frequently occur in food preparation, manufacturing, and assembly tasks, and have a variety of applications including situational support, automated supervision, and skill assessment. With the aim to stimulate research on recognizing manipulative gestures we introduce the 50 Salads dataset. It captures 25 people preparing 2 mixed salads each and contains over 4h of annotated accelerometer and RGB-D video data. Including detailed annotations, multiple sensor types, and two sequences per participant, the 50 Salads dataset may be used for research in areas such as activity recognition, activity spotting, sequence analysis, progress tracking, sensor fusion, transfer learning, and user-adaptation.\\n\\nThe dataset includes\\n\\nRGB video data 640\\u00d7480 pixels at 30 Hz\\nDepth maps 640\\u00d7480 pixels at 30 Hz\\n3-axis accelerometer data at 50 Hz of devices attached to a knife, a mixing spoon, a small spoon, a peeler, a glass, an oil bottle, and a pepper dispenser.\\nSynchronization parameters for temporal alignment of video and accelerometer data\\nAnnotations as temporal intervals of pre- core- and post-phases of activities corresponding to steps in a recipe\", \"PwC License Name\": \"CC BY-NC-SA 4.0\", \"PwC License URL\": null, \"S2 Citation Count (June 2024)\": \"\", \"S2 Date\": \"\"}, \"Bibtex\": \"@Book{Stein2013CombiningEA,\\n author = {Sebastian Stein and S. McKenna},\\n booktitle = {Ubiquitous Computing},\\n journal = {Proceedings of the 2013 ACM international joint conference on Pervasive and ubiquitous computing},\\n title = {Combining embedded accelerometers with computer vision for recognizing food preparation activities},\\n year = {2013}\\n}\\n\", \"License Use (DataProvenance)\": \"non-commercial\", \"License Attribution (DataProvenance)\": 1, \"License Share Alike (DataProvenance)\": 1, \"License Use (DataProvenance IgnoreOpenAI)\": \"non-commercial\", \"License Attribution (DataProvenance IgnoreOpenAI)\": 1, \"License Share Alike (DataProvenance IgnoreOpenAI)\": 1, \"License Type\": \"Non-Commercial/Academic\"}, {\"Unique Dataset Identifier\": \"egoschema\", \"Collection\": \"egoschema\", \"Collection URL\": \"https://arxiv.org/pdf/2308.09126\", \"Dataset Name\": \"EgoSchema: A Diagnostic Benchmark for Very Long-form Video Language Understanding\", \"Paper Title\": \"EgoSchema: A Diagnostic Benchmark for Very Long-form Video Language Understanding\", \"Paper URL\": \"https://arxiv.org/pdf/2308.09126\", \"GitHub URL\": \"\", \"Hugging Face URL\": \"\", \"Papers with Code URL\": \"https://paperswithcode.com/paper/egoschema-a-diagnostic-benchmark-for-very-1\", \"ArXiv URL\": \"https://arxiv.org/pdf/2308.09126\", \"Semantic Scholar Corpus ID\": 261031047, \"Year Released\": \"2024\", \"Text Sources\": [\"human\"], \"Licenses\": [{\"License\": \"Unspecified\", \"License URL\": \"\"}], \"Creators\": [\"UC Berkeley\"], \"Countries\": [\"United States of America\"], \"License Verified By\": \"Vivek Sharma\", \"Video Hours\": 250.0, \"Taken Down\": \"False\", \"Video Sources\": \"human\", \"Task Categories\": [\"Video Q&A\"], \"Inferred Metadata\": null, \"Bibtex\": null, \"License Use (DataProvenance)\": \"unspecified\", \"License Attribution (DataProvenance)\": 0, \"License Share Alike (DataProvenance)\": 0, \"License Use (DataProvenance IgnoreOpenAI)\": \"unspecified\", \"License Attribution (DataProvenance IgnoreOpenAI)\": 0, \"License Share Alike (DataProvenance IgnoreOpenAI)\": 0, \"License Type\": \"Unspecified\"}, {\"Unique Dataset Identifier\": \"kinetics-400\", \"Collection\": \"kinetics-400\", \"Collection URL\": \"https://arxiv.org/abs/1705.06950\", \"Dataset Name\": \"Kinetics 400\", \"Paper Title\": \"Kinetics 400\", \"Paper URL\": \"https://arxiv.org/abs/1705.06950\", \"GitHub URL\": \"https://github.com/cvdfoundation/kinetics-dataset\", \"Hugging Face URL\": \"\", \"Papers with Code URL\": \"https://paperswithcode.com/dataset/kinetics\", \"ArXiv URL\": \"https://arxiv.org/abs/1705.06950\", \"Semantic Scholar Corpus ID\": 27300853, \"Year Released\": \"2017\", \"Text Sources\": [\"human\"], \"Licenses\": [{\"License\": \"Unspecified\", \"License URL\": \"\"}], \"Creators\": [\"DeepMind\"], \"Countries\": [\"United Kingdom\"], \"License Verified By\": \"Vivek Sharma\", \"Video Hours\": 850.68, \"Taken Down\": \"False\", \"Video Sources\": \"human\", \"Task Categories\": [\"Video Classification\"], \"Inferred Metadata\": {\"GitHub License\": \"\", \"GitHub Stars (June 2024)\": 704, \"GitHub Topics\": [], \"Github Date\": \"\", \"HF Config\": \"\", \"HF Config License\": \"\", \"HF Dataset\": \"\", \"HF Date\": \"\", \"HF Downloads (June 2024)\": \"\", \"HF Likes (June 2024)\": \"\", \"HF Yaml License\": \"\", \"PwC Date\": \"2017-05-19\", \"PwC Description\": \"The Kinetics dataset is a large-scale, high-quality dataset for human action recognition in videos. The dataset consists of around 500,000 video clips covering 600 human action classes with at least 600 video clips for each action class. Each video clip lasts around 10 seconds and is labeled with a single action class. The videos are collected from YouTube.\", \"PwC License Name\": \"CC BY 4.0\", \"PwC License URL\": \"https://creativecommons.org/licenses/by/4.0/\", \"S2 Citation Count (June 2024)\": \"\", \"S2 Date\": \"\"}, \"Bibtex\": \"@Article{Kay2017TheKH,\\n author = {W. Kay and Jo\\u00e3o Carreira and K. Simonyan and Brian Zhang and Chloe Hillier and Sudheendra Vijayanarasimhan and Fabio Viola and Tim Green and T. Back and A. Natsev and Mustafa Suleyman and Andrew Zisserman},\\n booktitle = {arXiv.org},\\n journal = {ArXiv},\\n title = {The Kinetics Human Action Video Dataset},\\n volume = {abs/1705.06950},\\n year = {2017}\\n}\\n\", \"License Use (DataProvenance)\": \"unspecified\", \"License Attribution (DataProvenance)\": 0, \"License Share Alike (DataProvenance)\": 0, \"License Use (DataProvenance IgnoreOpenAI)\": \"unspecified\", \"License Attribution (DataProvenance IgnoreOpenAI)\": 0, \"License Share Alike (DataProvenance IgnoreOpenAI)\": 0, \"License Type\": \"Unspecified\"}, {\"Unique Dataset Identifier\": \"ego-exo4D\", \"Collection\": \"ego-exo4D\", \"Collection URL\": \"https://arxiv.org/abs/2311.18259\", \"Dataset Name\": \"Ego-Exo4D\", \"Paper Title\": \"Ego-Exo4D\", \"Paper URL\": \"https://arxiv.org/abs/2311.18259\", \"GitHub URL\": \"https://github.com/facebookresearch/Ego4d\", \"Hugging Face URL\": \"\", \"Papers with Code URL\": \"https://paperswithcode.com/paper/ego-exo4d-understanding-skilled-human/review/\", \"ArXiv URL\": \"https://arxiv.org/abs/2311.18259\", \"Semantic Scholar Corpus ID\": 265506384, \"Year Released\": \"2023\", \"Text Sources\": [\"human\"], \"Licenses\": [{\"License\": \"MIT License\", \"License URL\": \"https://github.com/facebookresearch/Ego4d/blob/main/LICENSE\"}], \"Creators\": [\"Meta\"], \"Countries\": [\"United States of America\"], \"License Verified By\": \"Vivek Sharma\", \"Video Hours\": 1422.0, \"Taken Down\": \"False\", \"Video Sources\": \"human\", \"Task Categories\": [\"Pose Estimation\", \"Video Classification\", \"Video Captioning\", \"Misc\"], \"Inferred Metadata\": {\"GitHub License\": \"MIT License\", \"GitHub Stars (June 2024)\": 297, \"GitHub Topics\": [\"computer-vision\", \"dataset\", \"feature-extraction\", \"video\", \"visuzalization\"], \"Github Date\": \"\", \"HF Config\": \"\", \"HF Config License\": \"\", \"HF Dataset\": \"\", \"HF Date\": \"\", \"HF Downloads (June 2024)\": \"\", \"HF Likes (June 2024)\": \"\", \"HF Yaml License\": \"\", \"PwC Date\": \"\", \"PwC Description\": \"Paper tables with annotated results for Ego-Exo4D: Understanding Skilled Human Activity from First- and Third-Person Perspectives\", \"PwC License Name\": \"\", \"PwC License URL\": \"\", \"S2 Citation Count (June 2024)\": \"\", \"S2 Date\": \"\"}, \"Bibtex\": \"@Article{Grauman2023EgoExo4DUS,\\n author = {K. Grauman and Andrew Westbury and L. Torresani and Kris Kitani and Jitendra Malik and Triantafyllos Afouras and Kumar Ashutosh and Vijay Baiyya and Siddhant Bansal and Bikram Boote and Eugene Byrne and Zachary Chavis and Joya Chen and Feng Cheng and Fu-Jen Chu and Sean Crane and Avijit Dasgupta and Jing Dong and Mar\\u00eda Escobar and Cristhian Forigua and A. Gebreselasie and S. Haresh and Jing Huang and Md Mohaiminul Islam and S. Jain and Rawal Khirodkar and Devansh Kukreja and Kevin J Liang and Jia-Wei Liu and Sagnik Majumder and Yongsen Mao and Miguel Martin and E. Mavroudi and Tushar Nagarajan and Francesco Ragusa and Santhosh K. Ramakrishnan and Luigi Seminara and Arjun Somayazulu and Yale Song and Shan Su and Zihui Xue and Edward Zhang and Jinxu Zhang and Angela Castillo and Changan Chen and Xinzhu Fu and Ryosuke Furuta and Cristina Gonzalez and Prince Gupta and Jiabo Hu and Yifei Huang and Yiming Huang and Weslie Khoo and Anush Kumar and Robert Kuo and Sach Lakhavani and Miao Liu and M. Luo and Zhengyi Luo and Brighid Meredith and Austin Miller and Oluwatumininu Oguntola and Xiaqing Pan and Penny Peng and Shraman Pramanick and Merey Ramazanova and Fiona Ryan and Wei Shan and Kiran Somasundaram and Chenan Song and Audrey Southerland and Masatoshi Tateno and Huiyu Wang and Yuchen Wang and Takuma Yagi and Mingfei Yan and Xitong Yang and Zecheng Yu and S. Zha and Chen Zhao and Ziwei Zhao and Zhifan Zhu and Jeff Zhuo and Pablo Arbel\\u00e1ez and Gedas Bertasius and David J. Crandall and D. Damen and J. Engel and G. Farinella and Antonino Furnari and Bernard Ghanem and Judy Hoffman and C. V. Jawahar and Richard A. Newcombe and Hyun Soo Park and James M. Rehg and Yoichi Sato and M. Savva and Jianbo Shi and Mike Zheng Shou and Michael Wray},\\n booktitle = {arXiv.org},\\n journal = {ArXiv},\\n title = {Ego-Exo4D: Understanding Skilled Human Activity from First- and Third-Person Perspectives},\\n volume = {abs/2311.18259},\\n year = {2023}\\n}\\n\", \"License Use (DataProvenance)\": \"commercial\", \"License Attribution (DataProvenance)\": 1, \"License Share Alike (DataProvenance)\": 0, \"License Use (DataProvenance IgnoreOpenAI)\": \"commercial\", \"License Attribution (DataProvenance IgnoreOpenAI)\": 1, \"License Share Alike (DataProvenance IgnoreOpenAI)\": 0, \"License Type\": \"Commercial\"}, {\"Unique Dataset Identifier\": \"project-aria-digital-twin-dataset\", \"Collection\": \"project-aria-digital-twin-dataset\", \"Collection URL\": \"https://arxiv.org/abs/2306.06362\", \"Dataset Name\": \"Aria Digital Twin\", \"Paper Title\": \"Aria Digital Twin\", \"Paper URL\": \"https://arxiv.org/abs/2306.06362\", \"GitHub URL\": \"https://github.com/facebookresearch/projectaria_tools\", \"Hugging Face URL\": \"\", \"Papers with Code URL\": \"https://paperswithcode.com/paper/aria-digital-twin-a-new-benchmark-dataset-for\", \"ArXiv URL\": \"https://arxiv.org/abs/2306.06362\", \"Semantic Scholar Corpus ID\": 261243365, \"Year Released\": \"2023\", \"Text Sources\": [\"human\"], \"Licenses\": [{\"License\": \"Apache License 2.0\", \"License URL\": \"https://github.com/facebookresearch/projectaria_tools/blob/main/LICENSE\"}], \"Creators\": [\"Meta\"], \"Countries\": [\"United States of America\"], \"License Verified By\": \"Vivek Sharma\", \"Video Hours\": 6.6, \"Taken Down\": \"False\", \"Video Sources\": \"human\", \"Task Categories\": [\"Video Object Detection\", \"Video Segmentation\"], \"Inferred Metadata\": {\"GitHub License\": \"Apache License 2.0\", \"GitHub Stars (June 2024)\": 361, \"GitHub Topics\": [], \"Github Date\": \"\", \"HF Config\": \"\", \"HF Config License\": \"\", \"HF Dataset\": \"\", \"HF Date\": \"\", \"HF Downloads (June 2024)\": \"\", \"HF Likes (June 2024)\": \"\", \"HF Yaml License\": \"\", \"PwC Date\": \"\", \"PwC Description\": \"Implemented in one code library.\", \"PwC License Name\": \"\", \"PwC License URL\": \"\", \"S2 Citation Count (June 2024)\": \"\", \"S2 Date\": \"\"}, \"Bibtex\": \"@Article{Somasundaram2023ProjectAA,\\n author = {K. Somasundaram and Jing Dong and Huixuan Tang and Julian Straub and Mingfei Yan and M. Goesele and Jakob J. Engel and R. D. Nardi and Richard A. Newcombe},\\n booktitle = {arXiv.org},\\n journal = {ArXiv},\\n title = {Project Aria: A New Tool for Egocentric Multi-Modal AI Research},\\n volume = {abs/2308.13561},\\n year = {2023}\\n}\\n\", \"License Use (DataProvenance)\": \"commercial\", \"License Attribution (DataProvenance)\": 1, \"License Share Alike (DataProvenance)\": 0, \"License Use (DataProvenance IgnoreOpenAI)\": \"commercial\", \"License Attribution (DataProvenance IgnoreOpenAI)\": 1, \"License Share Alike (DataProvenance IgnoreOpenAI)\": 0, \"License Type\": \"Commercial\"}, {\"Unique Dataset Identifier\": \"project-aria-dataset\", \"Collection\": \"project-aria-dataset\", \"Collection URL\": \"https://arxiv.org/pdf/2402.13349\", \"Dataset Name\": \"Aria Everyday Activities Dataset\", \"Paper Title\": \"Aria Everyday Activities Dataset\", \"Paper URL\": \"https://arxiv.org/pdf/2402.13349\", \"GitHub URL\": \"https://github.com/facebookresearch/projectaria_tools\", \"Hugging Face URL\": \"\", \"Papers with Code URL\": \"https://paperswithcode.com/paper/aria-everyday-activities-dataset\", \"ArXiv URL\": \"https://arxiv.org/pdf/2402.13349\", \"Semantic Scholar Corpus ID\": 267770215, \"Year Released\": \"2024\", \"Text Sources\": [\"human\"], \"Licenses\": [{\"License\": \"Apache License 2.0\", \"License URL\": \"https://github.com/facebookresearch/Aria_data_tools/blob/main/LICENSE\"}], \"Creators\": [\"Meta\"], \"Countries\": [\"United States of America\"], \"License Verified By\": \"Vivek Sharma\", \"Video Hours\": 1400.0, \"Taken Down\": \"False\", \"Video Sources\": \"human\", \"Task Categories\": [\"Misc (Scene Reconstruction)\"], \"Inferred Metadata\": {\"GitHub License\": \"Apache License 2.0\", \"GitHub Stars (June 2024)\": 361, \"GitHub Topics\": [], \"Github Date\": \"\", \"HF Config\": \"\", \"HF Config License\": \"\", \"HF Dataset\": \"\", \"HF Date\": \"\", \"HF Downloads (June 2024)\": \"\", \"HF Likes (June 2024)\": \"\", \"HF Yaml License\": \"\", \"PwC Date\": \"\", \"PwC Description\": \"Implemented in one code library.\", \"PwC License Name\": \"\", \"PwC License URL\": \"\", \"S2 Citation Count (June 2024)\": \"\", \"S2 Date\": \"\"}, \"Bibtex\": \"@Article{Lv2024AriaEA,\\n author = {Zhaoyang Lv and Nickolas Charron and Pierre Moulon and Alexander Gamino and Cheng Peng and Chris Sweeney and Edward Miller and Huixuan Tang and Jeff Meissner and Jing Dong and Kiran Somasundaram and Luis Pesqueira and Mark Schwesinger and Omkar M. Parkhi and Qiao Gu and R. D. Nardi and Shangyi Cheng and Steve Saarinen and Vijay Baiyya and Yuyang Zou and Richard A. Newcombe and J. Engel and Xiaqing Pan and Carl Ren},\\n booktitle = {arXiv.org},\\n journal = {ArXiv},\\n title = {Aria Everyday Activities Dataset},\\n volume = {abs/2402.13349},\\n year = {2024}\\n}\\n\", \"License Use (DataProvenance)\": \"commercial\", \"License Attribution (DataProvenance)\": 1, \"License Share Alike (DataProvenance)\": 0, \"License Use (DataProvenance IgnoreOpenAI)\": \"commercial\", \"License Attribution (DataProvenance IgnoreOpenAI)\": 1, \"License Share Alike (DataProvenance IgnoreOpenAI)\": 0, \"License Type\": \"Commercial\"}, {\"Unique Dataset Identifier\": \"ego-4d\", \"Collection\": \"ego-4d\", \"Collection URL\": \"https://arxiv.org/abs/2110.07058\", \"Dataset Name\": \"Ego4D: Around the World in 3,000 Hours of Egocentric Video\", \"Paper Title\": \"Ego4D: Around the World in 3,000 Hours of Egocentric Video\", \"Paper URL\": \"https://arxiv.org/abs/2110.07058\", \"GitHub URL\": \"https://github.com/EGO4D/forecasting\", \"Hugging Face URL\": \"\", \"Papers with Code URL\": \"https://paperswithcode.com/paper/ego4d-around-the-world-in-3000-hours-of\", \"ArXiv URL\": \"https://arxiv.org/abs/2110.07058\", \"Semantic Scholar Corpus ID\": 238856888, \"Year Released\": \"2022\", \"Text Sources\": [\"human\"], \"Licenses\": [{\"License\": \"Custom\", \"License URL\": \"https://ego4d-data.org/pdfs/Ego4D-Licenses-Draft.pdf\"}], \"Creators\": [\"Facebook AI Research\"], \"Countries\": [\"United States of America\"], \"License Verified By\": \"Vivek Sharma\", \"Video Hours\": 3670.0, \"Taken Down\": \"False\", \"Video Sources\": \"human\", \"Task Categories\": [\"Video Classification\", \"Temporal Action Segmentation\", \"Video Captioning\", \"Misc\"], \"Inferred Metadata\": {\"GitHub License\": \"MIT License\", \"GitHub Stars (June 2024)\": 63, \"GitHub Topics\": [], \"Github Date\": \"\", \"HF Config\": \"\", \"HF Config License\": \"\", \"HF Dataset\": \"\", \"HF Date\": \"\", \"HF Downloads (June 2024)\": \"\", \"HF Likes (June 2024)\": \"\", \"HF Yaml License\": \"\", \"PwC Date\": \"\", \"PwC Description\": \"Implemented in 8 code libraries.\", \"PwC License Name\": \"\", \"PwC License URL\": \"\", \"S2 Citation Count (June 2024)\": \"\", \"S2 Date\": \"\"}, \"Bibtex\": \"@Article{Grauman2021Ego4DAT,\\n author = {K. Grauman and Andrew Westbury and Eugene Byrne and Zachary Chavis and Antonino Furnari and Rohit Girdhar and Jackson Hamburger and Hao Jiang and Miao Liu and Xingyu Liu and Miguel Martin and Tushar Nagarajan and Ilija Radosavovic and Santhosh K. Ramakrishnan and Fiona Ryan and J. Sharma and Michael Wray and Mengmeng Xu and Eric Z. Xu and Chen Zhao and Siddhant Bansal and Dhruv Batra and Vincent Cartillier and S. Crane and Tien Do and Morrie Doulaty and Akshay Erapalli and Christoph Feichtenhofer and A. Fragomeni and Qichen Fu and Christian Fuegen and A. Gebreselasie and Cristina Gonz\\u00e1lez and James M. Hillis and Xuhua Huang and Yifei Huang and Wenqi Jia and Weslie Khoo and J. Kol\\u00e1r and Satwik Kottur and Anurag Kumar and F. Landini and Chao Li and Yanghao Li and Zhenqiang Li and K. Mangalam and Raghava Modhugu and Jonathan Munro and Tullie Murrell and Takumi Nishiyasu and Will Price and Paola Ruiz Puentes and Merey Ramazanova and Leda Sari and K. Somasundaram and Audrey Southerland and Yusuke Sugano and Ruijie Tao and Minh Vo and Yuchen Wang and Xindi Wu and Takuma Yagi and Yunyi Zhu and P. Arbel\\u00e1ez and David J. Crandall and D. Damen and G. Farinella and Bernard Ghanem and V. Ithapu and C. V. Jawahar and H. Joo and Kris Kitani and Haizhou Li and Richard A. Newcombe and A. Oliva and H. Park and James M. Rehg and Yoichi Sato and Jianbo Shi and Mike Zheng Shou and A. Torralba and L. Torresani and Mingfei Yan and J. Malik},\\n booktitle = {Computer Vision and Pattern Recognition},\\n journal = {2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},\\n pages = {18973-18990},\\n title = {Ego4D: Around the World in 3,000 Hours of Egocentric Video},\\n year = {2021}\\n}\\n\", \"License Use (DataProvenance)\": \"commercial\", \"License Attribution (DataProvenance)\": 1, \"License Share Alike (DataProvenance)\": 0, \"License Use (DataProvenance IgnoreOpenAI)\": \"commercial\", \"License Attribution (DataProvenance IgnoreOpenAI)\": 1, \"License Share Alike (DataProvenance IgnoreOpenAI)\": 0, \"License Type\": \"Commercial\"}, {\"Unique Dataset Identifier\": \"charades\", \"Collection\": \"charades\", \"Collection URL\": \"https://arxiv.org/abs/1604.01753\", \"Dataset Name\": \"Charades\", \"Paper Title\": \"Charades\", \"Paper URL\": \"https://arxiv.org/abs/1604.01753\", \"GitHub URL\": \"\", \"Hugging Face URL\": \"https://huggingface.co/datasets/HuggingFaceM4/charades\", \"Papers with Code URL\": \"https://paperswithcode.com/dataset/charades\", \"ArXiv URL\": \"https://arxiv.org/abs/1604.01753\", \"Semantic Scholar Corpus ID\": 18061547, \"Year Released\": \"2016\", \"Text Sources\": [\"human\"], \"Licenses\": [{\"License\": \"Custom\", \"License URL\": \"https://huggingface.co/datasets/HuggingFaceM4/charades#licensing-information\"}], \"Creators\": [\"Carnegie Mellon University\", \"Inria\", \"University of Washington\", \"Allen Institute for AI\"], \"Countries\": [\"United States of America\", \"France\"], \"License Verified By\": \"Vivek Sharma\", \"Video Hours\": 82.3, \"Taken Down\": \"False\", \"Video Sources\": \"human\", \"Task Categories\": [\"Video Captioning\"], \"Inferred Metadata\": {\"GitHub License\": \"\", \"GitHub Stars (June 2024)\": \"\", \"GitHub Topics\": \"\", \"Github Date\": \"\", \"HF Config\": \"default\", \"HF Config License\": \"\", \"HF Dataset\": \"HuggingFaceM4/charades\", \"HF Date\": \"2022-05-11\", \"HF Downloads (June 2024)\": 7, \"HF Likes (June 2024)\": 2, \"HF Yaml License\": \"Unspecified\", \"PwC Date\": \"2016-01-01\", \"PwC Description\": \"The Charades dataset is composed of 9,848 videos of daily indoors activities with an average length of 30 seconds, involving interactions with 46 objects classes in 15 types of indoor scenes and containing a vocabulary of 30 verbs leading to 157 action classes. Each video in this dataset is annotated by multiple free-text descriptions, action labels, action intervals and classes of interacting objects. 267 different users were presented with a sentence, which includes objects and actions from a fixed vocabulary, and they recorded a video acting out the sentence. In total, the dataset contains 66,500 temporal annotations for 157 action classes, 41,104 labels for 46 object classes, and 27,847 textual descriptions of the videos. In the standard split there are7,986 training video and 1,863 validation video.\", \"PwC License Name\": \"Non Commercial\", \"PwC License URL\": \"http://vuchallenge.org/license-charades.txt\", \"S2 Citation Count (June 2024)\": \"\", \"S2 Date\": \"\"}, \"Bibtex\": \"@Article{Sigurdsson2016HollywoodIH,\\n author = {Gunnar A. Sigurdsson and G\\u00fcl Varol and X. Wang and Ali Farhadi and I. Laptev and A. Gupta},\\n booktitle = {European Conference on Computer Vision},\\n pages = {510-526},\\n title = {Hollywood in Homes: Crowdsourcing Data Collection for Activity Understanding},\\n year = {2016}\\n}\\n\", \"License Use (DataProvenance)\": \"commercial\", \"License Attribution (DataProvenance)\": 1, \"License Share Alike (DataProvenance)\": 1, \"License Use (DataProvenance IgnoreOpenAI)\": \"commercial\", \"License Attribution (DataProvenance IgnoreOpenAI)\": 1, \"License Share Alike (DataProvenance IgnoreOpenAI)\": 1, \"License Type\": \"Commercial\"}, {\"Unique Dataset Identifier\": \"breakfast\", \"Collection\": \"breakfast\", \"Collection URL\": \"https://openaccess.thecvf.com/content_cvpr_2014/html/Kuehne_The_Language_of_2014_CVPR_paper.html\", \"Dataset Name\": \"Breakfast\", \"Paper Title\": \"Breakfast\", \"Paper URL\": \"https://openaccess.thecvf.com/content_cvpr_2014/html/Kuehne_The_Language_of_2014_CVPR_paper.html\", \"GitHub URL\": \"\", \"Hugging Face URL\": \"\", \"Papers with Code URL\": \"https://paperswithcode.com/dataset/breakfast\", \"ArXiv URL\": \"https://openaccess.thecvf.com/content_cvpr_2014/html/Kuehne_The_Language_of_2014_CVPR_paper.html\", \"Semantic Scholar Corpus ID\": 9621856, \"Year Released\": \"2014\", \"Text Sources\": [\"human\"], \"Licenses\": [{\"License\": \"CC BY 4.0\", \"License URL\": \"https://serre-lab.clps.brown.edu/resource/breakfast-actions-dataset/\"}], \"Creators\": [\"Fraunhofer FKIE\", \"Brown University\"], \"Countries\": [\"United States of America\", \"Germany\"], \"License Verified By\": \"Vivek Sharma\", \"Video Hours\": 77.0, \"Taken Down\": \"False\", \"Video Sources\": \"human\", \"Task Categories\": [\"Video Captioning\", \"Action Segmentation\"], \"Inferred Metadata\": {\"GitHub License\": \"\", \"GitHub Stars (June 2024)\": \"\", \"GitHub Topics\": \"\", \"Github Date\": \"\", \"HF Config\": \"\", \"HF Config License\": \"\", \"HF Dataset\": \"\", \"HF Date\": \"\", \"HF Downloads (June 2024)\": \"\", \"HF Likes (June 2024)\": \"\", \"HF Yaml License\": \"\", \"PwC Date\": \"2014-01-01\", \"PwC Description\": \"The Breakfast Actions Dataset comprises of 10 actions related to breakfast preparation, performed by 52 different individuals in 18 different kitchens. The dataset is one of the largest fully annotated datasets available. The actions are recorded \\u201cin the wild\\u201d as opposed to a single controlled lab environment. It consists of over 77 hours of video recordings.\", \"PwC License Name\": \"CC BY 4.0\", \"PwC License URL\": \"https://creativecommons.org/licenses/by/4.0/\", \"S2 Citation Count (June 2024)\": \"\", \"S2 Date\": \"\"}, \"Bibtex\": \"@Article{Kuehne2014TheLO,\\n author = {Hilde Kuehne and A. B. Arslan and Thomas Serre},\\n booktitle = {2014 IEEE Conference on Computer Vision and Pattern Recognition},\\n journal = {2014 IEEE Conference on Computer Vision and Pattern Recognition},\\n pages = {780-787},\\n title = {The Language of Actions: Recovering the Syntax and Semantics of Goal-Directed Human Activities},\\n year = {2014}\\n}\\n\", \"License Use (DataProvenance)\": \"commercial\", \"License Attribution (DataProvenance)\": 1, \"License Share Alike (DataProvenance)\": 0, \"License Use (DataProvenance IgnoreOpenAI)\": \"commercial\", \"License Attribution (DataProvenance IgnoreOpenAI)\": 1, \"License Share Alike (DataProvenance IgnoreOpenAI)\": 0, \"License Type\": \"Commercial\"}, {\"Unique Dataset Identifier\": \"toyota-smarthome\", \"Collection\": \"toyota-smarthome\", \"Collection URL\": \"https://openaccess.thecvf.com/content_ICCV_2019/papers/Das_Toyota_Smarthome_Real-World_Activities_of_Daily_Living_ICCV_2019_paper.pdf\", \"Dataset Name\": \"Toyota Smarthome: Real-World Activities of Daily Living (ICCV 2019)\", \"Paper Title\": \"Toyota Smarthome: Real-World Activities of Daily Living (ICCV 2019)\", \"Paper URL\": \"https://openaccess.thecvf.com/content_ICCV_2019/papers/Das_Toyota_Smarthome_Real-World_Activities_of_Daily_Living_ICCV_2019_paper.pdf\", \"GitHub URL\": \"\", \"Hugging Face URL\": \"\", \"Papers with Code URL\": \"\", \"ArXiv URL\": \"https://openaccess.thecvf.com/content_ICCV_2019/papers/Das_Toyota_Smarthome_Real-World_Activities_of_Daily_Living_ICCV_2019_paper.pdf\", \"Semantic Scholar Corpus ID\": 207971208, \"Year Released\": \"2019\", \"Text Sources\": [\"human\"], \"Licenses\": [{\"License\": \"Custom\", \"License URL\": \"https://project.inria.fr/toyotasmarthome/files/2020/12/License_v2.pdf\"}], \"Creators\": [\"Toyota\"], \"Countries\": [\"France\"], \"License Verified By\": \"Vivek Sharma\", \"Video Hours\": 268.58, \"Taken Down\": \"False\", \"Video Sources\": \"human\", \"Task Categories\": [\"Video Classification\"], \"Inferred Metadata\": {\"GitHub License\": \"\", \"GitHub Stars (June 2024)\": \"\", \"GitHub Topics\": \"\", \"Github Date\": \"\", \"HF Config\": \"\", \"HF Config License\": \"\", \"HF Dataset\": \"\", \"HF Date\": \"\", \"HF Downloads (June 2024)\": \"\", \"HF Likes (June 2024)\": \"\", \"HF Yaml License\": \"\", \"PwC Date\": \"\", \"PwC Description\": \"\", \"PwC License Name\": \"\", \"PwC License URL\": \"\", \"S2 Citation Count (June 2024)\": \"\", \"S2 Date\": \"\"}, \"Bibtex\": \"@Article{Das2019ToyotaSR,\\n author = {Srijan Das and Rui Dai and Michal Koperski and Luca Minciullo and L. Garattoni and F. Br\\u00e9mond and G. Francesca},\\n booktitle = {IEEE International Conference on Computer Vision},\\n journal = {2019 IEEE/CVF International Conference on Computer Vision (ICCV)},\\n pages = {833-842},\\n title = {Toyota Smarthome: Real-World Activities of Daily Living},\\n year = {2019}\\n}\\n\", \"License Use (DataProvenance)\": \"academic-only\", \"License Attribution (DataProvenance)\": 1, \"License Share Alike (DataProvenance)\": 0, \"License Use (DataProvenance IgnoreOpenAI)\": \"academic-only\", \"License Attribution (DataProvenance IgnoreOpenAI)\": 1, \"License Share Alike (DataProvenance IgnoreOpenAI)\": 0, \"License Type\": \"Non-Commercial/Academic\"}, {\"Unique Dataset Identifier\": \"mpii-cooking2\", \"Collection\": \"mpii-cooking2\", \"Collection URL\": \"https://arxiv.org/pdf/1502.06648.pdf\", \"Dataset Name\": \"MPII Cooking 2\", \"Paper Title\": \"MPII Cooking 2\", \"Paper URL\": \"https://arxiv.org/pdf/1502.06648.pdf\", \"GitHub URL\": \"\", \"Hugging Face URL\": \"\", \"Papers with Code URL\": \"https://paperswithcode.com/dataset/mpii-cooking-2-dataset\", \"ArXiv URL\": \"https://arxiv.org/pdf/1502.06648.pdf\", \"Semantic Scholar Corpus ID\": 14036544, \"Year Released\": \"2016\", \"Text Sources\": [\"human\"], \"Licenses\": [{\"License\": \"Custom\", \"License URL\": \"https://www.mpi-inf.mpg.de/departments/computer-vision-and-machine-learning/research/human-activity-recognition/mpii-cooking-2-dataset/\"}], \"Creators\": [\"Max Planck Institute for Informatics\", \"Saarland University\"], \"Countries\": [\"Germany\"], \"License Verified By\": \"Vivek Sharma\", \"Video Hours\": 27.0, \"Taken Down\": \"False\", \"Video Sources\": \"human\", \"Task Categories\": [\"Temporal Action Detection\"], \"Inferred Metadata\": {\"GitHub License\": \"\", \"GitHub Stars (June 2024)\": \"\", \"GitHub Topics\": \"\", \"Github Date\": \"\", \"HF Config\": \"\", \"HF Config License\": \"\", \"HF Dataset\": \"\", \"HF Date\": \"\", \"HF Downloads (June 2024)\": \"\", \"HF Likes (June 2024)\": \"\", \"HF Yaml License\": \"\", \"PwC Date\": \"\", \"PwC Description\": \"A dataset which provides detailed annotations for activity recognition.\", \"PwC License Name\": \"Unspecified\", \"PwC License URL\": null, \"S2 Citation Count (June 2024)\": \"\", \"S2 Date\": \"\"}, \"Bibtex\": \"@Article{Rohrbach2015RecognizingFA,\\n author = {Marcus Rohrbach and Anna Rohrbach and Michaela Regneri and S. Amin and Mykhaylo Andriluka and Manfred Pinkal and B. Schiele},\\n booktitle = {International Journal of Computer Vision},\\n journal = {International Journal of Computer Vision},\\n pages = {346 - 373},\\n title = {Recognizing Fine-Grained and Composite Activities Using Hand-Centric Features and Script Data},\\n volume = {119},\\n year = {2015}\\n}\\n\", \"License Use (DataProvenance)\": \"academic-only\", \"License Attribution (DataProvenance)\": 1, \"License Share Alike (DataProvenance)\": 0, \"License Use (DataProvenance IgnoreOpenAI)\": \"academic-only\", \"License Attribution (DataProvenance IgnoreOpenAI)\": 1, \"License Share Alike (DataProvenance IgnoreOpenAI)\": 0, \"License Type\": \"Non-Commercial/Academic\"}, {\"Unique Dataset Identifier\": \"haa500-dataset\", \"Collection\": \"haa500-dataset\", \"Collection URL\": \"https://arxiv.org/abs/2009.05224\", \"Dataset Name\": \"HAA500: Human-Centric Atomic Action Dataset with Curated Videos (ICCV 2021)\", \"Paper Title\": \"HAA500: Human-Centric Atomic Action Dataset with Curated Videos (ICCV 2021)\", \"Paper URL\": \"https://arxiv.org/abs/2009.05224\", \"GitHub URL\": \"\", \"Hugging Face URL\": \"\", \"Papers with Code URL\": \"https://paperswithcode.com/paper/haa500-human-centric-atomic-action-dataset\", \"ArXiv URL\": \"https://arxiv.org/abs/2009.05224\", \"Semantic Scholar Corpus ID\": 221640805, \"Year Released\": \"2020\", \"Text Sources\": [\"human\"], \"Licenses\": [{\"License\": \"Unspecified\", \"License URL\": \"\"}], \"Creators\": [\"Carnegie Mellon University\", \"The Hong Kong University of Science and Technology\", \"Princeton University\", \"Kuaishou Technology\"], \"Countries\": [\"China\", \"United States of America\"], \"License Verified By\": \"Vivek Sharma\", \"Video Hours\": 5.48, \"Taken Down\": \"False\", \"Video Sources\": \"human\", \"Task Categories\": [\"Video Classification\"], \"Inferred Metadata\": {\"GitHub License\": \"\", \"GitHub Stars (June 2024)\": \"\", \"GitHub Topics\": \"\", \"Github Date\": \"\", \"HF Config\": \"\", \"HF Config License\": \"\", \"HF Dataset\": \"\", \"HF Date\": \"\", \"HF Downloads (June 2024)\": \"\", \"HF Likes (June 2024)\": \"\", \"HF Yaml License\": \"\", \"PwC Date\": \"\", \"PwC Description\": \"\\ud83c\\udfc6 SOTA for Action Recognition on HAA500 (Top-1 (%) metric)\", \"PwC License Name\": \"\", \"PwC License URL\": \"\", \"S2 Citation Count (June 2024)\": \"\", \"S2 Date\": \"\"}, \"Bibtex\": \"@Article{Chung2020HAA500HA,\\n author = {Jihoon Chung and Cheng-hsin Wuu and Hsuan-ru Yang and Yu-Wing Tai and Chi-Keung Tang},\\n booktitle = {IEEE International Conference on Computer Vision},\\n journal = {2021 IEEE/CVF International Conference on Computer Vision (ICCV)},\\n pages = {13445-13454},\\n title = {HAA500: Human-Centric Atomic Action Dataset with Curated Videos},\\n year = {2020}\\n}\\n\", \"License Use (DataProvenance)\": \"unspecified\", \"License Attribution (DataProvenance)\": 0, \"License Share Alike (DataProvenance)\": 0, \"License Use (DataProvenance IgnoreOpenAI)\": \"unspecified\", \"License Attribution (DataProvenance IgnoreOpenAI)\": 0, \"License Share Alike (DataProvenance IgnoreOpenAI)\": 0, \"License Type\": \"Unspecified\"}, {\"Unique Dataset Identifier\": \"epic-kitchenes\", \"Collection\": \"epic-kitchenes\", \"Collection URL\": \"https://arxiv.org/abs/1804.02748\", \"Dataset Name\": \"EPIC-KITCHENS\", \"Paper Title\": \"EPIC-KITCHENS\", \"Paper URL\": \"https://arxiv.org/abs/1804.02748\", \"GitHub URL\": \"https://github.com/epic-kitchens\", \"Hugging Face URL\": \"\", \"Papers with Code URL\": \"https://paperswithcode.com/dataset/epic-kitchens-100\", \"ArXiv URL\": \"https://arxiv.org/abs/1804.02748\", \"Semantic Scholar Corpus ID\": 4710439, \"Year Released\": \"2018\", \"Text Sources\": [\"human\"], \"Licenses\": [{\"License\": \"CC BY-NC 4.0\", \"License URL\": \"https://epic-kitchens.github.io/2024\"}], \"Creators\": [\"University of Toronto\", \"University of Bristol\", \"University of Catania\"], \"Countries\": [\"United Kingdom\", \"Canada\", \"Spain\"], \"License Verified By\": \"Vivek Sharma\", \"Video Hours\": 100.0, \"Taken Down\": \"False\", \"Video Sources\": \"human\", \"Task Categories\": [\"Video Captioning\"], \"Inferred Metadata\": {\"GitHub License\": \"\", \"GitHub Stars (June 2024)\": 0, \"GitHub Topics\": [\"\"], \"Github Date\": \"\", \"HF Config\": \"\", \"HF Config License\": \"\", \"HF Dataset\": \"\", \"HF Date\": \"\", \"HF Downloads (June 2024)\": \"\", \"HF Likes (June 2024)\": \"\", \"HF Yaml License\": \"\", \"PwC Date\": \"2020-06-23\", \"PwC Description\": \"This paper introduces the pipeline to scale the largest dataset in egocentric vision EPIC-KITCHENS. The effort culminates in EPIC-KITCHENS-100, a collection of 100 hours, 20M frames, 90K actions in 700 variable-length videos, capturing long-term unscripted activities in 45 environments, using head-mounted cameras. Compared to its previous version (EPIC-KITCHENS-55), EPIC-KITCHENS-100 has been annotated using a novel pipeline that allows denser (54% more actions per minute) and more complete annotations of fine-grained actions (+128% more action segments). This collection also enables evaluating the \\\"test of time\\\" - i.e. whether models trained on data collected in 2018 can generalise to new footage collected under the same hypotheses albeit \\\"two years on\\\".\\nThe dataset is aligned with 6 challenges: action recognition (full and weak supervision), action detection, action anticipation, cross-modal retrieval (from captions), as well as unsupervised domain adaptation for action recognition. For each challenge, we define the task, provide baselines and evaluation metrics.\", \"PwC License Name\": \"CC BY-NC 4.0\", \"PwC License URL\": \"https://epic-kitchens.github.io/2021\", \"S2 Citation Count (June 2024)\": \"\", \"S2 Date\": \"\"}, \"Bibtex\": \"@Article{Damen2018ScalingEV,\\n author = {D. Damen and Hazel Doughty and G. Farinella and S. Fidler and Antonino Furnari and E. Kazakos and D. Moltisanti and Jonathan Munro and Toby Perrett and Will Price and Michael Wray},\\n booktitle = {arXiv.org},\\n journal = {ArXiv},\\n title = {Scaling Egocentric Vision: The EPIC-KITCHENS Dataset},\\n volume = {abs/1804.02748},\\n year = {2018}\\n}\\n\", \"License Use (DataProvenance)\": \"non-commercial\", \"License Attribution (DataProvenance)\": 1, \"License Share Alike (DataProvenance)\": 0, \"License Use (DataProvenance IgnoreOpenAI)\": \"non-commercial\", \"License Attribution (DataProvenance IgnoreOpenAI)\": 1, \"License Share Alike (DataProvenance IgnoreOpenAI)\": 0, \"License Type\": \"Non-Commercial/Academic\"}, {\"Unique Dataset Identifier\": \"mpii-cooking\", \"Collection\": \"mpii-cooking\", \"Collection URL\": \"https://arxiv.org/abs/1502.06648\", \"Dataset Name\": \"MPII-Cooking: Recognizing Fine-Grained and Composite Activities Using Hand-Centric Features and Script Data (IJCV 2015)\", \"Paper Title\": \"MPII-Cooking: Recognizing Fine-Grained and Composite Activities Using Hand-Centric Features and Script Data (IJCV 2015)\", \"Paper URL\": \"https://arxiv.org/abs/1502.06648\", \"GitHub URL\": \"\", \"Hugging Face URL\": \"\", \"Papers with Code URL\": \"https://paperswithcode.com/paper/recognizing-fine-grained-and-composite\", \"ArXiv URL\": \"https://arxiv.org/abs/1502.06648\", \"Semantic Scholar Corpus ID\": 14036544, \"Year Released\": \"2015\", \"Text Sources\": [\"human\"], \"Licenses\": [{\"License\": \"Custom\", \"License URL\": \"https://www.mpi-inf.mpg.de/departments/computer-vision-and-machine-learning/research/human-activity-recognition/mpii-cooking-activities-dataset\"}], \"Creators\": [\"Max Planck Institute for Informatics\"], \"Countries\": [\"Germany\"], \"License Verified By\": \"Vivek Sharma\", \"Video Hours\": 27.0, \"Taken Down\": \"False\", \"Video Sources\": \"human\", \"Task Categories\": [\"Video Classification\"], \"Inferred Metadata\": {\"GitHub License\": \"\", \"GitHub Stars (June 2024)\": \"\", \"GitHub Topics\": \"\", \"Github Date\": \"\", \"HF Config\": \"\", \"HF Config License\": \"\", \"HF Dataset\": \"\", \"HF Date\": \"\", \"HF Downloads (June 2024)\": \"\", \"HF Likes (June 2024)\": \"\", \"HF Yaml License\": \"\", \"PwC Date\": \"\", \"PwC Description\": \"No code available yet.\", \"PwC License Name\": \"\", \"PwC License URL\": \"\", \"S2 Citation Count (June 2024)\": \"\", \"S2 Date\": \"\"}, \"Bibtex\": \"@Article{Rohrbach2015RecognizingFA,\\n author = {Marcus Rohrbach and Anna Rohrbach and Michaela Regneri and S. Amin and Mykhaylo Andriluka and Manfred Pinkal and B. Schiele},\\n booktitle = {International Journal of Computer Vision},\\n journal = {International Journal of Computer Vision},\\n pages = {346 - 373},\\n title = {Recognizing Fine-Grained and Composite Activities Using Hand-Centric Features and Script Data},\\n volume = {119},\\n year = {2015}\\n}\\n\", \"License Use (DataProvenance)\": \"academic-only\", \"License Attribution (DataProvenance)\": 1, \"License Share Alike (DataProvenance)\": 0, \"License Use (DataProvenance IgnoreOpenAI)\": \"academic-only\", \"License Attribution (DataProvenance IgnoreOpenAI)\": 1, \"License Share Alike (DataProvenance IgnoreOpenAI)\": 0, \"License Type\": \"Non-Commercial/Academic\"}, {\"Unique Dataset Identifier\": \"uav-human\", \"Collection\": \"uav-human\", \"Collection URL\": \"https://arxiv.org/abs/2104.00946\", \"Dataset Name\": \"UAV-Human: A Large Benchmark for Human Behavior Understanding with Unmanned Aerial Vehicles\", \"Paper Title\": \"UAV-Human: A Large Benchmark for Human Behavior Understanding with Unmanned Aerial Vehicles\", \"Paper URL\": \"https://arxiv.org/abs/2104.00946\", \"GitHub URL\": \"https://github.com/sutdcv/UAV-Human\", \"Hugging Face URL\": \"\", \"Papers with Code URL\": \"https://paperswithcode.com/paper/uav-human-a-large-benchmark-for-human\", \"ArXiv URL\": \"https://arxiv.org/abs/2104.00946\", \"Semantic Scholar Corpus ID\": 233004700, \"Year Released\": \"2021\", \"Text Sources\": [\"human\"], \"Licenses\": [{\"License\": \"Custom\", \"License URL\": \"https://sutdcv.github.io/uav-human-web/\"}], \"Creators\": [\"Shandong University\", \"Singapore University of Technology and Design\"], \"Countries\": [\"Singapore\", \"China\"], \"License Verified By\": \"Vivek Sharma\", \"Video Hours\": 18.34, \"Taken Down\": \"False\", \"Video Sources\": \"human\", \"Task Categories\": [\"Video Classification\"], \"Inferred Metadata\": {\"GitHub License\": \"\", \"GitHub Stars (June 2024)\": 180, \"GitHub Topics\": [\"action-recognition\", \"dataset\", \"uav\"], \"Github Date\": \"\", \"HF Config\": \"\", \"HF Config License\": \"\", \"HF Dataset\": \"\", \"HF Date\": \"\", \"HF Downloads (June 2024)\": \"\", \"HF Likes (June 2024)\": \"\", \"HF Yaml License\": \"\", \"PwC Date\": \"\", \"PwC Description\": \"Implemented in 2 code libraries.\", \"PwC License Name\": \"\", \"PwC License URL\": \"\", \"S2 Citation Count (June 2024)\": \"\", \"S2 Date\": \"\"}, \"Bibtex\": \"@Article{Li2021UAVHumanAL,\\n author = {Tianjiao Li and Jun Liu and Wei Zhang and Yun Ni and Wenqian Wang and Zhiheng Li},\\n booktitle = {Computer Vision and Pattern Recognition},\\n journal = {2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},\\n pages = {16261-16270},\\n title = {UAV-Human: A Large Benchmark for Human Behavior Understanding with Unmanned Aerial Vehicles},\\n year = {2021}\\n}\\n\", \"License Use (DataProvenance)\": \"academic-only\", \"License Attribution (DataProvenance)\": 1, \"License Share Alike (DataProvenance)\": 0, \"License Use (DataProvenance IgnoreOpenAI)\": \"academic-only\", \"License Attribution (DataProvenance IgnoreOpenAI)\": 1, \"License Share Alike (DataProvenance IgnoreOpenAI)\": 0, \"License Type\": \"Non-Commercial/Academic\"}, {\"Unique Dataset Identifier\": \"mmact\", \"Collection\": \"mmact\", \"Collection URL\": \"https://ieeexplore.ieee.org/document/9009579\", \"Dataset Name\": \"MMAct\", \"Paper Title\": \"MMAct\", \"Paper URL\": \"https://ieeexplore.ieee.org/document/9009579\", \"GitHub URL\": \"\", \"Hugging Face URL\": \"\", \"Papers with Code URL\": \"https://paperswithcode.com/dataset/mmact\", \"ArXiv URL\": \"https://ieeexplore.ieee.org/document/9009579\", \"Semantic Scholar Corpus ID\": 207980205, \"Year Released\": \"2019\", \"Text Sources\": [\"human\"], \"Licenses\": [{\"License\": \"Custom\", \"License URL\": \"https://mmact19.github.io/2019/\"}], \"Creators\": [\"The Hong Kong University of Science and Technology\", \"Alibaba Group\"], \"Countries\": [\"Hong Kong\", \"China\"], \"License Verified By\": \"Vivek Sharma\", \"Video Hours\": 100.0, \"Taken Down\": \"False\", \"Video Sources\": \"human\", \"Task Categories\": [\"Temporal Localization\", \"Action Recognition\", \"Spatial-Temporal Action Localization\"], \"Inferred Metadata\": {\"GitHub License\": \"\", \"GitHub Stars (June 2024)\": \"\", \"GitHub Topics\": \"\", \"Github Date\": \"\", \"HF Config\": \"\", \"HF Config License\": \"\", \"HF Dataset\": \"\", \"HF Date\": \"\", \"HF Downloads (June 2024)\": \"\", \"HF Likes (June 2024)\": \"\", \"HF Yaml License\": \"\", \"PwC Date\": \"\", \"PwC Description\": \"MMAct is a large-scale dataset for multi/cross modal action understanding. This dataset has been recorded from 20 distinct subjects with seven different types of modalities: RGB videos, keypoints, acceleration, gyroscope, orientation, Wi-Fi and pressure signal. The dataset consists of more than 36k video clips for 37 action classes covering a wide range of daily life activities such as desktop-related and check-in-based ones in four different distinct scenarios.\", \"PwC License Name\": \"Unspecified\", \"PwC License URL\": null, \"S2 Citation Count (June 2024)\": \"\", \"S2 Date\": \"\"}, \"Bibtex\": \"@Article{Kong2019MMActAL,\\n author = {Quan Kong and Ziming Wu and Ziwei Deng and Martin Klinkigt and Bin Tong and Tomokazu Murakami},\\n booktitle = {IEEE International Conference on Computer Vision},\\n journal = {2019 IEEE/CVF International Conference on Computer Vision (ICCV)},\\n pages = {8657-8666},\\n title = {MMAct: A Large-Scale Dataset for Cross Modal Human Action Understanding},\\n year = {2019}\\n}\\n\", \"License Use (DataProvenance)\": \"academic-only\", \"License Attribution (DataProvenance)\": 1, \"License Share Alike (DataProvenance)\": 0, \"License Use (DataProvenance IgnoreOpenAI)\": \"academic-only\", \"License Attribution (DataProvenance IgnoreOpenAI)\": 1, \"License Share Alike (DataProvenance IgnoreOpenAI)\": 0, \"License Type\": \"Non-Commercial/Academic\"}, {\"Unique Dataset Identifier\": \"multi-moments-in-time-dataset\", \"Collection\": \"multi-moments-in-time-dataset\", \"Collection URL\": \"https://arxiv.org/pdf/1911.00232.pdf\", \"Dataset Name\": \"Multi-Moments in Time: Learning and Interpreting Models for Multi-Action Video Understanding\", \"Paper Title\": \"Multi-Moments in Time: Learning and Interpreting Models for Multi-Action Video Understanding\", \"Paper URL\": \"https://arxiv.org/pdf/1911.00232.pdf\", \"GitHub URL\": \"\", \"Hugging Face URL\": \"\", \"Papers with Code URL\": \"\", \"ArXiv URL\": \"https://arxiv.org/pdf/1911.00232.pdf\", \"Semantic Scholar Corpus ID\": 207780280, \"Year Released\": \"2021\", \"Text Sources\": [\"crowdsourced\", \"undisclosed web\"], \"Licenses\": [{\"License\": \"Unspecified\", \"License URL\": \"\"}], \"Creators\": [\"Massachusetts Institute of Technology\"], \"Countries\": [\"United States of America\"], \"License Verified By\": \"Vivek Sharma\", \"Video Hours\": 833.0, \"Taken Down\": \"False\", \"Video Sources\": \"undisclosed web\", \"Task Categories\": [\"Video Classification\"], \"Inferred Metadata\": {\"GitHub License\": \"\", \"GitHub Stars (June 2024)\": \"\", \"GitHub Topics\": \"\", \"Github Date\": \"\", \"HF Config\": \"\", \"HF Config License\": \"\", \"HF Dataset\": \"\", \"HF Date\": \"\", \"HF Downloads (June 2024)\": \"\", \"HF Likes (June 2024)\": \"\", \"HF Yaml License\": \"\", \"PwC Date\": \"\", \"PwC Description\": \"\", \"PwC License Name\": \"\", \"PwC License URL\": \"\", \"S2 Citation Count (June 2024)\": \"\", \"S2 Date\": \"\"}, \"Bibtex\": \"@Article{Monfort2019MultiMomentsIT,\\n author = {Mathew Monfort and K. Ramakrishnan and A. Andonian and Barry A. McNamara and A. Lascelles and Bowen Pan and Quanfu Fan and Dan Gutfreund and R. Feris and A. Oliva},\\n booktitle = {IEEE Transactions on Pattern Analysis and Machine Intelligence},\\n journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},\\n pages = {1-1},\\n title = {Multi-Moments in Time: Learning and Interpreting Models for Multi-Action Video Understanding},\\n volume = {PP},\\n year = {2019}\\n}\\n\", \"License Use (DataProvenance)\": \"unspecified\", \"License Attribution (DataProvenance)\": 0, \"License Share Alike (DataProvenance)\": 0, \"License Use (DataProvenance IgnoreOpenAI)\": \"unspecified\", \"License Attribution (DataProvenance IgnoreOpenAI)\": 0, \"License Share Alike (DataProvenance IgnoreOpenAI)\": 0, \"License Type\": \"Unspecified\"}, {\"Unique Dataset Identifier\": \"WebVid\", \"Collection\": \"WebVid\", \"Collection URL\": \"https://arxiv.org/abs/2104.00650\", \"Dataset Name\": \"WebVid\", \"Paper Title\": \"WebVid\", \"Paper URL\": \"https://arxiv.org/abs/2104.00650\", \"GitHub URL\": \"https://github.com/m-bain/webvid\", \"Hugging Face URL\": \"https://huggingface.co/datasets/TempoFunk/webvid-10M\", \"Papers with Code URL\": \"https://paperswithcode.com/dataset/webvid\", \"ArXiv URL\": \"https://arxiv.org/abs/2104.00650\", \"Semantic Scholar Corpus ID\": 232478955, \"Year Released\": \"2021\", \"Text Sources\": [\"undisclosed web\"], \"Licenses\": [{\"License\": \"Custom\", \"License URL\": \"https://github.com/m-bain/webvid/blob/main/TERMS.md\"}], \"Creators\": [\"University of Oxford\", \"CNRS\"], \"Countries\": [\"United Kingdom\", \"France\"], \"License Verified By\": \"Vivek Sharma\", \"Video Hours\": 13000.0, \"Taken Down\": \"True\", \"Video Sources\": \"undisclosed web\", \"Task Categories\": [\"Video Captioning\"], \"Inferred Metadata\": {\"GitHub License\": \"\", \"GitHub Stars (June 2024)\": 528, \"GitHub Topics\": [], \"Github Date\": \"\", \"HF Config\": \"default\", \"HF Config License\": \"\", \"HF Dataset\": \"TempoFunk/webvid-10M\", \"HF Date\": \"2023-06-16\", \"HF Downloads (June 2024)\": 360, \"HF Likes (June 2024)\": 28, \"HF Yaml License\": \"GNU General Public License v3.0\", \"PwC Date\": \"2021-04-01\", \"PwC Description\": \"WebVid contains 10 million video clips with captions, sourced from the web. The videos are diverse and rich in their content.\\n\\nBoth the full 10M set and a 2.5M subset is available for download:\\nhttps://github.com/m-bain/webvid-dataset\", \"PwC License Name\": \"Various\", \"PwC License URL\": \"https://github.com/m-bain/webvid/blob/main/TERMS.md\", \"S2 Citation Count (June 2024)\": \"\", \"S2 Date\": \"\"}, \"Bibtex\": \"@Article{Bain2021FrozenIT,\\n author = {Max Bain and Arsha Nagrani and G\\u00fcl Varol and Andrew Zisserman},\\n booktitle = {IEEE International Conference on Computer Vision},\\n journal = {2021 IEEE/CVF International Conference on Computer Vision (ICCV)},\\n pages = {1708-1718},\\n title = {Frozen in Time: A Joint Video and Image Encoder for End-to-End Retrieval},\\n year = {2021}\\n}\\n\", \"License Use (DataProvenance)\": \"academic-only\", \"License Attribution (DataProvenance)\": 0, \"License Share Alike (DataProvenance)\": 0, \"License Use (DataProvenance IgnoreOpenAI)\": \"academic-only\", \"License Attribution (DataProvenance IgnoreOpenAI)\": 0, \"License Share Alike (DataProvenance IgnoreOpenAI)\": 0, \"License Type\": \"Non-Commercial/Academic\"}, {\"Unique Dataset Identifier\": \"rare-act-dataset\", \"Collection\": \"rare-act-dataset\", \"Collection URL\": \"https://arxiv.org/pdf/2008.01018.pdf\", \"Dataset Name\": \"RareAct: A video dataset of unusual interactions\", \"Paper Title\": \"RareAct: A video dataset of unusual interactions\", \"Paper URL\": \"https://arxiv.org/pdf/2008.01018.pdf\", \"GitHub URL\": \"https://github.com/antoine77340/RareAct\", \"Hugging Face URL\": \"\", \"Papers with Code URL\": \"https://paperswithcode.com/dataset/rareact\", \"ArXiv URL\": \"https://arxiv.org/pdf/2008.01018.pdf\", \"Semantic Scholar Corpus ID\": 220936243, \"Year Released\": \"2020\", \"Text Sources\": [\"undisclosed web\"], \"Licenses\": [{\"License\": \"Unspecified\", \"License URL\": \"\"}], \"Creators\": [\"University of Oxford\", \"Ecole Normale Sup\\u00e9rieure\", \"Inria\", \"CIIRC\", \"Czech Technical University\"], \"Countries\": [\"United Kingdom\", \"France\", \"Czech Republic\"], \"License Verified By\": \"Vivek Sharma\", \"Video Hours\": 21.13, \"Taken Down\": \"False\", \"Video Sources\": \"undisclosed web\", \"Task Categories\": [\"Video Classification\"], \"Inferred Metadata\": {\"GitHub License\": \"Apache License 2.0\", \"GitHub Stars (June 2024)\": 32, \"GitHub Topics\": [], \"Github Date\": \"\", \"HF Config\": \"\", \"HF Config License\": \"\", \"HF Dataset\": \"\", \"HF Date\": \"\", \"HF Downloads (June 2024)\": \"\", \"HF Likes (June 2024)\": \"\", \"HF Yaml License\": \"\", \"PwC Date\": \"\", \"PwC Description\": \"RareAct is a video dataset of unusual actions, including actions like \\u201cblend phone\\u201d, \\u201ccut keyboard\\u201d and \\u201cmicrowave shoes\\u201d. It aims at evaluating the zero-shot and few-shot compositionality of action recognition models for unlikely compositions of common action verbs and object nouns. It contains 122 different actions which were obtained by combining verbs and nouns rarely co-occurring together in the large-scale textual corpus from HowTo100M, but that frequently appear separately.\", \"PwC License Name\": \"Unspecified\", \"PwC License URL\": null, \"S2 Citation Count (June 2024)\": \"\", \"S2 Date\": \"\"}, \"Bibtex\": \"@Article{Miech2020RareActAV,\\n author = {Antoine Miech and Jean-Baptiste Alayrac and I. Laptev and Josef Sivic and Andrew Zisserman},\\n booktitle = {arXiv.org},\\n journal = {ArXiv},\\n title = {RareAct: A video dataset of unusual interactions},\\n volume = {abs/2008.01018},\\n year = {2020}\\n}\\n\", \"License Use (DataProvenance)\": \"unspecified\", \"License Attribution (DataProvenance)\": 0, \"License Share Alike (DataProvenance)\": 0, \"License Use (DataProvenance IgnoreOpenAI)\": \"unspecified\", \"License Attribution (DataProvenance IgnoreOpenAI)\": 0, \"License Share Alike (DataProvenance IgnoreOpenAI)\": 0, \"License Type\": \"Unspecified\"}, {\"Unique Dataset Identifier\": \"finegym-dataset\", \"Collection\": \"finegym-dataset\", \"Collection URL\": \"https://arxiv.org/abs/2004.06704\", \"Dataset Name\": \"FineGym: A Hierarchical Video Dataset for Fine-grained Action Understanding (CVPR 2020)\", \"Paper Title\": \"FineGym: A Hierarchical Video Dataset for Fine-grained Action Understanding (CVPR 2020)\", \"Paper URL\": \"https://arxiv.org/abs/2004.06704\", \"GitHub URL\": \"https://github.com/SDOlivia/FineGym/\", \"Hugging Face URL\": \"\", \"Papers with Code URL\": \"https://paperswithcode.com/dataset/finegym\", \"ArXiv URL\": \"https://arxiv.org/abs/2004.06704\", \"Semantic Scholar Corpus ID\": 215754360, \"Year Released\": \"2020\", \"Text Sources\": [\"undisclosed web\"], \"Licenses\": [{\"License\": \"CC BY-NC 4.0\", \"License URL\": \"https://sdolivia.github.io/FineGym/\"}], \"Creators\": [\"The Chinese University of Hong Kong\"], \"Countries\": [\"China\"], \"License Verified By\": \"Vivek Sharma\", \"Video Hours\": 708.0, \"Taken Down\": \"False\", \"Video Sources\": \"undisclosed web\", \"Task Categories\": [\"Video Classification\"], \"Inferred Metadata\": {\"GitHub License\": \"\", \"GitHub Stars (June 2024)\": 124, \"GitHub Topics\": [], \"Github Date\": \"\", \"HF Config\": \"\", \"HF Config License\": \"\", \"HF Dataset\": \"\", \"HF Date\": \"\", \"HF Downloads (June 2024)\": \"\", \"HF Likes (June 2024)\": \"\", \"HF Yaml License\": \"\", \"PwC Date\": \"2020-04-14\", \"PwC Description\": \"FineGym is an action recognition dataset build on top of gymnasium videos. Compared to existing action recognition datasets, FineGym is distinguished in richness, quality, and diversity. In particular, it provides temporal annotations at both action and sub-action levels with a three-level semantic hierarchy. For example, a \\\"balance beam\\\" event will be annotated as a sequence of elementary sub-actions derived from five sets: \\\"leap-jumphop\\\", \\\"beam-turns\\\", \\\"flight-salto\\\", \\\"flight-handspring\\\", and \\\"dismount\\\", where the sub-action in each set will be further annotated with finely defined class labels. This new level of granularity presents significant challenges for action recognition, e.g. how to parse the temporal structures from a coherent action, and how to distinguish between subtly different action classes.\", \"PwC License Name\": \"CC BY-NC 4.0\", \"PwC License URL\": \"https://creativecommons.org/licenses/by-nc/4.0/\", \"S2 Citation Count (June 2024)\": \"\", \"S2 Date\": \"\"}, \"Bibtex\": \"@Article{Shao2020FineGymAH,\\n author = {Dian Shao and Yue Zhao and Bo Dai and Dahua Lin},\\n booktitle = {Computer Vision and Pattern Recognition},\\n journal = {2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},\\n pages = {2613-2622},\\n title = {FineGym: A Hierarchical Video Dataset for Fine-Grained Action Understanding},\\n year = {2020}\\n}\\n\", \"License Use (DataProvenance)\": \"non-commercial\", \"License Attribution (DataProvenance)\": 1, \"License Share Alike (DataProvenance)\": 0, \"License Use (DataProvenance IgnoreOpenAI)\": \"non-commercial\", \"License Attribution (DataProvenance IgnoreOpenAI)\": 1, \"License Share Alike (DataProvenance IgnoreOpenAI)\": 0, \"License Type\": \"Non-Commercial/Academic\"}, {\"Unique Dataset Identifier\": \"imagenet-vid\", \"Collection\": \"imagenet-vid\", \"Collection URL\": \"https://link.springer.com/article/10.1007/s11263-015-0816-y?sa_campaign=email/event/articleAuthor/onlineFirst#\", \"Dataset Name\": \"ImageNet VID\", \"Paper Title\": \"ImageNet VID\", \"Paper URL\": \"https://link.springer.com/article/10.1007/s11263-015-0816-y?sa_campaign=email/event/articleAuthor/onlineFirst#\", \"GitHub URL\": \"\", \"Hugging Face URL\": \"\", \"Papers with Code URL\": \"https://paperswithcode.com/sota/video-object-detection-on-imagenet-vid\", \"ArXiv URL\": \"https://link.springer.com/article/10.1007/s11263-015-0816-y?sa_campaign=email/event/articleAuthor/onlineFirst#\", \"Semantic Scholar Corpus ID\": 2930547, \"Year Released\": \"2017\", \"Text Sources\": [\"undisclosed web\"], \"Licenses\": [{\"License\": \"CC BY-NC 4.0\", \"License URL\": \"https://www.image-net.org/challenges/LSVRC/2017/index.php\"}], \"Creators\": [\"Stanford University\"], \"Countries\": [\"United States of America\"], \"License Verified By\": \"Vivek Sharma\", \"Video Hours\": 9.26, \"Taken Down\": \"False\", \"Video Sources\": \"undisclosed web\", \"Task Categories\": [\"Video Classification\"], \"Inferred Metadata\": {\"GitHub License\": \"\", \"GitHub Stars (June 2024)\": \"\", \"GitHub Topics\": \"\", \"Github Date\": \"\", \"HF Config\": \"\", \"HF Config License\": \"\", \"HF Dataset\": \"\", \"HF Date\": \"\", \"HF Downloads (June 2024)\": \"\", \"HF Likes (June 2024)\": \"\", \"HF Yaml License\": \"\", \"PwC Date\": \"\", \"PwC Description\": \"The current state-of-the-art on ImageNet VID is DiffusionVID (Swin-B). See a full comparison of 31 papers with code.\", \"PwC License Name\": \"\", \"PwC License URL\": \"\", \"S2 Citation Count (June 2024)\": \"\", \"S2 Date\": \"\"}, \"Bibtex\": \"@Article{Russakovsky2014ImageNetLS,\\n author = {Olga Russakovsky and Jia Deng and Hao Su and J. Krause and S. Satheesh and Sean Ma and Zhiheng Huang and A. Karpathy and A. Khosla and Michael S. Bernstein and A. Berg and Li Fei-Fei},\\n booktitle = {International Journal of Computer Vision},\\n journal = {International Journal of Computer Vision},\\n pages = {211 - 252},\\n title = {ImageNet Large Scale Visual Recognition Challenge},\\n volume = {115},\\n year = {2014}\\n}\\n\", \"License Use (DataProvenance)\": \"non-commercial\", \"License Attribution (DataProvenance)\": 1, \"License Share Alike (DataProvenance)\": 0, \"License Use (DataProvenance IgnoreOpenAI)\": \"non-commercial\", \"License Attribution (DataProvenance IgnoreOpenAI)\": 1, \"License Share Alike (DataProvenance IgnoreOpenAI)\": 0, \"License Type\": \"Non-Commercial/Academic\"}, {\"Unique Dataset Identifier\": \"mad\", \"Collection\": \"mad\", \"Collection URL\": \"https://arxiv.org/abs/2112.00431\", \"Dataset Name\": \"MAD: A Scalable Dataset for Language Grounding in Videos from Movie Audio Descriptions\", \"Paper Title\": \"MAD: A Scalable Dataset for Language Grounding in Videos from Movie Audio Descriptions\", \"Paper URL\": \"https://arxiv.org/abs/2112.00431\", \"GitHub URL\": \"https://github.com/Soldelli/MAD\", \"Hugging Face URL\": \"\", \"Papers with Code URL\": \"https://paperswithcode.com/dataset/mad\", \"ArXiv URL\": \"https://arxiv.org/abs/2112.00431\", \"Semantic Scholar Corpus ID\": 244773187, \"Year Released\": \"2022\", \"Text Sources\": [\"undisclosed web\"], \"Licenses\": [{\"License\": \"Custom\", \"License URL\": \"https://docs.google.com/forms/d/e/1FAIpQLSdtUV3uweS0u7AHAMIJAL_dRRdZ5MHpJS3fdZVbhnVt-Yb4NA/viewform\"}], \"Creators\": [\"King Abdullah University of Science and Technology (KAUST)\"], \"Countries\": [\"Saudi Arabia\"], \"License Verified By\": \"Vivek Sharma\", \"Video Hours\": 1207.3, \"Taken Down\": \"False\", \"Video Sources\": \"undisclosed web\", \"Task Categories\": [\"Video Captioning\"], \"Inferred Metadata\": {\"GitHub License\": \"MIT License\", \"GitHub Stars (June 2024)\": 138, \"GitHub Topics\": [], \"Github Date\": \"\", \"HF Config\": \"\", \"HF Config License\": \"\", \"HF Dataset\": \"\", \"HF Date\": \"\", \"HF Downloads (June 2024)\": \"\", \"HF Likes (June 2024)\": \"\", \"HF Yaml License\": \"\", \"PwC Date\": \"2021-12-01\", \"PwC Description\": \"MAD (Movie Audio Descriptions) is an automatically curated large-scale dataset for the task of natural language grounding in videos or natural language moment retrieval.\\nMAD exploits available audio descriptions of mainstream movies. Such audio descriptions are redacted for visually impaired audiences and are therefore highly descriptive of the visual content being displayed. \\nMAD contains over 384,000 natural language sentences grounded in over 1,200 hours of video, and provides a unique setup for video grounding as the visual stream is truly untrimmed with an average video duration of 110 minutes. 2 orders of magnitude longer than legacy datasets. \\n\\nTake a look at the paper for additional information.\\n\\nFrom the authors on availability: \\\"Due to copyright constraints, MAD\\u2019s videos will not be publicly released. However, we will provide all necessary features for our experiments\\u2019 reproducibility and promote future research in this direction\\\"\", \"PwC License Name\": \"Unspecified\", \"PwC License URL\": null, \"S2 Citation Count (June 2024)\": \"\", \"S2 Date\": \"\"}, \"Bibtex\": \"@Article{Soldan2021MADAS,\\n author = {Mattia Soldan and A. Pardo and Juan Le'on Alc'azar and Fabian Caba Heilbron and Chen Zhao and Silvio Giancola and Bernard Ghanem},\\n booktitle = {Computer Vision and Pattern Recognition},\\n journal = {2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},\\n pages = {5016-5025},\\n title = {MAD: A Scalable Dataset for Language Grounding in Videos from Movie Audio Descriptions},\\n year = {2021}\\n}\\n\", \"License Use (DataProvenance)\": \"academic-only\", \"License Attribution (DataProvenance)\": 1, \"License Share Alike (DataProvenance)\": 0, \"License Use (DataProvenance IgnoreOpenAI)\": \"academic-only\", \"License Attribution (DataProvenance IgnoreOpenAI)\": 1, \"License Share Alike (DataProvenance IgnoreOpenAI)\": 0, \"License Type\": \"Non-Commercial/Academic\"}, {\"Unique Dataset Identifier\": \"ferv39k-dataset\", \"Collection\": \"ferv39k-dataset\", \"Collection URL\": \"https://arxiv.org/abs/2203.09463\", \"Dataset Name\": \"FERV39k: A Large-Scale Multi-Scene Dataset for Facial Expression Recognition in Videos (CVPR 2022)\", \"Paper Title\": \"FERV39k: A Large-Scale Multi-Scene Dataset for Facial Expression Recognition in Videos (CVPR 2022)\", \"Paper URL\": \"https://arxiv.org/abs/2203.09463\", \"GitHub URL\": \"https://github.com/wangyanckxx/FERV39k\", \"Hugging Face URL\": \"\", \"Papers with Code URL\": \"https://paperswithcode.com/paper/ferv39k-a-large-scale-multi-scene-dataset-for\", \"ArXiv URL\": \"https://arxiv.org/abs/2203.09463\", \"Semantic Scholar Corpus ID\": 247518747, \"Year Released\": \"2022\", \"Text Sources\": [\"undisclosed web\"], \"Licenses\": [{\"License\": \"CC BY-NC 4.0\", \"License URL\": \"https://wangyanckxx.github.io/Proj_CVPR2022_FERV39k.html\"}], \"Creators\": [\"Fudan University\"], \"Countries\": [\"China\"], \"License Verified By\": \"Vivek Sharma\", \"Video Hours\": 16.47, \"Taken Down\": \"False\", \"Video Sources\": \"undisclosed web\", \"Task Categories\": [\"Video Classification\"], \"Inferred Metadata\": {\"GitHub License\": \"\", \"GitHub Stars (June 2024)\": 52, \"GitHub Topics\": [], \"Github Date\": \"\", \"HF Config\": \"\", \"HF Config License\": \"\", \"HF Dataset\": \"\", \"HF Date\": \"\", \"HF Downloads (June 2024)\": \"\", \"HF Likes (June 2024)\": \"\", \"HF Yaml License\": \"\", \"PwC Date\": \"\", \"PwC Description\": \"No code available yet.\", \"PwC License Name\": \"\", \"PwC License URL\": \"\", \"S2 Citation Count (June 2024)\": \"\", \"S2 Date\": \"\"}, \"Bibtex\": \"@Article{Wang2022FERV39kAL,\\n author = {Yan Wang and Yixuan Sun and Yiwen Huang and Zhongying Liu and Shuyong Gao and Wei Zhang and Weifeng Ge and Wenqiang Zhang},\\n booktitle = {Computer Vision and Pattern Recognition},\\n journal = {2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},\\n pages = {20890-20899},\\n title = {FERV39k: A Large-Scale Multi-Scene Dataset for Facial Expression Recognition in Videos},\\n year = {2022}\\n}\\n\", \"License Use (DataProvenance)\": \"non-commercial\", \"License Attribution (DataProvenance)\": 1, \"License Share Alike (DataProvenance)\": 0, \"License Use (DataProvenance IgnoreOpenAI)\": \"non-commercial\", \"License Attribution (DataProvenance IgnoreOpenAI)\": 1, \"License Share Alike (DataProvenance IgnoreOpenAI)\": 0, \"License Type\": \"Non-Commercial/Academic\"}, {\"Unique Dataset Identifier\": \"davis\", \"Collection\": \"davis\", \"Collection URL\": \"https://ieeexplore.ieee.org/document/7780454\", \"Dataset Name\": \"Davis\", \"Paper Title\": \"Davis\", \"Paper URL\": \"https://ieeexplore.ieee.org/document/7780454\", \"GitHub URL\": \"https://github.com/fperazzi/davis\", \"Hugging Face URL\": \"\", \"Papers with Code URL\": \"https://paperswithcode.com/paper/a-benchmark-dataset-and-evaluation\", \"ArXiv URL\": \"https://ieeexplore.ieee.org/document/7780454\", \"Semantic Scholar Corpus ID\": 3619941, \"Year Released\": \"2017\", \"Text Sources\": [\"undisclosed web\"], \"Licenses\": [{\"License\": \"Custom\", \"License URL\": \"https://github.com/fperazzi/davis/blob/main/LICENSE\"}], \"Creators\": [\"ETH Z\\u00fcrich\", \"Disney Research\"], \"Countries\": [\"Switzerland\"], \"License Verified By\": \"Vivek Sharma\", \"Video Hours\": 0.04, \"Taken Down\": \"False\", \"Video Sources\": \"undisclosed web\", \"Task Categories\": [\"Video Segmentation\", \"Video Classification\"], \"Inferred Metadata\": {\"GitHub License\": \"\", \"GitHub Stars (June 2024)\": 6, \"GitHub Topics\": [], \"Github Date\": \"\", \"HF Config\": \"\", \"HF Config License\": \"\", \"HF Dataset\": \"\", \"HF Date\": \"\", \"HF Downloads (June 2024)\": \"\", \"HF Likes (June 2024)\": \"\", \"HF Yaml License\": \"\", \"PwC Date\": \"\", \"PwC Description\": \"Implemented in one code library.\", \"PwC License Name\": \"\", \"PwC License URL\": \"\", \"S2 Citation Count (June 2024)\": \"\", \"S2 Date\": \"\"}, \"Bibtex\": \"@Article{Pont-Tuset2017The2D,\\n author = {J. Pont-Tuset and Federico Perazzi and Sergi Caelles and Pablo Arbel\\u00e1ez and A. Sorkine-Hornung and L. Gool},\\n booktitle = {arXiv.org},\\n journal = {ArXiv},\\n title = {The 2017 DAVIS Challenge on Video Object Segmentation},\\n volume = {abs/1704.00675},\\n year = {2017}\\n}\\n\", \"License Use (DataProvenance)\": \"commercial\", \"License Attribution (DataProvenance)\": 1, \"License Share Alike (DataProvenance)\": 0, \"License Use (DataProvenance IgnoreOpenAI)\": \"commercial\", \"License Attribution (DataProvenance IgnoreOpenAI)\": 1, \"License Share Alike (DataProvenance IgnoreOpenAI)\": 0, \"License Type\": \"Commercial\"}, {\"Unique Dataset Identifier\": \"eev-dataset\", \"Collection\": \"eev-dataset\", \"Collection URL\": \"https://arxiv.org/abs/2001.05488\", \"Dataset Name\": \"EEV: A Large-Scale Dataset for Studying Evoked Expressions from Video\", \"Paper Title\": \"EEV: A Large-Scale Dataset for Studying Evoked Expressions from Video\", \"Paper URL\": \"https://arxiv.org/abs/2001.05488\", \"GitHub URL\": \"https://github.com/google-research-datasets/eev\", \"Hugging Face URL\": \"\", \"Papers with Code URL\": \"\", \"ArXiv URL\": \"https://arxiv.org/abs/2001.05488\", \"Semantic Scholar Corpus ID\": 210701992, \"Year Released\": \"2020\", \"Text Sources\": [\"undisclosed web\"], \"Licenses\": [{\"License\": \"CC BY 4.0\", \"License URL\": \"https://github.com/google-research-datasets/eev?tab=readme-ov-file#license\"}], \"Creators\": [\"Google Research\", \"California Institute of Technology\"], \"Countries\": [\"United States of America\"], \"License Verified By\": \"Vivek Sharma\", \"Video Hours\": 370.0, \"Taken Down\": \"False\", \"Video Sources\": \"undisclosed web\", \"Task Categories\": [\"Video Classification\"], \"Inferred Metadata\": {\"GitHub License\": \"\", \"GitHub Stars (June 2024)\": 34, \"GitHub Topics\": [], \"Github Date\": \"\", \"HF Config\": \"\", \"HF Config License\": \"\", \"HF Dataset\": \"\", \"HF Date\": \"\", \"HF Downloads (June 2024)\": \"\", \"HF Likes (June 2024)\": \"\", \"HF Yaml License\": \"\", \"PwC Date\": \"\", \"PwC Description\": \"\", \"PwC License Name\": \"\", \"PwC License URL\": \"\", \"S2 Citation Count (June 2024)\": \"\", \"S2 Date\": \"\"}, \"Bibtex\": \"@Article{Sun2020EEVDP,\\n author = {Jennifer J. Sun and Ting Liu and Alan S. Cowen and Florian Schroff and Hartwig Adam and Gautam Prasad},\\n booktitle = {arXiv.org},\\n journal = {ArXiv},\\n title = {EEV Dataset: Predicting Expressions Evoked by Diverse Videos},\\n volume = {abs/2001.05488},\\n year = {2020}\\n}\\n\", \"License Use (DataProvenance)\": \"commercial\", \"License Attribution (DataProvenance)\": 1, \"License Share Alike (DataProvenance)\": 0, \"License Use (DataProvenance IgnoreOpenAI)\": \"commercial\", \"License Attribution (DataProvenance IgnoreOpenAI)\": 1, \"License Share Alike (DataProvenance IgnoreOpenAI)\": 0, \"License Type\": \"Commercial\"}, {\"Unique Dataset Identifier\": \"trecvid\", \"Collection\": \"trecvid\", \"Collection URL\": \"https://arxiv.org/abs/2009.09984\", \"Dataset Name\": \"TRECVID\", \"Paper Title\": \"TRECVID\", \"Paper URL\": \"https://arxiv.org/abs/2009.09984\", \"GitHub URL\": \"\", \"Hugging Face URL\": \"\", \"Papers with Code URL\": \"https://paperswithcode.com/paper/trecvid-2019-an-evaluation-campaign-to/review/\", \"ArXiv URL\": \"https://arxiv.org/abs/2009.09984\", \"Semantic Scholar Corpus ID\": 212694843, \"Year Released\": \"2019\", \"Text Sources\": [\"undisclosed web\", \"bbc\"], \"Licenses\": [{\"License\": \"CC BY-NC-SA 4.0\", \"License URL\": \"https://trecvid.nist.gov/\"}], \"Creators\": [\"National Institute of Standards and Technology\"], \"Countries\": [\"United States of America\"], \"License Verified By\": \"Vivek Sharma\", \"Video Hours\": 1000.0, \"Taken Down\": \"False\", \"Video Sources\": \"undisclosed web\", \"Task Categories\": [\"Video Captioning\", \"Video Classification\"], \"Inferred Metadata\": {\"GitHub License\": \"\", \"GitHub Stars (June 2024)\": \"\", \"GitHub Topics\": \"\", \"Github Date\": \"\", \"HF Config\": \"\", \"HF Config License\": \"\", \"HF Dataset\": \"\", \"HF Date\": \"\", \"HF Downloads (June 2024)\": \"\", \"HF Likes (June 2024)\": \"\", \"HF Yaml License\": \"\", \"PwC Date\": \"\", \"PwC Description\": \"Paper tables with annotated results for TRECVID 2019: An Evaluation Campaign to Benchmark Video Activity Detection, Video Captioning and Matching, and Video Search & Retrieval\", \"PwC License Name\": \"\", \"PwC License URL\": \"\", \"S2 Citation Count (June 2024)\": \"\", \"S2 Date\": \"\"}, \"Bibtex\": \"@Article{Awad2019TRECVID2A,\\n author = {G. Awad and A. Butt and Keith Curtis and Yooyoung Lee and Jonathan G. Fiscus and A. Godil and Andrew Delgado and Jesse Zhang and Eliot Godard and Lukas L. Diduch and A. Smeaton and Yyette Graham and Wessel Kraaij and G. Qu\\u00e9not},\\n booktitle = {TREC Video Retrieval Evaluation},\\n journal = {ArXiv},\\n title = {TRECVID 2019: An evaluation campaign to benchmark Video Activity Detection, Video Captioning and Matching, and Video Search & retrieval},\\n volume = {abs/2009.09984},\\n year = {2019}\\n}\\n\", \"License Use (DataProvenance)\": \"non-commercial\", \"License Attribution (DataProvenance)\": 1, \"License Share Alike (DataProvenance)\": 1, \"License Use (DataProvenance IgnoreOpenAI)\": \"non-commercial\", \"License Attribution (DataProvenance IgnoreOpenAI)\": 1, \"License Share Alike (DataProvenance IgnoreOpenAI)\": 1, \"License Type\": \"Non-Commercial/Academic\"}, {\"Unique Dataset Identifier\": \"hmdb-dataset\", \"Collection\": \"hmdb-dataset\", \"Collection URL\": \"https://ieeexplore.ieee.org/document/6126543\", \"Dataset Name\": \"HMDB: A Large Video Database for Human Motion Recognition (ICCV 2011)\", \"Paper Title\": \"HMDB: A Large Video Database for Human Motion Recognition (ICCV 2011)\", \"Paper URL\": \"https://ieeexplore.ieee.org/document/6126543\", \"GitHub URL\": \"\", \"Hugging Face URL\": \"\", \"Papers with Code URL\": \"https://paperswithcode.com/dataset/hmdb51\", \"ArXiv URL\": \"https://ieeexplore.ieee.org/document/6126543\", \"Semantic Scholar Corpus ID\": 206769852, \"Year Released\": \"2011\", \"Text Sources\": [\"movies\", \"prelinger archive\", \"undisclosed web\", \"youtube\", \"google videos\"], \"Licenses\": [{\"License\": \"CC BY 4.0\", \"License URL\": \"https://serre-lab.clps.brown.edu/resource/hmdb-a-large-human-motion-database/\"}], \"Creators\": [\"Karlsruhe Institute of Technology\", \"Massachusetts Institute of Technology\", \"Brown University\"], \"Countries\": [\"United States of America\", \"Germany\"], \"License Verified By\": \"Vivek Sharma\", \"Video Hours\": 7000.0, \"Taken Down\": \"False\", \"Video Sources\": \"undisclosed web\", \"Task Categories\": [\"Video Classification\"], \"Inferred Metadata\": {\"GitHub License\": \"\", \"GitHub Stars (June 2024)\": \"\", \"GitHub Topics\": \"\", \"Github Date\": \"\", \"HF Config\": \"\", \"HF Config License\": \"\", \"HF Dataset\": \"\", \"HF Date\": \"\", \"HF Downloads (June 2024)\": \"\", \"HF Likes (June 2024)\": \"\", \"HF Yaml License\": \"\", \"PwC Date\": \"2011-01-01\", \"PwC Description\": \"The HMDB51 dataset is a large collection of realistic videos from various sources, including movies and web videos. The dataset is composed of 6,766 video clips from 51 action categories (such as \\u201cjump\\u201d, \\u201ckiss\\u201d and \\u201claugh\\u201d), with each category containing at least 101 clips. The original evaluation scheme uses three different training/testing splits. In each split, each action class has 70 clips for training and 30 clips for testing. The average accuracy over these three splits is used to measure the final performance.\", \"PwC License Name\": \"CC BY 4.0\", \"PwC License URL\": \"https://creativecommons.org/licenses/by/4.0/\", \"S2 Citation Count (June 2024)\": \"\", \"S2 Date\": \"\"}, \"Bibtex\": \"@Article{Kuehne2011HMDBAL,\\n author = {Hilde Kuehne and Hueihan Jhuang and Est\\u00edbaliz Garrote and T. Poggio and Thomas Serre},\\n booktitle = {Vision},\\n journal = {2011 International Conference on Computer Vision},\\n pages = {2556-2563},\\n title = {HMDB: A large video database for human motion recognition},\\n year = {2011}\\n}\\n\", \"License Use (DataProvenance)\": \"commercial\", \"License Attribution (DataProvenance)\": 1, \"License Share Alike (DataProvenance)\": 0, \"License Use (DataProvenance IgnoreOpenAI)\": \"commercial\", \"License Attribution (DataProvenance IgnoreOpenAI)\": 1, \"License Share Alike (DataProvenance IgnoreOpenAI)\": 0, \"License Type\": \"Commercial\"}, {\"Unique Dataset Identifier\": \"activitynet\", \"Collection\": \"activitynet\", \"Collection URL\": \"https://ieeexplore.ieee.org/document/7298698\", \"Dataset Name\": \"ActivityNet\", \"Paper Title\": \"ActivityNet\", \"Paper URL\": \"https://ieeexplore.ieee.org/document/7298698\", \"GitHub URL\": \"\", \"Hugging Face URL\": \"https://huggingface.co/datasets/Leyo/ActivityNet_Captions\", \"Papers with Code URL\": \"https://paperswithcode.com/dataset/activitynet\", \"ArXiv URL\": \"https://ieeexplore.ieee.org/document/7298698\", \"Semantic Scholar Corpus ID\": 1710722, \"Year Released\": \"2015\", \"Text Sources\": [\"undisclosed web\"], \"Licenses\": [{\"License\": \"MIT License\", \"License URL\": \"https://github.com/facebookresearch/ActivityNet-Entities/blob/main/LICENSE\"}], \"Creators\": [\"Universidad del Norte\", \"King Abdullah University of Science and Technology (KAUST)\"], \"Countries\": [\"Saudi Arabia\", \"Colombia\"], \"License Verified By\": \"Vivek Sharma\", \"Video Hours\": 849.0, \"Taken Down\": \"False\", \"Video Sources\": \"undisclosed web\", \"Task Categories\": [\"Video Captioning\"], \"Inferred Metadata\": {\"GitHub License\": \"\", \"GitHub Stars (June 2024)\": \"\", \"GitHub Topics\": \"\", \"Github Date\": \"\", \"HF Config\": \"all\", \"HF Config License\": \"\", \"HF Dataset\": \"Leyo/ActivityNet_Captions\", \"HF Date\": \"2022-05-13\", \"HF Downloads (June 2024)\": 21, \"HF Likes (June 2024)\": 0, \"HF Yaml License\": \"Unspecified\", \"PwC Date\": \"2015-01-01\", \"PwC Description\": \"The ActivityNet dataset contains 200 different types of activities and a total of 849 hours of videos collected from YouTube. ActivityNet is the largest benchmark for temporal activity detection to date in terms of both the number of activity categories and number of videos, making the task particularly challenging. Version 1.3 of the dataset contains 19994 untrimmed videos in total and is divided into three disjoint subsets, training, validation, and testing by a ratio of 2:1:1. On average, each activity category has 137 untrimmed videos. Each video on average has 1.41 activities which are annotated with temporal boundaries. The ground-truth annotations of test videos are not public.\", \"PwC License Name\": \"Unspecified\", \"PwC License URL\": null, \"S2 Citation Count (June 2024)\": \"\", \"S2 Date\": \"\"}, \"Bibtex\": \"@Article{Heilbron2015ActivityNetAL,\\n author = {Fabian Caba Heilbron and Victor Escorcia and Bernard Ghanem and Juan Carlos Niebles},\\n booktitle = {Computer Vision and Pattern Recognition},\\n journal = {2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},\\n pages = {961-970},\\n title = {ActivityNet: A large-scale video benchmark for human activity understanding},\\n year = {2015}\\n}\\n\", \"License Use (DataProvenance)\": \"commercial\", \"License Attribution (DataProvenance)\": 1, \"License Share Alike (DataProvenance)\": 0, \"License Use (DataProvenance IgnoreOpenAI)\": \"commercial\", \"License Attribution (DataProvenance IgnoreOpenAI)\": 1, \"License Share Alike (DataProvenance IgnoreOpenAI)\": 0, \"License Type\": \"Commercial\"}, {\"Unique Dataset Identifier\": \"vtw\", \"Collection\": \"vtw\", \"Collection URL\": \"https://arxiv.org/abs/1608.07068\", \"Dataset Name\": \"VTW\", \"Paper Title\": \"VTW\", \"Paper URL\": \"https://arxiv.org/abs/1608.07068\", \"GitHub URL\": \"\", \"Hugging Face URL\": \"\", \"Papers with Code URL\": \"https://paperswithcode.com/paper/title-generation-for-user-generated-videos\", \"ArXiv URL\": \"https://arxiv.org/abs/1608.07068\", \"Semantic Scholar Corpus ID\": 6155397, \"Year Released\": \"2016\", \"Text Sources\": [\"undisclosed web\"], \"Licenses\": [{\"License\": \"Unspecified\", \"License URL\": \"\"}], \"Creators\": [\"Tsinghua University\", \"Stanford University\"], \"Countries\": [\"China\", \"United States of America\"], \"License Verified By\": \"Vivek Sharma\", \"Video Hours\": 213.2, \"Taken Down\": \"False\", \"Video Sources\": \"undisclosed web\", \"Task Categories\": [\"Video Captioning\"], \"Inferred Metadata\": {\"GitHub License\": \"\", \"GitHub Stars (June 2024)\": \"\", \"GitHub Topics\": \"\", \"Github Date\": \"\", \"HF Config\": \"\", \"HF Config License\": \"\", \"HF Dataset\": \"\", \"HF Date\": \"\", \"HF Downloads (June 2024)\": \"\", \"HF Likes (June 2024)\": \"\", \"HF Yaml License\": \"\", \"PwC Date\": \"\", \"PwC Description\": \"No code available yet.\", \"PwC License Name\": \"\", \"PwC License URL\": \"\", \"S2 Citation Count (June 2024)\": \"\", \"S2 Date\": \"\"}, \"Bibtex\": \"@Article{Zeng2016TitleGF,\\n author = {Kuo-Hao Zeng and Tseng-Hung Chen and Juan Carlos Niebles and Min Sun},\\n booktitle = {European Conference on Computer Vision},\\n pages = {609-625},\\n title = {Title Generation for User Generated Videos},\\n year = {2016}\\n}\\n\", \"License Use (DataProvenance)\": \"unspecified\", \"License Attribution (DataProvenance)\": 0, \"License Share Alike (DataProvenance)\": 0, \"License Use (DataProvenance IgnoreOpenAI)\": \"unspecified\", \"License Attribution (DataProvenance IgnoreOpenAI)\": 0, \"License Share Alike (DataProvenance IgnoreOpenAI)\": 0, \"License Type\": \"Unspecified\"}, {\"Unique Dataset Identifier\": \"msr-vtt\", \"Collection\": \"msr-vtt\", \"Collection URL\": \"https://ieeexplore.ieee.org/document/7780940\", \"Dataset Name\": \"MSR-VTT\", \"Paper Title\": \"MSR-VTT\", \"Paper URL\": \"https://ieeexplore.ieee.org/document/7780940\", \"GitHub URL\": \"\", \"Hugging Face URL\": \"\", \"Papers with Code URL\": \"https://paperswithcode.com/dataset/msr-vtt\", \"ArXiv URL\": \"https://ieeexplore.ieee.org/document/7780940\", \"Semantic Scholar Corpus ID\": 206594535, \"Year Released\": \"2016\", \"Text Sources\": [\"undisclosed web\"], \"Licenses\": [{\"License\": \"Unspecified\", \"License URL\": \"\"}], \"Creators\": [\"Microsoft Research\"], \"Countries\": [\"United States of America\"], \"License Verified By\": \"Vivek Sharma\", \"Video Hours\": 41.2, \"Taken Down\": \"False\", \"Video Sources\": \"undisclosed web\", \"Task Categories\": [\"Video Captioning\"], \"Inferred Metadata\": {\"GitHub License\": \"\", \"GitHub Stars (June 2024)\": \"\", \"GitHub Topics\": \"\", \"Github Date\": \"\", \"HF Config\": \"\", \"HF Config License\": \"\", \"HF Dataset\": \"\", \"HF Date\": \"\", \"HF Downloads (June 2024)\": \"\", \"HF Likes (June 2024)\": \"\", \"HF Yaml License\": \"\", \"PwC Date\": \"2016-01-01\", \"PwC Description\": \"MSR-VTT (Microsoft Research Video to Text) is a large-scale dataset for the open domain video captioning, which consists of 10,000 video clips from 20 categories, and each video clip is annotated with 20 English sentences by Amazon Mechanical Turks. There are about 29,000 unique words in all captions. The standard splits uses 6,513 clips for training, 497 clips for validation, and 2,990 clips for testing.\", \"PwC License Name\": \"Unspecified\", \"PwC License URL\": null, \"S2 Citation Count (June 2024)\": \"\", \"S2 Date\": \"\"}, \"Bibtex\": \"@Article{Xu2016MSRVTTAL,\\n author = {Jun Xu and Tao Mei and Ting Yao and Y. Rui},\\n booktitle = {Computer Vision and Pattern Recognition},\\n journal = {2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},\\n pages = {5288-5296},\\n title = {MSR-VTT: A Large Video Description Dataset for Bridging Video and Language},\\n year = {2016}\\n}\\n\", \"License Use (DataProvenance)\": \"unspecified\", \"License Attribution (DataProvenance)\": 0, \"License Share Alike (DataProvenance)\": 0, \"License Use (DataProvenance IgnoreOpenAI)\": \"unspecified\", \"License Attribution (DataProvenance IgnoreOpenAI)\": 0, \"License Share Alike (DataProvenance IgnoreOpenAI)\": 0, \"License Type\": \"Unspecified\"}, {\"Unique Dataset Identifier\": \"stroygraphs\", \"Collection\": \"stroygraphs\", \"Collection URL\": \"https://openaccess.thecvf.com/content_cvpr_2014/papers/Tapaswi_StoryGraphs_Visualizing_Character_2014_CVPR_paper.pdf\", \"Dataset Name\": \"StoryGraphs\", \"Paper Title\": \"StoryGraphs\", \"Paper URL\": \"https://openaccess.thecvf.com/content_cvpr_2014/papers/Tapaswi_StoryGraphs_Visualizing_Character_2014_CVPR_paper.pdf\", \"GitHub URL\": \"https://github.com/makarandtapaswi/StoryGraphs_CVPR2014\", \"Hugging Face URL\": \"\", \"Papers with Code URL\": \"https://paperswithcode.com/paper/storygraphs-visualizing-character\", \"ArXiv URL\": \"https://openaccess.thecvf.com/content_cvpr_2014/papers/Tapaswi_StoryGraphs_Visualizing_Character_2014_CVPR_paper.pdf\", \"Semantic Scholar Corpus ID\": 1055956, \"Year Released\": \"2014\", \"Text Sources\": [\"movies\"], \"Licenses\": [{\"License\": \"Unspecified\", \"License URL\": \"\"}], \"Creators\": [\"Karlsruhe Institute of Technology\"], \"Countries\": [\"Germany\"], \"License Verified By\": \"Vivek Sharma\", \"Video Hours\": 7.3, \"Taken Down\": \"False\", \"Video Sources\": \"movies\", \"Task Categories\": [\"Misc\"], \"Inferred Metadata\": {\"GitHub License\": \"\", \"GitHub Stars (June 2024)\": 20, \"GitHub Topics\": [], \"Github Date\": \"\", \"HF Config\": \"\", \"HF Config License\": \"\", \"HF Dataset\": \"\", \"HF Date\": \"\", \"HF Downloads (June 2024)\": \"\", \"HF Likes (June 2024)\": \"\", \"HF Yaml License\": \"\", \"PwC Date\": \"\", \"PwC Description\": \"Implemented in one code library.\", \"PwC License Name\": \"\", \"PwC License URL\": \"\", \"S2 Citation Count (June 2024)\": \"\", \"S2 Date\": \"\"}, \"Bibtex\": \"@Article{Tapaswi2014StoryGraphsVC,\\n author = {Makarand Tapaswi and M. B\\u00e4uml and R. Stiefelhagen},\\n booktitle = {2014 IEEE Conference on Computer Vision and Pattern Recognition},\\n journal = {2014 IEEE Conference on Computer Vision and Pattern Recognition},\\n pages = {827-834},\\n title = {StoryGraphs: Visualizing Character Interactions as a Timeline},\\n year = {2014}\\n}\\n\", \"License Use (DataProvenance)\": \"unspecified\", \"License Attribution (DataProvenance)\": 0, \"License Share Alike (DataProvenance)\": 0, \"License Use (DataProvenance IgnoreOpenAI)\": \"unspecified\", \"License Attribution (DataProvenance IgnoreOpenAI)\": 0, \"License Share Alike (DataProvenance IgnoreOpenAI)\": 0, \"License Type\": \"Unspecified\"}, {\"Unique Dataset Identifier\": \"movieqa\", \"Collection\": \"movieqa\", \"Collection URL\": \"https://arxiv.org/abs/1512.02902\", \"Dataset Name\": \"MovieQA\", \"Paper Title\": \"MovieQA\", \"Paper URL\": \"https://arxiv.org/abs/1512.02902\", \"GitHub URL\": \"https://github.com/makarandtapaswi/MovieQA_CVPR2016/\", \"Hugging Face URL\": \"\", \"Papers with Code URL\": \"https://paperswithcode.com/dataset/movieqa\", \"ArXiv URL\": \"https://arxiv.org/abs/1512.02902\", \"Semantic Scholar Corpus ID\": 1017389, \"Year Released\": \"2015\", \"Text Sources\": [\"movies\"], \"Licenses\": [{\"License\": \"Unspecified\", \"License URL\": \"\"}], \"Creators\": [\"Karlsruhe Institute of Technology\", \"Massachusetts Institute of Technology\", \"University of Toronto\"], \"Countries\": [\"Germany\", \"Canada\", \"United States of America\"], \"License Verified By\": \"Vivek Sharma\", \"Video Hours\": 381.0, \"Taken Down\": \"True\", \"Video Sources\": \"movies\", \"Task Categories\": [\"Video Question Answering\"], \"Inferred Metadata\": {\"GitHub License\": \"\", \"GitHub Stars (June 2024)\": 80, \"GitHub Topics\": [], \"Github Date\": \"\", \"HF Config\": \"\", \"HF Config License\": \"\", \"HF Dataset\": \"\", \"HF Date\": \"\", \"HF Downloads (June 2024)\": \"\", \"HF Likes (June 2024)\": \"\", \"HF Yaml License\": \"\", \"PwC Date\": \"2016-01-01\", \"PwC Description\": \"The MovieQA dataset is a dataset for movie question answering. to evaluate automatic story comprehension from both video and text. The data set consists of almost 15,000 multiple choice question answers obtained from over 400 movies and features high semantic diversity. Each question comes with a set of five highly plausible answers; only one of which is correct. The questions can be answered using multiple sources of information: movie clips, plots, subtitles, and for a subset scripts and DVS.\", \"PwC License Name\": \"Unspecified\", \"PwC License URL\": null, \"S2 Citation Count (June 2024)\": \"\", \"S2 Date\": \"\"}, \"Bibtex\": \"@Article{Tapaswi2015MovieQAUS,\\n author = {Makarand Tapaswi and Yukun Zhu and R. Stiefelhagen and A. Torralba and R. Urtasun and S. Fidler},\\n booktitle = {Computer Vision and Pattern Recognition},\\n journal = {2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},\\n pages = {4631-4640},\\n title = {MovieQA: Understanding Stories in Movies through Question-Answering},\\n year = {2015}\\n}\\n\", \"License Use (DataProvenance)\": \"unspecified\", \"License Attribution (DataProvenance)\": 0, \"License Share Alike (DataProvenance)\": 0, \"License Use (DataProvenance IgnoreOpenAI)\": \"unspecified\", \"License Attribution (DataProvenance IgnoreOpenAI)\": 0, \"License Share Alike (DataProvenance IgnoreOpenAI)\": 0, \"License Type\": \"Unspecified\"}, {\"Unique Dataset Identifier\": \"movie-net\", \"Collection\": \"movie-net\", \"Collection URL\": \"https://arxiv.org/abs/2007.10937\", \"Dataset Name\": \"MovieNet\", \"Paper Title\": \"MovieNet\", \"Paper URL\": \"https://arxiv.org/abs/2007.10937\", \"GitHub URL\": \"https://github.com/movienet/movienet-tools\", \"Hugging Face URL\": \"\", \"Papers with Code URL\": \"https://paperswithcode.com/dataset/movienet\", \"ArXiv URL\": \"https://arxiv.org/abs/2007.10937\", \"Semantic Scholar Corpus ID\": 220665753, \"Year Released\": \"2020\", \"Text Sources\": [\"movies\"], \"Licenses\": [{\"License\": \"Unspecified\", \"License URL\": \"\"}], \"Creators\": [\"The Chinese University of Hong Kong\"], \"Countries\": [\"China\"], \"License Verified By\": \"Vivek Sharma\", \"Video Hours\": 3000.0, \"Taken Down\": \"False\", \"Video Sources\": \"movies\", \"Task Categories\": [\"Video Summarization\", \"Misc\"], \"Inferred Metadata\": {\"GitHub License\": \"\", \"GitHub Stars (June 2024)\": 256, \"GitHub Topics\": [\"action-recognition\", \"computer-vision\", \"cross-modality\", \"deep-learning\", \"movie\", \"person-analysis\", \"shot-detection\", \"video-understanding\", \"vision-language\"], \"Github Date\": \"\", \"HF Config\": \"\", \"HF Config License\": \"\", \"HF Dataset\": \"\", \"HF Date\": \"\", \"HF Downloads (June 2024)\": \"\", \"HF Likes (June 2024)\": \"\", \"HF Yaml License\": \"\", \"PwC Date\": \"2020-07-21\", \"PwC Description\": \"MovieNet is a holistic dataset for movie understanding. MovieNet contains 1,100 movies with a large amount of multi-modal data, e.g. trailers, photos, plot descriptions, etc.. Besides, different aspects of manual annotations are provided in MovieNet, including 1.1M characters with bounding boxes and identities, 42K scene boundaries, 2.5K aligned description sentences, 65K tags of place and action, and 92 K tags of cinematic style.\", \"PwC License Name\": \"Unspecified\", \"PwC License URL\": null, \"S2 Citation Count (June 2024)\": \"\", \"S2 Date\": \"\"}, \"Bibtex\": \"@Article{Huang2020MovieNetAH,\\n author = {Qingqiu Huang and Yu Xiong and Anyi Rao and Jiaze Wang and Dahua Lin},\\n booktitle = {European Conference on Computer Vision},\\n pages = {709-727},\\n title = {MovieNet: A Holistic Dataset for Movie Understanding},\\n year = {2020}\\n}\\n\", \"License Use (DataProvenance)\": \"unspecified\", \"License Attribution (DataProvenance)\": 0, \"License Share Alike (DataProvenance)\": 0, \"License Use (DataProvenance IgnoreOpenAI)\": \"unspecified\", \"License Attribution (DataProvenance IgnoreOpenAI)\": 0, \"License Share Alike (DataProvenance IgnoreOpenAI)\": 0, \"License Type\": \"Unspecified\"}, {\"Unique Dataset Identifier\": \"msa\", \"Collection\": \"msa\", \"Collection URL\": \"https://arxiv.org/abs/1910.11009\", \"Dataset Name\": \"MSA\", \"Paper Title\": \"MSA\", \"Paper URL\": \"https://arxiv.org/abs/1910.11009\", \"GitHub URL\": \"https://github.com/ycxioooong/MovieSynopsisAssociation\", \"Hugging Face URL\": \"\", \"Papers with Code URL\": \"https://paperswithcode.com/paper/a-graph-based-framework-to-bridge-movies-and-1\", \"ArXiv URL\": \"https://arxiv.org/abs/1910.11009\", \"Semantic Scholar Corpus ID\": 204852218, \"Year Released\": \"2019\", \"Text Sources\": [\"movies\"], \"Licenses\": [{\"License\": \"Unspecified\", \"License URL\": \"\"}], \"Creators\": [\"The Chinese University of Hong Kong\", \"UC Berkeley\"], \"Countries\": [\"China\", \"United States of America\"], \"License Verified By\": \"Vivek Sharma\", \"Video Hours\": 516.0, \"Taken Down\": \"False\", \"Video Sources\": \"movies\", \"Task Categories\": [\"Video Summarization\"], \"Inferred Metadata\": {\"GitHub License\": \"\", \"GitHub Stars (June 2024)\": 52, \"GitHub Topics\": [], \"Github Date\": \"\", \"HF Config\": \"\", \"HF Config License\": \"\", \"HF Dataset\": \"\", \"HF Date\": \"\", \"HF Downloads (June 2024)\": \"\", \"HF Likes (June 2024)\": \"\", \"HF Yaml License\": \"\", \"PwC Date\": \"\", \"PwC Description\": \"No code available yet.\", \"PwC License Name\": \"\", \"PwC License URL\": \"\", \"S2 Citation Count (June 2024)\": \"\", \"S2 Date\": \"\"}, \"Bibtex\": \"@Article{Xiong2019AGF,\\n author = {Yu Xiong and Qingqiu Huang and Lingfeng Guo and Hang Zhou and Bolei Zhou and Dahua Lin},\\n booktitle = {IEEE International Conference on Computer Vision},\\n journal = {2019 IEEE/CVF International Conference on Computer Vision (ICCV)},\\n pages = {4591-4600},\\n title = {A Graph-Based Framework to Bridge Movies and Synopses},\\n year = {2019}\\n}\\n\", \"License Use (DataProvenance)\": \"unspecified\", \"License Attribution (DataProvenance)\": 0, \"License Share Alike (DataProvenance)\": 0, \"License Use (DataProvenance IgnoreOpenAI)\": \"unspecified\", \"License Attribution (DataProvenance IgnoreOpenAI)\": 0, \"License Share Alike (DataProvenance IgnoreOpenAI)\": 0, \"License Type\": \"Unspecified\"}, {\"Unique Dataset Identifier\": \"cinepile\", \"Collection\": \"cinepile\", \"Collection URL\": \"https://arxiv.org/pdf/2405.08813\", \"Dataset Name\": \"CinePile: A Long Video Question Answering Dataset and Benchmark\", \"Paper Title\": \"CinePile: A Long Video Question Answering Dataset and Benchmark\", \"Paper URL\": \"https://arxiv.org/pdf/2405.08813\", \"GitHub URL\": \"\", \"Hugging Face URL\": \"https://huggingface.co/datasets/tomg-group-umd/cinepile\", \"Papers with Code URL\": \"https://paperswithcode.com/dataset/cinepile\", \"ArXiv URL\": \"https://arxiv.org/pdf/2405.08813\", \"Semantic Scholar Corpus ID\": 269761335, \"Year Released\": \"2024\", \"Text Sources\": [\"youtube\", \"movies\"], \"Licenses\": [{\"License\": \"CC BY 4.0\", \"License URL\": \"https://creativecommons.org/licenses/by/4.0/\"}], \"Creators\": [\"University of Maryland\", \"Weizmann Institute of Science\"], \"Countries\": [\"United States of America\", \"Israel\"], \"License Verified By\": \"Vivek Sharma\", \"Video Hours\": 417.6, \"Taken Down\": \"False\", \"Video Sources\": \"movies\", \"Task Categories\": [\"Video Q&A\"], \"Inferred Metadata\": null, \"Bibtex\": null, \"License Use (DataProvenance)\": \"commercial\", \"License Attribution (DataProvenance)\": 1, \"License Share Alike (DataProvenance)\": 0, \"License Use (DataProvenance IgnoreOpenAI)\": \"commercial\", \"License Attribution (DataProvenance IgnoreOpenAI)\": 1, \"License Share Alike (DataProvenance IgnoreOpenAI)\": 0, \"License Type\": \"Commercial\"}, {\"Unique Dataset Identifier\": \"lsmdc\", \"Collection\": \"lsmdc\", \"Collection URL\": \"https://arxiv.org/pdf/1605.03705\", \"Dataset Name\": \"LSMDC\", \"Paper Title\": \"LSMDC\", \"Paper URL\": \"https://arxiv.org/pdf/1605.03705\", \"GitHub URL\": \"\", \"Hugging Face URL\": \"\", \"Papers with Code URL\": \"https://paperswithcode.com/paper/movie-description\", \"ArXiv URL\": \"https://arxiv.org/pdf/1605.03705\", \"Semantic Scholar Corpus ID\": 18217052, \"Year Released\": \"2017\", \"Text Sources\": [\"movies\"], \"Licenses\": [{\"License\": \"Custom\", \"License URL\": \"https://datasets.d2.mpi-inf.mpg.de/movieDescription/protected/lsmdc2016/README.txt\"}], \"Creators\": [\"UC Berkeley\", \"Disney Research\", \"Universite de Montreal\", \"Polytechnique Montr\\u00e9al\", \"Universit\\u00e9 de Sherbrooke\", \"Twitter\"], \"Countries\": [\"United States of America\", \"Canada\"], \"License Verified By\": \"Vivek Sharma\", \"Video Hours\": 158.0, \"Taken Down\": \"False\", \"Video Sources\": \"movies\", \"Task Categories\": [\"Video Summarization\", \"Misc\", \"Video Classification\"], \"Inferred Metadata\": {\"GitHub License\": \"\", \"GitHub Stars (June 2024)\": \"\", \"GitHub Topics\": \"\", \"Github Date\": \"\", \"HF Config\": \"\", \"HF Config License\": \"\", \"HF Dataset\": \"\", \"HF Date\": \"\", \"HF Downloads (June 2024)\": \"\", \"HF Likes (June 2024)\": \"\", \"HF Yaml License\": \"\", \"PwC Date\": \"\", \"PwC Description\": \"No code available yet.\", \"PwC License Name\": \"\", \"PwC License URL\": \"\", \"S2 Citation Count (June 2024)\": \"\", \"S2 Date\": \"\"}, \"Bibtex\": \"@Article{Rohrbach2016MovieD,\\n author = {Anna Rohrbach and Atousa Torabi and Marcus Rohrbach and Niket Tandon and C. Pal and H. Larochelle and Aaron C. Courville and B. Schiele},\\n booktitle = {International Journal of Computer Vision},\\n journal = {International Journal of Computer Vision},\\n pages = {94 - 120},\\n title = {Movie Description},\\n volume = {123},\\n year = {2016}\\n}\\n\", \"License Use (DataProvenance)\": \"academic-only\", \"License Attribution (DataProvenance)\": 1, \"License Share Alike (DataProvenance)\": 0, \"License Use (DataProvenance IgnoreOpenAI)\": \"academic-only\", \"License Attribution (DataProvenance IgnoreOpenAI)\": 1, \"License Share Alike (DataProvenance IgnoreOpenAI)\": 0, \"License Type\": \"Non-Commercial/Academic\"}, {\"Unique Dataset Identifier\": \"moviescenes\", \"Collection\": \"moviescenes\", \"Collection URL\": \"https://arxiv.org/abs/2004.02678\", \"Dataset Name\": \"MovieScenes\", \"Paper Title\": \"MovieScenes\", \"Paper URL\": \"https://arxiv.org/abs/2004.02678\", \"GitHub URL\": \"https://github.com/AnyiRao/SceneSeg\", \"Hugging Face URL\": \"\", \"Papers with Code URL\": \"https://paperswithcode.com/paper/a-local-to-global-approach-to-multi-modal\", \"ArXiv URL\": \"https://arxiv.org/abs/2004.02678\", \"Semantic Scholar Corpus ID\": 214802984, \"Year Released\": \"2020\", \"Text Sources\": [\"movies\"], \"Licenses\": [{\"License\": \"Unspecified\", \"License URL\": \"\"}], \"Creators\": [\"The Chinese University of Hong Kong\", \"UC Berkeley\"], \"Countries\": [\"China\", \"United States of America\"], \"License Verified By\": \"Vivek Sharma\", \"Video Hours\": 250.0, \"Taken Down\": \"False\", \"Video Sources\": \"movies\", \"Task Categories\": [\"Misc (Scene Segmentation)\"], \"Inferred Metadata\": {\"GitHub License\": \"\", \"GitHub Stars (June 2024)\": 213, \"GitHub Topics\": [\"boundary-detection\", \"scene\", \"segmentation\", \"video-analysis\"], \"Github Date\": \"\", \"HF Config\": \"\", \"HF Config License\": \"\", \"HF Dataset\": \"\", \"HF Date\": \"\", \"HF Downloads (June 2024)\": \"\", \"HF Likes (June 2024)\": \"\", \"HF Yaml License\": \"\", \"PwC Date\": \"\", \"PwC Description\": \"4 code implementations in PyTorch and TensorFlow. Scene, as the crucial unit of storytelling in movies, contains complex activities of actors and their interactions in a physical environment. Identifying the composition of scenes serves as a critical step towards semantic understanding of movies. This is very challenging -- compared to the videos studied in conventional vision problems, e.g. action recognition, as scenes in movies usually contain much richer temporal structures and more complex semantic information. Towards this goal, we scale up the scene segmentation task by building a large-scale video dataset MovieScenes, which contains 21K annotated scene segments from 150 movies. We further propose a local-to-global scene segmentation framework, which integrates multi-modal information across three levels, i.e. clip, segment, and movie. This framework is able to distill complex semantics from hierarchical temporal structures over a long movie, providing top-down guidance for scene segmentation. Our experiments show that the proposed network is able to segment a movie into scenes with high accuracy, consistently outperforming previous methods. We also found that pretraining on our MovieScenes can bring significant improvements to the existing approaches.\", \"PwC License Name\": \"\", \"PwC License URL\": \"\", \"S2 Citation Count (June 2024)\": \"\", \"S2 Date\": \"\"}, \"Bibtex\": \"@Article{Rao2020ALA,\\n author = {Anyi Rao and Linning Xu and Yu Xiong and Guodong Xu and Qingqiu Huang and Bolei Zhou and Dahua Lin},\\n booktitle = {Computer Vision and Pattern Recognition},\\n journal = {2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},\\n pages = {10143-10152},\\n title = {A Local-to-Global Approach to Multi-Modal Movie Scene Segmentation},\\n year = {2020}\\n}\\n\", \"License Use (DataProvenance)\": \"unspecified\", \"License Attribution (DataProvenance)\": 0, \"License Share Alike (DataProvenance)\": 0, \"License Use (DataProvenance IgnoreOpenAI)\": \"unspecified\", \"License Attribution (DataProvenance IgnoreOpenAI)\": 0, \"License Share Alike (DataProvenance IgnoreOpenAI)\": 0, \"License Type\": \"Unspecified\"}, {\"Unique Dataset Identifier\": \"condensed-movies\", \"Collection\": \"condensed-movies\", \"Collection URL\": \"https://arxiv.org/pdf/2005.04208\", \"Dataset Name\": \"Condensed Movies\", \"Paper Title\": \"Condensed Movies\", \"Paper URL\": \"https://arxiv.org/pdf/2005.04208\", \"GitHub URL\": \"https://github.com/m-bain/CondensedMovies\", \"Hugging Face URL\": \"\", \"Papers with Code URL\": \"https://paperswithcode.com/dataset/condensed-movies\", \"ArXiv URL\": \"https://arxiv.org/pdf/2005.04208\", \"Semantic Scholar Corpus ID\": 218571391, \"Year Released\": \"2020\", \"Text Sources\": [\"movies\"], \"Licenses\": [{\"License\": \"CC BY 4.0\", \"License URL\": \"https://www.robots.ox.ac.uk/~vgg/data/condensed-movies/#download\"}], \"Creators\": [\"University of Oxford\"], \"Countries\": [\"United Kingdom\"], \"License Verified By\": \"Vivek Sharma\", \"Video Hours\": 1270.0, \"Taken Down\": \"False\", \"Video Sources\": \"movies\", \"Task Categories\": [\"Video Summarization\"], \"Inferred Metadata\": {\"GitHub License\": \"\", \"GitHub Stars (June 2024)\": 153, \"GitHub Topics\": [\"dataset\", \"precomputed-features\", \"retrieval\", \"source-videos\", \"video-text-retrieval\"], \"Github Date\": \"\", \"HF Config\": \"\", \"HF Config License\": \"\", \"HF Dataset\": \"\", \"HF Date\": \"\", \"HF Downloads (June 2024)\": \"\", \"HF Likes (June 2024)\": \"\", \"HF Yaml License\": \"\", \"PwC Date\": \"2020-05-08\", \"PwC Description\": \"A large-scale video dataset, featuring clips from movies with detailed captions.\", \"PwC License Name\": \"Unspecified\", \"PwC License URL\": null, \"S2 Citation Count (June 2024)\": \"\", \"S2 Date\": \"\"}, \"Bibtex\": \"@Article{Bain2020CondensedMS,\\n author = {Max Bain and Arsha Nagrani and A. Brown and Andrew Zisserman},\\n booktitle = {Asian Conference on Computer Vision},\\n journal = {ArXiv},\\n title = {Condensed Movies: Story Based Retrieval with Contextual Embeddings},\\n volume = {abs/2005.04208},\\n year = {2020}\\n}\\n\", \"License Use (DataProvenance)\": \"commercial\", \"License Attribution (DataProvenance)\": 1, \"License Share Alike (DataProvenance)\": 0, \"License Use (DataProvenance IgnoreOpenAI)\": \"commercial\", \"License Attribution (DataProvenance IgnoreOpenAI)\": 1, \"License Share Alike (DataProvenance IgnoreOpenAI)\": 0, \"License Type\": \"Commercial\"}, {\"Unique Dataset Identifier\": \"apes\", \"Collection\": \"apes\", \"Collection URL\": \"https://arxiv.org/pdf/2106.01667\", \"Dataset Name\": \"Apes\", \"Paper Title\": \"Apes\", \"Paper URL\": \"https://arxiv.org/pdf/2106.01667\", \"GitHub URL\": \"https://github.com/fuankarion/audiovisual-person-search\", \"Hugging Face URL\": \"\", \"Papers with Code URL\": \"https://paperswithcode.com/paper/apes-audiovisual-person-search-in-untrimmed/review/\", \"ArXiv URL\": \"https://arxiv.org/pdf/2106.01667\", \"Semantic Scholar Corpus ID\": 235313698, \"Year Released\": \"2021\", \"Text Sources\": [\"movies\"], \"Licenses\": [{\"License\": \"Unspecified\", \"License URL\": \"\"}], \"Creators\": [\"Universidad de los Andes\", \"Adobe Research\", \"King Abdullah University of Science and Technology (KAUST)\"], \"Countries\": [\"Chile\", \"United States of America\", \"Saudi Arabia\"], \"License Verified By\": \"Vivek Sharma\", \"Video Hours\": 36.0, \"Taken Down\": \"False\", \"Video Sources\": \"movies\", \"Task Categories\": [\"Video Classification\"], \"Inferred Metadata\": {\"GitHub License\": \"\", \"GitHub Stars (June 2024)\": 4, \"GitHub Topics\": [], \"Github Date\": \"\", \"HF Config\": \"\", \"HF Config License\": \"\", \"HF Dataset\": \"\", \"HF Date\": \"\", \"HF Downloads (June 2024)\": \"\", \"HF Likes (June 2024)\": \"\", \"HF Yaml License\": \"\", \"PwC Date\": \"\", \"PwC Description\": \"Paper tables with annotated results for APES: Audiovisual Person Search in Untrimmed Video\", \"PwC License Name\": \"\", \"PwC License URL\": \"\", \"S2 Citation Count (June 2024)\": \"\", \"S2 Date\": \"\"}, \"Bibtex\": \"@Article{Alcazar2021APESAP,\\n author = {Juan Leon Alcazar and Long Mai and Federico Perazzi and Joon-Young Lee and Pablo Arbel\\u00e1ez and Bernard Ghanem and Fabian Caba Heilbron},\\n booktitle = {2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)},\\n journal = {2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)},\\n pages = {1720-1729},\\n title = {APES: Audiovisual Person Search in Untrimmed Video},\\n year = {2021}\\n}\\n\", \"License Use (DataProvenance)\": \"unspecified\", \"License Attribution (DataProvenance)\": 0, \"License Share Alike (DataProvenance)\": 0, \"License Use (DataProvenance IgnoreOpenAI)\": \"unspecified\", \"License Attribution (DataProvenance IgnoreOpenAI)\": 0, \"License Share Alike (DataProvenance IgnoreOpenAI)\": 0, \"License Type\": \"Unspecified\"}, {\"Unique Dataset Identifier\": \"mpii-md\", \"Collection\": \"mpii-md\", \"Collection URL\": \"https://arxiv.org/pdf/1501.02530.pdf\", \"Dataset Name\": \"MPII-MD: A Dataset for Movie Description\", \"Paper Title\": \"MPII-MD: A Dataset for Movie Description\", \"Paper URL\": \"https://arxiv.org/pdf/1501.02530.pdf\", \"GitHub URL\": \"\", \"Hugging Face URL\": \"\", \"Papers with Code URL\": \"\", \"ArXiv URL\": \"https://arxiv.org/pdf/1501.02530.pdf\", \"Semantic Scholar Corpus ID\": 15184723, \"Year Released\": \"2017\", \"Text Sources\": [\"movies\"], \"Licenses\": [{\"License\": \"Unspecified\", \"License URL\": \"\"}], \"Creators\": [\"Max Planck Institute for Informatics\"], \"Countries\": [\"Germany\"], \"License Verified By\": \"Vivek Sharma\", \"Video Hours\": 56.5, \"Taken Down\": \"False\", \"Video Sources\": \"movies\", \"Task Categories\": [\"Video Captioning\"], \"Inferred Metadata\": {\"GitHub License\": \"\", \"GitHub Stars (June 2024)\": \"\", \"GitHub Topics\": \"\", \"Github Date\": \"\", \"HF Config\": \"\", \"HF Config License\": \"\", \"HF Dataset\": \"\", \"HF Date\": \"\", \"HF Downloads (June 2024)\": \"\", \"HF Likes (June 2024)\": \"\", \"HF Yaml License\": \"\", \"PwC Date\": \"\", \"PwC Description\": \"\", \"PwC License Name\": \"\", \"PwC License URL\": \"\", \"S2 Citation Count (June 2024)\": \"\", \"S2 Date\": \"\"}, \"Bibtex\": \"@Article{Rohrbach2015ADF,\\n author = {Anna Rohrbach and Marcus Rohrbach and Niket Tandon and B. Schiele},\\n booktitle = {Computer Vision and Pattern Recognition},\\n journal = {2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},\\n pages = {3202-3212},\\n title = {A dataset for Movie Description},\\n year = {2015}\\n}\\n\", \"License Use (DataProvenance)\": \"unspecified\", \"License Attribution (DataProvenance)\": 0, \"License Share Alike (DataProvenance)\": 0, \"License Use (DataProvenance IgnoreOpenAI)\": \"unspecified\", \"License Attribution (DataProvenance IgnoreOpenAI)\": 0, \"License Share Alike (DataProvenance IgnoreOpenAI)\": 0, \"License Type\": \"Unspecified\"}, {\"Unique Dataset Identifier\": \"lsmdc-ordering\", \"Collection\": \"lsmdc-ordering\", \"Collection URL\": \"https://arxiv.org/pdf/2004.02205\", \"Dataset Name\": \"LSMDC Ordering\", \"Paper Title\": \"LSMDC Ordering\", \"Paper URL\": \"https://arxiv.org/pdf/2004.02205\", \"GitHub URL\": \"https://github.com/vivoutlaw/TCBP\", \"Hugging Face URL\": \"\", \"Papers with Code URL\": \"https://paperswithcode.com/paper/deep-multimodal-feature-encoding-for-video\", \"ArXiv URL\": \"https://arxiv.org/pdf/2004.02205\", \"Semantic Scholar Corpus ID\": 214802821, \"Year Released\": \"2020\", \"Text Sources\": [\"movies\"], \"Licenses\": [{\"License\": \"MIT License\", \"License URL\": \"https://github.com/vivoutlaw/tcbp/blob/master/LICENSE\"}], \"Creators\": [\"University of Toronto\", \"Karlsruhe Institute of Technology\", \"Inria\", \"Massachusetts Institute of Technology\"], \"Countries\": [\"Germany\", \"Canada\", \"United States of America\", \"France\"], \"License Verified By\": \"Vivek Sharma\", \"Video Hours\": 158.0, \"Taken Down\": \"False\", \"Video Sources\": \"movies\", \"Task Categories\": [\"Video Classification\"], \"Inferred Metadata\": {\"GitHub License\": \"MIT License\", \"GitHub Stars (June 2024)\": 11, \"GitHub Topics\": [], \"Github Date\": \"\", \"HF Config\": \"\", \"HF Config License\": \"\", \"HF Dataset\": \"\", \"HF Date\": \"\", \"HF Downloads (June 2024)\": \"\", \"HF Likes (June 2024)\": \"\", \"HF Yaml License\": \"\", \"PwC Date\": \"\", \"PwC Description\": \"1 code implementation in PyTorch. True understanding of videos comes from a joint analysis of all its modalities: the video frames, the audio track, and any accompanying text such as closed captions. We present a way to learn a compact multimodal feature representation that encodes all these modalities. Our model parameters are learned through a proxy task of inferring the temporal ordering of a set of unordered videos in a timeline. To this end, we create a new multimodal dataset for temporal ordering that consists of approximately 30K scenes (2-6 clips per scene) based on the \\\"Large Scale Movie Description Challenge\\\". We analyze and evaluate the individual and joint modalities on three challenging tasks: (i) inferring the temporal ordering of a set of videos; and (ii) action recognition. We demonstrate empirically that multimodal representations are indeed complementary, and can play a key role in improving the performance of many applications.\", \"PwC License Name\": \"\", \"PwC License URL\": \"\", \"S2 Citation Count (June 2024)\": \"\", \"S2 Date\": \"\"}, \"Bibtex\": \"@Article{Sharma2020DeepMF,\\n author = {Vivek Sharma and Makarand Tapaswi and R. Stiefelhagen},\\n booktitle = {arXiv.org},\\n journal = {ArXiv},\\n title = {Deep Multimodal Feature Encoding for Video Ordering},\\n volume = {abs/2004.02205},\\n year = {2020}\\n}\\n\", \"License Use (DataProvenance)\": \"commercial\", \"License Attribution (DataProvenance)\": 1, \"License Share Alike (DataProvenance)\": 0, \"License Use (DataProvenance IgnoreOpenAI)\": \"commercial\", \"License Attribution (DataProvenance IgnoreOpenAI)\": 1, \"License Share Alike (DataProvenance IgnoreOpenAI)\": 0, \"License Type\": \"Commercial\"}, {\"Unique Dataset Identifier\": \"hmdb-dataset\", \"Collection\": \"hmdb-dataset\", \"Collection URL\": \"https://ieeexplore.ieee.org/document/6126543\", \"Dataset Name\": \"HMDB: A Large Video Database for Human Motion Recognition (ICCV 2011)\", \"Paper Title\": \"HMDB: A Large Video Database for Human Motion Recognition (ICCV 2011)\", \"Paper URL\": \"https://ieeexplore.ieee.org/document/6126543\", \"GitHub URL\": \"\", \"Hugging Face URL\": \"\", \"Papers with Code URL\": \"https://paperswithcode.com/dataset/hmdb51\", \"ArXiv URL\": \"https://ieeexplore.ieee.org/document/6126543\", \"Semantic Scholar Corpus ID\": 206769852, \"Year Released\": \"2011\", \"Text Sources\": [\"movies\", \"prelinger archive\", \"undisclosed web\", \"youtube\", \"google videos\"], \"Licenses\": [{\"License\": \"CC BY 4.0\", \"License URL\": \"https://serre-lab.clps.brown.edu/resource/hmdb-a-large-human-motion-database/\"}], \"Creators\": [\"Karlsruhe Institute of Technology\", \"Massachusetts Institute of Technology\", \"Brown University\"], \"Countries\": [\"United States of America\", \"Germany\"], \"License Verified By\": \"Vivek Sharma\", \"Video Hours\": 7000.0, \"Taken Down\": \"False\", \"Video Sources\": \"movies\", \"Task Categories\": [\"Video Classification\"], \"Inferred Metadata\": {\"GitHub License\": \"\", \"GitHub Stars (June 2024)\": \"\", \"GitHub Topics\": \"\", \"Github Date\": \"\", \"HF Config\": \"\", \"HF Config License\": \"\", \"HF Dataset\": \"\", \"HF Date\": \"\", \"HF Downloads (June 2024)\": \"\", \"HF Likes (June 2024)\": \"\", \"HF Yaml License\": \"\", \"PwC Date\": \"2011-01-01\", \"PwC Description\": \"The HMDB51 dataset is a large collection of realistic videos from various sources, including movies and web videos. The dataset is composed of 6,766 video clips from 51 action categories (such as \\u201cjump\\u201d, \\u201ckiss\\u201d and \\u201claugh\\u201d), with each category containing at least 101 clips. The original evaluation scheme uses three different training/testing splits. In each split, each action class has 70 clips for training and 30 clips for testing. The average accuracy over these three splits is used to measure the final performance.\", \"PwC License Name\": \"CC BY 4.0\", \"PwC License URL\": \"https://creativecommons.org/licenses/by/4.0/\", \"S2 Citation Count (June 2024)\": \"\", \"S2 Date\": \"\"}, \"Bibtex\": \"@Article{Kuehne2011HMDBAL,\\n author = {Hilde Kuehne and Hueihan Jhuang and Est\\u00edbaliz Garrote and T. Poggio and Thomas Serre},\\n booktitle = {Vision},\\n journal = {2011 International Conference on Computer Vision},\\n pages = {2556-2563},\\n title = {HMDB: A large video database for human motion recognition},\\n year = {2011}\\n}\\n\", \"License Use (DataProvenance)\": \"commercial\", \"License Attribution (DataProvenance)\": 1, \"License Share Alike (DataProvenance)\": 0, \"License Use (DataProvenance IgnoreOpenAI)\": \"commercial\", \"License Attribution (DataProvenance IgnoreOpenAI)\": 1, \"License Share Alike (DataProvenance IgnoreOpenAI)\": 0, \"License Type\": \"Commercial\"}, {\"Unique Dataset Identifier\": \"moviegraphs\", \"Collection\": \"moviegraphs\", \"Collection URL\": \"https://arxiv.org/pdf/1712.06761\", \"Dataset Name\": \"MovieGraphs\", \"Paper Title\": \"MovieGraphs\", \"Paper URL\": \"https://arxiv.org/pdf/1712.06761\", \"GitHub URL\": \"\", \"Hugging Face URL\": \"\", \"Papers with Code URL\": \"https://paperswithcode.com/dataset/moviegraphs\", \"ArXiv URL\": \"https://arxiv.org/pdf/1712.06761\", \"Semantic Scholar Corpus ID\": 4856028, \"Year Released\": \"2018\", \"Text Sources\": [\"movies\"], \"Licenses\": [{\"License\": \"Custom\", \"License URL\": \"http://moviegraphs.cs.toronto.edu/download.html\"}], \"Creators\": [\"Vector Institute for Artificial Intelligence\", \"Montreal Institute of Learning Algorithms (Mila)\", \"University of Toronto\"], \"Countries\": [\"Canada\"], \"License Verified By\": \"Vivek Sharma\", \"Video Hours\": 93.9, \"Taken Down\": \"True\", \"Video Sources\": \"movies\", \"Task Categories\": [\"Misc (video retrieval\", \"interaction understanding via ordering\", \"reason prediction)\"], \"Inferred Metadata\": {\"GitHub License\": \"\", \"GitHub Stars (June 2024)\": \"\", \"GitHub Topics\": \"\", \"Github Date\": \"\", \"HF Config\": \"\", \"HF Config License\": \"\", \"HF Dataset\": \"\", \"HF Date\": \"\", \"HF Downloads (June 2024)\": \"\", \"HF Likes (June 2024)\": \"\", \"HF Yaml License\": \"\", \"PwC Date\": \"\", \"PwC Description\": \"Provides detailed, graph-based annotations of social situations depicted in movie clips. Each graph consists of several types of nodes, to capture who is present in the clip, their emotional and physical attributes, their relationships (i.e., parent/child), and the interactions between them. Most interactions are associated with topics that provide additional details, and reasons that give motivations for actions.\", \"PwC License Name\": \"Academic Research Purposes Only\", \"PwC License URL\": \"https://docs.google.com/forms/d/e/1FAIpQLScytuCn4kRBKFPPei0t01Sfadpu8Qh5i9fFvfODWAAJGyEs7g/viewform\", \"S2 Citation Count (June 2024)\": \"\", \"S2 Date\": \"\"}, \"Bibtex\": \"@Article{Vicol2017MovieGraphsTU,\\n author = {Paul Vicol and Makarand Tapaswi and Llu\\u00eds Castrej\\u00f3n and S. Fidler},\\n booktitle = {2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition},\\n journal = {2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition},\\n pages = {8581-8590},\\n title = {MovieGraphs: Towards Understanding Human-Centric Situations from Videos},\\n year = {2017}\\n}\\n\", \"License Use (DataProvenance)\": \"academic-only\", \"License Attribution (DataProvenance)\": 1, \"License Share Alike (DataProvenance)\": 0, \"License Use (DataProvenance IgnoreOpenAI)\": \"academic-only\", \"License Attribution (DataProvenance IgnoreOpenAI)\": 1, \"License Share Alike (DataProvenance IgnoreOpenAI)\": 0, \"License Type\": \"Non-Commercial/Academic\"}, {\"Unique Dataset Identifier\": \"hollywood-extended\", \"Collection\": \"hollywood-extended\", \"Collection URL\": \"https://arxiv.org/pdf/1407.1208\", \"Dataset Name\": \"Hollywood Extended\", \"Paper Title\": \"Hollywood Extended\", \"Paper URL\": \"https://arxiv.org/pdf/1407.1208\", \"GitHub URL\": \"https://github.com/piotr-bojanowski/action-ordering\", \"Hugging Face URL\": \"\", \"Papers with Code URL\": \"https://paperswithcode.com/paper/weakly-supervised-action-labeling-in-videos\", \"ArXiv URL\": \"https://arxiv.org/pdf/1407.1208\", \"Semantic Scholar Corpus ID\": 9342651, \"Year Released\": \"2014\", \"Text Sources\": [\"movies\"], \"Licenses\": [{\"License\": \"MIT License\", \"License URL\": \"https://github.com/piotr-bojanowski/action-ordering/blob/master/LICENSE\"}], \"Creators\": [\"Inria\"], \"Countries\": [\"France\"], \"License Verified By\": \"Vivek Sharma\", \"Video Hours\": 8.75, \"Taken Down\": \"False\", \"Video Sources\": \"movies\", \"Task Categories\": [\"Temporal Action Detection\", \"Action Recognition\"], \"Inferred Metadata\": {\"GitHub License\": \"MIT License\", \"GitHub Stars (June 2024)\": 12, \"GitHub Topics\": [], \"Github Date\": \"\", \"HF Config\": \"\", \"HF Config License\": \"\", \"HF Dataset\": \"\", \"HF Date\": \"\", \"HF Downloads (June 2024)\": \"\", \"HF Likes (June 2024)\": \"\", \"HF Yaml License\": \"\", \"PwC Date\": \"\", \"PwC Description\": \"No code available yet.\", \"PwC License Name\": \"\", \"PwC License URL\": \"\", \"S2 Citation Count (June 2024)\": \"\", \"S2 Date\": \"\"}, \"Bibtex\": \"@Article{Bojanowski2014WeaklySA,\\n author = {Piotr Bojanowski and R\\u00e9mi Lajugie and F. Bach and I. Laptev and J. Ponce and C. Schmid and Josef Sivic},\\n booktitle = {European Conference on Computer Vision},\\n journal = {ArXiv},\\n title = {Weakly Supervised Action Labeling in Videos under Ordering Constraints},\\n volume = {abs/1407.1208},\\n year = {2014}\\n}\\n\", \"License Use (DataProvenance)\": \"commercial\", \"License Attribution (DataProvenance)\": 1, \"License Share Alike (DataProvenance)\": 0, \"License Use (DataProvenance IgnoreOpenAI)\": \"commercial\", \"License Attribution (DataProvenance IgnoreOpenAI)\": 1, \"License Share Alike (DataProvenance IgnoreOpenAI)\": 0, \"License Type\": \"Commercial\"}, {\"Unique Dataset Identifier\": \"hollywood2-dataset\", \"Collection\": \"hollywood2-dataset\", \"Collection URL\": \"https://www.irisa.fr/vista/Papers/2009_cvpr_marszalek.pdf\", \"Dataset Name\": \"HOLLYWOOD2: Actions in Context (CVPR 2009)\", \"Paper Title\": \"HOLLYWOOD2: Actions in Context (CVPR 2009)\", \"Paper URL\": \"https://www.irisa.fr/vista/Papers/2009_cvpr_marszalek.pdf\", \"GitHub URL\": \"\", \"Hugging Face URL\": \"\", \"Papers with Code URL\": \"\", \"ArXiv URL\": \"https://www.irisa.fr/vista/Papers/2009_cvpr_marszalek.pdf\", \"Semantic Scholar Corpus ID\": 3155054, \"Year Released\": \"2009\", \"Text Sources\": [\"movies\"], \"Licenses\": [{\"License\": \"Unspecified\", \"License URL\": \"\"}], \"Creators\": [\"Inria\"], \"Countries\": [\"France\"], \"License Verified By\": \"Vivek Sharma\", \"Video Hours\": 20.1, \"Taken Down\": \"False\", \"Video Sources\": \"movies\", \"Task Categories\": [\"Video Classification\"], \"Inferred Metadata\": {\"GitHub License\": \"\", \"GitHub Stars (June 2024)\": \"\", \"GitHub Topics\": \"\", \"Github Date\": \"\", \"HF Config\": \"\", \"HF Config License\": \"\", \"HF Dataset\": \"\", \"HF Date\": \"\", \"HF Downloads (June 2024)\": \"\", \"HF Likes (June 2024)\": \"\", \"HF Yaml License\": \"\", \"PwC Date\": \"\", \"PwC Description\": \"\", \"PwC License Name\": \"\", \"PwC License URL\": \"\", \"S2 Citation Count (June 2024)\": \"\", \"S2 Date\": \"\"}, \"Bibtex\": \"@Article{Marszalek2009ActionsIC,\\n author = {Marcin Marszalek and I. Laptev and C. Schmid},\\n booktitle = {2009 IEEE Conference on Computer Vision and Pattern Recognition},\\n journal = {2009 IEEE Conference on Computer Vision and Pattern Recognition},\\n pages = {2929-2936},\\n title = {Actions in context},\\n year = {2009}\\n}\\n\", \"License Use (DataProvenance)\": \"unspecified\", \"License Attribution (DataProvenance)\": 0, \"License Share Alike (DataProvenance)\": 0, \"License Use (DataProvenance IgnoreOpenAI)\": \"unspecified\", \"License Attribution (DataProvenance IgnoreOpenAI)\": 0, \"License Share Alike (DataProvenance IgnoreOpenAI)\": 0, \"License Type\": \"Unspecified\"}, {\"Unique Dataset Identifier\": \"vatex\", \"Collection\": \"vatex\", \"Collection URL\": \"https://arxiv.org/abs/1904.03493\", \"Dataset Name\": \"VaTeX\", \"Paper Title\": \"VaTeX\", \"Paper URL\": \"https://arxiv.org/abs/1904.03493\", \"GitHub URL\": \"\", \"Hugging Face URL\": \"https://huggingface.co/datasets/HuggingFaceM4/vatex\", \"Papers with Code URL\": \"https://paperswithcode.com/dataset/vatex\", \"ArXiv URL\": \"https://arxiv.org/abs/1904.03493\", \"Semantic Scholar Corpus ID\": 102352148, \"Year Released\": \"2019\", \"Text Sources\": [\"youtube\"], \"Licenses\": [{\"License\": \"CC BY 4.0\", \"License URL\": \"https://eric-xw.github.io/vatex-website/index.html\"}], \"Creators\": [\"ByteDance AI Lab\", \"UC Santa Barbara\"], \"Countries\": [\"China\", \"United States of America\"], \"License Verified By\": \"Vivek Sharma\", \"Video Hours\": 114.58, \"Taken Down\": \"False\", \"Video Sources\": \"youtube\", \"Task Categories\": [\"Video Captioning\"], \"Inferred Metadata\": {\"GitHub License\": \"\", \"GitHub Stars (June 2024)\": \"\", \"GitHub Topics\": \"\", \"Github Date\": \"\", \"HF Config\": \"v1.1\", \"HF Config License\": \"\", \"HF Dataset\": \"HuggingFaceM4/vatex\", \"HF Date\": \"2022-05-13\", \"HF Downloads (June 2024)\": 112, \"HF Likes (June 2024)\": 4, \"HF Yaml License\": \"\", \"PwC Date\": \"2019-01-01\", \"PwC Description\": \"VATEX is multilingual, large, linguistically complex, and diverse dataset in terms of both video and natural language descriptions. It has two tasks for video-and-language research: (1) Multilingual Video Captioning, aimed at describing a video in various languages with a compact unified captioning model, and (2) Video-guided Machine Translation, to translate a source language description into the target language using the video information as additional spatiotemporal context.\", \"PwC License Name\": \"CC BY 4.0\", \"PwC License URL\": \"https://creativecommons.org/licenses/by/4.0/\", \"S2 Citation Count (June 2024)\": \"\", \"S2 Date\": \"\"}, \"Bibtex\": \"@Article{Wang2019VaTeXAL,\\n author = {Xin Eric Wang and Jiawei Wu and Junkun Chen and Lei Li and Yuan-fang Wang and William Yang Wang},\\n booktitle = {IEEE International Conference on Computer Vision},\\n journal = {2019 IEEE/CVF International Conference on Computer Vision (ICCV)},\\n pages = {4580-4590},\\n title = {VaTeX: A Large-Scale, High-Quality Multilingual Dataset for Video-and-Language Research},\\n year = {2019}\\n}\\n\", \"License Use (DataProvenance)\": \"commercial\", \"License Attribution (DataProvenance)\": 1, \"License Share Alike (DataProvenance)\": 0, \"License Use (DataProvenance IgnoreOpenAI)\": \"commercial\", \"License Attribution (DataProvenance IgnoreOpenAI)\": 1, \"License Share Alike (DataProvenance IgnoreOpenAI)\": 0, \"License Type\": \"Commercial\"}, {\"Unique Dataset Identifier\": \"ucf101-dataset\", \"Collection\": \"ucf101-dataset\", \"Collection URL\": \"https://arxiv.org/abs/1212.0402\", \"Dataset Name\": \"UCF101: A Dataset of 101 Human Actions Classes From Videos in The Wild\", \"Paper Title\": \"UCF101: A Dataset of 101 Human Actions Classes From Videos in The Wild\", \"Paper URL\": \"https://arxiv.org/abs/1212.0402\", \"GitHub URL\": \"\", \"Hugging Face URL\": \"\", \"Papers with Code URL\": \"https://paperswithcode.com/dataset/ucf101\", \"ArXiv URL\": \"https://arxiv.org/abs/1212.0402\", \"Semantic Scholar Corpus ID\": 7197134, \"Year Released\": \"2012\", \"Text Sources\": [\"youtube\"], \"Licenses\": [{\"License\": \"Unspecified\", \"License URL\": \"\"}], \"Creators\": [\"University of Central Florida\"], \"Countries\": [\"United States of America\"], \"License Verified By\": \"Vivek Sharma\", \"Video Hours\": 26.0, \"Taken Down\": \"False\", \"Video Sources\": \"youtube\", \"Task Categories\": [\"Video Classification\"], \"Inferred Metadata\": {\"GitHub License\": \"\", \"GitHub Stars (June 2024)\": \"\", \"GitHub Topics\": \"\", \"Github Date\": \"\", \"HF Config\": \"\", \"HF Config License\": \"\", \"HF Dataset\": \"\", \"HF Date\": \"\", \"HF Downloads (June 2024)\": \"\", \"HF Likes (June 2024)\": \"\", \"HF Yaml License\": \"\", \"PwC Date\": \"2012-12-03\", \"PwC Description\": \"UCF101 dataset is an extension of UCF50 and consists of 13,320 video clips, which are classified into 101 categories. These 101 categories can be classified into 5 types (Body motion, Human-human interactions, Human-object interactions, Playing musical instruments and Sports). The total length of these video clips is over 27 hours. All the videos are collected from YouTube and have a fixed frame rate of 25 FPS with the resolution of 320 \\u00d7 240.\", \"PwC License Name\": \"Unspecified\", \"PwC License URL\": null, \"S2 Citation Count (June 2024)\": \"\", \"S2 Date\": \"\"}, \"Bibtex\": \"@Article{Soomro2012UCF101AD,\\n author = {K. Soomro and Amir Zamir and M. Shah},\\n booktitle = {arXiv.org},\\n journal = {ArXiv},\\n title = {UCF101: A Dataset of 101 Human Actions Classes From Videos in The Wild},\\n volume = {abs/1212.0402},\\n year = {2012}\\n}\\n\", \"License Use (DataProvenance)\": \"unspecified\", \"License Attribution (DataProvenance)\": 0, \"License Share Alike (DataProvenance)\": 0, \"License Use (DataProvenance IgnoreOpenAI)\": \"unspecified\", \"License Attribution (DataProvenance IgnoreOpenAI)\": 0, \"License Share Alike (DataProvenance IgnoreOpenAI)\": 0, \"License Type\": \"Unspecified\"}, {\"Unique Dataset Identifier\": \"narrated-instruction-vids\", \"Collection\": \"narrated-instruction-vids\", \"Collection URL\": \"https://arxiv.org/abs/1506.09215\", \"Dataset Name\": \"Narrated Instruction Videos\", \"Paper Title\": \"Narrated Instruction Videos\", \"Paper URL\": \"https://arxiv.org/abs/1506.09215\", \"GitHub URL\": \"https://github.com/jalayrac/instructionVideos\", \"Hugging Face URL\": \"\", \"Papers with Code URL\": \"https://paperswithcode.com/dataset/youtube-inria-instructional\", \"ArXiv URL\": \"https://arxiv.org/abs/1506.09215\", \"Semantic Scholar Corpus ID\": 2617244, \"Year Released\": \"2016\", \"Text Sources\": [\"youtube\"], \"Licenses\": [{\"License\": \"MIT License\", \"License URL\": \"https://github.com/jalayrac/instructionVideos?tab=readme-ov-file#license\"}], \"Creators\": [\"CNRS\", \"Ecole Normale Sup\\u00e9rieure\", \"Inria\", \"International Institute of Information Technology - Hyderabad\"], \"Countries\": [\"United States of America\", \"India\"], \"License Verified By\": \"Vivek Sharma\", \"Video Hours\": 7.0, \"Taken Down\": \"False\", \"Video Sources\": \"youtube\", \"Task Categories\": [\"Video Captioning\"], \"Inferred Metadata\": {\"GitHub License\": \"MIT License\", \"GitHub Stars (June 2024)\": 19, \"GitHub Topics\": [], \"Github Date\": \"\", \"HF Config\": \"\", \"HF Config License\": \"\", \"HF Dataset\": \"\", \"HF Date\": \"\", \"HF Downloads (June 2024)\": \"\", \"HF Likes (June 2024)\": \"\", \"HF Yaml License\": \"\", \"PwC Date\": \"\", \"PwC Description\": \"We address the problem of automatically learning the main steps to complete a certain task, such as changing a car tire, from a set of narrated instruction videos. The contributions of this paper are three-fold. First, we develop a new unsupervised learning approach that takes advantage of the complementary nature of the input video and the associated narration. The method solves two clustering problems, one in text and one in video, applied one after each other and linked by joint constraints to obtain a single coherent sequence of steps in both modalities. Second, we collect and annotate a new challenging dataset of real-world instruction videos from the Internet. The dataset contains about 800,000 frames for five different tasks (How to : change a car tire, perform CardioPulmonary resuscitation (CPR), jump cars, repot a plant and make coffee) that include complex interactions between people and objects, and are captured in a variety of indoor and outdoor settings. Third, we experimentally demonstrate that the proposed method can automatically discover, in an unsupervised manner , the main steps to achieve the task and locate the steps in the input videos.\\n\\nThis video presents our results of automatically discovering the scenario for the two following task : changing a tire and performing CardioPulmonary Resuscitation (CPR). At the bottom of the videos, there are three bars. The first one corresponds to our ground truth annotation. The second one corresponds to our time interval prediction in video. Finally the third one corresponds to the constraints that we obtain from the text domain. On the right, there is a list of label. They corresponds to the label recovered by our NLP method in an unsupervised manner.\", \"PwC License Name\": \"Unspecified\", \"PwC License URL\": null, \"S2 Citation Count (June 2024)\": \"\", \"S2 Date\": \"\"}, \"Bibtex\": \"@Article{Alayrac2015UnsupervisedLF,\\n author = {Jean-Baptiste Alayrac and Piotr Bojanowski and Nishant Agrawal and Josef Sivic and I. Laptev and Simon Lacoste-Julien},\\n booktitle = {Computer Vision and Pattern Recognition},\\n journal = {2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},\\n pages = {4575-4583},\\n title = {Unsupervised Learning from Narrated Instruction Videos},\\n year = {2015}\\n}\\n\", \"License Use (DataProvenance)\": \"commercial\", \"License Attribution (DataProvenance)\": 1, \"License Share Alike (DataProvenance)\": 0, \"License Use (DataProvenance IgnoreOpenAI)\": \"commercial\", \"License Attribution (DataProvenance IgnoreOpenAI)\": 1, \"License Share Alike (DataProvenance IgnoreOpenAI)\": 0, \"License Type\": \"Commercial\"}, {\"Unique Dataset Identifier\": \"coin-dataset\", \"Collection\": \"coin-dataset\", \"Collection URL\": \"https://arxiv.org/abs/1903.02874\", \"Dataset Name\": \"COIN: A Large-scale Dataset for Comprehensive Instructional Video Analysis (CVPR 2019)\", \"Paper Title\": \"COIN: A Large-scale Dataset for Comprehensive Instructional Video Analysis (CVPR 2019)\", \"Paper URL\": \"https://arxiv.org/abs/1903.02874\", \"GitHub URL\": \"https://github.com/coin-dataset/annotations\", \"Hugging Face URL\": \"\", \"Papers with Code URL\": \"https://paperswithcode.com/dataset/coin#:~:text=The%20COIN%20dataset%20(a%20large,are%20all%20collected%20from%20YouTube.\", \"ArXiv URL\": \"https://arxiv.org/abs/1903.02874\", \"Semantic Scholar Corpus ID\": 71147568, \"Year Released\": \"2019\", \"Text Sources\": [\"youtube\"], \"Licenses\": [{\"License\": \"Custom\", \"License URL\": \"https://github.com/coin-dataset/annotations?tab=readme-ov-file#license\"}], \"Creators\": [\"Tsinghua University\", \"Meitu Inc.\"], \"Countries\": [\"China\"], \"License Verified By\": \"Vivek Sharma\", \"Video Hours\": 476.0, \"Taken Down\": \"False\", \"Video Sources\": \"youtube\", \"Task Categories\": [\"Video Classification\"], \"Inferred Metadata\": {\"GitHub License\": \"\", \"GitHub Stars (June 2024)\": 100, \"GitHub Topics\": [], \"Github Date\": \"\", \"HF Config\": \"\", \"HF Config License\": \"\", \"HF Dataset\": \"\", \"HF Date\": \"\", \"HF Downloads (June 2024)\": \"\", \"HF Likes (June 2024)\": \"\", \"HF Yaml License\": \"\", \"PwC Date\": \"\", \"PwC Description\": \"The COIN dataset (a large-scale dataset for COmprehensive INstructional video analysis) consists of 11,827 videos related to 180 different tasks in 12 domains (e.g., vehicles, gadgets, etc.) related to our daily life. The videos are all collected from YouTube. The average length of a video is 2.36 minutes. Each video is labelled with 3.91 step segments, where each segment lasts 14.91 seconds on average. In total, the dataset contains videos of 476 hours, with 46,354 annotated segments.\", \"PwC License Name\": \"Various\", \"PwC License URL\": \"https://coin-dataset.github.io/\", \"S2 Citation Count (June 2024)\": \"\", \"S2 Date\": \"\"}, \"Bibtex\": \"@Article{Tang2019COINAL,\\n author = {Yansong Tang and Dajun Ding and Yongming Rao and Yu Zheng and Danyang Zhang and Lili Zhao and Jiwen Lu and Jie Zhou},\\n booktitle = {Computer Vision and Pattern Recognition},\\n journal = {2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},\\n pages = {1207-1216},\\n title = {COIN: A Large-Scale Dataset for Comprehensive Instructional Video Analysis},\\n year = {2019}\\n}\\n\", \"License Use (DataProvenance)\": \"academic-only\", \"License Attribution (DataProvenance)\": 1, \"License Share Alike (DataProvenance)\": 0, \"License Use (DataProvenance IgnoreOpenAI)\": \"academic-only\", \"License Attribution (DataProvenance IgnoreOpenAI)\": 1, \"License Share Alike (DataProvenance IgnoreOpenAI)\": 0, \"License Type\": \"Non-Commercial/Academic\"}, {\"Unique Dataset Identifier\": \"cinepile\", \"Collection\": \"cinepile\", \"Collection URL\": \"https://arxiv.org/pdf/2405.08813\", \"Dataset Name\": \"CinePile: A Long Video Question Answering Dataset and Benchmark\", \"Paper Title\": \"CinePile: A Long Video Question Answering Dataset and Benchmark\", \"Paper URL\": \"https://arxiv.org/pdf/2405.08813\", \"GitHub URL\": \"\", \"Hugging Face URL\": \"https://huggingface.co/datasets/tomg-group-umd/cinepile\", \"Papers with Code URL\": \"https://paperswithcode.com/dataset/cinepile\", \"ArXiv URL\": \"https://arxiv.org/pdf/2405.08813\", \"Semantic Scholar Corpus ID\": 269761335, \"Year Released\": \"2024\", \"Text Sources\": [\"youtube\", \"movies\"], \"Licenses\": [{\"License\": \"CC BY 4.0\", \"License URL\": \"https://creativecommons.org/licenses/by/4.0/\"}], \"Creators\": [\"University of Maryland\", \"Weizmann Institute of Science\"], \"Countries\": [\"United States of America\", \"Israel\"], \"License Verified By\": \"Vivek Sharma\", \"Video Hours\": 417.6, \"Taken Down\": \"False\", \"Video Sources\": \"youtube\", \"Task Categories\": [\"Video Q&A\"], \"Inferred Metadata\": null, \"Bibtex\": null, \"License Use (DataProvenance)\": \"commercial\", \"License Attribution (DataProvenance)\": 1, \"License Share Alike (DataProvenance)\": 0, \"License Use (DataProvenance IgnoreOpenAI)\": \"commercial\", \"License Attribution (DataProvenance IgnoreOpenAI)\": 1, \"License Share Alike (DataProvenance IgnoreOpenAI)\": 0, \"License Type\": \"Commercial\"}, {\"Unique Dataset Identifier\": \"omnisource-web-dataset\", \"Collection\": \"omnisource-web-dataset\", \"Collection URL\": \"https://arxiv.org/abs/2003.13042\", \"Dataset Name\": \"OmniSource Web Dataset\", \"Paper Title\": \"OmniSource Web Dataset\", \"Paper URL\": \"https://arxiv.org/abs/2003.13042\", \"GitHub URL\": \"https://github.com/open-mmlab/mmaction\", \"Hugging Face URL\": \"\", \"Papers with Code URL\": \"https://paperswithcode.com/paper/omni-sourced-webly-supervised-learning-for#code\", \"ArXiv URL\": \"https://arxiv.org/abs/2003.13042\", \"Semantic Scholar Corpus ID\": 214714240, \"Year Released\": \"2020\", \"Text Sources\": [\"google videos\", \"instagram\", \"youtube\"], \"Licenses\": [{\"License\": \"Apache License 2.0\", \"License URL\": \"https://github.com/open-mmlab/mmaction?tab=readme-ov-file#license\"}], \"Creators\": [\"The Chinese University of Hong Kong\"], \"Countries\": [\"China\"], \"License Verified By\": \"Vivek Sharma\", \"Video Hours\": 13333.0, \"Taken Down\": \"False\", \"Video Sources\": \"youtube\", \"Task Categories\": [\"Video Classification\"], \"Inferred Metadata\": {\"GitHub License\": \"Apache License 2.0\", \"GitHub Stars (June 2024)\": 1856, \"GitHub Topics\": [\"action-detection\", \"action-recognition\", \"pytorch\", \"spatial-temporal-action-detection\", \"temporal-action-detection\", \"temporal-action-localization\", \"video-understanding\"], \"Github Date\": \"\", \"HF Config\": \"\", \"HF Config License\": \"\", \"HF Dataset\": \"\", \"HF Date\": \"\", \"HF Downloads (June 2024)\": \"\", \"HF Likes (June 2024)\": \"\", \"HF Yaml License\": \"\", \"PwC Date\": \"\", \"PwC Description\": \"3 code implementations in PyTorch. We introduce OmniSource, a novel framework for leveraging web data to train video recognition models. OmniSource overcomes the barriers between data formats, such as images, short videos, and long untrimmed videos for webly-supervised learning. First, data samples with multiple formats, curated by task-specific data collection and automatically filtered by a teacher model, are transformed into a unified form. Then a joint-training strategy is proposed to deal with the domain gaps between multiple data sources and formats in webly-supervised learning. Several good practices, including data balancing, resampling, and cross-dataset mixup are adopted in joint training. Experiments show that by utilizing data from multiple sources and formats, OmniSource is more data-efficient in training. With only 3.5M images and 800K minutes videos crawled from the internet without human labeling (less than 2% of prior works), our models learned with OmniSource improve Top-1 accuracy of 2D- and 3D-ConvNet baseline models by 3.0% and 3.9%, respectively, on the Kinetics-400 benchmark. With OmniSource, we establish new records with different pretraining strategies for video recognition. Our best models achieve 80.4%, 80.5%, and 83.6 Top-1 accuracies on the Kinetics-400 benchmark respectively for training-from-scratch, ImageNet pre-training and IG-65M pre-training.\", \"PwC License Name\": \"\", \"PwC License URL\": \"\", \"S2 Citation Count (June 2024)\": \"\", \"S2 Date\": \"\"}, \"Bibtex\": \"@Article{Duan2020OmnisourcedWL,\\n author = {Haodong Duan and Yue Zhao and Yuanjun Xiong and Wentao Liu and Dahua Lin},\\n booktitle = {European Conference on Computer Vision},\\n journal = {ArXiv},\\n title = {Omni-sourced Webly-supervised Learning for Video Recognition},\\n volume = {abs/2003.13042},\\n year = {2020}\\n}\\n\", \"License Use (DataProvenance)\": \"commercial\", \"License Attribution (DataProvenance)\": 1, \"License Share Alike (DataProvenance)\": 0, \"License Use (DataProvenance IgnoreOpenAI)\": \"commercial\", \"License Attribution (DataProvenance IgnoreOpenAI)\": 1, \"License Share Alike (DataProvenance IgnoreOpenAI)\": 0, \"License Type\": \"Commercial\"}, {\"Unique Dataset Identifier\": \"tvsum\", \"Collection\": \"tvsum\", \"Collection URL\": \"https://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Song_TVSum_Summarizing_Web_2015_CVPR_paper.pdf\", \"Dataset Name\": \"TVSum: Summarizing web videos using titles (CVPR 2015)\", \"Paper Title\": \"TVSum: Summarizing web videos using titles (CVPR 2015)\", \"Paper URL\": \"https://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Song_TVSum_Summarizing_Web_2015_CVPR_paper.pdf\", \"GitHub URL\": \"https://github.com/yalesong/tvsum\", \"Hugging Face URL\": \"\", \"Papers with Code URL\": \"https://paperswithcode.com/dataset/tvsum-1\", \"ArXiv URL\": \"https://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Song_TVSum_Summarizing_Web_2015_CVPR_paper.pdf\", \"Semantic Scholar Corpus ID\": 7675635, \"Year Released\": \"2015\", \"Text Sources\": [\"youtube\"], \"Licenses\": [{\"License\": \"CC BY 3.0\", \"License URL\": \"https://github.com/yalesong/tvsum\"}], \"Creators\": [\"Yahoo Labs\"], \"Countries\": [\"United States of America\"], \"License Verified By\": \"Vivek Sharma\", \"Video Hours\": 3.5, \"Taken Down\": \"False\", \"Video Sources\": \"youtube\", \"Task Categories\": [\"Video Summarization\"], \"Inferred Metadata\": {\"GitHub License\": \"\", \"GitHub Stars (June 2024)\": 116, \"GitHub Topics\": [], \"Github Date\": \"\", \"HF Config\": \"\", \"HF Config License\": \"\", \"HF Dataset\": \"\", \"HF Date\": \"\", \"HF Downloads (June 2024)\": \"\", \"HF Likes (June 2024)\": \"\", \"HF Yaml License\": \"\", \"PwC Date\": \"2015-06-01\", \"PwC Description\": \"Title-based Video Summarization (TVSum) dataset serves as a benchmark to validate video summarization techniques. It contains 50 videos of various genres (e.g., news, how-to, documentary, vlog, egocentric) and 1,000 annotations of shot-level importance scores obtained via crowdsourcing (20 per video).\", \"PwC License Name\": \"Unspecified\", \"PwC License URL\": null, \"S2 Citation Count (June 2024)\": \"\", \"S2 Date\": \"\"}, \"Bibtex\": \"@Article{Song2015TVSumSW,\\n author = {Yale Song and Jordi Vallmitjana and Amanda Stent and A. Jaimes},\\n booktitle = {Computer Vision and Pattern Recognition},\\n journal = {2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},\\n pages = {5179-5187},\\n title = {TVSum: Summarizing web videos using titles},\\n year = {2015}\\n}\\n\", \"License Use (DataProvenance)\": \"commercial\", \"License Attribution (DataProvenance)\": 1, \"License Share Alike (DataProvenance)\": 0, \"License Use (DataProvenance IgnoreOpenAI)\": \"commercial\", \"License Attribution (DataProvenance IgnoreOpenAI)\": 1, \"License Share Alike (DataProvenance IgnoreOpenAI)\": 0, \"License Type\": \"Commercial\"}, {\"Unique Dataset Identifier\": \"ava\", \"Collection\": \"ava\", \"Collection URL\": \"https://arxiv.org/pdf/1705.08421\", \"Dataset Name\": \"AVA\", \"Paper Title\": \"AVA\", \"Paper URL\": \"https://arxiv.org/pdf/1705.08421\", \"GitHub URL\": \"\", \"Hugging Face URL\": \"\", \"Papers with Code URL\": \"https://paperswithcode.com/paper/ava-a-video-dataset-of-spatio-temporally\", \"ArXiv URL\": \"https://arxiv.org/pdf/1705.08421\", \"Semantic Scholar Corpus ID\": 688013, \"Year Released\": \"2017\", \"Text Sources\": [\"youtube\"], \"Licenses\": [{\"License\": \"CC BY 4.0\", \"License URL\": \"https://research.google.com/ava/\"}], \"Creators\": [\"Google Research\"], \"Countries\": [\"United States of America\"], \"License Verified By\": \"Vivek Sharma\", \"Video Hours\": 107.5, \"Taken Down\": \"False\", \"Video Sources\": \"youtube\", \"Task Categories\": [\"Temporal Localization\", \"Video Classification\"], \"Inferred Metadata\": {\"GitHub License\": \"\", \"GitHub Stars (June 2024)\": \"\", \"GitHub Topics\": \"\", \"Github Date\": \"\", \"HF Config\": \"\", \"HF Config License\": \"\", \"HF Dataset\": \"\", \"HF Date\": \"\", \"HF Downloads (June 2024)\": \"\", \"HF Likes (June 2024)\": \"\", \"HF Yaml License\": \"\", \"PwC Date\": \"\", \"PwC Description\": \"#6 best model for Action Detection on UCF101-24 (Frame-mAP 0.5 metric)\", \"PwC License Name\": \"\", \"PwC License URL\": \"\", \"S2 Citation Count (June 2024)\": \"\", \"S2 Date\": \"\"}, \"Bibtex\": \"@Article{Gu2017AVAAV,\\n author = {Chunhui Gu and Chen Sun and Sudheendra Vijayanarasimhan and C. Pantofaru and David A. Ross and G. Toderici and Yeqing Li and Susanna Ricco and R. Sukthankar and C. Schmid and J. Malik},\\n booktitle = {2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition},\\n journal = {2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition},\\n pages = {6047-6056},\\n title = {AVA: A Video Dataset of Spatio-Temporally Localized Atomic Visual Actions},\\n year = {2017}\\n}\\n\", \"License Use (DataProvenance)\": \"commercial\", \"License Attribution (DataProvenance)\": 1, \"License Share Alike (DataProvenance)\": 0, \"License Use (DataProvenance IgnoreOpenAI)\": \"commercial\", \"License Attribution (DataProvenance IgnoreOpenAI)\": 1, \"License Share Alike (DataProvenance IgnoreOpenAI)\": 0, \"License Type\": \"Commercial\"}, {\"Unique Dataset Identifier\": \"moments-in-time-dataset\", \"Collection\": \"moments-in-time-dataset\", \"Collection URL\": \"https://arxiv.org/pdf/1801.03150.pdf\", \"Dataset Name\": \"Moments in Time Dataset: one million videos for event understanding (TPAMI 2019)\", \"Paper Title\": \"Moments in Time Dataset: one million videos for event understanding (TPAMI 2019)\", \"Paper URL\": \"https://arxiv.org/pdf/1801.03150.pdf\", \"GitHub URL\": \"\", \"Hugging Face URL\": \"\", \"Papers with Code URL\": \"\", \"ArXiv URL\": \"https://arxiv.org/pdf/1801.03150.pdf\", \"Semantic Scholar Corpus ID\": 11868155, \"Year Released\": \"2019\", \"Text Sources\": [\"youtube\", \"flickr\", \"vine\", \"metacafe\", \"peeks\", \"vimeo\", \"videoblocks\", \"bing videos\", \"giphy\", \"weather channel\", \"getty-images\"], \"Licenses\": [{\"License\": \"Custom\", \"License URL\": \"http://moments.csail.mit.edu/\"}], \"Creators\": [\"Massachusetts Institute of Technology\"], \"Countries\": [\"United States of America\"], \"License Verified By\": \"Vivek Sharma\", \"Video Hours\": 833.0, \"Taken Down\": \"False\", \"Video Sources\": \"youtube\", \"Task Categories\": [\"Video Classification\"], \"Inferred Metadata\": {\"GitHub License\": \"\", \"GitHub Stars (June 2024)\": \"\", \"GitHub Topics\": \"\", \"Github Date\": \"\", \"HF Config\": \"\", \"HF Config License\": \"\", \"HF Dataset\": \"\", \"HF Date\": \"\", \"HF Downloads (June 2024)\": \"\", \"HF Likes (June 2024)\": \"\", \"HF Yaml License\": \"\", \"PwC Date\": \"\", \"PwC Description\": \"\", \"PwC License Name\": \"\", \"PwC License URL\": \"\", \"S2 Citation Count (June 2024)\": \"\", \"S2 Date\": \"\"}, \"Bibtex\": \"@Article{Monfort2018MomentsIT,\\n author = {Mathew Monfort and Bolei Zhou and Sarah Adel Bargal and A. Andonian and Tom Yan and K. Ramakrishnan and L. Brown and Quanfu Fan and Dan Gutfreund and Carl Vondrick and A. Oliva},\\n booktitle = {IEEE Transactions on Pattern Analysis and Machine Intelligence},\\n journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},\\n pages = {502-508},\\n title = {Moments in Time Dataset: One Million Videos for Event Understanding},\\n volume = {42},\\n year = {2018}\\n}\\n\", \"License Use (DataProvenance)\": \"academic-only\", \"License Attribution (DataProvenance)\": 1, \"License Share Alike (DataProvenance)\": 0, \"License Use (DataProvenance IgnoreOpenAI)\": \"academic-only\", \"License Attribution (DataProvenance IgnoreOpenAI)\": 1, \"License Share Alike (DataProvenance IgnoreOpenAI)\": 0, \"License Type\": \"Non-Commercial/Academic\"}, {\"Unique Dataset Identifier\": \"YT-Temporal-1B\", \"Collection\": \"YT-Temporal-1B\", \"Collection URL\": \"https://arxiv.org/pdf/2201.02639\", \"Dataset Name\": \"MERLOT Reserve: Multimodal Neural Script Knowledge through Vision and Language and Sound\", \"Paper Title\": \"MERLOT Reserve: Multimodal Neural Script Knowledge through Vision and Language and Sound\", \"Paper URL\": \"https://arxiv.org/pdf/2201.02639\", \"GitHub URL\": \"\", \"Hugging Face URL\": \"\", \"Papers with Code URL\": \"https://paperswithcode.com/paper/merlot-reserve-neural-script-knowledge\", \"ArXiv URL\": \"https://arxiv.org/pdf/2201.02639\", \"Semantic Scholar Corpus ID\": 245837609, \"Year Released\": \"2022\", \"Text Sources\": [\"youtube\"], \"Licenses\": [{\"License\": \"MIT License\", \"License URL\": \"https://github.com/rowanz/merlot_reserve/blob/main/LICENSE\"}], \"Creators\": [\"University of Washington\", \"Allen Institute for AI\", \"University of Edinburgh\"], \"Countries\": [\"United States of America\", \"Scotland\"], \"License Verified By\": \"Vivek Sharma\", \"Video Hours\": 55555.0, \"Taken Down\": \"False\", \"Video Sources\": \"youtube\", \"Task Categories\": [\"Video Q&A\"], \"Inferred Metadata\": null, \"Bibtex\": null, \"License Use (DataProvenance)\": \"commercial\", \"License Attribution (DataProvenance)\": 1, \"License Share Alike (DataProvenance)\": 0, \"License Use (DataProvenance IgnoreOpenAI)\": \"commercial\", \"License Attribution (DataProvenance IgnoreOpenAI)\": 1, \"License Share Alike (DataProvenance IgnoreOpenAI)\": 0, \"License Type\": \"Commercial\"}, {\"Unique Dataset Identifier\": \"spoken-moments\", \"Collection\": \"spoken-moments\", \"Collection URL\": \"https://arxiv.org/abs/2105.04489\", \"Dataset Name\": \"Spoken Moments\", \"Paper Title\": \"Spoken Moments\", \"Paper URL\": \"https://arxiv.org/abs/2105.04489\", \"GitHub URL\": \"\", \"Hugging Face URL\": \"\", \"Papers with Code URL\": \"https://paperswithcode.com/paper/spoken-moments-learning-joint-audio-visual\", \"ArXiv URL\": \"https://arxiv.org/abs/2105.04489\", \"Semantic Scholar Corpus ID\": 233992735, \"Year Released\": \"2021\", \"Text Sources\": [\"youtube\", \"flickr\", \"vine\", \"metacafe\", \"peeks\", \"vimeo\", \"videoblocks\", \"bing videos\", \"giphy\", \"weather channel\", \"getty-images\"], \"Licenses\": [{\"License\": \"Custom\", \"License URL\": \"https://docs.google.com/forms/d/e/1FAIpQLSdUOScon6uWkslgcA2-BCkAhMp86SihgKtMqX53zRf-E2fIHw/formResponse\"}], \"Creators\": [\"Massachusetts Institute of Technology\", \"IBM\", \"The University of Texas at Austin\"], \"Countries\": [\"United States of America\"], \"License Verified By\": \"Vivek Sharma\", \"Video Hours\": 416.67, \"Taken Down\": \"False\", \"Video Sources\": \"youtube\", \"Task Categories\": [\"Video Captioning\"], \"Inferred Metadata\": {\"GitHub License\": \"\", \"GitHub Stars (June 2024)\": \"\", \"GitHub Topics\": \"\", \"Github Date\": \"\", \"HF Config\": \"\", \"HF Config License\": \"\", \"HF Dataset\": \"\", \"HF Date\": \"\", \"HF Downloads (June 2024)\": \"\", \"HF Likes (June 2024)\": \"\", \"HF Yaml License\": \"\", \"PwC Date\": \"\", \"PwC Description\": \"No code available yet.\", \"PwC License Name\": \"\", \"PwC License URL\": \"\", \"S2 Citation Count (June 2024)\": \"\", \"S2 Date\": \"\"}, \"Bibtex\": \"@Article{Monfort2021SpokenML,\\n author = {Mathew Monfort and SouYoung Jin},\\n booktitle = {Computer Vision and Pattern Recognition},\\n journal = {2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},\\n pages = {14866-14876},\\n title = {Spoken Moments: Learning Joint Audio-Visual Representations from Video Descriptions},\\n year = {2021}\\n}\\n\", \"License Use (DataProvenance)\": \"commercial\", \"License Attribution (DataProvenance)\": 0, \"License Share Alike (DataProvenance)\": 0, \"License Use (DataProvenance IgnoreOpenAI)\": \"commercial\", \"License Attribution (DataProvenance IgnoreOpenAI)\": 0, \"License Share Alike (DataProvenance IgnoreOpenAI)\": 0, \"License Type\": \"Commercial\"}, {\"Unique Dataset Identifier\": \"sports1M-dataset\", \"Collection\": \"sports1M-dataset\", \"Collection URL\": \"https://static.googleusercontent.com/media/research.google.com/en//pubs/archive/42455.pdf\", \"Dataset Name\": \"Sports-1M: Large-scale Video Classification with Convolutional Neural Networks\", \"Paper Title\": \"Sports-1M: Large-scale Video Classification with Convolutional Neural Networks\", \"Paper URL\": \"https://static.googleusercontent.com/media/research.google.com/en//pubs/archive/42455.pdf\", \"GitHub URL\": \"https://github.com/gtoderici/sports-1m-dataset\", \"Hugging Face URL\": \"\", \"Papers with Code URL\": \"https://paperswithcode.com/paper/large-scale-video-classification-with-1\", \"ArXiv URL\": \"https://static.googleusercontent.com/media/research.google.com/en//pubs/archive/42455.pdf\", \"Semantic Scholar Corpus ID\": 206592218, \"Year Released\": \"2014\", \"Text Sources\": [\"youtube\"], \"Licenses\": [{\"License\": \"CC BY 3.0\", \"License URL\": \"https://github.com/gtoderici/sports-1m-dataset\"}], \"Creators\": [\"Stanford University\"], \"Countries\": [\"United States of America\"], \"License Verified By\": \"Vivek Sharma\", \"Video Hours\": 105761.41, \"Taken Down\": \"False\", \"Video Sources\": \"youtube\", \"Task Categories\": [\"Video Classification\"], \"Inferred Metadata\": {\"GitHub License\": \"\", \"GitHub Stars (June 2024)\": 273, \"GitHub Topics\": [], \"Github Date\": \"\", \"HF Config\": \"\", \"HF Config License\": \"\", \"HF Dataset\": \"\", \"HF Date\": \"\", \"HF Downloads (June 2024)\": \"\", \"HF Likes (June 2024)\": \"\", \"HF Yaml License\": \"\", \"PwC Date\": \"\", \"PwC Description\": \"#9 best model for Action Recognition on Sports-1M (Video hit@1  metric)\", \"PwC License Name\": \"\", \"PwC License URL\": \"\", \"S2 Citation Count (June 2024)\": \"\", \"S2 Date\": \"\"}, \"Bibtex\": \"@Article{Karpathy2014LargeScaleVC,\\n author = {A. Karpathy and G. Toderici and Sanketh Shetty and Thomas Leung and R. Sukthankar and Li Fei-Fei},\\n booktitle = {2014 IEEE Conference on Computer Vision and Pattern Recognition},\\n journal = {2014 IEEE Conference on Computer Vision and Pattern Recognition},\\n pages = {1725-1732},\\n title = {Large-Scale Video Classification with Convolutional Neural Networks},\\n year = {2014}\\n}\\n\", \"License Use (DataProvenance)\": \"commercial\", \"License Attribution (DataProvenance)\": 1, \"License Share Alike (DataProvenance)\": 0, \"License Use (DataProvenance IgnoreOpenAI)\": \"commercial\", \"License Attribution (DataProvenance IgnoreOpenAI)\": 1, \"License Share Alike (DataProvenance IgnoreOpenAI)\": 0, \"License Type\": \"Commercial\"}, {\"Unique Dataset Identifier\": \"hmdb-dataset\", \"Collection\": \"hmdb-dataset\", \"Collection URL\": \"https://ieeexplore.ieee.org/document/6126543\", \"Dataset Name\": \"HMDB: A Large Video Database for Human Motion Recognition (ICCV 2011)\", \"Paper Title\": \"HMDB: A Large Video Database for Human Motion Recognition (ICCV 2011)\", \"Paper URL\": \"https://ieeexplore.ieee.org/document/6126543\", \"GitHub URL\": \"\", \"Hugging Face URL\": \"\", \"Papers with Code URL\": \"https://paperswithcode.com/dataset/hmdb51\", \"ArXiv URL\": \"https://ieeexplore.ieee.org/document/6126543\", \"Semantic Scholar Corpus ID\": 206769852, \"Year Released\": \"2011\", \"Text Sources\": [\"movies\", \"prelinger archive\", \"undisclosed web\", \"youtube\", \"google videos\"], \"Licenses\": [{\"License\": \"CC BY 4.0\", \"License URL\": \"https://serre-lab.clps.brown.edu/resource/hmdb-a-large-human-motion-database/\"}], \"Creators\": [\"Karlsruhe Institute of Technology\", \"Massachusetts Institute of Technology\", \"Brown University\"], \"Countries\": [\"United States of America\", \"Germany\"], \"License Verified By\": \"Vivek Sharma\", \"Video Hours\": 7000.0, \"Taken Down\": \"False\", \"Video Sources\": \"youtube\", \"Task Categories\": [\"Video Classification\"], \"Inferred Metadata\": {\"GitHub License\": \"\", \"GitHub Stars (June 2024)\": \"\", \"GitHub Topics\": \"\", \"Github Date\": \"\", \"HF Config\": \"\", \"HF Config License\": \"\", \"HF Dataset\": \"\", \"HF Date\": \"\", \"HF Downloads (June 2024)\": \"\", \"HF Likes (June 2024)\": \"\", \"HF Yaml License\": \"\", \"PwC Date\": \"2011-01-01\", \"PwC Description\": \"The HMDB51 dataset is a large collection of realistic videos from various sources, including movies and web videos. The dataset is composed of 6,766 video clips from 51 action categories (such as \\u201cjump\\u201d, \\u201ckiss\\u201d and \\u201claugh\\u201d), with each category containing at least 101 clips. The original evaluation scheme uses three different training/testing splits. In each split, each action class has 70 clips for training and 30 clips for testing. The average accuracy over these three splits is used to measure the final performance.\", \"PwC License Name\": \"CC BY 4.0\", \"PwC License URL\": \"https://creativecommons.org/licenses/by/4.0/\", \"S2 Citation Count (June 2024)\": \"\", \"S2 Date\": \"\"}, \"Bibtex\": \"@Article{Kuehne2011HMDBAL,\\n author = {Hilde Kuehne and Hueihan Jhuang and Est\\u00edbaliz Garrote and T. Poggio and Thomas Serre},\\n booktitle = {Vision},\\n journal = {2011 International Conference on Computer Vision},\\n pages = {2556-2563},\\n title = {HMDB: A large video database for human motion recognition},\\n year = {2011}\\n}\\n\", \"License Use (DataProvenance)\": \"commercial\", \"License Attribution (DataProvenance)\": 1, \"License Share Alike (DataProvenance)\": 0, \"License Use (DataProvenance IgnoreOpenAI)\": \"commercial\", \"License Attribution (DataProvenance IgnoreOpenAI)\": 1, \"License Share Alike (DataProvenance IgnoreOpenAI)\": 0, \"License Type\": \"Commercial\"}, {\"Unique Dataset Identifier\": \"multi-thumos-challenge\", \"Collection\": \"multi-thumos-challenge\", \"Collection URL\": \"https://arxiv.org/pdf/1507.05738v3.pdf\", \"Dataset Name\": \"MultiTHUMOS: Every Moment Counts: Dense Detailed Labeling of Actions in Complex Videos (IJCV 2017)\", \"Paper Title\": \"MultiTHUMOS: Every Moment Counts: Dense Detailed Labeling of Actions in Complex Videos (IJCV 2017)\", \"Paper URL\": \"https://arxiv.org/pdf/1507.05738v3.pdf\", \"GitHub URL\": \"\", \"Hugging Face URL\": \"\", \"Papers with Code URL\": \"https://paperswithcode.com/dataset/multithumos\", \"ArXiv URL\": \"https://arxiv.org/pdf/1507.05738v3.pdf\", \"Semantic Scholar Corpus ID\": 3337929, \"Year Released\": \"2017\", \"Text Sources\": [\"youtube\"], \"Licenses\": [{\"License\": \"CC BY 4.0\", \"License URL\": \"https://ai.stanford.edu/~syyeung/resources/multithumos.zip\"}], \"Creators\": [\"Stanford University\", \"Carnegie Mellon University\", \"Simon Fraser University\"], \"Countries\": [\"United States of America\", \"Canada\"], \"License Verified By\": \"Vivek Sharma\", \"Video Hours\": 30.0, \"Taken Down\": \"False\", \"Video Sources\": \"youtube\", \"Task Categories\": [\"Video Classification\"], \"Inferred Metadata\": {\"GitHub License\": \"\", \"GitHub Stars (June 2024)\": \"\", \"GitHub Topics\": \"\", \"Github Date\": \"\", \"HF Config\": \"\", \"HF Config License\": \"\", \"HF Dataset\": \"\", \"HF Date\": \"\", \"HF Downloads (June 2024)\": \"\", \"HF Likes (June 2024)\": \"\", \"HF Yaml License\": \"\", \"PwC Date\": \"2018-01-01\", \"PwC Description\": \"The MultiTHUMOS dataset contains dense, multilabel, frame-level action annotations for 30 hours across 400 videos in the THUMOS'14 action detection dataset. It consists of 38,690 annotations of 65 action classes, with an average of 1.5 labels per frame and 10.5 action classes per video.\", \"PwC License Name\": \"CC BY 4.0\", \"PwC License URL\": null, \"S2 Citation Count (June 2024)\": \"\", \"S2 Date\": \"\"}, \"Bibtex\": \"@Article{Yeung2015EveryMC,\\n author = {Serena Yeung and Olga Russakovsky and Ning Jin and Mykhaylo Andriluka and Greg Mori and Li Fei-Fei},\\n booktitle = {International Journal of Computer Vision},\\n journal = {International Journal of Computer Vision},\\n pages = {375 - 389},\\n title = {Every Moment Counts: Dense Detailed Labeling of Actions in Complex Videos},\\n volume = {126},\\n year = {2015}\\n}\\n\", \"License Use (DataProvenance)\": \"commercial\", \"License Attribution (DataProvenance)\": 1, \"License Share Alike (DataProvenance)\": 0, \"License Use (DataProvenance IgnoreOpenAI)\": \"commercial\", \"License Attribution (DataProvenance IgnoreOpenAI)\": 1, \"License Share Alike (DataProvenance IgnoreOpenAI)\": 0, \"License Type\": \"Commercial\"}, {\"Unique Dataset Identifier\": \"YT-Temporal-180m\", \"Collection\": \"YT-Temporal-180m\", \"Collection URL\": \"https://arxiv.org/pdf/2106.02636\", \"Dataset Name\": \"MERLOT: Multimodal Neural Script Knowledge Models\", \"Paper Title\": \"MERLOT: Multimodal Neural Script Knowledge Models\", \"Paper URL\": \"https://arxiv.org/pdf/2106.02636\", \"GitHub URL\": \"https://github.com/rowanz/merlot\", \"Hugging Face URL\": \"https://huggingface.co/datasets/HuggingFaceM4/yttemporal180m\", \"Papers with Code URL\": \"https://paperswithcode.com/paper/merlot-multimodal-neural-script-knowledge\", \"ArXiv URL\": \"https://arxiv.org/pdf/2106.02636\", \"Semantic Scholar Corpus ID\": 235352775, \"Year Released\": \"2021\", \"Text Sources\": [\"youtube\"], \"Licenses\": [{\"License\": \"MIT License\", \"License URL\": \"https://github.com/rowanz/merlot/blob/main/LICENSE\"}], \"Creators\": [\"University of Washington\", \"Allen Institute for AI\"], \"Countries\": [\"United States of America\"], \"License Verified By\": \"Vivek Sharma\", \"Video Hours\": 1515.0, \"Taken Down\": \"False\", \"Video Sources\": \"youtube\", \"Task Categories\": [\"Video Q&A\"], \"Inferred Metadata\": null, \"Bibtex\": null, \"License Use (DataProvenance)\": \"commercial\", \"License Attribution (DataProvenance)\": 1, \"License Share Alike (DataProvenance)\": 0, \"License Use (DataProvenance IgnoreOpenAI)\": \"commercial\", \"License Attribution (DataProvenance IgnoreOpenAI)\": 1, \"License Share Alike (DataProvenance IgnoreOpenAI)\": 0, \"License Type\": \"Commercial\"}, {\"Unique Dataset Identifier\": \"mimetics-dataset\", \"Collection\": \"mimetics-dataset\", \"Collection URL\": \"https://arxiv.org/abs/1912.07249\", \"Dataset Name\": \"Mimetics: Towards Understanding Human Actions Out of Context\", \"Paper Title\": \"Mimetics: Towards Understanding Human Actions Out of Context\", \"Paper URL\": \"https://arxiv.org/abs/1912.07249\", \"GitHub URL\": \"\", \"Hugging Face URL\": \"\", \"Papers with Code URL\": \"https://paperswithcode.com/paper/mimetics-towards-understanding-human-actions\", \"ArXiv URL\": \"https://arxiv.org/abs/1912.07249\", \"Semantic Scholar Corpus ID\": 209376248, \"Year Released\": \"2021\", \"Text Sources\": [\"youtube\"], \"Licenses\": [{\"License\": \"Unspecified\", \"License URL\": \"\"}], \"Creators\": [\"Naver\"], \"Countries\": [\"South Korea\"], \"License Verified By\": \"Vivek Sharma\", \"Video Hours\": 0.99, \"Taken Down\": \"False\", \"Video Sources\": \"youtube\", \"Task Categories\": [\"Video Classification\"], \"Inferred Metadata\": {\"GitHub License\": \"\", \"GitHub Stars (June 2024)\": \"\", \"GitHub Topics\": \"\", \"Github Date\": \"\", \"HF Config\": \"\", \"HF Config License\": \"\", \"HF Dataset\": \"\", \"HF Date\": \"\", \"HF Downloads (June 2024)\": \"\", \"HF Likes (June 2024)\": \"\", \"HF Yaml License\": \"\", \"PwC Date\": \"\", \"PwC Description\": \"No code available yet.\", \"PwC License Name\": \"\", \"PwC License URL\": \"\", \"S2 Citation Count (June 2024)\": \"\", \"S2 Date\": \"\"}, \"Bibtex\": \"@Article{Weinzaepfel2019MimeticsTU,\\n author = {Philippe Weinzaepfel and Gr\\u00e9gory Rogez},\\n booktitle = {International Journal of Computer Vision},\\n journal = {International Journal of Computer Vision},\\n pages = {1675 - 1690},\\n title = {Mimetics: Towards Understanding Human Actions Out of Context},\\n volume = {129},\\n year = {2019}\\n}\\n\", \"License Use (DataProvenance)\": \"unspecified\", \"License Attribution (DataProvenance)\": 0, \"License Share Alike (DataProvenance)\": 0, \"License Use (DataProvenance IgnoreOpenAI)\": \"unspecified\", \"License Attribution (DataProvenance IgnoreOpenAI)\": 0, \"License Share Alike (DataProvenance IgnoreOpenAI)\": 0, \"License Type\": \"Unspecified\"}, {\"Unique Dataset Identifier\": \"youcook-2\", \"Collection\": \"youcook-2\", \"Collection URL\": \"https://arxiv.org/abs/1703.09788\", \"Dataset Name\": \"YouCook2\", \"Paper Title\": \"YouCook2\", \"Paper URL\": \"https://arxiv.org/abs/1703.09788\", \"GitHub URL\": \"\", \"Hugging Face URL\": \"\", \"Papers with Code URL\": \"https://paperswithcode.com/dataset/youcook2\", \"ArXiv URL\": \"https://arxiv.org/abs/1703.09788\", \"Semantic Scholar Corpus ID\": 19713015, \"Year Released\": \"2017\", \"Text Sources\": [\"youtube\"], \"Licenses\": [{\"License\": \"MIT License\", \"License URL\": \"http://youcook2.eecs.umich.edu/static/YouCookII/LICENSE_YOUCOOK2.txt\"}], \"Creators\": [\"University of Rochester\", \"University of Michigan\"], \"Countries\": [\"United States of America\"], \"License Verified By\": \"Vivek Sharma\", \"Video Hours\": 175.6, \"Taken Down\": \"False\", \"Video Sources\": \"youtube\", \"Task Categories\": [\"Video Captioning\"], \"Inferred Metadata\": {\"GitHub License\": \"\", \"GitHub Stars (June 2024)\": \"\", \"GitHub Topics\": \"\", \"Github Date\": \"\", \"HF Config\": \"\", \"HF Config License\": \"\", \"HF Dataset\": \"\", \"HF Date\": \"\", \"HF Downloads (June 2024)\": \"\", \"HF Likes (June 2024)\": \"\", \"HF Yaml License\": \"\", \"PwC Date\": \"2018-01-01\", \"PwC Description\": \"YouCook2 is the largest task-oriented, instructional video dataset in the vision community. It contains 2000 long untrimmed videos from 89 cooking recipes; on average, each distinct recipe has 22 videos. The procedure steps for each video are annotated with temporal boundaries and described by imperative English sentences (see the example below). The videos were downloaded from YouTube and are all in the third-person viewpoint. All the videos are unconstrained and can be performed by individual persons at their houses with unfixed cameras. YouCook2 contains rich recipe types and various cooking styles from all over the world.\", \"PwC License Name\": \"Various\", \"PwC License URL\": \"http://youcook2.eecs.umich.edu/download\", \"S2 Citation Count (June 2024)\": \"\", \"S2 Date\": \"\"}, \"Bibtex\": \"@Article{Zhou2017TowardsAL,\\n author = {Luowei Zhou and Chenliang Xu and Jason J. Corso},\\n booktitle = {AAAI Conference on Artificial Intelligence},\\n pages = {7590-7598},\\n title = {Towards Automatic Learning of Procedures From Web Instructional Videos},\\n year = {2017}\\n}\\n\", \"License Use (DataProvenance)\": \"commercial\", \"License Attribution (DataProvenance)\": 1, \"License Share Alike (DataProvenance)\": 0, \"License Use (DataProvenance IgnoreOpenAI)\": \"commercial\", \"License Attribution (DataProvenance IgnoreOpenAI)\": 1, \"License Share Alike (DataProvenance IgnoreOpenAI)\": 0, \"License Type\": \"Commercial\"}, {\"Unique Dataset Identifier\": \"how2\", \"Collection\": \"how2\", \"Collection URL\": \"https://arxiv.org/abs/1811.00347\", \"Dataset Name\": \"How2: A Large-scale Dataset for Multimodal Language Understanding (NeurIPS 2018)\", \"Paper Title\": \"How2: A Large-scale Dataset for Multimodal Language Understanding (NeurIPS 2018)\", \"Paper URL\": \"https://arxiv.org/abs/1811.00347\", \"GitHub URL\": \"https://github.com/srvk/how2-dataset\", \"Hugging Face URL\": \"\", \"Papers with Code URL\": \"\", \"ArXiv URL\": \"https://arxiv.org/abs/1811.00347\", \"Semantic Scholar Corpus ID\": 53186236, \"Year Released\": \"2018\", \"Text Sources\": [\"youtube\"], \"Licenses\": [{\"License\": \"Various\", \"License URL\": \"https://github.com/srvk/how2-dataset?tab=readme-ov-file#how2-license\"}], \"Creators\": [\"Carnegie Mellon University\"], \"Countries\": [\"United States of America\"], \"License Verified By\": \"Vivek Sharma\", \"Video Hours\": 2300.0, \"Taken Down\": \"False\", \"Video Sources\": \"youtube\", \"Task Categories\": [\"Video Captioning\"], \"Inferred Metadata\": {\"GitHub License\": \"\", \"GitHub Stars (June 2024)\": 150, \"GitHub Topics\": [\"corpus\", \"dataset\", \"how2-dataset\", \"language\", \"machine-translation\", \"multimodality\", \"speech-recognition\", \"video\"], \"Github Date\": \"\", \"HF Config\": \"\", \"HF Config License\": \"\", \"HF Dataset\": \"\", \"HF Date\": \"\", \"HF Downloads (June 2024)\": \"\", \"HF Likes (June 2024)\": \"\", \"HF Yaml License\": \"\", \"PwC Date\": \"\", \"PwC Description\": \"\", \"PwC License Name\": \"\", \"PwC License URL\": \"\", \"S2 Citation Count (June 2024)\": \"\", \"S2 Date\": \"\"}, \"Bibtex\": \"@Article{Sanabria2018How2AL,\\n author = {Ramon Sanabria and Ozan Caglayan and Shruti Palaskar and Desmond Elliott and Lo\\u00efc Barrault and Lucia Specia and Florian Metze},\\n booktitle = {Neural Information Processing Systems},\\n journal = {ArXiv},\\n title = {How2: A Large-scale Dataset for Multimodal Language Understanding},\\n volume = {abs/1811.00347},\\n year = {2018}\\n}\\n\", \"License Use (DataProvenance)\": \"academic-only\", \"License Attribution (DataProvenance)\": 1, \"License Share Alike (DataProvenance)\": 1, \"License Use (DataProvenance IgnoreOpenAI)\": \"academic-only\", \"License Attribution (DataProvenance IgnoreOpenAI)\": 1, \"License Share Alike (DataProvenance IgnoreOpenAI)\": 1, \"License Type\": \"Non-Commercial/Academic\"}, {\"Unique Dataset Identifier\": \"crosstask\", \"Collection\": \"crosstask\", \"Collection URL\": \"https://arxiv.org/abs/1903.08225\", \"Dataset Name\": \"CrossTask: weakly supervised learning from instructional videos (CVPR 2019)\", \"Paper Title\": \"CrossTask: weakly supervised learning from instructional videos (CVPR 2019)\", \"Paper URL\": \"https://arxiv.org/abs/1903.08225\", \"GitHub URL\": \"https://github.com/DmZhukov/CrossTask\", \"Hugging Face URL\": \"\", \"Papers with Code URL\": \"https://paperswithcode.com/dataset/crosstask\", \"ArXiv URL\": \"https://arxiv.org/abs/1903.08225\", \"Semantic Scholar Corpus ID\": 84187266, \"Year Released\": \"2019\", \"Text Sources\": [\"youtube\"], \"Licenses\": [{\"License\": \"Unspecified\", \"License URL\": \"\"}], \"Creators\": [\"Ecole Normale Sup\\u00e9rieure\", \"Inria\", \"CIIRC\", \"Czech Technical University\", \"University of Michigan\"], \"Countries\": [\"France\", \"Turkey\", \"United States of America\", \"Czech Republic\"], \"License Verified By\": \"Vivek Sharma\", \"Video Hours\": 376.0, \"Taken Down\": \"False\", \"Video Sources\": \"youtube\", \"Task Categories\": [\"Temporal Action Localization\"], \"Inferred Metadata\": {\"GitHub License\": \"BSD 3-Clause License\", \"GitHub Stars (June 2024)\": 83, \"GitHub Topics\": [], \"Github Date\": \"\", \"HF Config\": \"\", \"HF Config License\": \"\", \"HF Dataset\": \"\", \"HF Date\": \"\", \"HF Downloads (June 2024)\": \"\", \"HF Likes (June 2024)\": \"\", \"HF Yaml License\": \"\", \"PwC Date\": \"2019-01-01\", \"PwC Description\": \"CrossTask dataset contains instructional videos, collected for 83 different tasks. For each task an ordered list of steps with manual descriptions is provided. The dataset is divided in two parts: 18 primary and 65 related tasks. Videos for the primary tasks are collected manually and provided with annotations for temporal step boundaries. Videos for the related tasks are collected automatically and don't have annotations.\", \"PwC License Name\": \"Unspecified\", \"PwC License URL\": null, \"S2 Citation Count (June 2024)\": \"\", \"S2 Date\": \"\"}, \"Bibtex\": \"@Article{Zhukov2019CrossTaskWS,\\n author = {Dimitri Zhukov and Jean-Baptiste Alayrac and R. G. Cinbis and D. Fouhey and I. Laptev and Josef Sivic},\\n booktitle = {Computer Vision and Pattern Recognition},\\n journal = {2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},\\n pages = {3532-3540},\\n title = {Cross-Task Weakly Supervised Learning From Instructional Videos},\\n year = {2019}\\n}\\n\", \"License Use (DataProvenance)\": \"unspecified\", \"License Attribution (DataProvenance)\": 0, \"License Share Alike (DataProvenance)\": 0, \"License Use (DataProvenance IgnoreOpenAI)\": \"unspecified\", \"License Attribution (DataProvenance IgnoreOpenAI)\": 0, \"License Share Alike (DataProvenance IgnoreOpenAI)\": 0, \"License Type\": \"Unspecified\"}, {\"Unique Dataset Identifier\": \"egopet\", \"Collection\": \"egopet\", \"Collection URL\": \"https://arxiv.org/pdf/2404.09991\", \"Dataset Name\": \"EgoPet: Egomotion and Interaction Data from an Animal's Perspective\", \"Paper Title\": \"EgoPet: Egomotion and Interaction Data from an Animal's Perspective\", \"Paper URL\": \"https://arxiv.org/pdf/2404.09991\", \"GitHub URL\": \"\", \"Hugging Face URL\": \"\", \"Papers with Code URL\": \"https://paperswithcode.com/paper/egopet-egomotion-and-interaction-data-from-an\", \"ArXiv URL\": \"https://arxiv.org/pdf/2404.09991\", \"Semantic Scholar Corpus ID\": 269148727, \"Year Released\": \"2024\", \"Text Sources\": [\"tiktok\", \"youtube\"], \"Licenses\": [{\"License\": \"CC BY-NC 4.0\", \"License URL\": \"https://github.com/DannyTran123/egopet/blob/main/LICENSE\"}], \"Creators\": [\"Tel Aviv University\", \"UC Berkeley\", \"New York University\"], \"Countries\": [\"United States of America\", \"Israel\"], \"License Verified By\": \"Vivek Sharma\", \"Video Hours\": 84.0, \"Taken Down\": \"False\", \"Video Sources\": \"youtube\", \"Task Categories\": [\"Misc (Locomotion Prediction\", \"Visual Interaction Prediction\", \"Vision to Proprioception Prediction)\"], \"Inferred Metadata\": null, \"Bibtex\": null, \"License Use (DataProvenance)\": \"non-commercial\", \"License Attribution (DataProvenance)\": 1, \"License Share Alike (DataProvenance)\": 0, \"License Use (DataProvenance IgnoreOpenAI)\": \"non-commercial\", \"License Attribution (DataProvenance IgnoreOpenAI)\": 1, \"License Share Alike (DataProvenance IgnoreOpenAI)\": 0, \"License Type\": \"Non-Commercial/Academic\"}, {\"Unique Dataset Identifier\": \"oops-dataset\", \"Collection\": \"oops-dataset\", \"Collection URL\": \"https://openaccess.thecvf.com/content_CVPR_2020/papers/Epstein_Oops_Predicting_Unintentional_Action_in_Video_CVPR_2020_paper.pdf\", \"Dataset Name\": \"Oops!: Predicting Unintentional Action in Video (CVPR 2020)\", \"Paper Title\": \"Oops!: Predicting Unintentional Action in Video (CVPR 2020)\", \"Paper URL\": \"https://openaccess.thecvf.com/content_CVPR_2020/papers/Epstein_Oops_Predicting_Unintentional_Action_in_Video_CVPR_2020_paper.pdf\", \"GitHub URL\": \"https://github.com/cvlab-columbia/oops\", \"Hugging Face URL\": \"\", \"Papers with Code URL\": \"https://paperswithcode.com/paper/oops-predicting-unintentional-action-in-video\", \"ArXiv URL\": \"https://openaccess.thecvf.com/content_CVPR_2020/papers/Epstein_Oops_Predicting_Unintentional_Action_in_Video_CVPR_2020_paper.pdf\", \"Semantic Scholar Corpus ID\": 208291335, \"Year Released\": \"2020\", \"Text Sources\": [\"youtube\"], \"Licenses\": [{\"License\": \"CC BY-NC-SA 4.0\", \"License URL\": \"https://oops.cs.columbia.edu/data/\"}], \"Creators\": [\"Columbia University\"], \"Countries\": [\"United States of America\"], \"License Verified By\": \"Vivek Sharma\", \"Video Hours\": 50.0, \"Taken Down\": \"False\", \"Video Sources\": \"youtube\", \"Task Categories\": [\"Video Classification\"], \"Inferred Metadata\": {\"GitHub License\": \"MIT License\", \"GitHub Stars (June 2024)\": 77, \"GitHub Topics\": [], \"Github Date\": \"\", \"HF Config\": \"\", \"HF Config License\": \"\", \"HF Dataset\": \"\", \"HF Date\": \"\", \"HF Downloads (June 2024)\": \"\", \"HF Likes (June 2024)\": \"\", \"HF Yaml License\": \"\", \"PwC Date\": \"\", \"PwC Description\": \"Implemented in one code library.\", \"PwC License Name\": \"\", \"PwC License URL\": \"\", \"S2 Citation Count (June 2024)\": \"\", \"S2 Date\": \"\"}, \"Bibtex\": \"@Article{Epstein2019OopsPU,\\n author = {Dave Epstein and Boyuan Chen and Carl Vondrick},\\n booktitle = {Computer Vision and Pattern Recognition},\\n journal = {2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},\\n pages = {916-926},\\n title = {Oops! Predicting Unintentional Action in Video},\\n year = {2019}\\n}\\n\", \"License Use (DataProvenance)\": \"non-commercial\", \"License Attribution (DataProvenance)\": 1, \"License Share Alike (DataProvenance)\": 1, \"License Use (DataProvenance IgnoreOpenAI)\": \"non-commercial\", \"License Attribution (DataProvenance IgnoreOpenAI)\": 1, \"License Share Alike (DataProvenance IgnoreOpenAI)\": 1, \"License Type\": \"Non-Commercial/Academic\"}, {\"Unique Dataset Identifier\": \"hacs-dataset\", \"Collection\": \"hacs-dataset\", \"Collection URL\": \"https://arxiv.org/abs/1712.09374\", \"Dataset Name\": \"HACS: Human Action Clips and Segments Dataset for Recognition and Temporal Localization\", \"Paper Title\": \"HACS: Human Action Clips and Segments Dataset for Recognition and Temporal Localization\", \"Paper URL\": \"https://arxiv.org/abs/1712.09374\", \"GitHub URL\": \"https://github.com/hangzhaomit/HACS-dataset\", \"Hugging Face URL\": \"\", \"Papers with Code URL\": \"https://paperswithcode.com/dataset/hacs\", \"ArXiv URL\": \"https://arxiv.org/abs/1712.09374\", \"Semantic Scholar Corpus ID\": 68049510, \"Year Released\": \"2019\", \"Text Sources\": [\"youtube\"], \"Licenses\": [{\"License\": \"Custom\", \"License URL\": \"https://github.com/hangzhaomit/HACS-dataset?tab=readme-ov-file#request-testing-videos-and-missing-videos-new\"}], \"Creators\": [\"Massachusetts Institute of Technology\", \"Dartmouth University\", \"UIUC\"], \"Countries\": [\"United States of America\"], \"License Verified By\": \"Vivek Sharma\", \"Video Hours\": 833.0, \"Taken Down\": \"False\", \"Video Sources\": \"youtube\", \"Task Categories\": [\"Video Classification\"], \"Inferred Metadata\": {\"GitHub License\": \"BSD 3-Clause License\", \"GitHub Stars (June 2024)\": 184, \"GitHub Topics\": [], \"Github Date\": \"\", \"HF Config\": \"\", \"HF Config License\": \"\", \"HF Dataset\": \"\", \"HF Date\": \"\", \"HF Downloads (June 2024)\": \"\", \"HF Likes (June 2024)\": \"\", \"HF Yaml License\": \"\", \"PwC Date\": \"\", \"PwC Description\": \"HACS is a dataset for human action recognition. It uses a taxonomy of 200 action classes, which is identical to that of the ActivityNet-v1.3 dataset. It has 504K videos retrieved from YouTube. Each one is strictly shorter than 4 minutes, and the average length is 2.6 minutes. A total of 1.5M clips of 2-second duration are sparsely sampled by methods based on both uniform randomness and consensus/disagreement of image classifiers. 0.6M and 0.9M clips are annotated as positive and negative samples, respectively.\\n\\nAuthors split the collection into training, validation and testing sets of size 1.4M, 50K and 50K clips, which are sampled\\nfrom 492K, 6K and 6K videos, respectively.\", \"PwC License Name\": \"BSD 3-Clause License\", \"PwC License URL\": \"https://github.com/hangzhaomit/HACS-dataset/blob/master/LICENSE\", \"S2 Citation Count (June 2024)\": \"\", \"S2 Date\": \"\"}, \"Bibtex\": \"@Article{Zhao2017HACSHA,\\n author = {Hang Zhao and A. Torralba and L. Torresani and Zhicheng Yan},\\n booktitle = {IEEE International Conference on Computer Vision},\\n journal = {2019 IEEE/CVF International Conference on Computer Vision (ICCV)},\\n pages = {8667-8677},\\n title = {HACS: Human Action Clips and Segments Dataset for Recognition and Temporal Localization},\\n year = {2017}\\n}\\n\", \"License Use (DataProvenance)\": \"academic-only\", \"License Attribution (DataProvenance)\": 1, \"License Share Alike (DataProvenance)\": 0, \"License Use (DataProvenance IgnoreOpenAI)\": \"academic-only\", \"License Attribution (DataProvenance IgnoreOpenAI)\": 1, \"License Share Alike (DataProvenance IgnoreOpenAI)\": 0, \"License Type\": \"Non-Commercial/Academic\"}, {\"Unique Dataset Identifier\": \"youtube-8m\", \"Collection\": \"youtube-8m\", \"Collection URL\": \"https://arxiv.org/abs/1609.08675\", \"Dataset Name\": \"Youtube-8M: A Large-Scale Video Classification Benchmark\", \"Paper Title\": \"Youtube-8M: A Large-Scale Video Classification Benchmark\", \"Paper URL\": \"https://arxiv.org/abs/1609.08675\", \"GitHub URL\": \"\", \"Hugging Face URL\": \"\", \"Papers with Code URL\": \"https://paperswithcode.com/dataset/youtube-8m\", \"ArXiv URL\": \"https://arxiv.org/abs/1609.08675\", \"Semantic Scholar Corpus ID\": 11241677, \"Year Released\": \"2016\", \"Text Sources\": [\"youtube\"], \"Licenses\": [{\"License\": \"Unspecified\", \"License URL\": \"\"}], \"Creators\": [\"Google Research\"], \"Countries\": [\"United States of America\"], \"License Verified By\": \"Vivek Sharma\", \"Video Hours\": 350000.0, \"Taken Down\": \"False\", \"Video Sources\": \"youtube\", \"Task Categories\": [\"Video Classification\"], \"Inferred Metadata\": {\"GitHub License\": \"\", \"GitHub Stars (June 2024)\": \"\", \"GitHub Topics\": \"\", \"Github Date\": \"\", \"HF Config\": \"\", \"HF Config License\": \"\", \"HF Dataset\": \"\", \"HF Date\": \"\", \"HF Downloads (June 2024)\": \"\", \"HF Likes (June 2024)\": \"\", \"HF Yaml License\": \"\", \"PwC Date\": \"2016-01-01\", \"PwC Description\": \"The YouTube-8M dataset is a large scale video dataset, which includes more than 7 million videos with 4716 classes labeled by the annotation system. The dataset consists of three parts: training set, validate set, and test set. In the training set, each class contains at least 100 training videos. Features of these videos are extracted by the state-of-the-art popular pre-trained models and released for public use. Each video contains audio and visual modality. Based on the visual information, videos are divided into 24 topics, such as sports, game, arts & entertainment, etc\", \"PwC License Name\": \"CC BY 4.0\", \"PwC License URL\": \"https://creativecommons.org/licenses/by/4.0/\", \"S2 Citation Count (June 2024)\": \"\", \"S2 Date\": \"\"}, \"Bibtex\": \"@Article{Abu-El-Haija2016YouTube8MAL,\\n author = {Sami Abu-El-Haija and Nisarg Kothari and Joonseok Lee and A. Natsev and G. Toderici and Balakrishnan Varadarajan and Sudheendra Vijayanarasimhan},\\n booktitle = {arXiv.org},\\n journal = {ArXiv},\\n title = {YouTube-8M: A Large-Scale Video Classification Benchmark},\\n volume = {abs/1609.08675},\\n year = {2016}\\n}\\n\", \"License Use (DataProvenance)\": \"unspecified\", \"License Attribution (DataProvenance)\": 0, \"License Share Alike (DataProvenance)\": 0, \"License Use (DataProvenance IgnoreOpenAI)\": \"unspecified\", \"License Attribution (DataProvenance IgnoreOpenAI)\": 0, \"License Share Alike (DataProvenance IgnoreOpenAI)\": 0, \"License Type\": \"Unspecified\"}, {\"Unique Dataset Identifier\": \"hd-vila-100m\", \"Collection\": \"hd-vila-100m\", \"Collection URL\": \"https://arxiv.org/abs/2111.10337\", \"Dataset Name\": \"Advancing High-Resolution Video-Language Representation with Large-Scale Video Transcriptions\", \"Paper Title\": \"Advancing High-Resolution Video-Language Representation with Large-Scale Video Transcriptions\", \"Paper URL\": \"https://arxiv.org/abs/2111.10337\", \"GitHub URL\": \"https://github.com/microsoft/XPretrain/blob/main/hd-vila-100m/README.md\", \"Hugging Face URL\": \"\", \"Papers with Code URL\": \"https://paperswithcode.com/paper/advancing-high-resolution-video-language/review/\", \"ArXiv URL\": \"https://arxiv.org/abs/2111.10337\", \"Semantic Scholar Corpus ID\": 244462849, \"Year Released\": \"2021\", \"Text Sources\": [\"youtube\"], \"Licenses\": [{\"License\": \"Custom\", \"License URL\": \"https://github.com/microsoft/XPretrain/blob/main/hd-vila-100m/README.md\"}], \"Creators\": [\"Microsoft\"], \"Countries\": [\"United States of America\"], \"License Verified By\": \"Vivek Sharma\", \"Video Hours\": 371.5, \"Taken Down\": \"False\", \"Video Sources\": \"youtube\", \"Task Categories\": [\"Video Captioning\"], \"Inferred Metadata\": {\"GitHub License\": \"\", \"GitHub Stars (June 2024)\": 0, \"GitHub Topics\": [\"\"], \"Github Date\": \"\", \"HF Config\": \"\", \"HF Config License\": \"\", \"HF Dataset\": \"\", \"HF Date\": \"\", \"HF Downloads (June 2024)\": \"\", \"HF Likes (June 2024)\": \"\", \"HF Yaml License\": \"\", \"PwC Date\": \"\", \"PwC Description\": \"Paper tables with annotated results for Advancing High-Resolution Video-Language Representation with Large-Scale Video Transcriptions\", \"PwC License Name\": \"\", \"PwC License URL\": \"\", \"S2 Citation Count (June 2024)\": \"\", \"S2 Date\": \"\"}, \"Bibtex\": \"@Article{Xue2021AdvancingHV,\\n author = {Hongwei Xue and Tiankai Hang and Yanhong Zeng and Yuchong Sun and Bei Liu and Huan Yang and Jianlong Fu and B. Guo},\\n booktitle = {Computer Vision and Pattern Recognition},\\n journal = {2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},\\n pages = {5026-5035},\\n title = {Advancing High-Resolution Video-Language Representation with Large-Scale Video Transcriptions},\\n year = {2021}\\n}\\n\", \"License Use (DataProvenance)\": \"academic-only\", \"License Attribution (DataProvenance)\": 1, \"License Share Alike (DataProvenance)\": 0, \"License Use (DataProvenance IgnoreOpenAI)\": \"academic-only\", \"License Attribution (DataProvenance IgnoreOpenAI)\": 1, \"License Share Alike (DataProvenance IgnoreOpenAI)\": 0, \"License Type\": \"Non-Commercial/Academic\"}, {\"Unique Dataset Identifier\": \"queryd\", \"Collection\": \"queryd\", \"Collection URL\": \"https://arxiv.org/abs/2011.11071\", \"Dataset Name\": \"QuerYD\", \"Paper Title\": \"QuerYD\", \"Paper URL\": \"https://arxiv.org/abs/2011.11071\", \"GitHub URL\": \"\", \"Hugging Face URL\": \"\", \"Papers with Code URL\": \"https://paperswithcode.com/dataset/queryd\", \"ArXiv URL\": \"https://arxiv.org/abs/2011.11071\", \"Semantic Scholar Corpus ID\": 261006321, \"Year Released\": \"2021\", \"Text Sources\": [\"youtube\", \"youdescribe\"], \"Licenses\": [{\"License\": \"Unspecified\", \"License URL\": \"\"}], \"Creators\": [\"University of Oxford\"], \"Countries\": [\"United Kingdom\"], \"License Verified By\": \"Vivek Sharma\", \"Video Hours\": 207.0, \"Taken Down\": \"False\", \"Video Sources\": \"youtube\", \"Task Categories\": [\"Video Captioning\"], \"Inferred Metadata\": {\"GitHub License\": \"\", \"GitHub Stars (June 2024)\": \"\", \"GitHub Topics\": \"\", \"Github Date\": \"\", \"HF Config\": \"\", \"HF Config License\": \"\", \"HF Dataset\": \"\", \"HF Date\": \"\", \"HF Downloads (June 2024)\": \"\", \"HF Likes (June 2024)\": \"\", \"HF Yaml License\": \"\", \"PwC Date\": \"\", \"PwC Description\": \"A large-scale dataset for retrieval and event localisation in video. A unique feature of the dataset is the availability of two audio tracks for each video: the original audio, and a high-quality spoken description of the visual content.\", \"PwC License Name\": \"Unspecified\", \"PwC License URL\": null, \"S2 Citation Count (June 2024)\": \"\", \"S2 Date\": \"\"}, \"Bibtex\": \"@Article{Oncescu2021QUERYDAV,\\n author = {Andreea-Maria Oncescu and Jo\\u00e3o F. Henriques and Yang Liu and Andrew Zisserman and Samuel Albanie},\\n booktitle = {IEEE International Conference on Acoustics, Speech, and Signal Processing},\\n journal = {ICASSP 2021 - 2021 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},\\n pages = {2265-2269},\\n title = {QUERYD: A Video Dataset with High-Quality Text and Audio Narrations},\\n year = {2021}\\n}\\n\", \"License Use (DataProvenance)\": \"unspecified\", \"License Attribution (DataProvenance)\": 0, \"License Share Alike (DataProvenance)\": 0, \"License Use (DataProvenance IgnoreOpenAI)\": \"unspecified\", \"License Attribution (DataProvenance IgnoreOpenAI)\": 0, \"License Share Alike (DataProvenance IgnoreOpenAI)\": 0, \"License Type\": \"Unspecified\"}, {\"Unique Dataset Identifier\": \"kinetics-700\", \"Collection\": \"kinetics-700\", \"Collection URL\": \"https://arxiv.org/pdf/2010.10864.pdf\", \"Dataset Name\": \"Kinetics-700\", \"Paper Title\": \"Kinetics-700\", \"Paper URL\": \"https://arxiv.org/pdf/2010.10864.pdf\", \"GitHub URL\": \"\", \"Hugging Face URL\": \"\", \"Papers with Code URL\": \"https://paperswithcode.com/dataset/kinetics-700\", \"ArXiv URL\": \"https://arxiv.org/pdf/2010.10864.pdf\", \"Semantic Scholar Corpus ID\": 196831809, \"Year Released\": \"2020\", \"Text Sources\": [\"youtube\"], \"Licenses\": [{\"License\": \"Unspecified\", \"License URL\": \"\"}], \"Creators\": [\"DeepMind\"], \"Countries\": [\"United Kingdom\"], \"License Verified By\": \"Vivek Sharma\", \"Video Hours\": 1805.56, \"Taken Down\": \"False\", \"Video Sources\": \"youtube\", \"Task Categories\": [\"Video Classification\"], \"Inferred Metadata\": {\"GitHub License\": \"\", \"GitHub Stars (June 2024)\": \"\", \"GitHub Topics\": \"\", \"Github Date\": \"\", \"HF Config\": \"\", \"HF Config License\": \"\", \"HF Dataset\": \"\", \"HF Date\": \"\", \"HF Downloads (June 2024)\": \"\", \"HF Likes (June 2024)\": \"\", \"HF Yaml License\": \"\", \"PwC Date\": \"2019-07-15\", \"PwC Description\": \"Kinetics-700 is a video dataset of 650,000 clips that covers 700 human action classes. The videos include human-object interactions such as playing instruments, as well as human-human interactions such as shaking hands and hugging. Each action class has at least 700 video clips. Each clip is annotated with an action class and lasts approximately 10 seconds.\", \"PwC License Name\": \"CC BY 4.0\", \"PwC License URL\": \"https://creativecommons.org/licenses/by/4.0/\", \"S2 Citation Count (June 2024)\": \"\", \"S2 Date\": \"\"}, \"Bibtex\": \"@Article{Carreira2019ASN,\\n author = {Jo\\u00e3o Carreira and Eric Noland and Chloe Hillier and Andrew Zisserman},\\n booktitle = {arXiv.org},\\n journal = {ArXiv},\\n title = {A Short Note on the Kinetics-700 Human Action Dataset},\\n volume = {abs/1907.06987},\\n year = {2019}\\n}\\n\", \"License Use (DataProvenance)\": \"unspecified\", \"License Attribution (DataProvenance)\": 0, \"License Share Alike (DataProvenance)\": 0, \"License Use (DataProvenance IgnoreOpenAI)\": \"unspecified\", \"License Attribution (DataProvenance IgnoreOpenAI)\": 0, \"License Share Alike (DataProvenance IgnoreOpenAI)\": 0, \"License Type\": \"Unspecified\"}, {\"Unique Dataset Identifier\": \"videolt-dataset\", \"Collection\": \"videolt-dataset\", \"Collection URL\": \"https://arxiv.org/abs/2105.02668\", \"Dataset Name\": \"VideoLT: Large-scale Long-tailed Video Recognition\", \"Paper Title\": \"VideoLT: Large-scale Long-tailed Video Recognition\", \"Paper URL\": \"https://arxiv.org/abs/2105.02668\", \"GitHub URL\": \"https://github.com/17Skye17/VideoLT\", \"Hugging Face URL\": \"\", \"Papers with Code URL\": \"https://paperswithcode.com/dataset/videolt\", \"ArXiv URL\": \"https://arxiv.org/abs/2105.02668\", \"Semantic Scholar Corpus ID\": 233864776, \"Year Released\": \"2021\", \"Text Sources\": [\"youtube\"], \"Licenses\": [{\"License\": \"Non Commercial\", \"License URL\": \"https://github.com/17Skye17/VideoLT?tab=readme-ov-file#data-preparation\"}], \"Creators\": [\"Fudan University\", \"Shanghai Collaborative Innovation Center of Intelligent Visual Computing\", \"Inception Institute of Artificial Intelligence\", \"University of Maryland\"], \"Countries\": [\"China\", \"UAE\"], \"License Verified By\": \"Vivek Sharma\", \"Video Hours\": 13664.96, \"Taken Down\": \"False\", \"Video Sources\": \"youtube\", \"Task Categories\": [\"Video Classification\"], \"Inferred Metadata\": {\"GitHub License\": \"\", \"GitHub Stars (June 2024)\": 33, \"GitHub Topics\": [\"long-tailed-recognition\", \"video-classification\", \"video-dataset\"], \"Github Date\": \"\", \"HF Config\": \"\", \"HF Config License\": \"\", \"HF Dataset\": \"\", \"HF Date\": \"\", \"HF Downloads (June 2024)\": \"\", \"HF Likes (June 2024)\": \"\", \"HF Yaml License\": \"\", \"PwC Date\": \"2021-05-06\", \"PwC Description\": \"VideoLT is a large-scale long-tailed video recognition dataset that contains 256,218 untrimmed videos, annotated into 1,004 classes with a long-tailed distribution.\", \"PwC License Name\": \"Unspecified\", \"PwC License URL\": null, \"S2 Citation Count (June 2024)\": \"\", \"S2 Date\": \"\"}, \"Bibtex\": \"@Article{Zhang2021VideoLTLL,\\n author = {Xing Zhang and Zuxuan Wu and Zejia Weng and H. Fu and Jingjing Chen and Yu-Gang Jiang and Larry S. Davis},\\n booktitle = {IEEE International Conference on Computer Vision},\\n journal = {2021 IEEE/CVF International Conference on Computer Vision (ICCV)},\\n pages = {7940-7949},\\n title = {VideoLT: Large-scale Long-tailed Video Recognition},\\n year = {2021}\\n}\\n\", \"License Use (DataProvenance)\": \"non-commercial\", \"License Attribution (DataProvenance)\": 0, \"License Share Alike (DataProvenance)\": 0, \"License Use (DataProvenance IgnoreOpenAI)\": \"non-commercial\", \"License Attribution (DataProvenance IgnoreOpenAI)\": 0, \"License Share Alike (DataProvenance IgnoreOpenAI)\": 0, \"License Type\": \"Non-Commercial/Academic\"}, {\"Unique Dataset Identifier\": \"howto100m\", \"Collection\": \"howto100m\", \"Collection URL\": \"https://arxiv.org/abs/1906.03327\", \"Dataset Name\": \"HowTo100M\", \"Paper Title\": \"HowTo100M\", \"Paper URL\": \"https://arxiv.org/abs/1906.03327\", \"GitHub URL\": \"https://github.com/antoine77340/howto100m\", \"Hugging Face URL\": \"\", \"Papers with Code URL\": \"https://paperswithcode.com/dataset/howto100m\", \"ArXiv URL\": \"https://arxiv.org/abs/1906.03327\", \"Semantic Scholar Corpus ID\": 182952863, \"Year Released\": \"2019\", \"Text Sources\": [\"youtube\"], \"Licenses\": [{\"License\": \"Unspecified\", \"License URL\": \"\"}], \"Creators\": [\"Ecole Normale Sup\\u00e9rieure\", \"Inria\", \"CIIRC\", \"Czech Technical University\"], \"Countries\": [\"France\", \"Czech Republic\"], \"License Verified By\": \"Vivek Sharma\", \"Video Hours\": 134472.0, \"Taken Down\": \"False\", \"Video Sources\": \"youtube\", \"Task Categories\": [\"Video Captioning\"], \"Inferred Metadata\": {\"GitHub License\": \"Apache License 2.0\", \"GitHub Stars (June 2024)\": 242, \"GitHub Topics\": [], \"Github Date\": \"\", \"HF Config\": \"\", \"HF Config License\": \"\", \"HF Dataset\": \"\", \"HF Date\": \"\", \"HF Downloads (June 2024)\": \"\", \"HF Likes (June 2024)\": \"\", \"HF Yaml License\": \"\", \"PwC Date\": \"2019-06-07\", \"PwC Description\": \"HowTo100M is a large-scale dataset of narrated videos with an emphasis on instructional videos where content creators teach complex tasks with an explicit intention of explaining the visual content on screen. HowTo100M features a total of:\\n\\n\\n136M video clips with captions sourced from 1.2M Youtube videos (15 years of video)\\n23k activities from domains such as cooking, hand crafting, personal care, gardening or fitness\\n\\nEach video is associated with a narration available as subtitles automatically downloaded from Youtube.\", \"PwC License Name\": \"Various\", \"PwC License URL\": \"https://www.di.ens.fr/willow/research/howto100m/\", \"S2 Citation Count (June 2024)\": \"\", \"S2 Date\": \"\"}, \"Bibtex\": \"@Article{Miech2019HowTo100MLA,\\n author = {Antoine Miech and Dimitri Zhukov and Jean-Baptiste Alayrac and Makarand Tapaswi and I. Laptev and Josef Sivic},\\n booktitle = {IEEE International Conference on Computer Vision},\\n journal = {2019 IEEE/CVF International Conference on Computer Vision (ICCV)},\\n pages = {2630-2640},\\n title = {HowTo100M: Learning a Text-Video Embedding by Watching Hundred Million Narrated Video Clips},\\n year = {2019}\\n}\\n\", \"License Use (DataProvenance)\": \"unspecified\", \"License Attribution (DataProvenance)\": 0, \"License Share Alike (DataProvenance)\": 0, \"License Use (DataProvenance IgnoreOpenAI)\": \"unspecified\", \"License Attribution (DataProvenance IgnoreOpenAI)\": 0, \"License Share Alike (DataProvenance IgnoreOpenAI)\": 0, \"License Type\": \"Unspecified\"}, {\"Unique Dataset Identifier\": \"videostory\", \"Collection\": \"videostory\", \"Collection URL\": \"https://isis-data.science.uva.nl/cgmsnoek/pub/habibian-videostory-mm2014.pdf\", \"Dataset Name\": \"VideoStory: A New Multimedia Embedding for Few-Example Recognition and Translation of Events\", \"Paper Title\": \"VideoStory: A New Multimedia Embedding for Few-Example Recognition and Translation of Events\", \"Paper URL\": \"https://isis-data.science.uva.nl/cgmsnoek/pub/habibian-videostory-mm2014.pdf\", \"GitHub URL\": \"\", \"Hugging Face URL\": \"\", \"Papers with Code URL\": \"\", \"ArXiv URL\": \"https://isis-data.science.uva.nl/cgmsnoek/pub/habibian-videostory-mm2014.pdf\", \"Semantic Scholar Corpus ID\": 28203, \"Year Released\": \"2014\", \"Text Sources\": [\"youtube\"], \"Licenses\": [{\"License\": \"Unspecified\", \"License URL\": \"\"}], \"Creators\": [\"University of Amsterdam\"], \"Countries\": [\"Netherlands\"], \"License Verified By\": \"Vivek Sharma\", \"Video Hours\": 743.0, \"Taken Down\": \"False\", \"Video Sources\": \"youtube\", \"Task Categories\": [\"Video Captioning\"], \"Inferred Metadata\": {\"GitHub License\": \"\", \"GitHub Stars (June 2024)\": \"\", \"GitHub Topics\": \"\", \"Github Date\": \"\", \"HF Config\": \"\", \"HF Config License\": \"\", \"HF Dataset\": \"\", \"HF Date\": \"\", \"HF Downloads (June 2024)\": \"\", \"HF Likes (June 2024)\": \"\", \"HF Yaml License\": \"\", \"PwC Date\": \"\", \"PwC Description\": \"\", \"PwC License Name\": \"\", \"PwC License URL\": \"\", \"S2 Citation Count (June 2024)\": \"\", \"S2 Date\": \"\"}, \"Bibtex\": \"@Book{Habibian2014VideoStoryAN,\\n author = {A. Habibian and Thomas Mensink and Cees G. M. Snoek},\\n booktitle = {ACM Multimedia},\\n journal = {Proceedings of the 22nd ACM international conference on Multimedia},\\n title = {VideoStory: A New Multimedia Embedding for Few-Example Recognition and Translation of Events},\\n year = {2014}\\n}\\n\", \"License Use (DataProvenance)\": \"unspecified\", \"License Attribution (DataProvenance)\": 0, \"License Share Alike (DataProvenance)\": 0, \"License Use (DataProvenance IgnoreOpenAI)\": \"unspecified\", \"License Attribution (DataProvenance IgnoreOpenAI)\": 0, \"License Share Alike (DataProvenance IgnoreOpenAI)\": 0, \"License Type\": \"Unspecified\"}, {\"Unique Dataset Identifier\": \"100doh\", \"Collection\": \"100doh\", \"Collection URL\": \"https://arxiv.org/abs/2006.06669\", \"Dataset Name\": \"100DOH: Understanding Human Hands in Contact at Internet Scale (CVPR 2020)\", \"Paper Title\": \"100DOH: Understanding Human Hands in Contact at Internet Scale (CVPR 2020)\", \"Paper URL\": \"https://arxiv.org/abs/2006.06669\", \"GitHub URL\": \"https://github.com/ddshan/hand_object_detector\", \"Hugging Face URL\": \"\", \"Papers with Code URL\": \"https://paperswithcode.com/paper/understanding-human-hands-in-contact-at-1\", \"ArXiv URL\": \"https://arxiv.org/abs/2006.06669\", \"Semantic Scholar Corpus ID\": 215413188, \"Year Released\": \"2020\", \"Text Sources\": [\"youtube\"], \"Licenses\": [{\"License\": \"Custom\", \"License URL\": \"https://fouheylab.eecs.umich.edu/~dandans/projects/100DOH/download.html\"}], \"Creators\": [\"University of Michigan\", \"Johns Hopkins University\"], \"Countries\": [\"United States of America\"], \"License Verified By\": \"Vivek Sharma\", \"Video Hours\": 4577.3, \"Taken Down\": \"False\", \"Video Sources\": \"youtube\", \"Task Categories\": [\"Misc (Hand/Object Detection)\"], \"Inferred Metadata\": {\"GitHub License\": \"MIT License\", \"GitHub Stars (June 2024)\": 213, \"GitHub Topics\": [\"cvpr2020\", \"dataset\", \"handobjectdetection\", \"interactiondetection\"], \"Github Date\": \"\", \"HF Config\": \"\", \"HF Config License\": \"\", \"HF Dataset\": \"\", \"HF Date\": \"\", \"HF Downloads (June 2024)\": \"\", \"HF Likes (June 2024)\": \"\", \"HF Yaml License\": \"\", \"PwC Date\": \"\", \"PwC Description\": \"Implemented in one code library.\", \"PwC License Name\": \"\", \"PwC License URL\": \"\", \"S2 Citation Count (June 2024)\": \"\", \"S2 Date\": \"\"}, \"Bibtex\": \"@Article{Shan2020UnderstandingHH,\\n author = {Dandan Shan and Jiaqi Geng and Michelle Shu and D. Fouhey},\\n booktitle = {Computer Vision and Pattern Recognition},\\n journal = {2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},\\n pages = {9866-9875},\\n title = {Understanding Human Hands in Contact at Internet Scale},\\n year = {2020}\\n}\\n\", \"License Use (DataProvenance)\": \"academic-only\", \"License Attribution (DataProvenance)\": 1, \"License Share Alike (DataProvenance)\": 0, \"License Use (DataProvenance IgnoreOpenAI)\": \"academic-only\", \"License Attribution (DataProvenance IgnoreOpenAI)\": 1, \"License Share Alike (DataProvenance IgnoreOpenAI)\": 0, \"License Type\": \"Non-Commercial/Academic\"}, {\"Unique Dataset Identifier\": \"violin\", \"Collection\": \"violin\", \"Collection URL\": \"https://arxiv.org/abs/2003.11618\", \"Dataset Name\": \"VIOLIN\", \"Paper Title\": \"VIOLIN\", \"Paper URL\": \"https://arxiv.org/abs/2003.11618\", \"GitHub URL\": \"https://github.com/jimmy646/violin\", \"Hugging Face URL\": \"\", \"Papers with Code URL\": \"https://paperswithcode.com/dataset/violin\", \"ArXiv URL\": \"https://arxiv.org/abs/2003.11618\", \"Semantic Scholar Corpus ID\": 214668012, \"Year Released\": \"2020\", \"Text Sources\": [\"youtube\"], \"Licenses\": [{\"License\": \"Unspecified\", \"License URL\": \"\"}], \"Creators\": [\"Microsoft\"], \"Countries\": [\"United States of America\"], \"License Verified By\": \"Vivek Sharma\", \"Video Hours\": 582.0, \"Taken Down\": \"False\", \"Video Sources\": \"youtube\", \"Task Categories\": [\"Video Captioning\"], \"Inferred Metadata\": {\"GitHub License\": \"MIT License\", \"GitHub Stars (June 2024)\": 156, \"GitHub Topics\": [], \"Github Date\": \"\", \"HF Config\": \"\", \"HF Config License\": \"\", \"HF Dataset\": \"\", \"HF Date\": \"\", \"HF Downloads (June 2024)\": \"\", \"HF Likes (June 2024)\": \"\", \"HF Yaml License\": \"\", \"PwC Date\": \"\", \"PwC Description\": \"Video-and-Language Inference is the task of joint multimodal understanding of video and text. Given a video clip with aligned subtitles as premise, paired with a natural language hypothesis based on the video content, a model needs to infer whether the hypothesis is entailed or contradicted by the given video clip. The Violin dataset is a dataset for this task which consists of 95,322 video-hypothesis pairs from 15,887 video clips, spanning over 582 hours of video. These video clips contain rich content with diverse temporal dynamics, event shifts, and people interactions, collected from two sources: (i) popular TV shows, and (ii) movie clips from YouTube channels.\", \"PwC License Name\": \"Unspecified\", \"PwC License URL\": null, \"S2 Citation Count (June 2024)\": \"\", \"S2 Date\": \"\"}, \"Bibtex\": \"@Article{Liu2020ViolinAL,\\n author = {J. Liu and Wenhu Chen and Yu Cheng and Zhe Gan and Licheng Yu and Yiming Yang and Jingjing Liu},\\n booktitle = {Computer Vision and Pattern Recognition},\\n journal = {2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},\\n pages = {10897-10907},\\n title = {Violin: A Large-Scale Dataset for Video-and-Language Inference},\\n year = {2020}\\n}\\n\", \"License Use (DataProvenance)\": \"unspecified\", \"License Attribution (DataProvenance)\": 0, \"License Share Alike (DataProvenance)\": 0, \"License Use (DataProvenance IgnoreOpenAI)\": \"unspecified\", \"License Attribution (DataProvenance IgnoreOpenAI)\": 0, \"License Share Alike (DataProvenance IgnoreOpenAI)\": 0, \"License Type\": \"Unspecified\"}, {\"Unique Dataset Identifier\": \"kinetics-600\", \"Collection\": \"kinetics-600\", \"Collection URL\": \"https://arxiv.org/abs/1808.01340\", \"Dataset Name\": \"Kinetics 600\", \"Paper Title\": \"Kinetics 600\", \"Paper URL\": \"https://arxiv.org/abs/1808.01340\", \"GitHub URL\": \"https://github.com/cvdfoundation/kinetics-dataset\", \"Hugging Face URL\": \"\", \"Papers with Code URL\": \"https://paperswithcode.com/dataset/kinetics-600\", \"ArXiv URL\": \"https://arxiv.org/abs/1808.01340\", \"Semantic Scholar Corpus ID\": 51927456, \"Year Released\": \"2018\", \"Text Sources\": [\"youtube\"], \"Licenses\": [{\"License\": \"Unspecified\", \"License URL\": \"\"}], \"Creators\": [\"DeepMind\"], \"Countries\": [\"United Kingdom\"], \"License Verified By\": \"Vivek Sharma\", \"Video Hours\": 1376.52, \"Taken Down\": \"False\", \"Video Sources\": \"youtube\", \"Task Categories\": [\"Video Classification\"], \"Inferred Metadata\": {\"GitHub License\": \"\", \"GitHub Stars (June 2024)\": 704, \"GitHub Topics\": [], \"Github Date\": \"\", \"HF Config\": \"\", \"HF Config License\": \"\", \"HF Dataset\": \"\", \"HF Date\": \"\", \"HF Downloads (June 2024)\": \"\", \"HF Likes (June 2024)\": \"\", \"HF Yaml License\": \"\", \"PwC Date\": \"2018-01-01\", \"PwC Description\": \"The Kinetics-600 is a large-scale action recognition dataset which consists of around 480K videos from 600 action categories. The 480K videos are divided into 390K, 30K, 60K for training, validation and test sets, respectively. Each video in the dataset is a 10-second clip of action moment annotated from raw YouTube video. It is an extensions of the Kinetics-400 dataset.\", \"PwC License Name\": \"CC BY 4.0\", \"PwC License URL\": \"https://creativecommons.org/licenses/by/4.0/\", \"S2 Citation Count (June 2024)\": \"\", \"S2 Date\": \"\"}, \"Bibtex\": \"@Article{Carreira2018ASN,\\n author = {Jo\\u00e3o Carreira and Eric Noland and Andras Banki-Horvath and Chloe Hillier and Andrew Zisserman},\\n booktitle = {arXiv.org},\\n journal = {ArXiv},\\n title = {A Short Note about Kinetics-600},\\n volume = {abs/1808.01340},\\n year = {2018}\\n}\\n\", \"License Use (DataProvenance)\": \"unspecified\", \"License Attribution (DataProvenance)\": 0, \"License Share Alike (DataProvenance)\": 0, \"License Use (DataProvenance IgnoreOpenAI)\": \"unspecified\", \"License Attribution (DataProvenance IgnoreOpenAI)\": 0, \"License Share Alike (DataProvenance IgnoreOpenAI)\": 0, \"License Type\": \"Unspecified\"}, {\"Unique Dataset Identifier\": \"voxceleb\", \"Collection\": \"voxceleb\", \"Collection URL\": \"https://arxiv.org/abs/1706.08612\", \"Dataset Name\": \"VoxCeleb\", \"Paper Title\": \"VoxCeleb\", \"Paper URL\": \"https://arxiv.org/abs/1706.08612\", \"GitHub URL\": \"https://github.com/a-nagrani/VGGVox\", \"Hugging Face URL\": \"\", \"Papers with Code URL\": \"https://cs.paperswithcode.com/paper/voxceleb-a-large-scale-speaker-identification\", \"ArXiv URL\": \"https://arxiv.org/abs/1706.08612\", \"Semantic Scholar Corpus ID\": 10475843, \"Year Released\": \"2017\", \"Text Sources\": [\"youtube\"], \"Licenses\": [{\"License\": \"Custom\", \"License URL\": \"https://www.robots.ox.ac.uk/~vgg/data/voxceleb/\"}], \"Creators\": [\"University of Oxford\"], \"Countries\": [\"United Kingdom\", \"South Korea\"], \"License Verified By\": \"Vivek Sharma\", \"Video Hours\": 2000.0, \"Taken Down\": \"False\", \"Video Sources\": \"youtube\", \"Task Categories\": [\"Video Classification\"], \"Inferred Metadata\": {\"GitHub License\": \"\", \"GitHub Stars (June 2024)\": 366, \"GitHub Topics\": [], \"Github Date\": \"\", \"HF Config\": \"\", \"HF Config License\": \"\", \"HF Dataset\": \"\", \"HF Date\": \"\", \"HF Downloads (June 2024)\": \"\", \"HF Likes (June 2024)\": \"\", \"HF Yaml License\": \"\", \"PwC Date\": \"\", \"PwC Description\": \"Implemented in 8 code libraries.\", \"PwC License Name\": \"\", \"PwC License URL\": \"\", \"S2 Citation Count (June 2024)\": \"\", \"S2 Date\": \"\"}, \"Bibtex\": \"@Article{Nagrani2017VoxCelebAL,\\n author = {Arsha Nagrani and Joon Son Chung and Andrew Zisserman},\\n booktitle = {Interspeech},\\n pages = {2616-2620},\\n title = {VoxCeleb: A Large-Scale Speaker Identification Dataset},\\n year = {2017}\\n}\\n\", \"License Use (DataProvenance)\": \"academic-only\", \"License Attribution (DataProvenance)\": 1, \"License Share Alike (DataProvenance)\": 0, \"License Use (DataProvenance IgnoreOpenAI)\": \"academic-only\", \"License Attribution (DataProvenance IgnoreOpenAI)\": 1, \"License Share Alike (DataProvenance IgnoreOpenAI)\": 0, \"License Type\": \"Non-Commercial/Academic\"}, {\"Unique Dataset Identifier\": \"volleyball-vids\", \"Collection\": \"volleyball-vids\", \"Collection URL\": \"https://arxiv.org/abs/1511.06040\", \"Dataset Name\": \"Volleyball: A Hierarchical Deep Temporal Model for Group Activity Recognition\", \"Paper Title\": \"Volleyball: A Hierarchical Deep Temporal Model for Group Activity Recognition\", \"Paper URL\": \"https://arxiv.org/abs/1511.06040\", \"GitHub URL\": \"https://github.com/mostafa-saad/deep-activity-rec#dataset\", \"Hugging Face URL\": \"\", \"Papers with Code URL\": \"https://paperswithcode.com/dataset/volleyball\", \"ArXiv URL\": \"https://arxiv.org/abs/1511.06040\", \"Semantic Scholar Corpus ID\": 8483403, \"Year Released\": \"2015\", \"Text Sources\": [\"youtube\"], \"Licenses\": [{\"License\": \"Unspecified\", \"License URL\": \"\"}], \"Creators\": [\"Simon Fraser University\"], \"Countries\": [\"Canada\"], \"License Verified By\": \"Vivek Sharma\", \"Video Hours\": 0.1, \"Taken Down\": \"False\", \"Video Sources\": \"youtube\", \"Task Categories\": [\"Group Activity Recognition\"], \"Inferred Metadata\": {\"GitHub License\": \"BSD 3-Clause License\", \"GitHub Stars (June 2024)\": 171, \"GitHub Topics\": [], \"Github Date\": \"\", \"HF Config\": \"\", \"HF Config License\": \"\", \"HF Dataset\": \"\", \"HF Date\": \"\", \"HF Downloads (June 2024)\": \"\", \"HF Likes (June 2024)\": \"\", \"HF Yaml License\": \"\", \"PwC Date\": \"2016-01-01\", \"PwC Description\": \"Volleyball is a video action recognition dataset. It has 4830 annotated frames that were handpicked from 55 videos with 9 player action labels and 8 team activity labels. It contains group activity annotations as well as individual activity annotations.\", \"PwC License Name\": \"Unspecified\", \"PwC License URL\": null, \"S2 Citation Count (June 2024)\": \"\", \"S2 Date\": \"\"}, \"Bibtex\": \"@Article{Ibrahim2016HierarchicalDT,\\n author = {Mostafa S. Ibrahim and S. Muralidharan and Zhiwei Deng and Arash Vahdat and Greg Mori},\\n booktitle = {arXiv.org},\\n journal = {ArXiv},\\n title = {Hierarchical Deep Temporal Models for Group Activity Recognition},\\n volume = {abs/1607.02643},\\n year = {2016}\\n}\\n\", \"License Use (DataProvenance)\": \"unspecified\", \"License Attribution (DataProvenance)\": 0, \"License Share Alike (DataProvenance)\": 0, \"License Use (DataProvenance IgnoreOpenAI)\": \"unspecified\", \"License Attribution (DataProvenance IgnoreOpenAI)\": 0, \"License Share Alike (DataProvenance IgnoreOpenAI)\": 0, \"License Type\": \"Unspecified\"}, {\"Unique Dataset Identifier\": \"thumos-challenge\", \"Collection\": \"thumos-challenge\", \"Collection URL\": \"https://arxiv.org/pdf/1604.06182.pdf\", \"Dataset Name\": \"The THUMOS Challenge on Action Recognition for Videos in the Wild\", \"Paper Title\": \"The THUMOS Challenge on Action Recognition for Videos in the Wild\", \"Paper URL\": \"https://arxiv.org/pdf/1604.06182.pdf\", \"GitHub URL\": \"\", \"Hugging Face URL\": \"\", \"Papers with Code URL\": \"https://paperswithcode.com/dataset/thumos14-1\", \"ArXiv URL\": \"https://arxiv.org/pdf/1604.06182.pdf\", \"Semantic Scholar Corpus ID\": 14049355, \"Year Released\": \"2014\", \"Text Sources\": [\"youtube\"], \"Licenses\": [{\"License\": \"Custom\", \"License URL\": \"https://docs.google.com/forms/d/e/1FAIpQLScs9davISAtYQS7SEF5qQNu0jUpLzNH3aHmPfuqk2q1VYDkmw/viewform\"}], \"Creators\": [\"Stanford University\", \"University of Central Florida\", \"Fudan University\", \"Inria\"], \"Countries\": [\"United States of America\", \"China\"], \"License Verified By\": \"Vivek Sharma\", \"Video Hours\": 254.0, \"Taken Down\": \"False\", \"Video Sources\": \"youtube\", \"Task Categories\": [\"Video Classification\"], \"Inferred Metadata\": {\"GitHub License\": \"\", \"GitHub Stars (June 2024)\": \"\", \"GitHub Topics\": \"\", \"Github Date\": \"\", \"HF Config\": \"\", \"HF Config License\": \"\", \"HF Dataset\": \"\", \"HF Date\": \"\", \"HF Downloads (June 2024)\": \"\", \"HF Likes (June 2024)\": \"\", \"HF Yaml License\": \"\", \"PwC Date\": \"\", \"PwC Description\": \"The THUMOS14 (THUMOS 2014) dataset is a large-scale video dataset that includes 1,010 videos for validation and 1,574 videos for testing from 20 classes. Among all the videos, there are 220 and 212 videos with temporal annotations in validation and testing set, respectively.\", \"PwC License Name\": \"Unspecified\", \"PwC License URL\": null, \"S2 Citation Count (June 2024)\": \"\", \"S2 Date\": \"\"}, \"Bibtex\": \"@Misc{None,\\n author = {Haroon Idrees and Amir Zamir and Yu-Gang Jiang and Alexander N. Gorban and I. Laptev and R. Sukthankar and M. Shah},\\n title = {Computer Vision and Image Understanding}\\n}\\n\", \"License Use (DataProvenance)\": \"academic-only\", \"License Attribution (DataProvenance)\": 1, \"License Share Alike (DataProvenance)\": 0, \"License Use (DataProvenance IgnoreOpenAI)\": \"academic-only\", \"License Attribution (DataProvenance IgnoreOpenAI)\": 1, \"License Share Alike (DataProvenance IgnoreOpenAI)\": 0, \"License Type\": \"Non-Commercial/Academic\"}, {\"Unique Dataset Identifier\": \"youcook\", \"Collection\": \"youcook\", \"Collection URL\": \"https://openaccess.thecvf.com/content_cvpr_2013/papers/Das_A_Thousand_Frames_2013_CVPR_paper.pdf\", \"Dataset Name\": \"YouCook: A Thousand Frames in Just a Few Words: Lingual Description of Videos through Latent Topics and Sparse Object Stitching (CVPR 2013)\", \"Paper Title\": \"YouCook: A Thousand Frames in Just a Few Words: Lingual Description of Videos through Latent Topics and Sparse Object Stitching (CVPR 2013)\", \"Paper URL\": \"https://openaccess.thecvf.com/content_cvpr_2013/papers/Das_A_Thousand_Frames_2013_CVPR_paper.pdf\", \"GitHub URL\": \"\", \"Hugging Face URL\": \"\", \"Papers with Code URL\": \"https://paperswithcode.com/dataset/youcook\", \"ArXiv URL\": \"https://openaccess.thecvf.com/content_cvpr_2013/papers/Das_A_Thousand_Frames_2013_CVPR_paper.pdf\", \"Semantic Scholar Corpus ID\": 12284555, \"Year Released\": \"2013\", \"Text Sources\": [\"youtube\"], \"Licenses\": [{\"License\": \"Unspecified\", \"License URL\": \"\"}], \"Creators\": [\"University at Buffalo\"], \"Countries\": [\"United States of America\"], \"License Verified By\": \"Vivek Sharma\", \"Video Hours\": 1000.0, \"Taken Down\": \"False\", \"Video Sources\": \"youtube\", \"Task Categories\": [\"Video Captioning\"], \"Inferred Metadata\": {\"GitHub License\": \"\", \"GitHub Stars (June 2024)\": \"\", \"GitHub Topics\": \"\", \"Github Date\": \"\", \"HF Config\": \"\", \"HF Config License\": \"\", \"HF Dataset\": \"\", \"HF Date\": \"\", \"HF Downloads (June 2024)\": \"\", \"HF Likes (June 2024)\": \"\", \"HF Yaml License\": \"\", \"PwC Date\": \"\", \"PwC Description\": \"This data set was prepared from 88 open-source YouTube cooking videos. The YouCook dataset contains videos of people cooking various recipes. The videos were downloaded from YouTube and are all in the third-person viewpoint; they represent a significantly more challenging visual problem than existing cooking and kitchen datasets (the background kitchen/scene is different for many and most videos have dynamic camera changes). In addition, frame-by-frame object and action annotations are provided for training data (as well as a number of precomputed low-level features). Finally, each video has a number of human provided natural language descriptions (on average, there are eight different descriptions per video). This dataset has been created to serve as a benchmark in describing complex real-world videos with natural language descriptions.\", \"PwC License Name\": \"Unspecified\", \"PwC License URL\": null, \"S2 Citation Count (June 2024)\": \"\", \"S2 Date\": \"\"}, \"Bibtex\": \"@Article{Das2013ATF,\\n author = {Pradipto Das and Chenliang Xu and Richard F. Doell and Jason J. Corso},\\n booktitle = {2013 IEEE Conference on Computer Vision and Pattern Recognition},\\n journal = {2013 IEEE Conference on Computer Vision and Pattern Recognition},\\n pages = {2634-2641},\\n title = {A Thousand Frames in Just a Few Words: Lingual Description of Videos through Latent Topics and Sparse Object Stitching},\\n year = {2013}\\n}\\n\", \"License Use (DataProvenance)\": \"unspecified\", \"License Attribution (DataProvenance)\": 0, \"License Share Alike (DataProvenance)\": 0, \"License Use (DataProvenance IgnoreOpenAI)\": \"unspecified\", \"License Attribution (DataProvenance IgnoreOpenAI)\": 0, \"License Share Alike (DataProvenance IgnoreOpenAI)\": 0, \"License Type\": \"Unspecified\"}, {\"Unique Dataset Identifier\": \"vlog-vids\", \"Collection\": \"vlog-vids\", \"Collection URL\": \"https://arxiv.org/abs/1712.02310\", \"Dataset Name\": \"VLOG: From Lifestyle Vlogs to Everyday Interactions (CVPR 2018)\", \"Paper Title\": \"VLOG: From Lifestyle Vlogs to Everyday Interactions (CVPR 2018)\", \"Paper URL\": \"https://arxiv.org/abs/1712.02310\", \"GitHub URL\": \"\", \"Hugging Face URL\": \"\", \"Papers with Code URL\": \"https://paperswithcode.com/dataset/vlog-dataset\", \"ArXiv URL\": \"https://arxiv.org/abs/1712.02310\", \"Semantic Scholar Corpus ID\": 22264672, \"Year Released\": \"2018\", \"Text Sources\": [\"youtube\"], \"Licenses\": [{\"License\": \"Custom\", \"License URL\": \"https://web.eecs.umich.edu/~fouhey/2017/VLOG/index.html\"}], \"Creators\": [\"UC Berkeley\"], \"Countries\": [\"United States of America\"], \"License Verified By\": \"Vivek Sharma\", \"Video Hours\": 336.0, \"Taken Down\": \"False\", \"Video Sources\": \"youtube\", \"Task Categories\": [\"Video Classification\"], \"Inferred Metadata\": {\"GitHub License\": \"\", \"GitHub Stars (June 2024)\": \"\", \"GitHub Topics\": \"\", \"Github Date\": \"\", \"HF Config\": \"\", \"HF Config License\": \"\", \"HF Dataset\": \"\", \"HF Date\": \"\", \"HF Downloads (June 2024)\": \"\", \"HF Likes (June 2024)\": \"\", \"HF Yaml License\": \"\", \"PwC Date\": \"\", \"PwC Description\": \"A large collection of interaction-rich video data which are annotated and analyzed.\", \"PwC License Name\": \"Unspecified\", \"PwC License URL\": null, \"S2 Citation Count (June 2024)\": \"\", \"S2 Date\": \"\"}, \"Bibtex\": \"@Article{Fouhey2017FromLV,\\n author = {D. Fouhey and Weicheng Kuo and Alexei A. Efros and Jitendra Malik},\\n booktitle = {2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition},\\n journal = {2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition},\\n pages = {4991-5000},\\n title = {From Lifestyle Vlogs to Everyday Interactions},\\n year = {2017}\\n}\\n\", \"License Use (DataProvenance)\": \"academic-only\", \"License Attribution (DataProvenance)\": 1, \"License Share Alike (DataProvenance)\": 0, \"License Use (DataProvenance IgnoreOpenAI)\": \"academic-only\", \"License Attribution (DataProvenance IgnoreOpenAI)\": 1, \"License Share Alike (DataProvenance IgnoreOpenAI)\": 0, \"License Type\": \"Non-Commercial/Academic\"}, {\"Unique Dataset Identifier\": \"hyu-vids\", \"Collection\": \"hyu-vids\", \"Collection URL\": \"https://arxiv.org/abs/1904.11451\", \"Dataset Name\": \"HVU: Large Scale Holistic Video Understanding (ECCV 2020)\", \"Paper Title\": \"HVU: Large Scale Holistic Video Understanding (ECCV 2020)\", \"Paper URL\": \"https://arxiv.org/abs/1904.11451\", \"GitHub URL\": \"https://github.com/holistic-video-understanding/HVU-Dataset\", \"Hugging Face URL\": \"\", \"Papers with Code URL\": \"https://paperswithcode.com/paper/holistic-large-scale-video-understanding\", \"ArXiv URL\": \"https://arxiv.org/abs/1904.11451\", \"Semantic Scholar Corpus ID\": 131777079, \"Year Released\": \"2020\", \"Text Sources\": [\"youtube\"], \"Licenses\": [{\"License\": \"Custom\", \"License URL\": \"https://github.com/holistic-video-understanding/HVU-Dataset\"}], \"Creators\": [\"Karlsruhe Institute of Technology\", \"ETH Z\\u00fcrich\", \"KU Leuven\", \"University of Bonn\", \"Sensifai\"], \"Countries\": [\"Switzerland\", \"Belgium\", \"Germany\"], \"License Verified By\": \"Vivek Sharma\", \"Video Hours\": 96166.67, \"Taken Down\": \"False\", \"Video Sources\": \"youtube\", \"Task Categories\": [\"Video Classification\"], \"Inferred Metadata\": {\"GitHub License\": \"\", \"GitHub Stars (June 2024)\": 70, \"GitHub Topics\": [], \"Github Date\": \"\", \"HF Config\": \"\", \"HF Config License\": \"\", \"HF Dataset\": \"\", \"HF Date\": \"\", \"HF Downloads (June 2024)\": \"\", \"HF Likes (June 2024)\": \"\", \"HF Yaml License\": \"\", \"PwC Date\": \"\", \"PwC Description\": \"#11 best model for Action Recognition on UCF101 (3-fold Accuracy metric)\", \"PwC License Name\": \"\", \"PwC License URL\": \"\", \"S2 Citation Count (June 2024)\": \"\", \"S2 Date\": \"\"}, \"Bibtex\": \"@Article{Diba2019HolisticLS,\\n author = {Ali Diba and Mohsen Fayyaz and Vivek Sharma and Manohar Paluri and Juergen Gall and R. Stiefelhagen and L. Gool},\\n booktitle = {arXiv.org},\\n journal = {ArXiv},\\n title = {Holistic Large Scale Video Understanding},\\n volume = {abs/1904.11451},\\n year = {2019}\\n}\\n\", \"License Use (DataProvenance)\": \"academic-only\", \"License Attribution (DataProvenance)\": 1, \"License Share Alike (DataProvenance)\": 0, \"License Use (DataProvenance IgnoreOpenAI)\": \"academic-only\", \"License Attribution (DataProvenance IgnoreOpenAI)\": 1, \"License Share Alike (DataProvenance IgnoreOpenAI)\": 0, \"License Type\": \"Non-Commercial/Academic\"}, {\"Unique Dataset Identifier\": \"spoken-moments\", \"Collection\": \"spoken-moments\", \"Collection URL\": \"https://arxiv.org/abs/2105.04489\", \"Dataset Name\": \"Spoken Moments\", \"Paper Title\": \"Spoken Moments\", \"Paper URL\": \"https://arxiv.org/abs/2105.04489\", \"GitHub URL\": \"\", \"Hugging Face URL\": \"\", \"Papers with Code URL\": \"https://paperswithcode.com/paper/spoken-moments-learning-joint-audio-visual\", \"ArXiv URL\": \"https://arxiv.org/abs/2105.04489\", \"Semantic Scholar Corpus ID\": 233992735, \"Year Released\": \"2021\", \"Text Sources\": [\"youtube\", \"flickr\", \"vine\", \"metacafe\", \"peeks\", \"vimeo\", \"videoblocks\", \"bing videos\", \"giphy\", \"weather channel\", \"getty-images\"], \"Licenses\": [{\"License\": \"Custom\", \"License URL\": \"https://docs.google.com/forms/d/e/1FAIpQLSdUOScon6uWkslgcA2-BCkAhMp86SihgKtMqX53zRf-E2fIHw/formResponse\"}], \"Creators\": [\"Massachusetts Institute of Technology\", \"IBM\", \"The University of Texas at Austin\"], \"Countries\": [\"United States of America\"], \"License Verified By\": \"Vivek Sharma\", \"Video Hours\": 416.67, \"Taken Down\": \"False\", \"Video Sources\": \"Other\", \"Task Categories\": [\"Video Captioning\"], \"Inferred Metadata\": {\"GitHub License\": \"\", \"GitHub Stars (June 2024)\": \"\", \"GitHub Topics\": \"\", \"Github Date\": \"\", \"HF Config\": \"\", \"HF Config License\": \"\", \"HF Dataset\": \"\", \"HF Date\": \"\", \"HF Downloads (June 2024)\": \"\", \"HF Likes (June 2024)\": \"\", \"HF Yaml License\": \"\", \"PwC Date\": \"\", \"PwC Description\": \"No code available yet.\", \"PwC License Name\": \"\", \"PwC License URL\": \"\", \"S2 Citation Count (June 2024)\": \"\", \"S2 Date\": \"\"}, \"Bibtex\": \"@Article{Monfort2021SpokenML,\\n author = {Mathew Monfort and SouYoung Jin},\\n booktitle = {Computer Vision and Pattern Recognition},\\n journal = {2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},\\n pages = {14866-14876},\\n title = {Spoken Moments: Learning Joint Audio-Visual Representations from Video Descriptions},\\n year = {2021}\\n}\\n\", \"License Use (DataProvenance)\": \"commercial\", \"License Attribution (DataProvenance)\": 0, \"License Share Alike (DataProvenance)\": 0, \"License Use (DataProvenance IgnoreOpenAI)\": \"commercial\", \"License Attribution (DataProvenance IgnoreOpenAI)\": 0, \"License Share Alike (DataProvenance IgnoreOpenAI)\": 0, \"License Type\": \"Commercial\"}, {\"Unique Dataset Identifier\": \"spoken-moments\", \"Collection\": \"spoken-moments\", \"Collection URL\": \"https://arxiv.org/abs/2105.04489\", \"Dataset Name\": \"Spoken Moments\", \"Paper Title\": \"Spoken Moments\", \"Paper URL\": \"https://arxiv.org/abs/2105.04489\", \"GitHub URL\": \"\", \"Hugging Face URL\": \"\", \"Papers with Code URL\": \"https://paperswithcode.com/paper/spoken-moments-learning-joint-audio-visual\", \"ArXiv URL\": \"https://arxiv.org/abs/2105.04489\", \"Semantic Scholar Corpus ID\": 233992735, \"Year Released\": \"2021\", \"Text Sources\": [\"youtube\", \"flickr\", \"vine\", \"metacafe\", \"peeks\", \"vimeo\", \"videoblocks\", \"bing videos\", \"giphy\", \"weather channel\", \"getty-images\"], \"Licenses\": [{\"License\": \"Custom\", \"License URL\": \"https://docs.google.com/forms/d/e/1FAIpQLSdUOScon6uWkslgcA2-BCkAhMp86SihgKtMqX53zRf-E2fIHw/formResponse\"}], \"Creators\": [\"Massachusetts Institute of Technology\", \"IBM\", \"The University of Texas at Austin\"], \"Countries\": [\"United States of America\"], \"License Verified By\": \"Vivek Sharma\", \"Video Hours\": 416.67, \"Taken Down\": \"False\", \"Video Sources\": \"Other\", \"Task Categories\": [\"Video Captioning\"], \"Inferred Metadata\": {\"GitHub License\": \"\", \"GitHub Stars (June 2024)\": \"\", \"GitHub Topics\": \"\", \"Github Date\": \"\", \"HF Config\": \"\", \"HF Config License\": \"\", \"HF Dataset\": \"\", \"HF Date\": \"\", \"HF Downloads (June 2024)\": \"\", \"HF Likes (June 2024)\": \"\", \"HF Yaml License\": \"\", \"PwC Date\": \"\", \"PwC Description\": \"No code available yet.\", \"PwC License Name\": \"\", \"PwC License URL\": \"\", \"S2 Citation Count (June 2024)\": \"\", \"S2 Date\": \"\"}, \"Bibtex\": \"@Article{Monfort2021SpokenML,\\n author = {Mathew Monfort and SouYoung Jin},\\n booktitle = {Computer Vision and Pattern Recognition},\\n journal = {2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},\\n pages = {14866-14876},\\n title = {Spoken Moments: Learning Joint Audio-Visual Representations from Video Descriptions},\\n year = {2021}\\n}\\n\", \"License Use (DataProvenance)\": \"commercial\", \"License Attribution (DataProvenance)\": 0, \"License Share Alike (DataProvenance)\": 0, \"License Use (DataProvenance IgnoreOpenAI)\": \"commercial\", \"License Attribution (DataProvenance IgnoreOpenAI)\": 0, \"License Share Alike (DataProvenance IgnoreOpenAI)\": 0, \"License Type\": \"Commercial\"}, {\"Unique Dataset Identifier\": \"ava-dataset\", \"Collection\": \"ava-dataset\", \"Collection URL\": \"https://arxiv.org/abs/1901.01342\", \"Dataset Name\": \"AVA Active Speaker\", \"Paper Title\": \"AVA Active Speaker\", \"Paper URL\": \"https://arxiv.org/abs/1901.01342\", \"GitHub URL\": \"https://github.com/cvdfoundation/ava-dataset\", \"Hugging Face URL\": \"\", \"Papers with Code URL\": \"https://paperswithcode.com/dataset/ava-activespeaker\", \"ArXiv URL\": \"https://arxiv.org/abs/1901.01342\", \"Semantic Scholar Corpus ID\": 216211909, \"Year Released\": \"2019\", \"Text Sources\": [\"Not Prohibited\"], \"Licenses\": [{\"License\": \"CC BY 4.0\", \"License URL\": \"https://research.google.com/ava/download.html#ava_active_speaker_download\"}], \"Creators\": [\"Google Research\"], \"Countries\": [\"United States of America\"], \"License Verified By\": \"Vivek Sharma\", \"Video Hours\": 38.5, \"Taken Down\": \"False\", \"Video Sources\": \"Other\", \"Task Categories\": [\"Video Classification\"], \"Inferred Metadata\": {\"GitHub License\": \"\", \"GitHub Stars (June 2024)\": 305, \"GitHub Topics\": [], \"Github Date\": \"\", \"HF Config\": \"\", \"HF Config License\": \"\", \"HF Dataset\": \"\", \"HF Date\": \"\", \"HF Downloads (June 2024)\": \"\", \"HF Likes (June 2024)\": \"\", \"HF Yaml License\": \"\", \"PwC Date\": \"\", \"PwC Description\": \"Contains temporally labeled face tracks in video, where each face instance is labeled as speaking or not, and whether the speech is audible. This dataset contains about 3.65 million human labeled frames or about 38.5 hours of face tracks, and the corresponding audio.\", \"PwC License Name\": \"Unspecified\", \"PwC License URL\": null, \"S2 Citation Count (June 2024)\": \"\", \"S2 Date\": \"\"}, \"Bibtex\": \"@Article{Roth2019AvaAS,\\n author = {Joseph Roth and Sourish Chaudhuri and Ondrej Klejch and Radhika Marvin and Andrew C. Gallagher and Liat Kaver and S. Ramaswamy and Arkadiusz Stopczynski and C. Schmid and Zhonghua Xi and C. Pantofaru},\\n booktitle = {IEEE International Conference on Acoustics, Speech, and Signal Processing},\\n journal = {ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},\\n pages = {4492-4496},\\n title = {Ava Active Speaker: An Audio-Visual Dataset for Active Speaker Detection},\\n year = {2019}\\n}\\n\", \"License Use (DataProvenance)\": \"commercial\", \"License Attribution (DataProvenance)\": 1, \"License Share Alike (DataProvenance)\": 0, \"License Use (DataProvenance IgnoreOpenAI)\": \"commercial\", \"License Attribution (DataProvenance IgnoreOpenAI)\": 1, \"License Share Alike (DataProvenance IgnoreOpenAI)\": 0, \"License Type\": \"Commercial\"}, {\"Unique Dataset Identifier\": \"spoken-moments\", \"Collection\": \"spoken-moments\", \"Collection URL\": \"https://arxiv.org/abs/2105.04489\", \"Dataset Name\": \"Spoken Moments\", \"Paper Title\": \"Spoken Moments\", \"Paper URL\": \"https://arxiv.org/abs/2105.04489\", \"GitHub URL\": \"\", \"Hugging Face URL\": \"\", \"Papers with Code URL\": \"https://paperswithcode.com/paper/spoken-moments-learning-joint-audio-visual\", \"ArXiv URL\": \"https://arxiv.org/abs/2105.04489\", \"Semantic Scholar Corpus ID\": 233992735, \"Year Released\": \"2021\", \"Text Sources\": [\"youtube\", \"flickr\", \"vine\", \"metacafe\", \"peeks\", \"vimeo\", \"videoblocks\", \"bing videos\", \"giphy\", \"weather channel\", \"getty-images\"], \"Licenses\": [{\"License\": \"Custom\", \"License URL\": \"https://docs.google.com/forms/d/e/1FAIpQLSdUOScon6uWkslgcA2-BCkAhMp86SihgKtMqX53zRf-E2fIHw/formResponse\"}], \"Creators\": [\"Massachusetts Institute of Technology\", \"IBM\", \"The University of Texas at Austin\"], \"Countries\": [\"United States of America\"], \"License Verified By\": \"Vivek Sharma\", \"Video Hours\": 416.67, \"Taken Down\": \"False\", \"Video Sources\": \"Other\", \"Task Categories\": [\"Video Captioning\"], \"Inferred Metadata\": {\"GitHub License\": \"\", \"GitHub Stars (June 2024)\": \"\", \"GitHub Topics\": \"\", \"Github Date\": \"\", \"HF Config\": \"\", \"HF Config License\": \"\", \"HF Dataset\": \"\", \"HF Date\": \"\", \"HF Downloads (June 2024)\": \"\", \"HF Likes (June 2024)\": \"\", \"HF Yaml License\": \"\", \"PwC Date\": \"\", \"PwC Description\": \"No code available yet.\", \"PwC License Name\": \"\", \"PwC License URL\": \"\", \"S2 Citation Count (June 2024)\": \"\", \"S2 Date\": \"\"}, \"Bibtex\": \"@Article{Monfort2021SpokenML,\\n author = {Mathew Monfort and SouYoung Jin},\\n booktitle = {Computer Vision and Pattern Recognition},\\n journal = {2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},\\n pages = {14866-14876},\\n title = {Spoken Moments: Learning Joint Audio-Visual Representations from Video Descriptions},\\n year = {2021}\\n}\\n\", \"License Use (DataProvenance)\": \"commercial\", \"License Attribution (DataProvenance)\": 0, \"License Share Alike (DataProvenance)\": 0, \"License Use (DataProvenance IgnoreOpenAI)\": \"commercial\", \"License Attribution (DataProvenance IgnoreOpenAI)\": 0, \"License Share Alike (DataProvenance IgnoreOpenAI)\": 0, \"License Type\": \"Commercial\"}, {\"Unique Dataset Identifier\": \"spoken-moments\", \"Collection\": \"spoken-moments\", \"Collection URL\": \"https://arxiv.org/abs/2105.04489\", \"Dataset Name\": \"Spoken Moments\", \"Paper Title\": \"Spoken Moments\", \"Paper URL\": \"https://arxiv.org/abs/2105.04489\", \"GitHub URL\": \"\", \"Hugging Face URL\": \"\", \"Papers with Code URL\": \"https://paperswithcode.com/paper/spoken-moments-learning-joint-audio-visual\", \"ArXiv URL\": \"https://arxiv.org/abs/2105.04489\", \"Semantic Scholar Corpus ID\": 233992735, \"Year Released\": \"2021\", \"Text Sources\": [\"youtube\", \"flickr\", \"vine\", \"metacafe\", \"peeks\", \"vimeo\", \"videoblocks\", \"bing videos\", \"giphy\", \"weather channel\", \"getty-images\"], \"Licenses\": [{\"License\": \"Custom\", \"License URL\": \"https://docs.google.com/forms/d/e/1FAIpQLSdUOScon6uWkslgcA2-BCkAhMp86SihgKtMqX53zRf-E2fIHw/formResponse\"}], \"Creators\": [\"Massachusetts Institute of Technology\", \"IBM\", \"The University of Texas at Austin\"], \"Countries\": [\"United States of America\"], \"License Verified By\": \"Vivek Sharma\", \"Video Hours\": 416.67, \"Taken Down\": \"False\", \"Video Sources\": \"Other\", \"Task Categories\": [\"Video Captioning\"], \"Inferred Metadata\": {\"GitHub License\": \"\", \"GitHub Stars (June 2024)\": \"\", \"GitHub Topics\": \"\", \"Github Date\": \"\", \"HF Config\": \"\", \"HF Config License\": \"\", \"HF Dataset\": \"\", \"HF Date\": \"\", \"HF Downloads (June 2024)\": \"\", \"HF Likes (June 2024)\": \"\", \"HF Yaml License\": \"\", \"PwC Date\": \"\", \"PwC Description\": \"No code available yet.\", \"PwC License Name\": \"\", \"PwC License URL\": \"\", \"S2 Citation Count (June 2024)\": \"\", \"S2 Date\": \"\"}, \"Bibtex\": \"@Article{Monfort2021SpokenML,\\n author = {Mathew Monfort and SouYoung Jin},\\n booktitle = {Computer Vision and Pattern Recognition},\\n journal = {2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},\\n pages = {14866-14876},\\n title = {Spoken Moments: Learning Joint Audio-Visual Representations from Video Descriptions},\\n year = {2021}\\n}\\n\", \"License Use (DataProvenance)\": \"commercial\", \"License Attribution (DataProvenance)\": 0, \"License Share Alike (DataProvenance)\": 0, \"License Use (DataProvenance IgnoreOpenAI)\": \"commercial\", \"License Attribution (DataProvenance IgnoreOpenAI)\": 0, \"License Share Alike (DataProvenance IgnoreOpenAI)\": 0, \"License Type\": \"Commercial\"}, {\"Unique Dataset Identifier\": \"spoken-moments\", \"Collection\": \"spoken-moments\", \"Collection URL\": \"https://arxiv.org/abs/2105.04489\", \"Dataset Name\": \"Spoken Moments\", \"Paper Title\": \"Spoken Moments\", \"Paper URL\": \"https://arxiv.org/abs/2105.04489\", \"GitHub URL\": \"\", \"Hugging Face URL\": \"\", \"Papers with Code URL\": \"https://paperswithcode.com/paper/spoken-moments-learning-joint-audio-visual\", \"ArXiv URL\": \"https://arxiv.org/abs/2105.04489\", \"Semantic Scholar Corpus ID\": 233992735, \"Year Released\": \"2021\", \"Text Sources\": [\"youtube\", \"flickr\", \"vine\", \"metacafe\", \"peeks\", \"vimeo\", \"videoblocks\", \"bing videos\", \"giphy\", \"weather channel\", \"getty-images\"], \"Licenses\": [{\"License\": \"Custom\", \"License URL\": \"https://docs.google.com/forms/d/e/1FAIpQLSdUOScon6uWkslgcA2-BCkAhMp86SihgKtMqX53zRf-E2fIHw/formResponse\"}], \"Creators\": [\"Massachusetts Institute of Technology\", \"IBM\", \"The University of Texas at Austin\"], \"Countries\": [\"United States of America\"], \"License Verified By\": \"Vivek Sharma\", \"Video Hours\": 416.67, \"Taken Down\": \"False\", \"Video Sources\": \"Other\", \"Task Categories\": [\"Video Captioning\"], \"Inferred Metadata\": {\"GitHub License\": \"\", \"GitHub Stars (June 2024)\": \"\", \"GitHub Topics\": \"\", \"Github Date\": \"\", \"HF Config\": \"\", \"HF Config License\": \"\", \"HF Dataset\": \"\", \"HF Date\": \"\", \"HF Downloads (June 2024)\": \"\", \"HF Likes (June 2024)\": \"\", \"HF Yaml License\": \"\", \"PwC Date\": \"\", \"PwC Description\": \"No code available yet.\", \"PwC License Name\": \"\", \"PwC License URL\": \"\", \"S2 Citation Count (June 2024)\": \"\", \"S2 Date\": \"\"}, \"Bibtex\": \"@Article{Monfort2021SpokenML,\\n author = {Mathew Monfort and SouYoung Jin},\\n booktitle = {Computer Vision and Pattern Recognition},\\n journal = {2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},\\n pages = {14866-14876},\\n title = {Spoken Moments: Learning Joint Audio-Visual Representations from Video Descriptions},\\n year = {2021}\\n}\\n\", \"License Use (DataProvenance)\": \"commercial\", \"License Attribution (DataProvenance)\": 0, \"License Share Alike (DataProvenance)\": 0, \"License Use (DataProvenance IgnoreOpenAI)\": \"commercial\", \"License Attribution (DataProvenance IgnoreOpenAI)\": 0, \"License Share Alike (DataProvenance IgnoreOpenAI)\": 0, \"License Type\": \"Commercial\"}, {\"Unique Dataset Identifier\": \"hmdb-dataset\", \"Collection\": \"hmdb-dataset\", \"Collection URL\": \"https://ieeexplore.ieee.org/document/6126543\", \"Dataset Name\": \"HMDB: A Large Video Database for Human Motion Recognition (ICCV 2011)\", \"Paper Title\": \"HMDB: A Large Video Database for Human Motion Recognition (ICCV 2011)\", \"Paper URL\": \"https://ieeexplore.ieee.org/document/6126543\", \"GitHub URL\": \"\", \"Hugging Face URL\": \"\", \"Papers with Code URL\": \"https://paperswithcode.com/dataset/hmdb51\", \"ArXiv URL\": \"https://ieeexplore.ieee.org/document/6126543\", \"Semantic Scholar Corpus ID\": 206769852, \"Year Released\": \"2011\", \"Text Sources\": [\"movies\", \"prelinger archive\", \"undisclosed web\", \"youtube\", \"google videos\"], \"Licenses\": [{\"License\": \"CC BY 4.0\", \"License URL\": \"https://serre-lab.clps.brown.edu/resource/hmdb-a-large-human-motion-database/\"}], \"Creators\": [\"Karlsruhe Institute of Technology\", \"Massachusetts Institute of Technology\", \"Brown University\"], \"Countries\": [\"United States of America\", \"Germany\"], \"License Verified By\": \"Vivek Sharma\", \"Video Hours\": 7000.0, \"Taken Down\": \"False\", \"Video Sources\": \"Other\", \"Task Categories\": [\"Video Classification\"], \"Inferred Metadata\": {\"GitHub License\": \"\", \"GitHub Stars (June 2024)\": \"\", \"GitHub Topics\": \"\", \"Github Date\": \"\", \"HF Config\": \"\", \"HF Config License\": \"\", \"HF Dataset\": \"\", \"HF Date\": \"\", \"HF Downloads (June 2024)\": \"\", \"HF Likes (June 2024)\": \"\", \"HF Yaml License\": \"\", \"PwC Date\": \"2011-01-01\", \"PwC Description\": \"The HMDB51 dataset is a large collection of realistic videos from various sources, including movies and web videos. The dataset is composed of 6,766 video clips from 51 action categories (such as \\u201cjump\\u201d, \\u201ckiss\\u201d and \\u201claugh\\u201d), with each category containing at least 101 clips. The original evaluation scheme uses three different training/testing splits. In each split, each action class has 70 clips for training and 30 clips for testing. The average accuracy over these three splits is used to measure the final performance.\", \"PwC License Name\": \"CC BY 4.0\", \"PwC License URL\": \"https://creativecommons.org/licenses/by/4.0/\", \"S2 Citation Count (June 2024)\": \"\", \"S2 Date\": \"\"}, \"Bibtex\": \"@Article{Kuehne2011HMDBAL,\\n author = {Hilde Kuehne and Hueihan Jhuang and Est\\u00edbaliz Garrote and T. Poggio and Thomas Serre},\\n booktitle = {Vision},\\n journal = {2011 International Conference on Computer Vision},\\n pages = {2556-2563},\\n title = {HMDB: A large video database for human motion recognition},\\n year = {2011}\\n}\\n\", \"License Use (DataProvenance)\": \"commercial\", \"License Attribution (DataProvenance)\": 1, \"License Share Alike (DataProvenance)\": 0, \"License Use (DataProvenance IgnoreOpenAI)\": \"commercial\", \"License Attribution (DataProvenance IgnoreOpenAI)\": 1, \"License Share Alike (DataProvenance IgnoreOpenAI)\": 0, \"License Type\": \"Commercial\"}, {\"Unique Dataset Identifier\": \"spoken-moments\", \"Collection\": \"spoken-moments\", \"Collection URL\": \"https://arxiv.org/abs/2105.04489\", \"Dataset Name\": \"Spoken Moments\", \"Paper Title\": \"Spoken Moments\", \"Paper URL\": \"https://arxiv.org/abs/2105.04489\", \"GitHub URL\": \"\", \"Hugging Face URL\": \"\", \"Papers with Code URL\": \"https://paperswithcode.com/paper/spoken-moments-learning-joint-audio-visual\", \"ArXiv URL\": \"https://arxiv.org/abs/2105.04489\", \"Semantic Scholar Corpus ID\": 233992735, \"Year Released\": \"2021\", \"Text Sources\": [\"youtube\", \"flickr\", \"vine\", \"metacafe\", \"peeks\", \"vimeo\", \"videoblocks\", \"bing videos\", \"giphy\", \"weather channel\", \"getty-images\"], \"Licenses\": [{\"License\": \"Custom\", \"License URL\": \"https://docs.google.com/forms/d/e/1FAIpQLSdUOScon6uWkslgcA2-BCkAhMp86SihgKtMqX53zRf-E2fIHw/formResponse\"}], \"Creators\": [\"Massachusetts Institute of Technology\", \"IBM\", \"The University of Texas at Austin\"], \"Countries\": [\"United States of America\"], \"License Verified By\": \"Vivek Sharma\", \"Video Hours\": 416.67, \"Taken Down\": \"False\", \"Video Sources\": \"Other\", \"Task Categories\": [\"Video Captioning\"], \"Inferred Metadata\": {\"GitHub License\": \"\", \"GitHub Stars (June 2024)\": \"\", \"GitHub Topics\": \"\", \"Github Date\": \"\", \"HF Config\": \"\", \"HF Config License\": \"\", \"HF Dataset\": \"\", \"HF Date\": \"\", \"HF Downloads (June 2024)\": \"\", \"HF Likes (June 2024)\": \"\", \"HF Yaml License\": \"\", \"PwC Date\": \"\", \"PwC Description\": \"No code available yet.\", \"PwC License Name\": \"\", \"PwC License URL\": \"\", \"S2 Citation Count (June 2024)\": \"\", \"S2 Date\": \"\"}, \"Bibtex\": \"@Article{Monfort2021SpokenML,\\n author = {Mathew Monfort and SouYoung Jin},\\n booktitle = {Computer Vision and Pattern Recognition},\\n journal = {2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},\\n pages = {14866-14876},\\n title = {Spoken Moments: Learning Joint Audio-Visual Representations from Video Descriptions},\\n year = {2021}\\n}\\n\", \"License Use (DataProvenance)\": \"commercial\", \"License Attribution (DataProvenance)\": 0, \"License Share Alike (DataProvenance)\": 0, \"License Use (DataProvenance IgnoreOpenAI)\": \"commercial\", \"License Attribution (DataProvenance IgnoreOpenAI)\": 0, \"License Share Alike (DataProvenance IgnoreOpenAI)\": 0, \"License Type\": \"Commercial\"}, {\"Unique Dataset Identifier\": \"trecvid\", \"Collection\": \"trecvid\", \"Collection URL\": \"https://arxiv.org/abs/2009.09984\", \"Dataset Name\": \"TRECVID\", \"Paper Title\": \"TRECVID\", \"Paper URL\": \"https://arxiv.org/abs/2009.09984\", \"GitHub URL\": \"\", \"Hugging Face URL\": \"\", \"Papers with Code URL\": \"https://paperswithcode.com/paper/trecvid-2019-an-evaluation-campaign-to/review/\", \"ArXiv URL\": \"https://arxiv.org/abs/2009.09984\", \"Semantic Scholar Corpus ID\": 212694843, \"Year Released\": \"2019\", \"Text Sources\": [\"undisclosed web\", \"bbc\"], \"Licenses\": [{\"License\": \"CC BY-NC-SA 4.0\", \"License URL\": \"https://trecvid.nist.gov/\"}], \"Creators\": [\"National Institute of Standards and Technology\"], \"Countries\": [\"United States of America\"], \"License Verified By\": \"Vivek Sharma\", \"Video Hours\": 1000.0, \"Taken Down\": \"False\", \"Video Sources\": \"Other\", \"Task Categories\": [\"Video Captioning\", \"Video Classification\"], \"Inferred Metadata\": {\"GitHub License\": \"\", \"GitHub Stars (June 2024)\": \"\", \"GitHub Topics\": \"\", \"Github Date\": \"\", \"HF Config\": \"\", \"HF Config License\": \"\", \"HF Dataset\": \"\", \"HF Date\": \"\", \"HF Downloads (June 2024)\": \"\", \"HF Likes (June 2024)\": \"\", \"HF Yaml License\": \"\", \"PwC Date\": \"\", \"PwC Description\": \"Paper tables with annotated results for TRECVID 2019: An Evaluation Campaign to Benchmark Video Activity Detection, Video Captioning and Matching, and Video Search & Retrieval\", \"PwC License Name\": \"\", \"PwC License URL\": \"\", \"S2 Citation Count (June 2024)\": \"\", \"S2 Date\": \"\"}, \"Bibtex\": \"@Article{Awad2019TRECVID2A,\\n author = {G. Awad and A. Butt and Keith Curtis and Yooyoung Lee and Jonathan G. Fiscus and A. Godil and Andrew Delgado and Jesse Zhang and Eliot Godard and Lukas L. Diduch and A. Smeaton and Yyette Graham and Wessel Kraaij and G. Qu\\u00e9not},\\n booktitle = {TREC Video Retrieval Evaluation},\\n journal = {ArXiv},\\n title = {TRECVID 2019: An evaluation campaign to benchmark Video Activity Detection, Video Captioning and Matching, and Video Search & retrieval},\\n volume = {abs/2009.09984},\\n year = {2019}\\n}\\n\", \"License Use (DataProvenance)\": \"non-commercial\", \"License Attribution (DataProvenance)\": 1, \"License Share Alike (DataProvenance)\": 1, \"License Use (DataProvenance IgnoreOpenAI)\": \"non-commercial\", \"License Attribution (DataProvenance IgnoreOpenAI)\": 1, \"License Share Alike (DataProvenance IgnoreOpenAI)\": 1, \"License Type\": \"Non-Commercial/Academic\"}, {\"Unique Dataset Identifier\": \"spoken-moments\", \"Collection\": \"spoken-moments\", \"Collection URL\": \"https://arxiv.org/abs/2105.04489\", \"Dataset Name\": \"Spoken Moments\", \"Paper Title\": \"Spoken Moments\", \"Paper URL\": \"https://arxiv.org/abs/2105.04489\", \"GitHub URL\": \"\", \"Hugging Face URL\": \"\", \"Papers with Code URL\": \"https://paperswithcode.com/paper/spoken-moments-learning-joint-audio-visual\", \"ArXiv URL\": \"https://arxiv.org/abs/2105.04489\", \"Semantic Scholar Corpus ID\": 233992735, \"Year Released\": \"2021\", \"Text Sources\": [\"youtube\", \"flickr\", \"vine\", \"metacafe\", \"peeks\", \"vimeo\", \"videoblocks\", \"bing videos\", \"giphy\", \"weather channel\", \"getty-images\"], \"Licenses\": [{\"License\": \"Custom\", \"License URL\": \"https://docs.google.com/forms/d/e/1FAIpQLSdUOScon6uWkslgcA2-BCkAhMp86SihgKtMqX53zRf-E2fIHw/formResponse\"}], \"Creators\": [\"Massachusetts Institute of Technology\", \"IBM\", \"The University of Texas at Austin\"], \"Countries\": [\"United States of America\"], \"License Verified By\": \"Vivek Sharma\", \"Video Hours\": 416.67, \"Taken Down\": \"False\", \"Video Sources\": \"Other\", \"Task Categories\": [\"Video Captioning\"], \"Inferred Metadata\": {\"GitHub License\": \"\", \"GitHub Stars (June 2024)\": \"\", \"GitHub Topics\": \"\", \"Github Date\": \"\", \"HF Config\": \"\", \"HF Config License\": \"\", \"HF Dataset\": \"\", \"HF Date\": \"\", \"HF Downloads (June 2024)\": \"\", \"HF Likes (June 2024)\": \"\", \"HF Yaml License\": \"\", \"PwC Date\": \"\", \"PwC Description\": \"No code available yet.\", \"PwC License Name\": \"\", \"PwC License URL\": \"\", \"S2 Citation Count (June 2024)\": \"\", \"S2 Date\": \"\"}, \"Bibtex\": \"@Article{Monfort2021SpokenML,\\n author = {Mathew Monfort and SouYoung Jin},\\n booktitle = {Computer Vision and Pattern Recognition},\\n journal = {2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},\\n pages = {14866-14876},\\n title = {Spoken Moments: Learning Joint Audio-Visual Representations from Video Descriptions},\\n year = {2021}\\n}\\n\", \"License Use (DataProvenance)\": \"commercial\", \"License Attribution (DataProvenance)\": 0, \"License Share Alike (DataProvenance)\": 0, \"License Use (DataProvenance IgnoreOpenAI)\": \"commercial\", \"License Attribution (DataProvenance IgnoreOpenAI)\": 0, \"License Share Alike (DataProvenance IgnoreOpenAI)\": 0, \"License Type\": \"Commercial\"}, {\"Unique Dataset Identifier\": \"spoken-moments\", \"Collection\": \"spoken-moments\", \"Collection URL\": \"https://arxiv.org/abs/2105.04489\", \"Dataset Name\": \"Spoken Moments\", \"Paper Title\": \"Spoken Moments\", \"Paper URL\": \"https://arxiv.org/abs/2105.04489\", \"GitHub URL\": \"\", \"Hugging Face URL\": \"\", \"Papers with Code URL\": \"https://paperswithcode.com/paper/spoken-moments-learning-joint-audio-visual\", \"ArXiv URL\": \"https://arxiv.org/abs/2105.04489\", \"Semantic Scholar Corpus ID\": 233992735, \"Year Released\": \"2021\", \"Text Sources\": [\"youtube\", \"flickr\", \"vine\", \"metacafe\", \"peeks\", \"vimeo\", \"videoblocks\", \"bing videos\", \"giphy\", \"weather channel\", \"getty-images\"], \"Licenses\": [{\"License\": \"Custom\", \"License URL\": \"https://docs.google.com/forms/d/e/1FAIpQLSdUOScon6uWkslgcA2-BCkAhMp86SihgKtMqX53zRf-E2fIHw/formResponse\"}], \"Creators\": [\"Massachusetts Institute of Technology\", \"IBM\", \"The University of Texas at Austin\"], \"Countries\": [\"United States of America\"], \"License Verified By\": \"Vivek Sharma\", \"Video Hours\": 416.67, \"Taken Down\": \"False\", \"Video Sources\": \"Other\", \"Task Categories\": [\"Video Captioning\"], \"Inferred Metadata\": {\"GitHub License\": \"\", \"GitHub Stars (June 2024)\": \"\", \"GitHub Topics\": \"\", \"Github Date\": \"\", \"HF Config\": \"\", \"HF Config License\": \"\", \"HF Dataset\": \"\", \"HF Date\": \"\", \"HF Downloads (June 2024)\": \"\", \"HF Likes (June 2024)\": \"\", \"HF Yaml License\": \"\", \"PwC Date\": \"\", \"PwC Description\": \"No code available yet.\", \"PwC License Name\": \"\", \"PwC License URL\": \"\", \"S2 Citation Count (June 2024)\": \"\", \"S2 Date\": \"\"}, \"Bibtex\": \"@Article{Monfort2021SpokenML,\\n author = {Mathew Monfort and SouYoung Jin},\\n booktitle = {Computer Vision and Pattern Recognition},\\n journal = {2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},\\n pages = {14866-14876},\\n title = {Spoken Moments: Learning Joint Audio-Visual Representations from Video Descriptions},\\n year = {2021}\\n}\\n\", \"License Use (DataProvenance)\": \"commercial\", \"License Attribution (DataProvenance)\": 0, \"License Share Alike (DataProvenance)\": 0, \"License Use (DataProvenance IgnoreOpenAI)\": \"commercial\", \"License Attribution (DataProvenance IgnoreOpenAI)\": 0, \"License Share Alike (DataProvenance IgnoreOpenAI)\": 0, \"License Type\": \"Commercial\"}, {\"Unique Dataset Identifier\": \"egopet\", \"Collection\": \"egopet\", \"Collection URL\": \"https://arxiv.org/pdf/2404.09991\", \"Dataset Name\": \"EgoPet: Egomotion and Interaction Data from an Animal's Perspective\", \"Paper Title\": \"EgoPet: Egomotion and Interaction Data from an Animal's Perspective\", \"Paper URL\": \"https://arxiv.org/pdf/2404.09991\", \"GitHub URL\": \"\", \"Hugging Face URL\": \"\", \"Papers with Code URL\": \"https://paperswithcode.com/paper/egopet-egomotion-and-interaction-data-from-an\", \"ArXiv URL\": \"https://arxiv.org/pdf/2404.09991\", \"Semantic Scholar Corpus ID\": 269148727, \"Year Released\": \"2024\", \"Text Sources\": [\"tiktok\", \"youtube\"], \"Licenses\": [{\"License\": \"CC BY-NC 4.0\", \"License URL\": \"https://github.com/DannyTran123/egopet/blob/main/LICENSE\"}], \"Creators\": [\"Tel Aviv University\", \"UC Berkeley\", \"New York University\"], \"Countries\": [\"United States of America\", \"Israel\"], \"License Verified By\": \"Vivek Sharma\", \"Video Hours\": 84.0, \"Taken Down\": \"False\", \"Video Sources\": \"Other\", \"Task Categories\": [\"Misc (Locomotion Prediction\", \"Visual Interaction Prediction\", \"Vision to Proprioception Prediction)\"], \"Inferred Metadata\": null, \"Bibtex\": null, \"License Use (DataProvenance)\": \"non-commercial\", \"License Attribution (DataProvenance)\": 1, \"License Share Alike (DataProvenance)\": 0, \"License Use (DataProvenance IgnoreOpenAI)\": \"non-commercial\", \"License Attribution (DataProvenance IgnoreOpenAI)\": 1, \"License Share Alike (DataProvenance IgnoreOpenAI)\": 0, \"License Type\": \"Non-Commercial/Academic\"}, {\"Unique Dataset Identifier\": \"queryd\", \"Collection\": \"queryd\", \"Collection URL\": \"https://arxiv.org/abs/2011.11071\", \"Dataset Name\": \"QuerYD\", \"Paper Title\": \"QuerYD\", \"Paper URL\": \"https://arxiv.org/abs/2011.11071\", \"GitHub URL\": \"\", \"Hugging Face URL\": \"\", \"Papers with Code URL\": \"https://paperswithcode.com/dataset/queryd\", \"ArXiv URL\": \"https://arxiv.org/abs/2011.11071\", \"Semantic Scholar Corpus ID\": 261006321, \"Year Released\": \"2021\", \"Text Sources\": [\"youtube\", \"youdescribe\"], \"Licenses\": [{\"License\": \"Unspecified\", \"License URL\": \"\"}], \"Creators\": [\"University of Oxford\"], \"Countries\": [\"United Kingdom\"], \"License Verified By\": \"Vivek Sharma\", \"Video Hours\": 207.0, \"Taken Down\": \"False\", \"Video Sources\": \"Other\", \"Task Categories\": [\"Video Captioning\"], \"Inferred Metadata\": {\"GitHub License\": \"\", \"GitHub Stars (June 2024)\": \"\", \"GitHub Topics\": \"\", \"Github Date\": \"\", \"HF Config\": \"\", \"HF Config License\": \"\", \"HF Dataset\": \"\", \"HF Date\": \"\", \"HF Downloads (June 2024)\": \"\", \"HF Likes (June 2024)\": \"\", \"HF Yaml License\": \"\", \"PwC Date\": \"\", \"PwC Description\": \"A large-scale dataset for retrieval and event localisation in video. A unique feature of the dataset is the availability of two audio tracks for each video: the original audio, and a high-quality spoken description of the visual content.\", \"PwC License Name\": \"Unspecified\", \"PwC License URL\": null, \"S2 Citation Count (June 2024)\": \"\", \"S2 Date\": \"\"}, \"Bibtex\": \"@Article{Oncescu2021QUERYDAV,\\n author = {Andreea-Maria Oncescu and Jo\\u00e3o F. Henriques and Yang Liu and Andrew Zisserman and Samuel Albanie},\\n booktitle = {IEEE International Conference on Acoustics, Speech, and Signal Processing},\\n journal = {ICASSP 2021 - 2021 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},\\n pages = {2265-2269},\\n title = {QUERYD: A Video Dataset with High-Quality Text and Audio Narrations},\\n year = {2021}\\n}\\n\", \"License Use (DataProvenance)\": \"unspecified\", \"License Attribution (DataProvenance)\": 0, \"License Share Alike (DataProvenance)\": 0, \"License Use (DataProvenance IgnoreOpenAI)\": \"unspecified\", \"License Attribution (DataProvenance IgnoreOpenAI)\": 0, \"License Share Alike (DataProvenance IgnoreOpenAI)\": 0, \"License Type\": \"Unspecified\"}, {\"Unique Dataset Identifier\": \"moments-in-time-dataset\", \"Collection\": \"moments-in-time-dataset\", \"Collection URL\": \"https://arxiv.org/pdf/1801.03150.pdf\", \"Dataset Name\": \"Moments in Time Dataset: one million videos for event understanding (TPAMI 2019)\", \"Paper Title\": \"Moments in Time Dataset: one million videos for event understanding (TPAMI 2019)\", \"Paper URL\": \"https://arxiv.org/pdf/1801.03150.pdf\", \"GitHub URL\": \"\", \"Hugging Face URL\": \"\", \"Papers with Code URL\": \"\", \"ArXiv URL\": \"https://arxiv.org/pdf/1801.03150.pdf\", \"Semantic Scholar Corpus ID\": 11868155, \"Year Released\": \"2019\", \"Text Sources\": [\"youtube\", \"flickr\", \"vine\", \"metacafe\", \"peeks\", \"vimeo\", \"videoblocks\", \"bing videos\", \"giphy\", \"weather channel\", \"getty-images\"], \"Licenses\": [{\"License\": \"Custom\", \"License URL\": \"http://moments.csail.mit.edu/\"}], \"Creators\": [\"Massachusetts Institute of Technology\"], \"Countries\": [\"United States of America\"], \"License Verified By\": \"Vivek Sharma\", \"Video Hours\": 833.0, \"Taken Down\": \"False\", \"Video Sources\": \"Other\", \"Task Categories\": [\"Video Classification\"], \"Inferred Metadata\": {\"GitHub License\": \"\", \"GitHub Stars (June 2024)\": \"\", \"GitHub Topics\": \"\", \"Github Date\": \"\", \"HF Config\": \"\", \"HF Config License\": \"\", \"HF Dataset\": \"\", \"HF Date\": \"\", \"HF Downloads (June 2024)\": \"\", \"HF Likes (June 2024)\": \"\", \"HF Yaml License\": \"\", \"PwC Date\": \"\", \"PwC Description\": \"\", \"PwC License Name\": \"\", \"PwC License URL\": \"\", \"S2 Citation Count (June 2024)\": \"\", \"S2 Date\": \"\"}, \"Bibtex\": \"@Article{Monfort2018MomentsIT,\\n author = {Mathew Monfort and Bolei Zhou and Sarah Adel Bargal and A. Andonian and Tom Yan and K. Ramakrishnan and L. Brown and Quanfu Fan and Dan Gutfreund and Carl Vondrick and A. Oliva},\\n booktitle = {IEEE Transactions on Pattern Analysis and Machine Intelligence},\\n journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},\\n pages = {502-508},\\n title = {Moments in Time Dataset: One Million Videos for Event Understanding},\\n volume = {42},\\n year = {2018}\\n}\\n\", \"License Use (DataProvenance)\": \"academic-only\", \"License Attribution (DataProvenance)\": 1, \"License Share Alike (DataProvenance)\": 0, \"License Use (DataProvenance IgnoreOpenAI)\": \"academic-only\", \"License Attribution (DataProvenance IgnoreOpenAI)\": 1, \"License Share Alike (DataProvenance IgnoreOpenAI)\": 0, \"License Type\": \"Non-Commercial/Academic\"}, {\"Unique Dataset Identifier\": \"moments-in-time-dataset\", \"Collection\": \"moments-in-time-dataset\", \"Collection URL\": \"https://arxiv.org/pdf/1801.03150.pdf\", \"Dataset Name\": \"Moments in Time Dataset: one million videos for event understanding (TPAMI 2019)\", \"Paper Title\": \"Moments in Time Dataset: one million videos for event understanding (TPAMI 2019)\", \"Paper URL\": \"https://arxiv.org/pdf/1801.03150.pdf\", \"GitHub URL\": \"\", \"Hugging Face URL\": \"\", \"Papers with Code URL\": \"\", \"ArXiv URL\": \"https://arxiv.org/pdf/1801.03150.pdf\", \"Semantic Scholar Corpus ID\": 11868155, \"Year Released\": \"2019\", \"Text Sources\": [\"youtube\", \"flickr\", \"vine\", \"metacafe\", \"peeks\", \"vimeo\", \"videoblocks\", \"bing videos\", \"giphy\", \"weather channel\", \"getty-images\"], \"Licenses\": [{\"License\": \"Custom\", \"License URL\": \"http://moments.csail.mit.edu/\"}], \"Creators\": [\"Massachusetts Institute of Technology\"], \"Countries\": [\"United States of America\"], \"License Verified By\": \"Vivek Sharma\", \"Video Hours\": 833.0, \"Taken Down\": \"False\", \"Video Sources\": \"Other\", \"Task Categories\": [\"Video Classification\"], \"Inferred Metadata\": {\"GitHub License\": \"\", \"GitHub Stars (June 2024)\": \"\", \"GitHub Topics\": \"\", \"Github Date\": \"\", \"HF Config\": \"\", \"HF Config License\": \"\", \"HF Dataset\": \"\", \"HF Date\": \"\", \"HF Downloads (June 2024)\": \"\", \"HF Likes (June 2024)\": \"\", \"HF Yaml License\": \"\", \"PwC Date\": \"\", \"PwC Description\": \"\", \"PwC License Name\": \"\", \"PwC License URL\": \"\", \"S2 Citation Count (June 2024)\": \"\", \"S2 Date\": \"\"}, \"Bibtex\": \"@Article{Monfort2018MomentsIT,\\n author = {Mathew Monfort and Bolei Zhou and Sarah Adel Bargal and A. Andonian and Tom Yan and K. Ramakrishnan and L. Brown and Quanfu Fan and Dan Gutfreund and Carl Vondrick and A. Oliva},\\n booktitle = {IEEE Transactions on Pattern Analysis and Machine Intelligence},\\n journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},\\n pages = {502-508},\\n title = {Moments in Time Dataset: One Million Videos for Event Understanding},\\n volume = {42},\\n year = {2018}\\n}\\n\", \"License Use (DataProvenance)\": \"academic-only\", \"License Attribution (DataProvenance)\": 1, \"License Share Alike (DataProvenance)\": 0, \"License Use (DataProvenance IgnoreOpenAI)\": \"academic-only\", \"License Attribution (DataProvenance IgnoreOpenAI)\": 1, \"License Share Alike (DataProvenance IgnoreOpenAI)\": 0, \"License Type\": \"Non-Commercial/Academic\"}, {\"Unique Dataset Identifier\": \"soa-dataset\", \"Collection\": \"soa-dataset\", \"Collection URL\": \"https://arxiv.org/pdf/1904.11451\", \"Dataset Name\": \"Scenes-objects-actions: A multi-task, multi-label video dataset\", \"Paper Title\": \"Scenes-objects-actions: A multi-task, multi-label video dataset\", \"Paper URL\": \"https://arxiv.org/pdf/1904.11451\", \"GitHub URL\": \"\", \"Hugging Face URL\": \"\", \"Papers with Code URL\": \"https://paperswithcode.com/paper/scenes-objects-actions-a-multi-task-multi\", \"ArXiv URL\": \"https://arxiv.org/pdf/1904.11451\", \"Semantic Scholar Corpus ID\": 52968009, \"Year Released\": \"2018\", \"Text Sources\": [\"facebook\"], \"Licenses\": [{\"License\": \"Unspecified\", \"License URL\": \"\"}], \"Creators\": [\"Meta\"], \"Countries\": [\"United States of America\"], \"License Verified By\": \"Vivek Sharma\", \"Video Hours\": 1561.1, \"Taken Down\": \"False\", \"Video Sources\": \"Other\", \"Task Categories\": [\"Video Classification\"], \"Inferred Metadata\": {\"GitHub License\": \"\", \"GitHub Stars (June 2024)\": \"\", \"GitHub Topics\": \"\", \"Github Date\": \"\", \"HF Config\": \"\", \"HF Config License\": \"\", \"HF Dataset\": \"\", \"HF Date\": \"\", \"HF Downloads (June 2024)\": \"\", \"HF Likes (June 2024)\": \"\", \"HF Yaml License\": \"\", \"PwC Date\": \"\", \"PwC Description\": \"No code available yet.\", \"PwC License Name\": \"\", \"PwC License URL\": \"\", \"S2 Citation Count (June 2024)\": \"\", \"S2 Date\": \"\"}, \"Bibtex\": \"@Article{Ray2018ScenesObjectsActionsAM,\\n author = {Jamie Ray and Heng Wang and Du Tran and Yufei Wang and Matt Feiszli and L. Torresani and Manohar Paluri},\\n booktitle = {European Conference on Computer Vision},\\n pages = {660-676},\\n title = {Scenes-Objects-Actions: A Multi-task, Multi-label Video Dataset},\\n year = {2018}\\n}\\n\", \"License Use (DataProvenance)\": \"unspecified\", \"License Attribution (DataProvenance)\": 0, \"License Share Alike (DataProvenance)\": 0, \"License Use (DataProvenance IgnoreOpenAI)\": \"unspecified\", \"License Attribution (DataProvenance IgnoreOpenAI)\": 0, \"License Share Alike (DataProvenance IgnoreOpenAI)\": 0, \"License Type\": \"Unspecified\"}, {\"Unique Dataset Identifier\": \"moments-in-time-dataset\", \"Collection\": \"moments-in-time-dataset\", \"Collection URL\": \"https://arxiv.org/pdf/1801.03150.pdf\", \"Dataset Name\": \"Moments in Time Dataset: one million videos for event understanding (TPAMI 2019)\", \"Paper Title\": \"Moments in Time Dataset: one million videos for event understanding (TPAMI 2019)\", \"Paper URL\": \"https://arxiv.org/pdf/1801.03150.pdf\", \"GitHub URL\": \"\", \"Hugging Face URL\": \"\", \"Papers with Code URL\": \"\", \"ArXiv URL\": \"https://arxiv.org/pdf/1801.03150.pdf\", \"Semantic Scholar Corpus ID\": 11868155, \"Year Released\": \"2019\", \"Text Sources\": [\"youtube\", \"flickr\", \"vine\", \"metacafe\", \"peeks\", \"vimeo\", \"videoblocks\", \"bing videos\", \"giphy\", \"weather channel\", \"getty-images\"], \"Licenses\": [{\"License\": \"Custom\", \"License URL\": \"http://moments.csail.mit.edu/\"}], \"Creators\": [\"Massachusetts Institute of Technology\"], \"Countries\": [\"United States of America\"], \"License Verified By\": \"Vivek Sharma\", \"Video Hours\": 833.0, \"Taken Down\": \"False\", \"Video Sources\": \"Other\", \"Task Categories\": [\"Video Classification\"], \"Inferred Metadata\": {\"GitHub License\": \"\", \"GitHub Stars (June 2024)\": \"\", \"GitHub Topics\": \"\", \"Github Date\": \"\", \"HF Config\": \"\", \"HF Config License\": \"\", \"HF Dataset\": \"\", \"HF Date\": \"\", \"HF Downloads (June 2024)\": \"\", \"HF Likes (June 2024)\": \"\", \"HF Yaml License\": \"\", \"PwC Date\": \"\", \"PwC Description\": \"\", \"PwC License Name\": \"\", \"PwC License URL\": \"\", \"S2 Citation Count (June 2024)\": \"\", \"S2 Date\": \"\"}, \"Bibtex\": \"@Article{Monfort2018MomentsIT,\\n author = {Mathew Monfort and Bolei Zhou and Sarah Adel Bargal and A. Andonian and Tom Yan and K. Ramakrishnan and L. Brown and Quanfu Fan and Dan Gutfreund and Carl Vondrick and A. Oliva},\\n booktitle = {IEEE Transactions on Pattern Analysis and Machine Intelligence},\\n journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},\\n pages = {502-508},\\n title = {Moments in Time Dataset: One Million Videos for Event Understanding},\\n volume = {42},\\n year = {2018}\\n}\\n\", \"License Use (DataProvenance)\": \"academic-only\", \"License Attribution (DataProvenance)\": 1, \"License Share Alike (DataProvenance)\": 0, \"License Use (DataProvenance IgnoreOpenAI)\": \"academic-only\", \"License Attribution (DataProvenance IgnoreOpenAI)\": 1, \"License Share Alike (DataProvenance IgnoreOpenAI)\": 0, \"License Type\": \"Non-Commercial/Academic\"}, {\"Unique Dataset Identifier\": \"moments-in-time-dataset\", \"Collection\": \"moments-in-time-dataset\", \"Collection URL\": \"https://arxiv.org/pdf/1801.03150.pdf\", \"Dataset Name\": \"Moments in Time Dataset: one million videos for event understanding (TPAMI 2019)\", \"Paper Title\": \"Moments in Time Dataset: one million videos for event understanding (TPAMI 2019)\", \"Paper URL\": \"https://arxiv.org/pdf/1801.03150.pdf\", \"GitHub URL\": \"\", \"Hugging Face URL\": \"\", \"Papers with Code URL\": \"\", \"ArXiv URL\": \"https://arxiv.org/pdf/1801.03150.pdf\", \"Semantic Scholar Corpus ID\": 11868155, \"Year Released\": \"2019\", \"Text Sources\": [\"youtube\", \"flickr\", \"vine\", \"metacafe\", \"peeks\", \"vimeo\", \"videoblocks\", \"bing videos\", \"giphy\", \"weather channel\", \"getty-images\"], \"Licenses\": [{\"License\": \"Custom\", \"License URL\": \"http://moments.csail.mit.edu/\"}], \"Creators\": [\"Massachusetts Institute of Technology\"], \"Countries\": [\"United States of America\"], \"License Verified By\": \"Vivek Sharma\", \"Video Hours\": 833.0, \"Taken Down\": \"False\", \"Video Sources\": \"Other\", \"Task Categories\": [\"Video Classification\"], \"Inferred Metadata\": {\"GitHub License\": \"\", \"GitHub Stars (June 2024)\": \"\", \"GitHub Topics\": \"\", \"Github Date\": \"\", \"HF Config\": \"\", \"HF Config License\": \"\", \"HF Dataset\": \"\", \"HF Date\": \"\", \"HF Downloads (June 2024)\": \"\", \"HF Likes (June 2024)\": \"\", \"HF Yaml License\": \"\", \"PwC Date\": \"\", \"PwC Description\": \"\", \"PwC License Name\": \"\", \"PwC License URL\": \"\", \"S2 Citation Count (June 2024)\": \"\", \"S2 Date\": \"\"}, \"Bibtex\": \"@Article{Monfort2018MomentsIT,\\n author = {Mathew Monfort and Bolei Zhou and Sarah Adel Bargal and A. Andonian and Tom Yan and K. Ramakrishnan and L. Brown and Quanfu Fan and Dan Gutfreund and Carl Vondrick and A. Oliva},\\n booktitle = {IEEE Transactions on Pattern Analysis and Machine Intelligence},\\n journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},\\n pages = {502-508},\\n title = {Moments in Time Dataset: One Million Videos for Event Understanding},\\n volume = {42},\\n year = {2018}\\n}\\n\", \"License Use (DataProvenance)\": \"academic-only\", \"License Attribution (DataProvenance)\": 1, \"License Share Alike (DataProvenance)\": 0, \"License Use (DataProvenance IgnoreOpenAI)\": \"academic-only\", \"License Attribution (DataProvenance IgnoreOpenAI)\": 1, \"License Share Alike (DataProvenance IgnoreOpenAI)\": 0, \"License Type\": \"Non-Commercial/Academic\"}, {\"Unique Dataset Identifier\": \"moments-in-time-dataset\", \"Collection\": \"moments-in-time-dataset\", \"Collection URL\": \"https://arxiv.org/pdf/1801.03150.pdf\", \"Dataset Name\": \"Moments in Time Dataset: one million videos for event understanding (TPAMI 2019)\", \"Paper Title\": \"Moments in Time Dataset: one million videos for event understanding (TPAMI 2019)\", \"Paper URL\": \"https://arxiv.org/pdf/1801.03150.pdf\", \"GitHub URL\": \"\", \"Hugging Face URL\": \"\", \"Papers with Code URL\": \"\", \"ArXiv URL\": \"https://arxiv.org/pdf/1801.03150.pdf\", \"Semantic Scholar Corpus ID\": 11868155, \"Year Released\": \"2019\", \"Text Sources\": [\"youtube\", \"flickr\", \"vine\", \"metacafe\", \"peeks\", \"vimeo\", \"videoblocks\", \"bing videos\", \"giphy\", \"weather channel\", \"getty-images\"], \"Licenses\": [{\"License\": \"Custom\", \"License URL\": \"http://moments.csail.mit.edu/\"}], \"Creators\": [\"Massachusetts Institute of Technology\"], \"Countries\": [\"United States of America\"], \"License Verified By\": \"Vivek Sharma\", \"Video Hours\": 833.0, \"Taken Down\": \"False\", \"Video Sources\": \"Other\", \"Task Categories\": [\"Video Classification\"], \"Inferred Metadata\": {\"GitHub License\": \"\", \"GitHub Stars (June 2024)\": \"\", \"GitHub Topics\": \"\", \"Github Date\": \"\", \"HF Config\": \"\", \"HF Config License\": \"\", \"HF Dataset\": \"\", \"HF Date\": \"\", \"HF Downloads (June 2024)\": \"\", \"HF Likes (June 2024)\": \"\", \"HF Yaml License\": \"\", \"PwC Date\": \"\", \"PwC Description\": \"\", \"PwC License Name\": \"\", \"PwC License URL\": \"\", \"S2 Citation Count (June 2024)\": \"\", \"S2 Date\": \"\"}, \"Bibtex\": \"@Article{Monfort2018MomentsIT,\\n author = {Mathew Monfort and Bolei Zhou and Sarah Adel Bargal and A. Andonian and Tom Yan and K. Ramakrishnan and L. Brown and Quanfu Fan and Dan Gutfreund and Carl Vondrick and A. Oliva},\\n booktitle = {IEEE Transactions on Pattern Analysis and Machine Intelligence},\\n journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},\\n pages = {502-508},\\n title = {Moments in Time Dataset: One Million Videos for Event Understanding},\\n volume = {42},\\n year = {2018}\\n}\\n\", \"License Use (DataProvenance)\": \"academic-only\", \"License Attribution (DataProvenance)\": 1, \"License Share Alike (DataProvenance)\": 0, \"License Use (DataProvenance IgnoreOpenAI)\": \"academic-only\", \"License Attribution (DataProvenance IgnoreOpenAI)\": 1, \"License Share Alike (DataProvenance IgnoreOpenAI)\": 0, \"License Type\": \"Non-Commercial/Academic\"}, {\"Unique Dataset Identifier\": \"moments-in-time-dataset\", \"Collection\": \"moments-in-time-dataset\", \"Collection URL\": \"https://arxiv.org/pdf/1801.03150.pdf\", \"Dataset Name\": \"Moments in Time Dataset: one million videos for event understanding (TPAMI 2019)\", \"Paper Title\": \"Moments in Time Dataset: one million videos for event understanding (TPAMI 2019)\", \"Paper URL\": \"https://arxiv.org/pdf/1801.03150.pdf\", \"GitHub URL\": \"\", \"Hugging Face URL\": \"\", \"Papers with Code URL\": \"\", \"ArXiv URL\": \"https://arxiv.org/pdf/1801.03150.pdf\", \"Semantic Scholar Corpus ID\": 11868155, \"Year Released\": \"2019\", \"Text Sources\": [\"youtube\", \"flickr\", \"vine\", \"metacafe\", \"peeks\", \"vimeo\", \"videoblocks\", \"bing videos\", \"giphy\", \"weather channel\", \"getty-images\"], \"Licenses\": [{\"License\": \"Custom\", \"License URL\": \"http://moments.csail.mit.edu/\"}], \"Creators\": [\"Massachusetts Institute of Technology\"], \"Countries\": [\"United States of America\"], \"License Verified By\": \"Vivek Sharma\", \"Video Hours\": 833.0, \"Taken Down\": \"False\", \"Video Sources\": \"Other\", \"Task Categories\": [\"Video Classification\"], \"Inferred Metadata\": {\"GitHub License\": \"\", \"GitHub Stars (June 2024)\": \"\", \"GitHub Topics\": \"\", \"Github Date\": \"\", \"HF Config\": \"\", \"HF Config License\": \"\", \"HF Dataset\": \"\", \"HF Date\": \"\", \"HF Downloads (June 2024)\": \"\", \"HF Likes (June 2024)\": \"\", \"HF Yaml License\": \"\", \"PwC Date\": \"\", \"PwC Description\": \"\", \"PwC License Name\": \"\", \"PwC License URL\": \"\", \"S2 Citation Count (June 2024)\": \"\", \"S2 Date\": \"\"}, \"Bibtex\": \"@Article{Monfort2018MomentsIT,\\n author = {Mathew Monfort and Bolei Zhou and Sarah Adel Bargal and A. Andonian and Tom Yan and K. Ramakrishnan and L. Brown and Quanfu Fan and Dan Gutfreund and Carl Vondrick and A. Oliva},\\n booktitle = {IEEE Transactions on Pattern Analysis and Machine Intelligence},\\n journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},\\n pages = {502-508},\\n title = {Moments in Time Dataset: One Million Videos for Event Understanding},\\n volume = {42},\\n year = {2018}\\n}\\n\", \"License Use (DataProvenance)\": \"academic-only\", \"License Attribution (DataProvenance)\": 1, \"License Share Alike (DataProvenance)\": 0, \"License Use (DataProvenance IgnoreOpenAI)\": \"academic-only\", \"License Attribution (DataProvenance IgnoreOpenAI)\": 1, \"License Share Alike (DataProvenance IgnoreOpenAI)\": 0, \"License Type\": \"Non-Commercial/Academic\"}, {\"Unique Dataset Identifier\": \"moments-in-time-dataset\", \"Collection\": \"moments-in-time-dataset\", \"Collection URL\": \"https://arxiv.org/pdf/1801.03150.pdf\", \"Dataset Name\": \"Moments in Time Dataset: one million videos for event understanding (TPAMI 2019)\", \"Paper Title\": \"Moments in Time Dataset: one million videos for event understanding (TPAMI 2019)\", \"Paper URL\": \"https://arxiv.org/pdf/1801.03150.pdf\", \"GitHub URL\": \"\", \"Hugging Face URL\": \"\", \"Papers with Code URL\": \"\", \"ArXiv URL\": \"https://arxiv.org/pdf/1801.03150.pdf\", \"Semantic Scholar Corpus ID\": 11868155, \"Year Released\": \"2019\", \"Text Sources\": [\"youtube\", \"flickr\", \"vine\", \"metacafe\", \"peeks\", \"vimeo\", \"videoblocks\", \"bing videos\", \"giphy\", \"weather channel\", \"getty-images\"], \"Licenses\": [{\"License\": \"Custom\", \"License URL\": \"http://moments.csail.mit.edu/\"}], \"Creators\": [\"Massachusetts Institute of Technology\"], \"Countries\": [\"United States of America\"], \"License Verified By\": \"Vivek Sharma\", \"Video Hours\": 833.0, \"Taken Down\": \"False\", \"Video Sources\": \"Other\", \"Task Categories\": [\"Video Classification\"], \"Inferred Metadata\": {\"GitHub License\": \"\", \"GitHub Stars (June 2024)\": \"\", \"GitHub Topics\": \"\", \"Github Date\": \"\", \"HF Config\": \"\", \"HF Config License\": \"\", \"HF Dataset\": \"\", \"HF Date\": \"\", \"HF Downloads (June 2024)\": \"\", \"HF Likes (June 2024)\": \"\", \"HF Yaml License\": \"\", \"PwC Date\": \"\", \"PwC Description\": \"\", \"PwC License Name\": \"\", \"PwC License URL\": \"\", \"S2 Citation Count (June 2024)\": \"\", \"S2 Date\": \"\"}, \"Bibtex\": \"@Article{Monfort2018MomentsIT,\\n author = {Mathew Monfort and Bolei Zhou and Sarah Adel Bargal and A. Andonian and Tom Yan and K. Ramakrishnan and L. Brown and Quanfu Fan and Dan Gutfreund and Carl Vondrick and A. Oliva},\\n booktitle = {IEEE Transactions on Pattern Analysis and Machine Intelligence},\\n journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},\\n pages = {502-508},\\n title = {Moments in Time Dataset: One Million Videos for Event Understanding},\\n volume = {42},\\n year = {2018}\\n}\\n\", \"License Use (DataProvenance)\": \"academic-only\", \"License Attribution (DataProvenance)\": 1, \"License Share Alike (DataProvenance)\": 0, \"License Use (DataProvenance IgnoreOpenAI)\": \"academic-only\", \"License Attribution (DataProvenance IgnoreOpenAI)\": 1, \"License Share Alike (DataProvenance IgnoreOpenAI)\": 0, \"License Type\": \"Non-Commercial/Academic\"}, {\"Unique Dataset Identifier\": \"tgif\", \"Collection\": \"tgif\", \"Collection URL\": \"https://arxiv.org/abs/1604.02748\", \"Dataset Name\": \"TGIF\", \"Paper Title\": \"TGIF\", \"Paper URL\": \"https://arxiv.org/abs/1604.02748\", \"GitHub URL\": \"https://github.com/raingo/TGIF-Release\", \"Hugging Face URL\": \"https://huggingface.co/datasets/HuggingFaceM4/TGIF\", \"Papers with Code URL\": \"https://paperswithcode.com/dataset/tgif\", \"ArXiv URL\": \"https://arxiv.org/abs/1604.02748\", \"Semantic Scholar Corpus ID\": 6262415, \"Year Released\": \"2016\", \"Text Sources\": [\"tumblr\"], \"Licenses\": [{\"License\": \"Custom\", \"License URL\": \"https://github.com/raingo/TGIF-Release\"}], \"Creators\": [\"University of Rochester\", \"Yahoo! Inc.\", \"AiCure\"], \"Countries\": [\"United States of America\"], \"License Verified By\": \"Vivek Sharma\", \"Video Hours\": 86.1, \"Taken Down\": \"False\", \"Video Sources\": \"Other\", \"Task Categories\": [\"Video Captioning\"], \"Inferred Metadata\": {\"GitHub License\": \"BSD 3-Clause License\", \"GitHub Stars (June 2024)\": 111, \"GitHub Topics\": [], \"Github Date\": \"\", \"HF Config\": \"all\", \"HF Config License\": \"\", \"HF Dataset\": \"HuggingFaceM4/TGIF\", \"HF Date\": \"2022-05-17\", \"HF Downloads (June 2024)\": 9, \"HF Likes (June 2024)\": 11, \"HF Yaml License\": \"Unspecified\", \"PwC Date\": \"2016-04-10\", \"PwC Description\": \"The Tumblr GIF (TGIF) dataset contains 100K animated GIFs and 120K sentences describing visual content of the animated GIFs. The animated GIFs have been collected from Tumblr, from randomly selected posts published between May and June of 2015. The dataset provides the URLs of animated GIFs. The sentences are collected via crowdsourcing, with a carefully designed annotation interface that ensures high quality dataset. There is one sentence per animated GIF for the training and validation splits, and three sentences per GIF for the test split. The dataset can be used to evaluate animated GIF/video description techniques.\", \"PwC License Name\": \"Academic Research Purposes Only\", \"PwC License URL\": \"https://github.com/raingo/TGIF-Release#license\", \"S2 Citation Count (June 2024)\": \"\", \"S2 Date\": \"\"}, \"Bibtex\": \"@Article{Li2016TGIFAN,\\n author = {Yuncheng Li and Yale Song and Liangliang Cao and Joel R. Tetreault and Larry Goldberg and A. Jaimes and Jiebo Luo},\\n booktitle = {Computer Vision and Pattern Recognition},\\n journal = {2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},\\n pages = {4641-4650},\\n title = {TGIF: A New Dataset and Benchmark on Animated GIF Description},\\n year = {2016}\\n}\\n\", \"License Use (DataProvenance)\": \"commercial\", \"License Attribution (DataProvenance)\": 1, \"License Share Alike (DataProvenance)\": 0, \"License Use (DataProvenance IgnoreOpenAI)\": \"commercial\", \"License Attribution (DataProvenance IgnoreOpenAI)\": 1, \"License Share Alike (DataProvenance IgnoreOpenAI)\": 0, \"License Type\": \"Commercial\"}, {\"Unique Dataset Identifier\": \"moments-in-time-dataset\", \"Collection\": \"moments-in-time-dataset\", \"Collection URL\": \"https://arxiv.org/pdf/1801.03150.pdf\", \"Dataset Name\": \"Moments in Time Dataset: one million videos for event understanding (TPAMI 2019)\", \"Paper Title\": \"Moments in Time Dataset: one million videos for event understanding (TPAMI 2019)\", \"Paper URL\": \"https://arxiv.org/pdf/1801.03150.pdf\", \"GitHub URL\": \"\", \"Hugging Face URL\": \"\", \"Papers with Code URL\": \"\", \"ArXiv URL\": \"https://arxiv.org/pdf/1801.03150.pdf\", \"Semantic Scholar Corpus ID\": 11868155, \"Year Released\": \"2019\", \"Text Sources\": [\"youtube\", \"flickr\", \"vine\", \"metacafe\", \"peeks\", \"vimeo\", \"videoblocks\", \"bing videos\", \"giphy\", \"weather channel\", \"getty-images\"], \"Licenses\": [{\"License\": \"Custom\", \"License URL\": \"http://moments.csail.mit.edu/\"}], \"Creators\": [\"Massachusetts Institute of Technology\"], \"Countries\": [\"United States of America\"], \"License Verified By\": \"Vivek Sharma\", \"Video Hours\": 833.0, \"Taken Down\": \"False\", \"Video Sources\": \"Other\", \"Task Categories\": [\"Video Classification\"], \"Inferred Metadata\": {\"GitHub License\": \"\", \"GitHub Stars (June 2024)\": \"\", \"GitHub Topics\": \"\", \"Github Date\": \"\", \"HF Config\": \"\", \"HF Config License\": \"\", \"HF Dataset\": \"\", \"HF Date\": \"\", \"HF Downloads (June 2024)\": \"\", \"HF Likes (June 2024)\": \"\", \"HF Yaml License\": \"\", \"PwC Date\": \"\", \"PwC Description\": \"\", \"PwC License Name\": \"\", \"PwC License URL\": \"\", \"S2 Citation Count (June 2024)\": \"\", \"S2 Date\": \"\"}, \"Bibtex\": \"@Article{Monfort2018MomentsIT,\\n author = {Mathew Monfort and Bolei Zhou and Sarah Adel Bargal and A. Andonian and Tom Yan and K. Ramakrishnan and L. Brown and Quanfu Fan and Dan Gutfreund and Carl Vondrick and A. Oliva},\\n booktitle = {IEEE Transactions on Pattern Analysis and Machine Intelligence},\\n journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},\\n pages = {502-508},\\n title = {Moments in Time Dataset: One Million Videos for Event Understanding},\\n volume = {42},\\n year = {2018}\\n}\\n\", \"License Use (DataProvenance)\": \"academic-only\", \"License Attribution (DataProvenance)\": 1, \"License Share Alike (DataProvenance)\": 0, \"License Use (DataProvenance IgnoreOpenAI)\": \"academic-only\", \"License Attribution (DataProvenance IgnoreOpenAI)\": 1, \"License Share Alike (DataProvenance IgnoreOpenAI)\": 0, \"License Type\": \"Non-Commercial/Academic\"}, {\"Unique Dataset Identifier\": \"omnisource-web-dataset\", \"Collection\": \"omnisource-web-dataset\", \"Collection URL\": \"https://arxiv.org/abs/2003.13042\", \"Dataset Name\": \"OmniSource Web Dataset\", \"Paper Title\": \"OmniSource Web Dataset\", \"Paper URL\": \"https://arxiv.org/abs/2003.13042\", \"GitHub URL\": \"https://github.com/open-mmlab/mmaction\", \"Hugging Face URL\": \"\", \"Papers with Code URL\": \"https://paperswithcode.com/paper/omni-sourced-webly-supervised-learning-for#code\", \"ArXiv URL\": \"https://arxiv.org/abs/2003.13042\", \"Semantic Scholar Corpus ID\": 214714240, \"Year Released\": \"2020\", \"Text Sources\": [\"google videos\", \"instagram\", \"youtube\"], \"Licenses\": [{\"License\": \"Apache License 2.0\", \"License URL\": \"https://github.com/open-mmlab/mmaction?tab=readme-ov-file#license\"}], \"Creators\": [\"The Chinese University of Hong Kong\"], \"Countries\": [\"China\"], \"License Verified By\": \"Vivek Sharma\", \"Video Hours\": 13333.0, \"Taken Down\": \"False\", \"Video Sources\": \"Other\", \"Task Categories\": [\"Video Classification\"], \"Inferred Metadata\": {\"GitHub License\": \"Apache License 2.0\", \"GitHub Stars (June 2024)\": 1856, \"GitHub Topics\": [\"action-detection\", \"action-recognition\", \"pytorch\", \"spatial-temporal-action-detection\", \"temporal-action-detection\", \"temporal-action-localization\", \"video-understanding\"], \"Github Date\": \"\", \"HF Config\": \"\", \"HF Config License\": \"\", \"HF Dataset\": \"\", \"HF Date\": \"\", \"HF Downloads (June 2024)\": \"\", \"HF Likes (June 2024)\": \"\", \"HF Yaml License\": \"\", \"PwC Date\": \"\", \"PwC Description\": \"3 code implementations in PyTorch. We introduce OmniSource, a novel framework for leveraging web data to train video recognition models. OmniSource overcomes the barriers between data formats, such as images, short videos, and long untrimmed videos for webly-supervised learning. First, data samples with multiple formats, curated by task-specific data collection and automatically filtered by a teacher model, are transformed into a unified form. Then a joint-training strategy is proposed to deal with the domain gaps between multiple data sources and formats in webly-supervised learning. Several good practices, including data balancing, resampling, and cross-dataset mixup are adopted in joint training. Experiments show that by utilizing data from multiple sources and formats, OmniSource is more data-efficient in training. With only 3.5M images and 800K minutes videos crawled from the internet without human labeling (less than 2% of prior works), our models learned with OmniSource improve Top-1 accuracy of 2D- and 3D-ConvNet baseline models by 3.0% and 3.9%, respectively, on the Kinetics-400 benchmark. With OmniSource, we establish new records with different pretraining strategies for video recognition. Our best models achieve 80.4%, 80.5%, and 83.6 Top-1 accuracies on the Kinetics-400 benchmark respectively for training-from-scratch, ImageNet pre-training and IG-65M pre-training.\", \"PwC License Name\": \"\", \"PwC License URL\": \"\", \"S2 Citation Count (June 2024)\": \"\", \"S2 Date\": \"\"}, \"Bibtex\": \"@Article{Duan2020OmnisourcedWL,\\n author = {Haodong Duan and Yue Zhao and Yuanjun Xiong and Wentao Liu and Dahua Lin},\\n booktitle = {European Conference on Computer Vision},\\n journal = {ArXiv},\\n title = {Omni-sourced Webly-supervised Learning for Video Recognition},\\n volume = {abs/2003.13042},\\n year = {2020}\\n}\\n\", \"License Use (DataProvenance)\": \"commercial\", \"License Attribution (DataProvenance)\": 1, \"License Share Alike (DataProvenance)\": 0, \"License Use (DataProvenance IgnoreOpenAI)\": \"commercial\", \"License Attribution (DataProvenance IgnoreOpenAI)\": 1, \"License Share Alike (DataProvenance IgnoreOpenAI)\": 0, \"License Type\": \"Commercial\"}, {\"Unique Dataset Identifier\": \"spoken-moments\", \"Collection\": \"spoken-moments\", \"Collection URL\": \"https://arxiv.org/abs/2105.04489\", \"Dataset Name\": \"Spoken Moments\", \"Paper Title\": \"Spoken Moments\", \"Paper URL\": \"https://arxiv.org/abs/2105.04489\", \"GitHub URL\": \"\", \"Hugging Face URL\": \"\", \"Papers with Code URL\": \"https://paperswithcode.com/paper/spoken-moments-learning-joint-audio-visual\", \"ArXiv URL\": \"https://arxiv.org/abs/2105.04489\", \"Semantic Scholar Corpus ID\": 233992735, \"Year Released\": \"2021\", \"Text Sources\": [\"youtube\", \"flickr\", \"vine\", \"metacafe\", \"peeks\", \"vimeo\", \"videoblocks\", \"bing videos\", \"giphy\", \"weather channel\", \"getty-images\"], \"Licenses\": [{\"License\": \"Custom\", \"License URL\": \"https://docs.google.com/forms/d/e/1FAIpQLSdUOScon6uWkslgcA2-BCkAhMp86SihgKtMqX53zRf-E2fIHw/formResponse\"}], \"Creators\": [\"Massachusetts Institute of Technology\", \"IBM\", \"The University of Texas at Austin\"], \"Countries\": [\"United States of America\"], \"License Verified By\": \"Vivek Sharma\", \"Video Hours\": 416.67, \"Taken Down\": \"False\", \"Video Sources\": \"getty-images\", \"Task Categories\": [\"Video Captioning\"], \"Inferred Metadata\": {\"GitHub License\": \"\", \"GitHub Stars (June 2024)\": \"\", \"GitHub Topics\": \"\", \"Github Date\": \"\", \"HF Config\": \"\", \"HF Config License\": \"\", \"HF Dataset\": \"\", \"HF Date\": \"\", \"HF Downloads (June 2024)\": \"\", \"HF Likes (June 2024)\": \"\", \"HF Yaml License\": \"\", \"PwC Date\": \"\", \"PwC Description\": \"No code available yet.\", \"PwC License Name\": \"\", \"PwC License URL\": \"\", \"S2 Citation Count (June 2024)\": \"\", \"S2 Date\": \"\"}, \"Bibtex\": \"@Article{Monfort2021SpokenML,\\n author = {Mathew Monfort and SouYoung Jin},\\n booktitle = {Computer Vision and Pattern Recognition},\\n journal = {2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},\\n pages = {14866-14876},\\n title = {Spoken Moments: Learning Joint Audio-Visual Representations from Video Descriptions},\\n year = {2021}\\n}\\n\", \"License Use (DataProvenance)\": \"commercial\", \"License Attribution (DataProvenance)\": 0, \"License Share Alike (DataProvenance)\": 0, \"License Use (DataProvenance IgnoreOpenAI)\": \"commercial\", \"License Attribution (DataProvenance IgnoreOpenAI)\": 0, \"License Share Alike (DataProvenance IgnoreOpenAI)\": 0, \"License Type\": \"Commercial\"}, {\"Unique Dataset Identifier\": \"moments-in-time-dataset\", \"Collection\": \"moments-in-time-dataset\", \"Collection URL\": \"https://arxiv.org/pdf/1801.03150.pdf\", \"Dataset Name\": \"Moments in Time Dataset: one million videos for event understanding (TPAMI 2019)\", \"Paper Title\": \"Moments in Time Dataset: one million videos for event understanding (TPAMI 2019)\", \"Paper URL\": \"https://arxiv.org/pdf/1801.03150.pdf\", \"GitHub URL\": \"\", \"Hugging Face URL\": \"\", \"Papers with Code URL\": \"\", \"ArXiv URL\": \"https://arxiv.org/pdf/1801.03150.pdf\", \"Semantic Scholar Corpus ID\": 11868155, \"Year Released\": \"2019\", \"Text Sources\": [\"youtube\", \"flickr\", \"vine\", \"metacafe\", \"peeks\", \"vimeo\", \"videoblocks\", \"bing videos\", \"giphy\", \"weather channel\", \"getty-images\"], \"Licenses\": [{\"License\": \"Custom\", \"License URL\": \"http://moments.csail.mit.edu/\"}], \"Creators\": [\"Massachusetts Institute of Technology\"], \"Countries\": [\"United States of America\"], \"License Verified By\": \"Vivek Sharma\", \"Video Hours\": 833.0, \"Taken Down\": \"False\", \"Video Sources\": \"getty-images\", \"Task Categories\": [\"Video Classification\"], \"Inferred Metadata\": {\"GitHub License\": \"\", \"GitHub Stars (June 2024)\": \"\", \"GitHub Topics\": \"\", \"Github Date\": \"\", \"HF Config\": \"\", \"HF Config License\": \"\", \"HF Dataset\": \"\", \"HF Date\": \"\", \"HF Downloads (June 2024)\": \"\", \"HF Likes (June 2024)\": \"\", \"HF Yaml License\": \"\", \"PwC Date\": \"\", \"PwC Description\": \"\", \"PwC License Name\": \"\", \"PwC License URL\": \"\", \"S2 Citation Count (June 2024)\": \"\", \"S2 Date\": \"\"}, \"Bibtex\": \"@Article{Monfort2018MomentsIT,\\n author = {Mathew Monfort and Bolei Zhou and Sarah Adel Bargal and A. Andonian and Tom Yan and K. Ramakrishnan and L. Brown and Quanfu Fan and Dan Gutfreund and Carl Vondrick and A. Oliva},\\n booktitle = {IEEE Transactions on Pattern Analysis and Machine Intelligence},\\n journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},\\n pages = {502-508},\\n title = {Moments in Time Dataset: One Million Videos for Event Understanding},\\n volume = {42},\\n year = {2018}\\n}\\n\", \"License Use (DataProvenance)\": \"academic-only\", \"License Attribution (DataProvenance)\": 1, \"License Share Alike (DataProvenance)\": 0, \"License Use (DataProvenance IgnoreOpenAI)\": \"academic-only\", \"License Attribution (DataProvenance IgnoreOpenAI)\": 1, \"License Share Alike (DataProvenance IgnoreOpenAI)\": 0, \"License Type\": \"Non-Commercial/Academic\"}, {\"Unique Dataset Identifier\": \"spoken-moments\", \"Collection\": \"spoken-moments\", \"Collection URL\": \"https://arxiv.org/abs/2105.04489\", \"Dataset Name\": \"Spoken Moments\", \"Paper Title\": \"Spoken Moments\", \"Paper URL\": \"https://arxiv.org/abs/2105.04489\", \"GitHub URL\": \"\", \"Hugging Face URL\": \"\", \"Papers with Code URL\": \"https://paperswithcode.com/paper/spoken-moments-learning-joint-audio-visual\", \"ArXiv URL\": \"https://arxiv.org/abs/2105.04489\", \"Semantic Scholar Corpus ID\": 233992735, \"Year Released\": \"2021\", \"Text Sources\": [\"youtube\", \"flickr\", \"vine\", \"metacafe\", \"peeks\", \"vimeo\", \"videoblocks\", \"bing videos\", \"giphy\", \"weather channel\", \"getty-images\"], \"Licenses\": [{\"License\": \"Custom\", \"License URL\": \"https://docs.google.com/forms/d/e/1FAIpQLSdUOScon6uWkslgcA2-BCkAhMp86SihgKtMqX53zRf-E2fIHw/formResponse\"}], \"Creators\": [\"Massachusetts Institute of Technology\", \"IBM\", \"The University of Texas at Austin\"], \"Countries\": [\"United States of America\"], \"License Verified By\": \"Vivek Sharma\", \"Video Hours\": 416.67, \"Taken Down\": \"False\", \"Video Sources\": \"flickr\", \"Task Categories\": [\"Video Captioning\"], \"Inferred Metadata\": {\"GitHub License\": \"\", \"GitHub Stars (June 2024)\": \"\", \"GitHub Topics\": \"\", \"Github Date\": \"\", \"HF Config\": \"\", \"HF Config License\": \"\", \"HF Dataset\": \"\", \"HF Date\": \"\", \"HF Downloads (June 2024)\": \"\", \"HF Likes (June 2024)\": \"\", \"HF Yaml License\": \"\", \"PwC Date\": \"\", \"PwC Description\": \"No code available yet.\", \"PwC License Name\": \"\", \"PwC License URL\": \"\", \"S2 Citation Count (June 2024)\": \"\", \"S2 Date\": \"\"}, \"Bibtex\": \"@Article{Monfort2021SpokenML,\\n author = {Mathew Monfort and SouYoung Jin},\\n booktitle = {Computer Vision and Pattern Recognition},\\n journal = {2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},\\n pages = {14866-14876},\\n title = {Spoken Moments: Learning Joint Audio-Visual Representations from Video Descriptions},\\n year = {2021}\\n}\\n\", \"License Use (DataProvenance)\": \"commercial\", \"License Attribution (DataProvenance)\": 0, \"License Share Alike (DataProvenance)\": 0, \"License Use (DataProvenance IgnoreOpenAI)\": \"commercial\", \"License Attribution (DataProvenance IgnoreOpenAI)\": 0, \"License Share Alike (DataProvenance IgnoreOpenAI)\": 0, \"License Type\": \"Commercial\"}, {\"Unique Dataset Identifier\": \"didemo\", \"Collection\": \"didemo\", \"Collection URL\": \"https://paperswithcode.com/dataset/didemo\", \"Dataset Name\": \"DiDeMo: Localizing Moments in Video with Temporal Language (EMNLP 2018)\", \"Paper Title\": \"DiDeMo: Localizing Moments in Video with Temporal Language (EMNLP 2018)\", \"Paper URL\": \"https://paperswithcode.com/dataset/didemo\", \"GitHub URL\": \"https://github.com/LisaAnne/TemporalLanguageRelease\", \"Hugging Face URL\": \"\", \"Papers with Code URL\": \"\", \"ArXiv URL\": \"https://paperswithcode.com/dataset/didemo\", \"Semantic Scholar Corpus ID\": 52164739, \"Year Released\": \"2018\", \"Text Sources\": [\"flickr\"], \"Licenses\": [{\"License\": \"BSD 2-Clause License\", \"License URL\": \"https://github.com/LisaAnne/TemporalLanguageRelease\"}], \"Creators\": [\"UC Berkeley\"], \"Countries\": [\"United States of America\"], \"License Verified By\": \"Vivek Sharma\", \"Video Hours\": 275.0, \"Taken Down\": \"False\", \"Video Sources\": \"flickr\", \"Task Categories\": [\"Temporal Action Localization\"], \"Inferred Metadata\": {\"GitHub License\": \"\", \"GitHub Stars (June 2024)\": 40, \"GitHub Topics\": [], \"Github Date\": \"\", \"HF Config\": \"\", \"HF Config License\": \"\", \"HF Dataset\": \"\", \"HF Date\": \"\", \"HF Downloads (June 2024)\": \"\", \"HF Likes (June 2024)\": \"\", \"HF Yaml License\": \"\", \"PwC Date\": \"\", \"PwC Description\": \"\", \"PwC License Name\": \"\", \"PwC License URL\": \"\", \"S2 Citation Count (June 2024)\": \"\", \"S2 Date\": \"\"}, \"Bibtex\": \"@Article{Hendricks2018LocalizingMI,\\n author = {Lisa Anne Hendricks and Oliver Wang and Eli Shechtman and Josef Sivic and Trevor Darrell and Bryan C. Russell},\\n booktitle = {Conference on Empirical Methods in Natural Language Processing},\\n journal = {ArXiv},\\n title = {Localizing Moments in Video with Temporal Language},\\n volume = {abs/1809.01337},\\n year = {2018}\\n}\\n\", \"License Use (DataProvenance)\": \"commercial\", \"License Attribution (DataProvenance)\": 1, \"License Share Alike (DataProvenance)\": 0, \"License Use (DataProvenance IgnoreOpenAI)\": \"commercial\", \"License Attribution (DataProvenance IgnoreOpenAI)\": 1, \"License Share Alike (DataProvenance IgnoreOpenAI)\": 0, \"License Type\": \"Commercial\"}, {\"Unique Dataset Identifier\": \"moments-in-time-dataset\", \"Collection\": \"moments-in-time-dataset\", \"Collection URL\": \"https://arxiv.org/pdf/1801.03150.pdf\", \"Dataset Name\": \"Moments in Time Dataset: one million videos for event understanding (TPAMI 2019)\", \"Paper Title\": \"Moments in Time Dataset: one million videos for event understanding (TPAMI 2019)\", \"Paper URL\": \"https://arxiv.org/pdf/1801.03150.pdf\", \"GitHub URL\": \"\", \"Hugging Face URL\": \"\", \"Papers with Code URL\": \"\", \"ArXiv URL\": \"https://arxiv.org/pdf/1801.03150.pdf\", \"Semantic Scholar Corpus ID\": 11868155, \"Year Released\": \"2019\", \"Text Sources\": [\"youtube\", \"flickr\", \"vine\", \"metacafe\", \"peeks\", \"vimeo\", \"videoblocks\", \"bing videos\", \"giphy\", \"weather channel\", \"getty-images\"], \"Licenses\": [{\"License\": \"Custom\", \"License URL\": \"http://moments.csail.mit.edu/\"}], \"Creators\": [\"Massachusetts Institute of Technology\"], \"Countries\": [\"United States of America\"], \"License Verified By\": \"Vivek Sharma\", \"Video Hours\": 833.0, \"Taken Down\": \"False\", \"Video Sources\": \"flickr\", \"Task Categories\": [\"Video Classification\"], \"Inferred Metadata\": {\"GitHub License\": \"\", \"GitHub Stars (June 2024)\": \"\", \"GitHub Topics\": \"\", \"Github Date\": \"\", \"HF Config\": \"\", \"HF Config License\": \"\", \"HF Dataset\": \"\", \"HF Date\": \"\", \"HF Downloads (June 2024)\": \"\", \"HF Likes (June 2024)\": \"\", \"HF Yaml License\": \"\", \"PwC Date\": \"\", \"PwC Description\": \"\", \"PwC License Name\": \"\", \"PwC License URL\": \"\", \"S2 Citation Count (June 2024)\": \"\", \"S2 Date\": \"\"}, \"Bibtex\": \"@Article{Monfort2018MomentsIT,\\n author = {Mathew Monfort and Bolei Zhou and Sarah Adel Bargal and A. Andonian and Tom Yan and K. Ramakrishnan and L. Brown and Quanfu Fan and Dan Gutfreund and Carl Vondrick and A. Oliva},\\n booktitle = {IEEE Transactions on Pattern Analysis and Machine Intelligence},\\n journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},\\n pages = {502-508},\\n title = {Moments in Time Dataset: One Million Videos for Event Understanding},\\n volume = {42},\\n year = {2018}\\n}\\n\", \"License Use (DataProvenance)\": \"academic-only\", \"License Attribution (DataProvenance)\": 1, \"License Share Alike (DataProvenance)\": 0, \"License Use (DataProvenance IgnoreOpenAI)\": \"academic-only\", \"License Attribution (DataProvenance IgnoreOpenAI)\": 1, \"License Share Alike (DataProvenance IgnoreOpenAI)\": 0, \"License Type\": \"Non-Commercial/Academic\"}, {\"Unique Dataset Identifier\": \"omnisource-web-dataset\", \"Collection\": \"omnisource-web-dataset\", \"Collection URL\": \"https://arxiv.org/abs/2003.13042\", \"Dataset Name\": \"OmniSource Web Dataset\", \"Paper Title\": \"OmniSource Web Dataset\", \"Paper URL\": \"https://arxiv.org/abs/2003.13042\", \"GitHub URL\": \"https://github.com/open-mmlab/mmaction\", \"Hugging Face URL\": \"\", \"Papers with Code URL\": \"https://paperswithcode.com/paper/omni-sourced-webly-supervised-learning-for#code\", \"ArXiv URL\": \"https://arxiv.org/abs/2003.13042\", \"Semantic Scholar Corpus ID\": 214714240, \"Year Released\": \"2020\", \"Text Sources\": [\"google videos\", \"instagram\", \"youtube\"], \"Licenses\": [{\"License\": \"Apache License 2.0\", \"License URL\": \"https://github.com/open-mmlab/mmaction?tab=readme-ov-file#license\"}], \"Creators\": [\"The Chinese University of Hong Kong\"], \"Countries\": [\"China\"], \"License Verified By\": \"Vivek Sharma\", \"Video Hours\": 13333.0, \"Taken Down\": \"False\", \"Video Sources\": \"google videos\", \"Task Categories\": [\"Video Classification\"], \"Inferred Metadata\": {\"GitHub License\": \"Apache License 2.0\", \"GitHub Stars (June 2024)\": 1856, \"GitHub Topics\": [\"action-detection\", \"action-recognition\", \"pytorch\", \"spatial-temporal-action-detection\", \"temporal-action-detection\", \"temporal-action-localization\", \"video-understanding\"], \"Github Date\": \"\", \"HF Config\": \"\", \"HF Config License\": \"\", \"HF Dataset\": \"\", \"HF Date\": \"\", \"HF Downloads (June 2024)\": \"\", \"HF Likes (June 2024)\": \"\", \"HF Yaml License\": \"\", \"PwC Date\": \"\", \"PwC Description\": \"3 code implementations in PyTorch. We introduce OmniSource, a novel framework for leveraging web data to train video recognition models. OmniSource overcomes the barriers between data formats, such as images, short videos, and long untrimmed videos for webly-supervised learning. First, data samples with multiple formats, curated by task-specific data collection and automatically filtered by a teacher model, are transformed into a unified form. Then a joint-training strategy is proposed to deal with the domain gaps between multiple data sources and formats in webly-supervised learning. Several good practices, including data balancing, resampling, and cross-dataset mixup are adopted in joint training. Experiments show that by utilizing data from multiple sources and formats, OmniSource is more data-efficient in training. With only 3.5M images and 800K minutes videos crawled from the internet without human labeling (less than 2% of prior works), our models learned with OmniSource improve Top-1 accuracy of 2D- and 3D-ConvNet baseline models by 3.0% and 3.9%, respectively, on the Kinetics-400 benchmark. With OmniSource, we establish new records with different pretraining strategies for video recognition. Our best models achieve 80.4%, 80.5%, and 83.6 Top-1 accuracies on the Kinetics-400 benchmark respectively for training-from-scratch, ImageNet pre-training and IG-65M pre-training.\", \"PwC License Name\": \"\", \"PwC License URL\": \"\", \"S2 Citation Count (June 2024)\": \"\", \"S2 Date\": \"\"}, \"Bibtex\": \"@Article{Duan2020OmnisourcedWL,\\n author = {Haodong Duan and Yue Zhao and Yuanjun Xiong and Wentao Liu and Dahua Lin},\\n booktitle = {European Conference on Computer Vision},\\n journal = {ArXiv},\\n title = {Omni-sourced Webly-supervised Learning for Video Recognition},\\n volume = {abs/2003.13042},\\n year = {2020}\\n}\\n\", \"License Use (DataProvenance)\": \"commercial\", \"License Attribution (DataProvenance)\": 1, \"License Share Alike (DataProvenance)\": 0, \"License Use (DataProvenance IgnoreOpenAI)\": \"commercial\", \"License Attribution (DataProvenance IgnoreOpenAI)\": 1, \"License Share Alike (DataProvenance IgnoreOpenAI)\": 0, \"License Type\": \"Commercial\"}, {\"Unique Dataset Identifier\": \"hmdb-dataset\", \"Collection\": \"hmdb-dataset\", \"Collection URL\": \"https://ieeexplore.ieee.org/document/6126543\", \"Dataset Name\": \"HMDB: A Large Video Database for Human Motion Recognition (ICCV 2011)\", \"Paper Title\": \"HMDB: A Large Video Database for Human Motion Recognition (ICCV 2011)\", \"Paper URL\": \"https://ieeexplore.ieee.org/document/6126543\", \"GitHub URL\": \"\", \"Hugging Face URL\": \"\", \"Papers with Code URL\": \"https://paperswithcode.com/dataset/hmdb51\", \"ArXiv URL\": \"https://ieeexplore.ieee.org/document/6126543\", \"Semantic Scholar Corpus ID\": 206769852, \"Year Released\": \"2011\", \"Text Sources\": [\"movies\", \"prelinger archive\", \"undisclosed web\", \"youtube\", \"google videos\"], \"Licenses\": [{\"License\": \"CC BY 4.0\", \"License URL\": \"https://serre-lab.clps.brown.edu/resource/hmdb-a-large-human-motion-database/\"}], \"Creators\": [\"Karlsruhe Institute of Technology\", \"Massachusetts Institute of Technology\", \"Brown University\"], \"Countries\": [\"United States of America\", \"Germany\"], \"License Verified By\": \"Vivek Sharma\", \"Video Hours\": 7000.0, \"Taken Down\": \"False\", \"Video Sources\": \"google videos\", \"Task Categories\": [\"Video Classification\"], \"Inferred Metadata\": {\"GitHub License\": \"\", \"GitHub Stars (June 2024)\": \"\", \"GitHub Topics\": \"\", \"Github Date\": \"\", \"HF Config\": \"\", \"HF Config License\": \"\", \"HF Dataset\": \"\", \"HF Date\": \"\", \"HF Downloads (June 2024)\": \"\", \"HF Likes (June 2024)\": \"\", \"HF Yaml License\": \"\", \"PwC Date\": \"2011-01-01\", \"PwC Description\": \"The HMDB51 dataset is a large collection of realistic videos from various sources, including movies and web videos. The dataset is composed of 6,766 video clips from 51 action categories (such as \\u201cjump\\u201d, \\u201ckiss\\u201d and \\u201claugh\\u201d), with each category containing at least 101 clips. The original evaluation scheme uses three different training/testing splits. In each split, each action class has 70 clips for training and 30 clips for testing. The average accuracy over these three splits is used to measure the final performance.\", \"PwC License Name\": \"CC BY 4.0\", \"PwC License URL\": \"https://creativecommons.org/licenses/by/4.0/\", \"S2 Citation Count (June 2024)\": \"\", \"S2 Date\": \"\"}, \"Bibtex\": \"@Article{Kuehne2011HMDBAL,\\n author = {Hilde Kuehne and Hueihan Jhuang and Est\\u00edbaliz Garrote and T. Poggio and Thomas Serre},\\n booktitle = {Vision},\\n journal = {2011 International Conference on Computer Vision},\\n pages = {2556-2563},\\n title = {HMDB: A large video database for human motion recognition},\\n year = {2011}\\n}\\n\", \"License Use (DataProvenance)\": \"commercial\", \"License Attribution (DataProvenance)\": 1, \"License Share Alike (DataProvenance)\": 0, \"License Use (DataProvenance IgnoreOpenAI)\": \"commercial\", \"License Attribution (DataProvenance IgnoreOpenAI)\": 1, \"License Share Alike (DataProvenance IgnoreOpenAI)\": 0, \"License Type\": \"Commercial\"}]}}, {\"mode\": \"vega-lite\"});\n",
       "</script>"
      ],
      "text/plain": [
       "alt.LayerChart(...)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base = alt.Chart(df_videosourcelicences).mark_bar().encode(\n",
    "    x=alt.X(\n",
    "        \"Video Sources:N\",\n",
    "        title=\"Video Sources\",\n",
    "        sort=videosources_order,\n",
    "        axis=alt.Axis(labelAngle=-30)\n",
    "    ),\n",
    "    y=alt.Y(\n",
    "        \"count():Q\",\n",
    "        stack=\"normalize\",\n",
    "        axis=alt.Axis(format=\"%\"),\n",
    "        title=\"Pct. Datasets\"\n",
    "    ),\n",
    "    color=alt.Color(\n",
    "        \"License Type:N\",\n",
    "        scale=alt.Scale(\n",
    "            domain=LICENSE_ORDER,\n",
    "            range=LICENSE_PALETTE\n",
    "        ),\n",
    "        title=\"License Type\"\n",
    "    )\n",
    ").properties(\n",
    "    width=600,\n",
    "    height=100\n",
    ")\n",
    "\n",
    "text = alt.Chart(df_videosourcelicences).mark_text(\n",
    "    dy=-68,\n",
    "    align=\"center\",\n",
    "    baseline=\"top\",\n",
    "    fontSize=12\n",
    ").encode(\n",
    "    x=alt.X(\n",
    "        \"Video Sources:N\",\n",
    "        title=\"Video Sources\",\n",
    "        sort=videosources_order\n",
    "    ),\n",
    "    text=\"count():Q\"\n",
    ")\n",
    "\n",
    "chart = (base + text).configure_axis(\n",
    "    labelFontSize=FONT_SIZE,\n",
    "    titleFontSize=FONT_SIZE\n",
    ").configure_legend(\n",
    "    labelFontSize=FONT_SIZE,\n",
    "    titleFontSize=FONT_SIZE,\n",
    "    orient=LEGEND_POSITION,\n",
    "    labelLimit=MAX_LABELLIMIT\n",
    ")\n",
    "\n",
    "if PLOT_TOFILE:\n",
    "    chart.save(\n",
    "        os.path.join(PLOT_DIR, \"video_sources-licenses.png\"),\n",
    "        ppi=PLOT_PPI\n",
    "    )\n",
    "\n",
    "chart"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Source Category by Year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "58    2009\n",
       "88    2009\n",
       "63    2011\n",
       "63    2011\n",
       "63    2011\n",
       "Name: Year Released, dtype: category\n",
       "Categories (22, object): ['<2004' < '2004' < '2005' < '2006' ... '2021' < '2022' < '2023' < '2024']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "INCLUDE_TOP_N_CATEGORIES = 6\n",
    "df_videosourceyears = df_video.explode(\"Video Sources\")\n",
    "df_videosourceyears = reduce_categories_to_topk(df_videosourceyears, \"Video Sources\", INCLUDE_TOP_N_CATEGORIES)\n",
    "\n",
    "df_videosourceyears = df_videosourceyears.sort_values(by=\"Year Released\")\n",
    "df_videosourceyears.head()['Year Released']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "  #altair-viz-94122d1aa03240d4929ef09039627045.vega-embed {\n",
       "    width: 100%;\n",
       "    display: flex;\n",
       "  }\n",
       "\n",
       "  #altair-viz-94122d1aa03240d4929ef09039627045.vega-embed details,\n",
       "  #altair-viz-94122d1aa03240d4929ef09039627045.vega-embed details summary {\n",
       "    position: relative;\n",
       "  }\n",
       "</style>\n",
       "<div id=\"altair-viz-94122d1aa03240d4929ef09039627045\"></div>\n",
       "<script type=\"text/javascript\">\n",
       "  var VEGA_DEBUG = (typeof VEGA_DEBUG == \"undefined\") ? {} : VEGA_DEBUG;\n",
       "  (function(spec, embedOpt){\n",
       "    let outputDiv = document.currentScript.previousElementSibling;\n",
       "    if (outputDiv.id !== \"altair-viz-94122d1aa03240d4929ef09039627045\") {\n",
       "      outputDiv = document.getElementById(\"altair-viz-94122d1aa03240d4929ef09039627045\");\n",
       "    }\n",
       "    const paths = {\n",
       "      \"vega\": \"https://cdn.jsdelivr.net/npm/vega@5?noext\",\n",
       "      \"vega-lib\": \"https://cdn.jsdelivr.net/npm/vega-lib?noext\",\n",
       "      \"vega-lite\": \"https://cdn.jsdelivr.net/npm/vega-lite@5.20.1?noext\",\n",
       "      \"vega-embed\": \"https://cdn.jsdelivr.net/npm/vega-embed@6?noext\",\n",
       "    };\n",
       "\n",
       "    function maybeLoadScript(lib, version) {\n",
       "      var key = `${lib.replace(\"-\", \"\")}_version`;\n",
       "      return (VEGA_DEBUG[key] == version) ?\n",
       "        Promise.resolve(paths[lib]) :\n",
       "        new Promise(function(resolve, reject) {\n",
       "          var s = document.createElement('script');\n",
       "          document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
       "          s.async = true;\n",
       "          s.onload = () => {\n",
       "            VEGA_DEBUG[key] = version;\n",
       "            return resolve(paths[lib]);\n",
       "          };\n",
       "          s.onerror = () => reject(`Error loading script: ${paths[lib]}`);\n",
       "          s.src = paths[lib];\n",
       "        });\n",
       "    }\n",
       "\n",
       "    function showError(err) {\n",
       "      outputDiv.innerHTML = `<div class=\"error\" style=\"color:red;\">${err}</div>`;\n",
       "      throw err;\n",
       "    }\n",
       "\n",
       "    function displayChart(vegaEmbed) {\n",
       "      vegaEmbed(outputDiv, spec, embedOpt)\n",
       "        .catch(err => showError(`Javascript Error: ${err.message}<br>This usually means there's a typo in your chart specification. See the javascript console for the full traceback.`));\n",
       "    }\n",
       "\n",
       "    if(typeof define === \"function\" && define.amd) {\n",
       "      requirejs.config({paths});\n",
       "      require([\"vega-embed\"], displayChart, err => showError(`Error loading script: ${err.message}`));\n",
       "    } else {\n",
       "      maybeLoadScript(\"vega\", \"5\")\n",
       "        .then(() => maybeLoadScript(\"vega-lite\", \"5.20.1\"))\n",
       "        .then(() => maybeLoadScript(\"vega-embed\", \"6\"))\n",
       "        .catch(showError)\n",
       "        .then(() => displayChart(vegaEmbed));\n",
       "    }\n",
       "  })({\"config\": {\"title\": {\"font\": \"Times New Roman\"}, \"axis\": {\"labelFont\": \"Times New Roman\", \"titleFont\": \"Times New Roman\", \"labelFontSize\": 20, \"titleFontSize\": 20}, \"header\": {\"labelFont\": \"Times New Roman\", \"titleFont\": \"Times New Roman\"}, \"legend\": {\"labelFont\": \"Times New Roman\", \"titleFont\": \"Times New Roman\", \"columns\": 4, \"labelFontSize\": 20, \"labelLimit\": 1000, \"orient\": \"bottom\", \"titleFontSize\": 20}, \"text\": {\"font\": \"Times New Roman\"}}, \"layer\": [{\"mark\": {\"type\": \"bar\"}, \"encoding\": {\"color\": {\"field\": \"Video Sources\", \"title\": \"Video Sources\", \"type\": \"nominal\"}, \"x\": {\"axis\": {\"labelAngle\": -30}, \"field\": \"Year Released\", \"sort\": [\"<2004\", \"2004\", \"2005\", \"2006\", \"2007\", \"2008\", \"2009\", \"2010\", \"2011\", \"2012\", \"2013\", \"2014\", \"2015\", \"2016\", \"2017\", \"2018\", \"2019\", \"2020\", \"2021\", \"2022\", \"2023\", \"2024\"], \"title\": \"Year Released\", \"type\": \"nominal\"}, \"y\": {\"aggregate\": \"count\", \"axis\": {\"format\": \"%\"}, \"stack\": \"normalize\", \"title\": \"Pct. Datasets\", \"type\": \"quantitative\"}}}, {\"mark\": {\"type\": \"text\", \"align\": \"center\", \"baseline\": \"top\", \"dy\": -90, \"fontSize\": 12}, \"encoding\": {\"text\": {\"aggregate\": \"count\", \"type\": \"quantitative\"}, \"x\": {\"field\": \"Year Released\", \"sort\": [\"<2004\", \"2004\", \"2005\", \"2006\", \"2007\", \"2008\", \"2009\", \"2010\", \"2011\", \"2012\", \"2013\", \"2014\", \"2015\", \"2016\", \"2017\", \"2018\", \"2019\", \"2020\", \"2021\", \"2022\", \"2023\", \"2024\"], \"title\": \"Year Released\", \"type\": \"nominal\"}}}], \"data\": {\"name\": \"data-1e9feeadaef45a8f9a62e99c28759dee\"}, \"height\": 160, \"width\": 600, \"$schema\": \"https://vega.github.io/schema/vega-lite/v5.20.1.json\", \"datasets\": {\"data-1e9feeadaef45a8f9a62e99c28759dee\": [{\"Unique Dataset Identifier\": \"collective\", \"Collection\": \"collective\", \"Collection URL\": \"https://cvgl.stanford.edu/papers/Wongun_CollectiveActivityRecognition09.pdf\", \"Dataset Name\": \"Collective: What are they doing? : Collective activity classification using spatio-temporal relationship among people\", \"Paper Title\": \"Collective: What are they doing? : Collective activity classification using spatio-temporal relationship among people\", \"Paper URL\": \"https://cvgl.stanford.edu/papers/Wongun_CollectiveActivityRecognition09.pdf\", \"GitHub URL\": \"\", \"Hugging Face URL\": \"\", \"Papers with Code URL\": \"\", \"ArXiv URL\": \"https://cvgl.stanford.edu/papers/Wongun_CollectiveActivityRecognition09.pdf\", \"Semantic Scholar Corpus ID\": 5925915, \"Year Released\": \"2009\", \"Text Sources\": [\"human\"], \"Licenses\": [{\"License\": \"Unspecified\", \"License URL\": \"\"}], \"Creators\": [\"University of Michigan\"], \"Countries\": [\"United States of America\"], \"License Verified By\": \"Vivek Sharma\", \"Video Hours\": 0.1, \"Taken Down\": \"False\", \"Video Sources\": \"human\", \"Task Categories\": [\"Group Activity Recognition\"], \"Inferred Metadata\": {\"GitHub License\": \"\", \"GitHub Stars (June 2024)\": \"\", \"GitHub Topics\": \"\", \"Github Date\": \"\", \"HF Config\": \"\", \"HF Config License\": \"\", \"HF Dataset\": \"\", \"HF Date\": \"\", \"HF Downloads (June 2024)\": \"\", \"HF Likes (June 2024)\": \"\", \"HF Yaml License\": \"\", \"PwC Date\": \"\", \"PwC Description\": \"\", \"PwC License Name\": \"\", \"PwC License URL\": \"\", \"S2 Citation Count (June 2024)\": \"\", \"S2 Date\": \"\"}, \"Bibtex\": \"@Article{Choi2009WhatAT,\\n author = {Wongun Choi and Khuram Shahid and S. Savarese},\\n booktitle = {2009 IEEE 12th International Conference on Computer Vision Workshops, ICCV Workshops},\\n journal = {2009 IEEE 12th International Conference on Computer Vision Workshops, ICCV Workshops},\\n pages = {1282-1289},\\n title = {What are they doing? : Collective activity classification using spatio-temporal relationship among people},\\n year = {2009}\\n}\\n\", \"License Use (DataProvenance)\": \"unspecified\", \"License Attribution (DataProvenance)\": 0, \"License Share Alike (DataProvenance)\": 0, \"License Use (DataProvenance IgnoreOpenAI)\": \"unspecified\", \"License Attribution (DataProvenance IgnoreOpenAI)\": 0, \"License Share Alike (DataProvenance IgnoreOpenAI)\": 0, \"License Type\": \"Unspecified\"}, {\"Unique Dataset Identifier\": \"hollywood2-dataset\", \"Collection\": \"hollywood2-dataset\", \"Collection URL\": \"https://www.irisa.fr/vista/Papers/2009_cvpr_marszalek.pdf\", \"Dataset Name\": \"HOLLYWOOD2: Actions in Context (CVPR 2009)\", \"Paper Title\": \"HOLLYWOOD2: Actions in Context (CVPR 2009)\", \"Paper URL\": \"https://www.irisa.fr/vista/Papers/2009_cvpr_marszalek.pdf\", \"GitHub URL\": \"\", \"Hugging Face URL\": \"\", \"Papers with Code URL\": \"\", \"ArXiv URL\": \"https://www.irisa.fr/vista/Papers/2009_cvpr_marszalek.pdf\", \"Semantic Scholar Corpus ID\": 3155054, \"Year Released\": \"2009\", \"Text Sources\": [\"movies\"], \"Licenses\": [{\"License\": \"Unspecified\", \"License URL\": \"\"}], \"Creators\": [\"Inria\"], \"Countries\": [\"France\"], \"License Verified By\": \"Vivek Sharma\", \"Video Hours\": 20.1, \"Taken Down\": \"False\", \"Video Sources\": \"movies\", \"Task Categories\": [\"Video Classification\"], \"Inferred Metadata\": {\"GitHub License\": \"\", \"GitHub Stars (June 2024)\": \"\", \"GitHub Topics\": \"\", \"Github Date\": \"\", \"HF Config\": \"\", \"HF Config License\": \"\", \"HF Dataset\": \"\", \"HF Date\": \"\", \"HF Downloads (June 2024)\": \"\", \"HF Likes (June 2024)\": \"\", \"HF Yaml License\": \"\", \"PwC Date\": \"\", \"PwC Description\": \"\", \"PwC License Name\": \"\", \"PwC License URL\": \"\", \"S2 Citation Count (June 2024)\": \"\", \"S2 Date\": \"\"}, \"Bibtex\": \"@Article{Marszalek2009ActionsIC,\\n author = {Marcin Marszalek and I. Laptev and C. Schmid},\\n booktitle = {2009 IEEE Conference on Computer Vision and Pattern Recognition},\\n journal = {2009 IEEE Conference on Computer Vision and Pattern Recognition},\\n pages = {2929-2936},\\n title = {Actions in context},\\n year = {2009}\\n}\\n\", \"License Use (DataProvenance)\": \"unspecified\", \"License Attribution (DataProvenance)\": 0, \"License Share Alike (DataProvenance)\": 0, \"License Use (DataProvenance IgnoreOpenAI)\": \"unspecified\", \"License Attribution (DataProvenance IgnoreOpenAI)\": 0, \"License Share Alike (DataProvenance IgnoreOpenAI)\": 0, \"License Type\": \"Unspecified\"}, {\"Unique Dataset Identifier\": \"hmdb-dataset\", \"Collection\": \"hmdb-dataset\", \"Collection URL\": \"https://ieeexplore.ieee.org/document/6126543\", \"Dataset Name\": \"HMDB: A Large Video Database for Human Motion Recognition (ICCV 2011)\", \"Paper Title\": \"HMDB: A Large Video Database for Human Motion Recognition (ICCV 2011)\", \"Paper URL\": \"https://ieeexplore.ieee.org/document/6126543\", \"GitHub URL\": \"\", \"Hugging Face URL\": \"\", \"Papers with Code URL\": \"https://paperswithcode.com/dataset/hmdb51\", \"ArXiv URL\": \"https://ieeexplore.ieee.org/document/6126543\", \"Semantic Scholar Corpus ID\": 206769852, \"Year Released\": \"2011\", \"Text Sources\": [\"movies\", \"prelinger archive\", \"undisclosed web\", \"youtube\", \"google videos\"], \"Licenses\": [{\"License\": \"CC BY 4.0\", \"License URL\": \"https://serre-lab.clps.brown.edu/resource/hmdb-a-large-human-motion-database/\"}], \"Creators\": [\"Karlsruhe Institute of Technology\", \"Massachusetts Institute of Technology\", \"Brown University\"], \"Countries\": [\"United States of America\", \"Germany\"], \"License Verified By\": \"Vivek Sharma\", \"Video Hours\": 7000.0, \"Taken Down\": \"False\", \"Video Sources\": \"youtube\", \"Task Categories\": [\"Video Classification\"], \"Inferred Metadata\": {\"GitHub License\": \"\", \"GitHub Stars (June 2024)\": \"\", \"GitHub Topics\": \"\", \"Github Date\": \"\", \"HF Config\": \"\", \"HF Config License\": \"\", \"HF Dataset\": \"\", \"HF Date\": \"\", \"HF Downloads (June 2024)\": \"\", \"HF Likes (June 2024)\": \"\", \"HF Yaml License\": \"\", \"PwC Date\": \"2011-01-01\", \"PwC Description\": \"The HMDB51 dataset is a large collection of realistic videos from various sources, including movies and web videos. The dataset is composed of 6,766 video clips from 51 action categories (such as \\u201cjump\\u201d, \\u201ckiss\\u201d and \\u201claugh\\u201d), with each category containing at least 101 clips. The original evaluation scheme uses three different training/testing splits. In each split, each action class has 70 clips for training and 30 clips for testing. The average accuracy over these three splits is used to measure the final performance.\", \"PwC License Name\": \"CC BY 4.0\", \"PwC License URL\": \"https://creativecommons.org/licenses/by/4.0/\", \"S2 Citation Count (June 2024)\": \"\", \"S2 Date\": \"\"}, \"Bibtex\": \"@Article{Kuehne2011HMDBAL,\\n author = {Hilde Kuehne and Hueihan Jhuang and Est\\u00edbaliz Garrote and T. Poggio and Thomas Serre},\\n booktitle = {Vision},\\n journal = {2011 International Conference on Computer Vision},\\n pages = {2556-2563},\\n title = {HMDB: A large video database for human motion recognition},\\n year = {2011}\\n}\\n\", \"License Use (DataProvenance)\": \"commercial\", \"License Attribution (DataProvenance)\": 1, \"License Share Alike (DataProvenance)\": 0, \"License Use (DataProvenance IgnoreOpenAI)\": \"commercial\", \"License Attribution (DataProvenance IgnoreOpenAI)\": 1, \"License Share Alike (DataProvenance IgnoreOpenAI)\": 0, \"License Type\": \"Commercial\"}, {\"Unique Dataset Identifier\": \"hmdb-dataset\", \"Collection\": \"hmdb-dataset\", \"Collection URL\": \"https://ieeexplore.ieee.org/document/6126543\", \"Dataset Name\": \"HMDB: A Large Video Database for Human Motion Recognition (ICCV 2011)\", \"Paper Title\": \"HMDB: A Large Video Database for Human Motion Recognition (ICCV 2011)\", \"Paper URL\": \"https://ieeexplore.ieee.org/document/6126543\", \"GitHub URL\": \"\", \"Hugging Face URL\": \"\", \"Papers with Code URL\": \"https://paperswithcode.com/dataset/hmdb51\", \"ArXiv URL\": \"https://ieeexplore.ieee.org/document/6126543\", \"Semantic Scholar Corpus ID\": 206769852, \"Year Released\": \"2011\", \"Text Sources\": [\"movies\", \"prelinger archive\", \"undisclosed web\", \"youtube\", \"google videos\"], \"Licenses\": [{\"License\": \"CC BY 4.0\", \"License URL\": \"https://serre-lab.clps.brown.edu/resource/hmdb-a-large-human-motion-database/\"}], \"Creators\": [\"Karlsruhe Institute of Technology\", \"Massachusetts Institute of Technology\", \"Brown University\"], \"Countries\": [\"United States of America\", \"Germany\"], \"License Verified By\": \"Vivek Sharma\", \"Video Hours\": 7000.0, \"Taken Down\": \"False\", \"Video Sources\": \"undisclosed web\", \"Task Categories\": [\"Video Classification\"], \"Inferred Metadata\": {\"GitHub License\": \"\", \"GitHub Stars (June 2024)\": \"\", \"GitHub Topics\": \"\", \"Github Date\": \"\", \"HF Config\": \"\", \"HF Config License\": \"\", \"HF Dataset\": \"\", \"HF Date\": \"\", \"HF Downloads (June 2024)\": \"\", \"HF Likes (June 2024)\": \"\", \"HF Yaml License\": \"\", \"PwC Date\": \"2011-01-01\", \"PwC Description\": \"The HMDB51 dataset is a large collection of realistic videos from various sources, including movies and web videos. The dataset is composed of 6,766 video clips from 51 action categories (such as \\u201cjump\\u201d, \\u201ckiss\\u201d and \\u201claugh\\u201d), with each category containing at least 101 clips. The original evaluation scheme uses three different training/testing splits. In each split, each action class has 70 clips for training and 30 clips for testing. The average accuracy over these three splits is used to measure the final performance.\", \"PwC License Name\": \"CC BY 4.0\", \"PwC License URL\": \"https://creativecommons.org/licenses/by/4.0/\", \"S2 Citation Count (June 2024)\": \"\", \"S2 Date\": \"\"}, \"Bibtex\": \"@Article{Kuehne2011HMDBAL,\\n author = {Hilde Kuehne and Hueihan Jhuang and Est\\u00edbaliz Garrote and T. Poggio and Thomas Serre},\\n booktitle = {Vision},\\n journal = {2011 International Conference on Computer Vision},\\n pages = {2556-2563},\\n title = {HMDB: A large video database for human motion recognition},\\n year = {2011}\\n}\\n\", \"License Use (DataProvenance)\": \"commercial\", \"License Attribution (DataProvenance)\": 1, \"License Share Alike (DataProvenance)\": 0, \"License Use (DataProvenance IgnoreOpenAI)\": \"commercial\", \"License Attribution (DataProvenance IgnoreOpenAI)\": 1, \"License Share Alike (DataProvenance IgnoreOpenAI)\": 0, \"License Type\": \"Commercial\"}, {\"Unique Dataset Identifier\": \"hmdb-dataset\", \"Collection\": \"hmdb-dataset\", \"Collection URL\": \"https://ieeexplore.ieee.org/document/6126543\", \"Dataset Name\": \"HMDB: A Large Video Database for Human Motion Recognition (ICCV 2011)\", \"Paper Title\": \"HMDB: A Large Video Database for Human Motion Recognition (ICCV 2011)\", \"Paper URL\": \"https://ieeexplore.ieee.org/document/6126543\", \"GitHub URL\": \"\", \"Hugging Face URL\": \"\", \"Papers with Code URL\": \"https://paperswithcode.com/dataset/hmdb51\", \"ArXiv URL\": \"https://ieeexplore.ieee.org/document/6126543\", \"Semantic Scholar Corpus ID\": 206769852, \"Year Released\": \"2011\", \"Text Sources\": [\"movies\", \"prelinger archive\", \"undisclosed web\", \"youtube\", \"google videos\"], \"Licenses\": [{\"License\": \"CC BY 4.0\", \"License URL\": \"https://serre-lab.clps.brown.edu/resource/hmdb-a-large-human-motion-database/\"}], \"Creators\": [\"Karlsruhe Institute of Technology\", \"Massachusetts Institute of Technology\", \"Brown University\"], \"Countries\": [\"United States of America\", \"Germany\"], \"License Verified By\": \"Vivek Sharma\", \"Video Hours\": 7000.0, \"Taken Down\": \"False\", \"Video Sources\": \"Other\", \"Task Categories\": [\"Video Classification\"], \"Inferred Metadata\": {\"GitHub License\": \"\", \"GitHub Stars (June 2024)\": \"\", \"GitHub Topics\": \"\", \"Github Date\": \"\", \"HF Config\": \"\", \"HF Config License\": \"\", \"HF Dataset\": \"\", \"HF Date\": \"\", \"HF Downloads (June 2024)\": \"\", \"HF Likes (June 2024)\": \"\", \"HF Yaml License\": \"\", \"PwC Date\": \"2011-01-01\", \"PwC Description\": \"The HMDB51 dataset is a large collection of realistic videos from various sources, including movies and web videos. The dataset is composed of 6,766 video clips from 51 action categories (such as \\u201cjump\\u201d, \\u201ckiss\\u201d and \\u201claugh\\u201d), with each category containing at least 101 clips. The original evaluation scheme uses three different training/testing splits. In each split, each action class has 70 clips for training and 30 clips for testing. The average accuracy over these three splits is used to measure the final performance.\", \"PwC License Name\": \"CC BY 4.0\", \"PwC License URL\": \"https://creativecommons.org/licenses/by/4.0/\", \"S2 Citation Count (June 2024)\": \"\", \"S2 Date\": \"\"}, \"Bibtex\": \"@Article{Kuehne2011HMDBAL,\\n author = {Hilde Kuehne and Hueihan Jhuang and Est\\u00edbaliz Garrote and T. Poggio and Thomas Serre},\\n booktitle = {Vision},\\n journal = {2011 International Conference on Computer Vision},\\n pages = {2556-2563},\\n title = {HMDB: A large video database for human motion recognition},\\n year = {2011}\\n}\\n\", \"License Use (DataProvenance)\": \"commercial\", \"License Attribution (DataProvenance)\": 1, \"License Share Alike (DataProvenance)\": 0, \"License Use (DataProvenance IgnoreOpenAI)\": \"commercial\", \"License Attribution (DataProvenance IgnoreOpenAI)\": 1, \"License Share Alike (DataProvenance IgnoreOpenAI)\": 0, \"License Type\": \"Commercial\"}, {\"Unique Dataset Identifier\": \"hmdb-dataset\", \"Collection\": \"hmdb-dataset\", \"Collection URL\": \"https://ieeexplore.ieee.org/document/6126543\", \"Dataset Name\": \"HMDB: A Large Video Database for Human Motion Recognition (ICCV 2011)\", \"Paper Title\": \"HMDB: A Large Video Database for Human Motion Recognition (ICCV 2011)\", \"Paper URL\": \"https://ieeexplore.ieee.org/document/6126543\", \"GitHub URL\": \"\", \"Hugging Face URL\": \"\", \"Papers with Code URL\": \"https://paperswithcode.com/dataset/hmdb51\", \"ArXiv URL\": \"https://ieeexplore.ieee.org/document/6126543\", \"Semantic Scholar Corpus ID\": 206769852, \"Year Released\": \"2011\", \"Text Sources\": [\"movies\", \"prelinger archive\", \"undisclosed web\", \"youtube\", \"google videos\"], \"Licenses\": [{\"License\": \"CC BY 4.0\", \"License URL\": \"https://serre-lab.clps.brown.edu/resource/hmdb-a-large-human-motion-database/\"}], \"Creators\": [\"Karlsruhe Institute of Technology\", \"Massachusetts Institute of Technology\", \"Brown University\"], \"Countries\": [\"United States of America\", \"Germany\"], \"License Verified By\": \"Vivek Sharma\", \"Video Hours\": 7000.0, \"Taken Down\": \"False\", \"Video Sources\": \"movies\", \"Task Categories\": [\"Video Classification\"], \"Inferred Metadata\": {\"GitHub License\": \"\", \"GitHub Stars (June 2024)\": \"\", \"GitHub Topics\": \"\", \"Github Date\": \"\", \"HF Config\": \"\", \"HF Config License\": \"\", \"HF Dataset\": \"\", \"HF Date\": \"\", \"HF Downloads (June 2024)\": \"\", \"HF Likes (June 2024)\": \"\", \"HF Yaml License\": \"\", \"PwC Date\": \"2011-01-01\", \"PwC Description\": \"The HMDB51 dataset is a large collection of realistic videos from various sources, including movies and web videos. The dataset is composed of 6,766 video clips from 51 action categories (such as \\u201cjump\\u201d, \\u201ckiss\\u201d and \\u201claugh\\u201d), with each category containing at least 101 clips. The original evaluation scheme uses three different training/testing splits. In each split, each action class has 70 clips for training and 30 clips for testing. The average accuracy over these three splits is used to measure the final performance.\", \"PwC License Name\": \"CC BY 4.0\", \"PwC License URL\": \"https://creativecommons.org/licenses/by/4.0/\", \"S2 Citation Count (June 2024)\": \"\", \"S2 Date\": \"\"}, \"Bibtex\": \"@Article{Kuehne2011HMDBAL,\\n author = {Hilde Kuehne and Hueihan Jhuang and Est\\u00edbaliz Garrote and T. Poggio and Thomas Serre},\\n booktitle = {Vision},\\n journal = {2011 International Conference on Computer Vision},\\n pages = {2556-2563},\\n title = {HMDB: A large video database for human motion recognition},\\n year = {2011}\\n}\\n\", \"License Use (DataProvenance)\": \"commercial\", \"License Attribution (DataProvenance)\": 1, \"License Share Alike (DataProvenance)\": 0, \"License Use (DataProvenance IgnoreOpenAI)\": \"commercial\", \"License Attribution (DataProvenance IgnoreOpenAI)\": 1, \"License Share Alike (DataProvenance IgnoreOpenAI)\": 0, \"License Type\": \"Commercial\"}, {\"Unique Dataset Identifier\": \"hmdb-dataset\", \"Collection\": \"hmdb-dataset\", \"Collection URL\": \"https://ieeexplore.ieee.org/document/6126543\", \"Dataset Name\": \"HMDB: A Large Video Database for Human Motion Recognition (ICCV 2011)\", \"Paper Title\": \"HMDB: A Large Video Database for Human Motion Recognition (ICCV 2011)\", \"Paper URL\": \"https://ieeexplore.ieee.org/document/6126543\", \"GitHub URL\": \"\", \"Hugging Face URL\": \"\", \"Papers with Code URL\": \"https://paperswithcode.com/dataset/hmdb51\", \"ArXiv URL\": \"https://ieeexplore.ieee.org/document/6126543\", \"Semantic Scholar Corpus ID\": 206769852, \"Year Released\": \"2011\", \"Text Sources\": [\"movies\", \"prelinger archive\", \"undisclosed web\", \"youtube\", \"google videos\"], \"Licenses\": [{\"License\": \"CC BY 4.0\", \"License URL\": \"https://serre-lab.clps.brown.edu/resource/hmdb-a-large-human-motion-database/\"}], \"Creators\": [\"Karlsruhe Institute of Technology\", \"Massachusetts Institute of Technology\", \"Brown University\"], \"Countries\": [\"United States of America\", \"Germany\"], \"License Verified By\": \"Vivek Sharma\", \"Video Hours\": 7000.0, \"Taken Down\": \"False\", \"Video Sources\": \"Other\", \"Task Categories\": [\"Video Classification\"], \"Inferred Metadata\": {\"GitHub License\": \"\", \"GitHub Stars (June 2024)\": \"\", \"GitHub Topics\": \"\", \"Github Date\": \"\", \"HF Config\": \"\", \"HF Config License\": \"\", \"HF Dataset\": \"\", \"HF Date\": \"\", \"HF Downloads (June 2024)\": \"\", \"HF Likes (June 2024)\": \"\", \"HF Yaml License\": \"\", \"PwC Date\": \"2011-01-01\", \"PwC Description\": \"The HMDB51 dataset is a large collection of realistic videos from various sources, including movies and web videos. The dataset is composed of 6,766 video clips from 51 action categories (such as \\u201cjump\\u201d, \\u201ckiss\\u201d and \\u201claugh\\u201d), with each category containing at least 101 clips. The original evaluation scheme uses three different training/testing splits. In each split, each action class has 70 clips for training and 30 clips for testing. The average accuracy over these three splits is used to measure the final performance.\", \"PwC License Name\": \"CC BY 4.0\", \"PwC License URL\": \"https://creativecommons.org/licenses/by/4.0/\", \"S2 Citation Count (June 2024)\": \"\", \"S2 Date\": \"\"}, \"Bibtex\": \"@Article{Kuehne2011HMDBAL,\\n author = {Hilde Kuehne and Hueihan Jhuang and Est\\u00edbaliz Garrote and T. Poggio and Thomas Serre},\\n booktitle = {Vision},\\n journal = {2011 International Conference on Computer Vision},\\n pages = {2556-2563},\\n title = {HMDB: A large video database for human motion recognition},\\n year = {2011}\\n}\\n\", \"License Use (DataProvenance)\": \"commercial\", \"License Attribution (DataProvenance)\": 1, \"License Share Alike (DataProvenance)\": 0, \"License Use (DataProvenance IgnoreOpenAI)\": \"commercial\", \"License Attribution (DataProvenance IgnoreOpenAI)\": 1, \"License Share Alike (DataProvenance IgnoreOpenAI)\": 0, \"License Type\": \"Commercial\"}, {\"Unique Dataset Identifier\": \"ucf101-dataset\", \"Collection\": \"ucf101-dataset\", \"Collection URL\": \"https://arxiv.org/abs/1212.0402\", \"Dataset Name\": \"UCF101: A Dataset of 101 Human Actions Classes From Videos in The Wild\", \"Paper Title\": \"UCF101: A Dataset of 101 Human Actions Classes From Videos in The Wild\", \"Paper URL\": \"https://arxiv.org/abs/1212.0402\", \"GitHub URL\": \"\", \"Hugging Face URL\": \"\", \"Papers with Code URL\": \"https://paperswithcode.com/dataset/ucf101\", \"ArXiv URL\": \"https://arxiv.org/abs/1212.0402\", \"Semantic Scholar Corpus ID\": 7197134, \"Year Released\": \"2012\", \"Text Sources\": [\"youtube\"], \"Licenses\": [{\"License\": \"Unspecified\", \"License URL\": \"\"}], \"Creators\": [\"University of Central Florida\"], \"Countries\": [\"United States of America\"], \"License Verified By\": \"Vivek Sharma\", \"Video Hours\": 26.0, \"Taken Down\": \"False\", \"Video Sources\": \"youtube\", \"Task Categories\": [\"Video Classification\"], \"Inferred Metadata\": {\"GitHub License\": \"\", \"GitHub Stars (June 2024)\": \"\", \"GitHub Topics\": \"\", \"Github Date\": \"\", \"HF Config\": \"\", \"HF Config License\": \"\", \"HF Dataset\": \"\", \"HF Date\": \"\", \"HF Downloads (June 2024)\": \"\", \"HF Likes (June 2024)\": \"\", \"HF Yaml License\": \"\", \"PwC Date\": \"2012-12-03\", \"PwC Description\": \"UCF101 dataset is an extension of UCF50 and consists of 13,320 video clips, which are classified into 101 categories. These 101 categories can be classified into 5 types (Body motion, Human-human interactions, Human-object interactions, Playing musical instruments and Sports). The total length of these video clips is over 27 hours. All the videos are collected from YouTube and have a fixed frame rate of 25 FPS with the resolution of 320 \\u00d7 240.\", \"PwC License Name\": \"Unspecified\", \"PwC License URL\": null, \"S2 Citation Count (June 2024)\": \"\", \"S2 Date\": \"\"}, \"Bibtex\": \"@Article{Soomro2012UCF101AD,\\n author = {K. Soomro and Amir Zamir and M. Shah},\\n booktitle = {arXiv.org},\\n journal = {ArXiv},\\n title = {UCF101: A Dataset of 101 Human Actions Classes From Videos in The Wild},\\n volume = {abs/1212.0402},\\n year = {2012}\\n}\\n\", \"License Use (DataProvenance)\": \"unspecified\", \"License Attribution (DataProvenance)\": 0, \"License Share Alike (DataProvenance)\": 0, \"License Use (DataProvenance IgnoreOpenAI)\": \"unspecified\", \"License Attribution (DataProvenance IgnoreOpenAI)\": 0, \"License Share Alike (DataProvenance IgnoreOpenAI)\": 0, \"License Type\": \"Unspecified\"}, {\"Unique Dataset Identifier\": \"50salads\", \"Collection\": \"50salads\", \"Collection URL\": \"https://citeseerx.ist.psu.edu/document?repid=rep1&type=pdf&doi=b9410401cec076baef045e83953f3ff24f25d149\", \"Dataset Name\": \"50Salads\", \"Paper Title\": \"50Salads\", \"Paper URL\": \"https://citeseerx.ist.psu.edu/document?repid=rep1&type=pdf&doi=b9410401cec076baef045e83953f3ff24f25d149\", \"GitHub URL\": \"\", \"Hugging Face URL\": \"\", \"Papers with Code URL\": \"https://paperswithcode.com/dataset/50-salads\", \"ArXiv URL\": \"https://citeseerx.ist.psu.edu/document?repid=rep1&type=pdf&doi=b9410401cec076baef045e83953f3ff24f25d149\", \"Semantic Scholar Corpus ID\": 2333743, \"Year Released\": \"2013\", \"Text Sources\": [\"human\"], \"Licenses\": [{\"License\": \"CC BY-NC-SA 4.0\", \"License URL\": \"https://cvip.computing.dundee.ac.uk/datasets/foodpreparation/50salads/\"}], \"Creators\": [\"University of Dundee\"], \"Countries\": [\"United Kingdom\"], \"License Verified By\": \"Vivek Sharma\", \"Video Hours\": 40.0, \"Taken Down\": \"False\", \"Video Sources\": \"human\", \"Task Categories\": [\"Action Segmentation\", \"Action Localization\"], \"Inferred Metadata\": {\"GitHub License\": \"\", \"GitHub Stars (June 2024)\": \"\", \"GitHub Topics\": \"\", \"Github Date\": \"\", \"HF Config\": \"\", \"HF Config License\": \"\", \"HF Dataset\": \"\", \"HF Date\": \"\", \"HF Downloads (June 2024)\": \"\", \"HF Likes (June 2024)\": \"\", \"HF Yaml License\": \"\", \"PwC Date\": \"2013-09-08\", \"PwC Description\": \"Activity recognition research has shifted focus from distinguishing full-body motion patterns to recognizing complex interactions of multiple entities. Manipulative gestures \\u2013 characterized by interactions between hands, tools, and manipulable objects \\u2013 frequently occur in food preparation, manufacturing, and assembly tasks, and have a variety of applications including situational support, automated supervision, and skill assessment. With the aim to stimulate research on recognizing manipulative gestures we introduce the 50 Salads dataset. It captures 25 people preparing 2 mixed salads each and contains over 4h of annotated accelerometer and RGB-D video data. Including detailed annotations, multiple sensor types, and two sequences per participant, the 50 Salads dataset may be used for research in areas such as activity recognition, activity spotting, sequence analysis, progress tracking, sensor fusion, transfer learning, and user-adaptation.\\n\\nThe dataset includes\\n\\nRGB video data 640\\u00d7480 pixels at 30 Hz\\nDepth maps 640\\u00d7480 pixels at 30 Hz\\n3-axis accelerometer data at 50 Hz of devices attached to a knife, a mixing spoon, a small spoon, a peeler, a glass, an oil bottle, and a pepper dispenser.\\nSynchronization parameters for temporal alignment of video and accelerometer data\\nAnnotations as temporal intervals of pre- core- and post-phases of activities corresponding to steps in a recipe\", \"PwC License Name\": \"CC BY-NC-SA 4.0\", \"PwC License URL\": null, \"S2 Citation Count (June 2024)\": \"\", \"S2 Date\": \"\"}, \"Bibtex\": \"@Book{Stein2013CombiningEA,\\n author = {Sebastian Stein and S. McKenna},\\n booktitle = {Ubiquitous Computing},\\n journal = {Proceedings of the 2013 ACM international joint conference on Pervasive and ubiquitous computing},\\n title = {Combining embedded accelerometers with computer vision for recognizing food preparation activities},\\n year = {2013}\\n}\\n\", \"License Use (DataProvenance)\": \"non-commercial\", \"License Attribution (DataProvenance)\": 1, \"License Share Alike (DataProvenance)\": 1, \"License Use (DataProvenance IgnoreOpenAI)\": \"non-commercial\", \"License Attribution (DataProvenance IgnoreOpenAI)\": 1, \"License Share Alike (DataProvenance IgnoreOpenAI)\": 1, \"License Type\": \"Non-Commercial/Academic\"}, {\"Unique Dataset Identifier\": \"youcook\", \"Collection\": \"youcook\", \"Collection URL\": \"https://openaccess.thecvf.com/content_cvpr_2013/papers/Das_A_Thousand_Frames_2013_CVPR_paper.pdf\", \"Dataset Name\": \"YouCook: A Thousand Frames in Just a Few Words: Lingual Description of Videos through Latent Topics and Sparse Object Stitching (CVPR 2013)\", \"Paper Title\": \"YouCook: A Thousand Frames in Just a Few Words: Lingual Description of Videos through Latent Topics and Sparse Object Stitching (CVPR 2013)\", \"Paper URL\": \"https://openaccess.thecvf.com/content_cvpr_2013/papers/Das_A_Thousand_Frames_2013_CVPR_paper.pdf\", \"GitHub URL\": \"\", \"Hugging Face URL\": \"\", \"Papers with Code URL\": \"https://paperswithcode.com/dataset/youcook\", \"ArXiv URL\": \"https://openaccess.thecvf.com/content_cvpr_2013/papers/Das_A_Thousand_Frames_2013_CVPR_paper.pdf\", \"Semantic Scholar Corpus ID\": 12284555, \"Year Released\": \"2013\", \"Text Sources\": [\"youtube\"], \"Licenses\": [{\"License\": \"Unspecified\", \"License URL\": \"\"}], \"Creators\": [\"University at Buffalo\"], \"Countries\": [\"United States of America\"], \"License Verified By\": \"Vivek Sharma\", \"Video Hours\": 1000.0, \"Taken Down\": \"False\", \"Video Sources\": \"youtube\", \"Task Categories\": [\"Video Captioning\"], \"Inferred Metadata\": {\"GitHub License\": \"\", \"GitHub Stars (June 2024)\": \"\", \"GitHub Topics\": \"\", \"Github Date\": \"\", \"HF Config\": \"\", \"HF Config License\": \"\", \"HF Dataset\": \"\", \"HF Date\": \"\", \"HF Downloads (June 2024)\": \"\", \"HF Likes (June 2024)\": \"\", \"HF Yaml License\": \"\", \"PwC Date\": \"\", \"PwC Description\": \"This data set was prepared from 88 open-source YouTube cooking videos. The YouCook dataset contains videos of people cooking various recipes. The videos were downloaded from YouTube and are all in the third-person viewpoint; they represent a significantly more challenging visual problem than existing cooking and kitchen datasets (the background kitchen/scene is different for many and most videos have dynamic camera changes). In addition, frame-by-frame object and action annotations are provided for training data (as well as a number of precomputed low-level features). Finally, each video has a number of human provided natural language descriptions (on average, there are eight different descriptions per video). This dataset has been created to serve as a benchmark in describing complex real-world videos with natural language descriptions.\", \"PwC License Name\": \"Unspecified\", \"PwC License URL\": null, \"S2 Citation Count (June 2024)\": \"\", \"S2 Date\": \"\"}, \"Bibtex\": \"@Article{Das2013ATF,\\n author = {Pradipto Das and Chenliang Xu and Richard F. Doell and Jason J. Corso},\\n booktitle = {2013 IEEE Conference on Computer Vision and Pattern Recognition},\\n journal = {2013 IEEE Conference on Computer Vision and Pattern Recognition},\\n pages = {2634-2641},\\n title = {A Thousand Frames in Just a Few Words: Lingual Description of Videos through Latent Topics and Sparse Object Stitching},\\n year = {2013}\\n}\\n\", \"License Use (DataProvenance)\": \"unspecified\", \"License Attribution (DataProvenance)\": 0, \"License Share Alike (DataProvenance)\": 0, \"License Use (DataProvenance IgnoreOpenAI)\": \"unspecified\", \"License Attribution (DataProvenance IgnoreOpenAI)\": 0, \"License Share Alike (DataProvenance IgnoreOpenAI)\": 0, \"License Type\": \"Unspecified\"}, {\"Unique Dataset Identifier\": \"summe\", \"Collection\": \"summe\", \"Collection URL\": \"https://link.springer.com/chapter/10.1007/978-3-319-10584-0_33\", \"Dataset Name\": \"SumMe: Creating Summaries from User Videos (ECCV 2014)\", \"Paper Title\": \"SumMe: Creating Summaries from User Videos (ECCV 2014)\", \"Paper URL\": \"https://link.springer.com/chapter/10.1007/978-3-319-10584-0_33\", \"GitHub URL\": \"\", \"Hugging Face URL\": \"\", \"Papers with Code URL\": \"https://paperswithcode.com/dataset/summe\", \"ArXiv URL\": \"https://link.springer.com/chapter/10.1007/978-3-319-10584-0_33\", \"Semantic Scholar Corpus ID\": 2111093, \"Year Released\": \"2014\", \"Text Sources\": [\"crowdsourced\"], \"Licenses\": [{\"License\": \"Unspecified\", \"License URL\": \"\"}], \"Creators\": [\"ETH Z\\u00fcrich\", \"KU Leuven\", \"upicto GmbH\"], \"Countries\": [\"Belgium\", \"Switzerland\"], \"License Verified By\": \"Vivek Sharma\", \"Video Hours\": 1.11, \"Taken Down\": \"False\", \"Video Sources\": \"crowdsourced\", \"Task Categories\": [\"Video Summarization\"], \"Inferred Metadata\": {\"GitHub License\": \"\", \"GitHub Stars (June 2024)\": \"\", \"GitHub Topics\": \"\", \"Github Date\": \"\", \"HF Config\": \"\", \"HF Config License\": \"\", \"HF Dataset\": \"\", \"HF Date\": \"\", \"HF Downloads (June 2024)\": \"\", \"HF Likes (June 2024)\": \"\", \"HF Yaml License\": \"\", \"PwC Date\": \"2014-01-01\", \"PwC Description\": \"The SumMe dataset is a video summarization dataset consisting of 25 videos, each annotated with at least 15 human summaries (390 in total).\", \"PwC License Name\": \"Academic Research Purposes Only\", \"PwC License URL\": \"https://gyglim.github.io/me/vsum/index.html#benchmark\", \"S2 Citation Count (June 2024)\": \"\", \"S2 Date\": \"\"}, \"Bibtex\": \"@Article{Gygli2014CreatingSF,\\n author = {Michael Gygli and H. Grabner and Hayko Riemenschneider and L. Gool},\\n booktitle = {European Conference on Computer Vision},\\n pages = {505-520},\\n title = {Creating Summaries from User Videos},\\n year = {2014}\\n}\\n\", \"License Use (DataProvenance)\": \"unspecified\", \"License Attribution (DataProvenance)\": 0, \"License Share Alike (DataProvenance)\": 0, \"License Use (DataProvenance IgnoreOpenAI)\": \"unspecified\", \"License Attribution (DataProvenance IgnoreOpenAI)\": 0, \"License Share Alike (DataProvenance IgnoreOpenAI)\": 0, \"License Type\": \"Unspecified\"}, {\"Unique Dataset Identifier\": \"thumos-challenge\", \"Collection\": \"thumos-challenge\", \"Collection URL\": \"https://arxiv.org/pdf/1604.06182.pdf\", \"Dataset Name\": \"The THUMOS Challenge on Action Recognition for Videos in the Wild\", \"Paper Title\": \"The THUMOS Challenge on Action Recognition for Videos in the Wild\", \"Paper URL\": \"https://arxiv.org/pdf/1604.06182.pdf\", \"GitHub URL\": \"\", \"Hugging Face URL\": \"\", \"Papers with Code URL\": \"https://paperswithcode.com/dataset/thumos14-1\", \"ArXiv URL\": \"https://arxiv.org/pdf/1604.06182.pdf\", \"Semantic Scholar Corpus ID\": 14049355, \"Year Released\": \"2014\", \"Text Sources\": [\"youtube\"], \"Licenses\": [{\"License\": \"Custom\", \"License URL\": \"https://docs.google.com/forms/d/e/1FAIpQLScs9davISAtYQS7SEF5qQNu0jUpLzNH3aHmPfuqk2q1VYDkmw/viewform\"}], \"Creators\": [\"Stanford University\", \"University of Central Florida\", \"Fudan University\", \"Inria\"], \"Countries\": [\"United States of America\", \"China\"], \"License Verified By\": \"Vivek Sharma\", \"Video Hours\": 254.0, \"Taken Down\": \"False\", \"Video Sources\": \"youtube\", \"Task Categories\": [\"Video Classification\"], \"Inferred Metadata\": {\"GitHub License\": \"\", \"GitHub Stars (June 2024)\": \"\", \"GitHub Topics\": \"\", \"Github Date\": \"\", \"HF Config\": \"\", \"HF Config License\": \"\", \"HF Dataset\": \"\", \"HF Date\": \"\", \"HF Downloads (June 2024)\": \"\", \"HF Likes (June 2024)\": \"\", \"HF Yaml License\": \"\", \"PwC Date\": \"\", \"PwC Description\": \"The THUMOS14 (THUMOS 2014) dataset is a large-scale video dataset that includes 1,010 videos for validation and 1,574 videos for testing from 20 classes. Among all the videos, there are 220 and 212 videos with temporal annotations in validation and testing set, respectively.\", \"PwC License Name\": \"Unspecified\", \"PwC License URL\": null, \"S2 Citation Count (June 2024)\": \"\", \"S2 Date\": \"\"}, \"Bibtex\": \"@Misc{None,\\n author = {Haroon Idrees and Amir Zamir and Yu-Gang Jiang and Alexander N. Gorban and I. Laptev and R. Sukthankar and M. Shah},\\n title = {Computer Vision and Image Understanding}\\n}\\n\", \"License Use (DataProvenance)\": \"academic-only\", \"License Attribution (DataProvenance)\": 1, \"License Share Alike (DataProvenance)\": 0, \"License Use (DataProvenance IgnoreOpenAI)\": \"academic-only\", \"License Attribution (DataProvenance IgnoreOpenAI)\": 1, \"License Share Alike (DataProvenance IgnoreOpenAI)\": 0, \"License Type\": \"Non-Commercial/Academic\"}, {\"Unique Dataset Identifier\": \"breakfast\", \"Collection\": \"breakfast\", \"Collection URL\": \"https://openaccess.thecvf.com/content_cvpr_2014/html/Kuehne_The_Language_of_2014_CVPR_paper.html\", \"Dataset Name\": \"Breakfast\", \"Paper Title\": \"Breakfast\", \"Paper URL\": \"https://openaccess.thecvf.com/content_cvpr_2014/html/Kuehne_The_Language_of_2014_CVPR_paper.html\", \"GitHub URL\": \"\", \"Hugging Face URL\": \"\", \"Papers with Code URL\": \"https://paperswithcode.com/dataset/breakfast\", \"ArXiv URL\": \"https://openaccess.thecvf.com/content_cvpr_2014/html/Kuehne_The_Language_of_2014_CVPR_paper.html\", \"Semantic Scholar Corpus ID\": 9621856, \"Year Released\": \"2014\", \"Text Sources\": [\"human\"], \"Licenses\": [{\"License\": \"CC BY 4.0\", \"License URL\": \"https://serre-lab.clps.brown.edu/resource/breakfast-actions-dataset/\"}], \"Creators\": [\"Fraunhofer FKIE\", \"Brown University\"], \"Countries\": [\"United States of America\", \"Germany\"], \"License Verified By\": \"Vivek Sharma\", \"Video Hours\": 77.0, \"Taken Down\": \"False\", \"Video Sources\": \"human\", \"Task Categories\": [\"Video Captioning\", \"Action Segmentation\"], \"Inferred Metadata\": {\"GitHub License\": \"\", \"GitHub Stars (June 2024)\": \"\", \"GitHub Topics\": \"\", \"Github Date\": \"\", \"HF Config\": \"\", \"HF Config License\": \"\", \"HF Dataset\": \"\", \"HF Date\": \"\", \"HF Downloads (June 2024)\": \"\", \"HF Likes (June 2024)\": \"\", \"HF Yaml License\": \"\", \"PwC Date\": \"2014-01-01\", \"PwC Description\": \"The Breakfast Actions Dataset comprises of 10 actions related to breakfast preparation, performed by 52 different individuals in 18 different kitchens. The dataset is one of the largest fully annotated datasets available. The actions are recorded \\u201cin the wild\\u201d as opposed to a single controlled lab environment. It consists of over 77 hours of video recordings.\", \"PwC License Name\": \"CC BY 4.0\", \"PwC License URL\": \"https://creativecommons.org/licenses/by/4.0/\", \"S2 Citation Count (June 2024)\": \"\", \"S2 Date\": \"\"}, \"Bibtex\": \"@Article{Kuehne2014TheLO,\\n author = {Hilde Kuehne and A. B. Arslan and Thomas Serre},\\n booktitle = {2014 IEEE Conference on Computer Vision and Pattern Recognition},\\n journal = {2014 IEEE Conference on Computer Vision and Pattern Recognition},\\n pages = {780-787},\\n title = {The Language of Actions: Recovering the Syntax and Semantics of Goal-Directed Human Activities},\\n year = {2014}\\n}\\n\", \"License Use (DataProvenance)\": \"commercial\", \"License Attribution (DataProvenance)\": 1, \"License Share Alike (DataProvenance)\": 0, \"License Use (DataProvenance IgnoreOpenAI)\": \"commercial\", \"License Attribution (DataProvenance IgnoreOpenAI)\": 1, \"License Share Alike (DataProvenance IgnoreOpenAI)\": 0, \"License Type\": \"Commercial\"}, {\"Unique Dataset Identifier\": \"hollywood-extended\", \"Collection\": \"hollywood-extended\", \"Collection URL\": \"https://arxiv.org/pdf/1407.1208\", \"Dataset Name\": \"Hollywood Extended\", \"Paper Title\": \"Hollywood Extended\", \"Paper URL\": \"https://arxiv.org/pdf/1407.1208\", \"GitHub URL\": \"https://github.com/piotr-bojanowski/action-ordering\", \"Hugging Face URL\": \"\", \"Papers with Code URL\": \"https://paperswithcode.com/paper/weakly-supervised-action-labeling-in-videos\", \"ArXiv URL\": \"https://arxiv.org/pdf/1407.1208\", \"Semantic Scholar Corpus ID\": 9342651, \"Year Released\": \"2014\", \"Text Sources\": [\"movies\"], \"Licenses\": [{\"License\": \"MIT License\", \"License URL\": \"https://github.com/piotr-bojanowski/action-ordering/blob/master/LICENSE\"}], \"Creators\": [\"Inria\"], \"Countries\": [\"France\"], \"License Verified By\": \"Vivek Sharma\", \"Video Hours\": 8.75, \"Taken Down\": \"False\", \"Video Sources\": \"movies\", \"Task Categories\": [\"Temporal Action Detection\", \"Action Recognition\"], \"Inferred Metadata\": {\"GitHub License\": \"MIT License\", \"GitHub Stars (June 2024)\": 12, \"GitHub Topics\": [], \"Github Date\": \"\", \"HF Config\": \"\", \"HF Config License\": \"\", \"HF Dataset\": \"\", \"HF Date\": \"\", \"HF Downloads (June 2024)\": \"\", \"HF Likes (June 2024)\": \"\", \"HF Yaml License\": \"\", \"PwC Date\": \"\", \"PwC Description\": \"No code available yet.\", \"PwC License Name\": \"\", \"PwC License URL\": \"\", \"S2 Citation Count (June 2024)\": \"\", \"S2 Date\": \"\"}, \"Bibtex\": \"@Article{Bojanowski2014WeaklySA,\\n author = {Piotr Bojanowski and R\\u00e9mi Lajugie and F. Bach and I. Laptev and J. Ponce and C. Schmid and Josef Sivic},\\n booktitle = {European Conference on Computer Vision},\\n journal = {ArXiv},\\n title = {Weakly Supervised Action Labeling in Videos under Ordering Constraints},\\n volume = {abs/1407.1208},\\n year = {2014}\\n}\\n\", \"License Use (DataProvenance)\": \"commercial\", \"License Attribution (DataProvenance)\": 1, \"License Share Alike (DataProvenance)\": 0, \"License Use (DataProvenance IgnoreOpenAI)\": \"commercial\", \"License Attribution (DataProvenance IgnoreOpenAI)\": 1, \"License Share Alike (DataProvenance IgnoreOpenAI)\": 0, \"License Type\": \"Commercial\"}, {\"Unique Dataset Identifier\": \"stroygraphs\", \"Collection\": \"stroygraphs\", \"Collection URL\": \"https://openaccess.thecvf.com/content_cvpr_2014/papers/Tapaswi_StoryGraphs_Visualizing_Character_2014_CVPR_paper.pdf\", \"Dataset Name\": \"StoryGraphs\", \"Paper Title\": \"StoryGraphs\", \"Paper URL\": \"https://openaccess.thecvf.com/content_cvpr_2014/papers/Tapaswi_StoryGraphs_Visualizing_Character_2014_CVPR_paper.pdf\", \"GitHub URL\": \"https://github.com/makarandtapaswi/StoryGraphs_CVPR2014\", \"Hugging Face URL\": \"\", \"Papers with Code URL\": \"https://paperswithcode.com/paper/storygraphs-visualizing-character\", \"ArXiv URL\": \"https://openaccess.thecvf.com/content_cvpr_2014/papers/Tapaswi_StoryGraphs_Visualizing_Character_2014_CVPR_paper.pdf\", \"Semantic Scholar Corpus ID\": 1055956, \"Year Released\": \"2014\", \"Text Sources\": [\"movies\"], \"Licenses\": [{\"License\": \"Unspecified\", \"License URL\": \"\"}], \"Creators\": [\"Karlsruhe Institute of Technology\"], \"Countries\": [\"Germany\"], \"License Verified By\": \"Vivek Sharma\", \"Video Hours\": 7.3, \"Taken Down\": \"False\", \"Video Sources\": \"movies\", \"Task Categories\": [\"Misc\"], \"Inferred Metadata\": {\"GitHub License\": \"\", \"GitHub Stars (June 2024)\": 20, \"GitHub Topics\": [], \"Github Date\": \"\", \"HF Config\": \"\", \"HF Config License\": \"\", \"HF Dataset\": \"\", \"HF Date\": \"\", \"HF Downloads (June 2024)\": \"\", \"HF Likes (June 2024)\": \"\", \"HF Yaml License\": \"\", \"PwC Date\": \"\", \"PwC Description\": \"Implemented in one code library.\", \"PwC License Name\": \"\", \"PwC License URL\": \"\", \"S2 Citation Count (June 2024)\": \"\", \"S2 Date\": \"\"}, \"Bibtex\": \"@Article{Tapaswi2014StoryGraphsVC,\\n author = {Makarand Tapaswi and M. B\\u00e4uml and R. Stiefelhagen},\\n booktitle = {2014 IEEE Conference on Computer Vision and Pattern Recognition},\\n journal = {2014 IEEE Conference on Computer Vision and Pattern Recognition},\\n pages = {827-834},\\n title = {StoryGraphs: Visualizing Character Interactions as a Timeline},\\n year = {2014}\\n}\\n\", \"License Use (DataProvenance)\": \"unspecified\", \"License Attribution (DataProvenance)\": 0, \"License Share Alike (DataProvenance)\": 0, \"License Use (DataProvenance IgnoreOpenAI)\": \"unspecified\", \"License Attribution (DataProvenance IgnoreOpenAI)\": 0, \"License Share Alike (DataProvenance IgnoreOpenAI)\": 0, \"License Type\": \"Unspecified\"}, {\"Unique Dataset Identifier\": \"sports1M-dataset\", \"Collection\": \"sports1M-dataset\", \"Collection URL\": \"https://static.googleusercontent.com/media/research.google.com/en//pubs/archive/42455.pdf\", \"Dataset Name\": \"Sports-1M: Large-scale Video Classification with Convolutional Neural Networks\", \"Paper Title\": \"Sports-1M: Large-scale Video Classification with Convolutional Neural Networks\", \"Paper URL\": \"https://static.googleusercontent.com/media/research.google.com/en//pubs/archive/42455.pdf\", \"GitHub URL\": \"https://github.com/gtoderici/sports-1m-dataset\", \"Hugging Face URL\": \"\", \"Papers with Code URL\": \"https://paperswithcode.com/paper/large-scale-video-classification-with-1\", \"ArXiv URL\": \"https://static.googleusercontent.com/media/research.google.com/en//pubs/archive/42455.pdf\", \"Semantic Scholar Corpus ID\": 206592218, \"Year Released\": \"2014\", \"Text Sources\": [\"youtube\"], \"Licenses\": [{\"License\": \"CC BY 3.0\", \"License URL\": \"https://github.com/gtoderici/sports-1m-dataset\"}], \"Creators\": [\"Stanford University\"], \"Countries\": [\"United States of America\"], \"License Verified By\": \"Vivek Sharma\", \"Video Hours\": 105761.41, \"Taken Down\": \"False\", \"Video Sources\": \"youtube\", \"Task Categories\": [\"Video Classification\"], \"Inferred Metadata\": {\"GitHub License\": \"\", \"GitHub Stars (June 2024)\": 273, \"GitHub Topics\": [], \"Github Date\": \"\", \"HF Config\": \"\", \"HF Config License\": \"\", \"HF Dataset\": \"\", \"HF Date\": \"\", \"HF Downloads (June 2024)\": \"\", \"HF Likes (June 2024)\": \"\", \"HF Yaml License\": \"\", \"PwC Date\": \"\", \"PwC Description\": \"#9 best model for Action Recognition on Sports-1M (Video hit@1  metric)\", \"PwC License Name\": \"\", \"PwC License URL\": \"\", \"S2 Citation Count (June 2024)\": \"\", \"S2 Date\": \"\"}, \"Bibtex\": \"@Article{Karpathy2014LargeScaleVC,\\n author = {A. Karpathy and G. Toderici and Sanketh Shetty and Thomas Leung and R. Sukthankar and Li Fei-Fei},\\n booktitle = {2014 IEEE Conference on Computer Vision and Pattern Recognition},\\n journal = {2014 IEEE Conference on Computer Vision and Pattern Recognition},\\n pages = {1725-1732},\\n title = {Large-Scale Video Classification with Convolutional Neural Networks},\\n year = {2014}\\n}\\n\", \"License Use (DataProvenance)\": \"commercial\", \"License Attribution (DataProvenance)\": 1, \"License Share Alike (DataProvenance)\": 0, \"License Use (DataProvenance IgnoreOpenAI)\": \"commercial\", \"License Attribution (DataProvenance IgnoreOpenAI)\": 1, \"License Share Alike (DataProvenance IgnoreOpenAI)\": 0, \"License Type\": \"Commercial\"}, {\"Unique Dataset Identifier\": \"videostory\", \"Collection\": \"videostory\", \"Collection URL\": \"https://isis-data.science.uva.nl/cgmsnoek/pub/habibian-videostory-mm2014.pdf\", \"Dataset Name\": \"VideoStory: A New Multimedia Embedding for Few-Example Recognition and Translation of Events\", \"Paper Title\": \"VideoStory: A New Multimedia Embedding for Few-Example Recognition and Translation of Events\", \"Paper URL\": \"https://isis-data.science.uva.nl/cgmsnoek/pub/habibian-videostory-mm2014.pdf\", \"GitHub URL\": \"\", \"Hugging Face URL\": \"\", \"Papers with Code URL\": \"\", \"ArXiv URL\": \"https://isis-data.science.uva.nl/cgmsnoek/pub/habibian-videostory-mm2014.pdf\", \"Semantic Scholar Corpus ID\": 28203, \"Year Released\": \"2014\", \"Text Sources\": [\"youtube\"], \"Licenses\": [{\"License\": \"Unspecified\", \"License URL\": \"\"}], \"Creators\": [\"University of Amsterdam\"], \"Countries\": [\"Netherlands\"], \"License Verified By\": \"Vivek Sharma\", \"Video Hours\": 743.0, \"Taken Down\": \"False\", \"Video Sources\": \"youtube\", \"Task Categories\": [\"Video Captioning\"], \"Inferred Metadata\": {\"GitHub License\": \"\", \"GitHub Stars (June 2024)\": \"\", \"GitHub Topics\": \"\", \"Github Date\": \"\", \"HF Config\": \"\", \"HF Config License\": \"\", \"HF Dataset\": \"\", \"HF Date\": \"\", \"HF Downloads (June 2024)\": \"\", \"HF Likes (June 2024)\": \"\", \"HF Yaml License\": \"\", \"PwC Date\": \"\", \"PwC Description\": \"\", \"PwC License Name\": \"\", \"PwC License URL\": \"\", \"S2 Citation Count (June 2024)\": \"\", \"S2 Date\": \"\"}, \"Bibtex\": \"@Book{Habibian2014VideoStoryAN,\\n author = {A. Habibian and Thomas Mensink and Cees G. M. Snoek},\\n booktitle = {ACM Multimedia},\\n journal = {Proceedings of the 22nd ACM international conference on Multimedia},\\n title = {VideoStory: A New Multimedia Embedding for Few-Example Recognition and Translation of Events},\\n year = {2014}\\n}\\n\", \"License Use (DataProvenance)\": \"unspecified\", \"License Attribution (DataProvenance)\": 0, \"License Share Alike (DataProvenance)\": 0, \"License Use (DataProvenance IgnoreOpenAI)\": \"unspecified\", \"License Attribution (DataProvenance IgnoreOpenAI)\": 0, \"License Share Alike (DataProvenance IgnoreOpenAI)\": 0, \"License Type\": \"Unspecified\"}, {\"Unique Dataset Identifier\": \"tvsum\", \"Collection\": \"tvsum\", \"Collection URL\": \"https://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Song_TVSum_Summarizing_Web_2015_CVPR_paper.pdf\", \"Dataset Name\": \"TVSum: Summarizing web videos using titles (CVPR 2015)\", \"Paper Title\": \"TVSum: Summarizing web videos using titles (CVPR 2015)\", \"Paper URL\": \"https://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Song_TVSum_Summarizing_Web_2015_CVPR_paper.pdf\", \"GitHub URL\": \"https://github.com/yalesong/tvsum\", \"Hugging Face URL\": \"\", \"Papers with Code URL\": \"https://paperswithcode.com/dataset/tvsum-1\", \"ArXiv URL\": \"https://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Song_TVSum_Summarizing_Web_2015_CVPR_paper.pdf\", \"Semantic Scholar Corpus ID\": 7675635, \"Year Released\": \"2015\", \"Text Sources\": [\"youtube\"], \"Licenses\": [{\"License\": \"CC BY 3.0\", \"License URL\": \"https://github.com/yalesong/tvsum\"}], \"Creators\": [\"Yahoo Labs\"], \"Countries\": [\"United States of America\"], \"License Verified By\": \"Vivek Sharma\", \"Video Hours\": 3.5, \"Taken Down\": \"False\", \"Video Sources\": \"youtube\", \"Task Categories\": [\"Video Summarization\"], \"Inferred Metadata\": {\"GitHub License\": \"\", \"GitHub Stars (June 2024)\": 116, \"GitHub Topics\": [], \"Github Date\": \"\", \"HF Config\": \"\", \"HF Config License\": \"\", \"HF Dataset\": \"\", \"HF Date\": \"\", \"HF Downloads (June 2024)\": \"\", \"HF Likes (June 2024)\": \"\", \"HF Yaml License\": \"\", \"PwC Date\": \"2015-06-01\", \"PwC Description\": \"Title-based Video Summarization (TVSum) dataset serves as a benchmark to validate video summarization techniques. It contains 50 videos of various genres (e.g., news, how-to, documentary, vlog, egocentric) and 1,000 annotations of shot-level importance scores obtained via crowdsourcing (20 per video).\", \"PwC License Name\": \"Unspecified\", \"PwC License URL\": null, \"S2 Citation Count (June 2024)\": \"\", \"S2 Date\": \"\"}, \"Bibtex\": \"@Article{Song2015TVSumSW,\\n author = {Yale Song and Jordi Vallmitjana and Amanda Stent and A. Jaimes},\\n booktitle = {Computer Vision and Pattern Recognition},\\n journal = {2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},\\n pages = {5179-5187},\\n title = {TVSum: Summarizing web videos using titles},\\n year = {2015}\\n}\\n\", \"License Use (DataProvenance)\": \"commercial\", \"License Attribution (DataProvenance)\": 1, \"License Share Alike (DataProvenance)\": 0, \"License Use (DataProvenance IgnoreOpenAI)\": \"commercial\", \"License Attribution (DataProvenance IgnoreOpenAI)\": 1, \"License Share Alike (DataProvenance IgnoreOpenAI)\": 0, \"License Type\": \"Commercial\"}, {\"Unique Dataset Identifier\": \"mpii-cooking\", \"Collection\": \"mpii-cooking\", \"Collection URL\": \"https://arxiv.org/abs/1502.06648\", \"Dataset Name\": \"MPII-Cooking: Recognizing Fine-Grained and Composite Activities Using Hand-Centric Features and Script Data (IJCV 2015)\", \"Paper Title\": \"MPII-Cooking: Recognizing Fine-Grained and Composite Activities Using Hand-Centric Features and Script Data (IJCV 2015)\", \"Paper URL\": \"https://arxiv.org/abs/1502.06648\", \"GitHub URL\": \"\", \"Hugging Face URL\": \"\", \"Papers with Code URL\": \"https://paperswithcode.com/paper/recognizing-fine-grained-and-composite\", \"ArXiv URL\": \"https://arxiv.org/abs/1502.06648\", \"Semantic Scholar Corpus ID\": 14036544, \"Year Released\": \"2015\", \"Text Sources\": [\"human\"], \"Licenses\": [{\"License\": \"Custom\", \"License URL\": \"https://www.mpi-inf.mpg.de/departments/computer-vision-and-machine-learning/research/human-activity-recognition/mpii-cooking-activities-dataset\"}], \"Creators\": [\"Max Planck Institute for Informatics\"], \"Countries\": [\"Germany\"], \"License Verified By\": \"Vivek Sharma\", \"Video Hours\": 27.0, \"Taken Down\": \"False\", \"Video Sources\": \"human\", \"Task Categories\": [\"Video Classification\"], \"Inferred Metadata\": {\"GitHub License\": \"\", \"GitHub Stars (June 2024)\": \"\", \"GitHub Topics\": \"\", \"Github Date\": \"\", \"HF Config\": \"\", \"HF Config License\": \"\", \"HF Dataset\": \"\", \"HF Date\": \"\", \"HF Downloads (June 2024)\": \"\", \"HF Likes (June 2024)\": \"\", \"HF Yaml License\": \"\", \"PwC Date\": \"\", \"PwC Description\": \"No code available yet.\", \"PwC License Name\": \"\", \"PwC License URL\": \"\", \"S2 Citation Count (June 2024)\": \"\", \"S2 Date\": \"\"}, \"Bibtex\": \"@Article{Rohrbach2015RecognizingFA,\\n author = {Marcus Rohrbach and Anna Rohrbach and Michaela Regneri and S. Amin and Mykhaylo Andriluka and Manfred Pinkal and B. Schiele},\\n booktitle = {International Journal of Computer Vision},\\n journal = {International Journal of Computer Vision},\\n pages = {346 - 373},\\n title = {Recognizing Fine-Grained and Composite Activities Using Hand-Centric Features and Script Data},\\n volume = {119},\\n year = {2015}\\n}\\n\", \"License Use (DataProvenance)\": \"academic-only\", \"License Attribution (DataProvenance)\": 1, \"License Share Alike (DataProvenance)\": 0, \"License Use (DataProvenance IgnoreOpenAI)\": \"academic-only\", \"License Attribution (DataProvenance IgnoreOpenAI)\": 1, \"License Share Alike (DataProvenance IgnoreOpenAI)\": 0, \"License Type\": \"Non-Commercial/Academic\"}, {\"Unique Dataset Identifier\": \"activitynet\", \"Collection\": \"activitynet\", \"Collection URL\": \"https://ieeexplore.ieee.org/document/7298698\", \"Dataset Name\": \"ActivityNet\", \"Paper Title\": \"ActivityNet\", \"Paper URL\": \"https://ieeexplore.ieee.org/document/7298698\", \"GitHub URL\": \"\", \"Hugging Face URL\": \"https://huggingface.co/datasets/Leyo/ActivityNet_Captions\", \"Papers with Code URL\": \"https://paperswithcode.com/dataset/activitynet\", \"ArXiv URL\": \"https://ieeexplore.ieee.org/document/7298698\", \"Semantic Scholar Corpus ID\": 1710722, \"Year Released\": \"2015\", \"Text Sources\": [\"undisclosed web\"], \"Licenses\": [{\"License\": \"MIT License\", \"License URL\": \"https://github.com/facebookresearch/ActivityNet-Entities/blob/main/LICENSE\"}], \"Creators\": [\"Universidad del Norte\", \"King Abdullah University of Science and Technology (KAUST)\"], \"Countries\": [\"Saudi Arabia\", \"Colombia\"], \"License Verified By\": \"Vivek Sharma\", \"Video Hours\": 849.0, \"Taken Down\": \"False\", \"Video Sources\": \"undisclosed web\", \"Task Categories\": [\"Video Captioning\"], \"Inferred Metadata\": {\"GitHub License\": \"\", \"GitHub Stars (June 2024)\": \"\", \"GitHub Topics\": \"\", \"Github Date\": \"\", \"HF Config\": \"all\", \"HF Config License\": \"\", \"HF Dataset\": \"Leyo/ActivityNet_Captions\", \"HF Date\": \"2022-05-13\", \"HF Downloads (June 2024)\": 21, \"HF Likes (June 2024)\": 0, \"HF Yaml License\": \"Unspecified\", \"PwC Date\": \"2015-01-01\", \"PwC Description\": \"The ActivityNet dataset contains 200 different types of activities and a total of 849 hours of videos collected from YouTube. ActivityNet is the largest benchmark for temporal activity detection to date in terms of both the number of activity categories and number of videos, making the task particularly challenging. Version 1.3 of the dataset contains 19994 untrimmed videos in total and is divided into three disjoint subsets, training, validation, and testing by a ratio of 2:1:1. On average, each activity category has 137 untrimmed videos. Each video on average has 1.41 activities which are annotated with temporal boundaries. The ground-truth annotations of test videos are not public.\", \"PwC License Name\": \"Unspecified\", \"PwC License URL\": null, \"S2 Citation Count (June 2024)\": \"\", \"S2 Date\": \"\"}, \"Bibtex\": \"@Article{Heilbron2015ActivityNetAL,\\n author = {Fabian Caba Heilbron and Victor Escorcia and Bernard Ghanem and Juan Carlos Niebles},\\n booktitle = {Computer Vision and Pattern Recognition},\\n journal = {2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},\\n pages = {961-970},\\n title = {ActivityNet: A large-scale video benchmark for human activity understanding},\\n year = {2015}\\n}\\n\", \"License Use (DataProvenance)\": \"commercial\", \"License Attribution (DataProvenance)\": 1, \"License Share Alike (DataProvenance)\": 0, \"License Use (DataProvenance IgnoreOpenAI)\": \"commercial\", \"License Attribution (DataProvenance IgnoreOpenAI)\": 1, \"License Share Alike (DataProvenance IgnoreOpenAI)\": 0, \"License Type\": \"Commercial\"}, {\"Unique Dataset Identifier\": \"movieqa\", \"Collection\": \"movieqa\", \"Collection URL\": \"https://arxiv.org/abs/1512.02902\", \"Dataset Name\": \"MovieQA\", \"Paper Title\": \"MovieQA\", \"Paper URL\": \"https://arxiv.org/abs/1512.02902\", \"GitHub URL\": \"https://github.com/makarandtapaswi/MovieQA_CVPR2016/\", \"Hugging Face URL\": \"\", \"Papers with Code URL\": \"https://paperswithcode.com/dataset/movieqa\", \"ArXiv URL\": \"https://arxiv.org/abs/1512.02902\", \"Semantic Scholar Corpus ID\": 1017389, \"Year Released\": \"2015\", \"Text Sources\": [\"movies\"], \"Licenses\": [{\"License\": \"Unspecified\", \"License URL\": \"\"}], \"Creators\": [\"Karlsruhe Institute of Technology\", \"Massachusetts Institute of Technology\", \"University of Toronto\"], \"Countries\": [\"Germany\", \"Canada\", \"United States of America\"], \"License Verified By\": \"Vivek Sharma\", \"Video Hours\": 381.0, \"Taken Down\": \"True\", \"Video Sources\": \"movies\", \"Task Categories\": [\"Video Question Answering\"], \"Inferred Metadata\": {\"GitHub License\": \"\", \"GitHub Stars (June 2024)\": 80, \"GitHub Topics\": [], \"Github Date\": \"\", \"HF Config\": \"\", \"HF Config License\": \"\", \"HF Dataset\": \"\", \"HF Date\": \"\", \"HF Downloads (June 2024)\": \"\", \"HF Likes (June 2024)\": \"\", \"HF Yaml License\": \"\", \"PwC Date\": \"2016-01-01\", \"PwC Description\": \"The MovieQA dataset is a dataset for movie question answering. to evaluate automatic story comprehension from both video and text. The data set consists of almost 15,000 multiple choice question answers obtained from over 400 movies and features high semantic diversity. Each question comes with a set of five highly plausible answers; only one of which is correct. The questions can be answered using multiple sources of information: movie clips, plots, subtitles, and for a subset scripts and DVS.\", \"PwC License Name\": \"Unspecified\", \"PwC License URL\": null, \"S2 Citation Count (June 2024)\": \"\", \"S2 Date\": \"\"}, \"Bibtex\": \"@Article{Tapaswi2015MovieQAUS,\\n author = {Makarand Tapaswi and Yukun Zhu and R. Stiefelhagen and A. Torralba and R. Urtasun and S. Fidler},\\n booktitle = {Computer Vision and Pattern Recognition},\\n journal = {2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},\\n pages = {4631-4640},\\n title = {MovieQA: Understanding Stories in Movies through Question-Answering},\\n year = {2015}\\n}\\n\", \"License Use (DataProvenance)\": \"unspecified\", \"License Attribution (DataProvenance)\": 0, \"License Share Alike (DataProvenance)\": 0, \"License Use (DataProvenance IgnoreOpenAI)\": \"unspecified\", \"License Attribution (DataProvenance IgnoreOpenAI)\": 0, \"License Share Alike (DataProvenance IgnoreOpenAI)\": 0, \"License Type\": \"Unspecified\"}, {\"Unique Dataset Identifier\": \"volleyball-vids\", \"Collection\": \"volleyball-vids\", \"Collection URL\": \"https://arxiv.org/abs/1511.06040\", \"Dataset Name\": \"Volleyball: A Hierarchical Deep Temporal Model for Group Activity Recognition\", \"Paper Title\": \"Volleyball: A Hierarchical Deep Temporal Model for Group Activity Recognition\", \"Paper URL\": \"https://arxiv.org/abs/1511.06040\", \"GitHub URL\": \"https://github.com/mostafa-saad/deep-activity-rec#dataset\", \"Hugging Face URL\": \"\", \"Papers with Code URL\": \"https://paperswithcode.com/dataset/volleyball\", \"ArXiv URL\": \"https://arxiv.org/abs/1511.06040\", \"Semantic Scholar Corpus ID\": 8483403, \"Year Released\": \"2015\", \"Text Sources\": [\"youtube\"], \"Licenses\": [{\"License\": \"Unspecified\", \"License URL\": \"\"}], \"Creators\": [\"Simon Fraser University\"], \"Countries\": [\"Canada\"], \"License Verified By\": \"Vivek Sharma\", \"Video Hours\": 0.1, \"Taken Down\": \"False\", \"Video Sources\": \"youtube\", \"Task Categories\": [\"Group Activity Recognition\"], \"Inferred Metadata\": {\"GitHub License\": \"BSD 3-Clause License\", \"GitHub Stars (June 2024)\": 171, \"GitHub Topics\": [], \"Github Date\": \"\", \"HF Config\": \"\", \"HF Config License\": \"\", \"HF Dataset\": \"\", \"HF Date\": \"\", \"HF Downloads (June 2024)\": \"\", \"HF Likes (June 2024)\": \"\", \"HF Yaml License\": \"\", \"PwC Date\": \"2016-01-01\", \"PwC Description\": \"Volleyball is a video action recognition dataset. It has 4830 annotated frames that were handpicked from 55 videos with 9 player action labels and 8 team activity labels. It contains group activity annotations as well as individual activity annotations.\", \"PwC License Name\": \"Unspecified\", \"PwC License URL\": null, \"S2 Citation Count (June 2024)\": \"\", \"S2 Date\": \"\"}, \"Bibtex\": \"@Article{Ibrahim2016HierarchicalDT,\\n author = {Mostafa S. Ibrahim and S. Muralidharan and Zhiwei Deng and Arash Vahdat and Greg Mori},\\n booktitle = {arXiv.org},\\n journal = {ArXiv},\\n title = {Hierarchical Deep Temporal Models for Group Activity Recognition},\\n volume = {abs/1607.02643},\\n year = {2016}\\n}\\n\", \"License Use (DataProvenance)\": \"unspecified\", \"License Attribution (DataProvenance)\": 0, \"License Share Alike (DataProvenance)\": 0, \"License Use (DataProvenance IgnoreOpenAI)\": \"unspecified\", \"License Attribution (DataProvenance IgnoreOpenAI)\": 0, \"License Share Alike (DataProvenance IgnoreOpenAI)\": 0, \"License Type\": \"Unspecified\"}, {\"Unique Dataset Identifier\": \"msr-vtt\", \"Collection\": \"msr-vtt\", \"Collection URL\": \"https://ieeexplore.ieee.org/document/7780940\", \"Dataset Name\": \"MSR-VTT\", \"Paper Title\": \"MSR-VTT\", \"Paper URL\": \"https://ieeexplore.ieee.org/document/7780940\", \"GitHub URL\": \"\", \"Hugging Face URL\": \"\", \"Papers with Code URL\": \"https://paperswithcode.com/dataset/msr-vtt\", \"ArXiv URL\": \"https://ieeexplore.ieee.org/document/7780940\", \"Semantic Scholar Corpus ID\": 206594535, \"Year Released\": \"2016\", \"Text Sources\": [\"undisclosed web\"], \"Licenses\": [{\"License\": \"Unspecified\", \"License URL\": \"\"}], \"Creators\": [\"Microsoft Research\"], \"Countries\": [\"United States of America\"], \"License Verified By\": \"Vivek Sharma\", \"Video Hours\": 41.2, \"Taken Down\": \"False\", \"Video Sources\": \"undisclosed web\", \"Task Categories\": [\"Video Captioning\"], \"Inferred Metadata\": {\"GitHub License\": \"\", \"GitHub Stars (June 2024)\": \"\", \"GitHub Topics\": \"\", \"Github Date\": \"\", \"HF Config\": \"\", \"HF Config License\": \"\", \"HF Dataset\": \"\", \"HF Date\": \"\", \"HF Downloads (June 2024)\": \"\", \"HF Likes (June 2024)\": \"\", \"HF Yaml License\": \"\", \"PwC Date\": \"2016-01-01\", \"PwC Description\": \"MSR-VTT (Microsoft Research Video to Text) is a large-scale dataset for the open domain video captioning, which consists of 10,000 video clips from 20 categories, and each video clip is annotated with 20 English sentences by Amazon Mechanical Turks. There are about 29,000 unique words in all captions. The standard splits uses 6,513 clips for training, 497 clips for validation, and 2,990 clips for testing.\", \"PwC License Name\": \"Unspecified\", \"PwC License URL\": null, \"S2 Citation Count (June 2024)\": \"\", \"S2 Date\": \"\"}, \"Bibtex\": \"@Article{Xu2016MSRVTTAL,\\n author = {Jun Xu and Tao Mei and Ting Yao and Y. Rui},\\n booktitle = {Computer Vision and Pattern Recognition},\\n journal = {2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},\\n pages = {5288-5296},\\n title = {MSR-VTT: A Large Video Description Dataset for Bridging Video and Language},\\n year = {2016}\\n}\\n\", \"License Use (DataProvenance)\": \"unspecified\", \"License Attribution (DataProvenance)\": 0, \"License Share Alike (DataProvenance)\": 0, \"License Use (DataProvenance IgnoreOpenAI)\": \"unspecified\", \"License Attribution (DataProvenance IgnoreOpenAI)\": 0, \"License Share Alike (DataProvenance IgnoreOpenAI)\": 0, \"License Type\": \"Unspecified\"}, {\"Unique Dataset Identifier\": \"vtw\", \"Collection\": \"vtw\", \"Collection URL\": \"https://arxiv.org/abs/1608.07068\", \"Dataset Name\": \"VTW\", \"Paper Title\": \"VTW\", \"Paper URL\": \"https://arxiv.org/abs/1608.07068\", \"GitHub URL\": \"\", \"Hugging Face URL\": \"\", \"Papers with Code URL\": \"https://paperswithcode.com/paper/title-generation-for-user-generated-videos\", \"ArXiv URL\": \"https://arxiv.org/abs/1608.07068\", \"Semantic Scholar Corpus ID\": 6155397, \"Year Released\": \"2016\", \"Text Sources\": [\"undisclosed web\"], \"Licenses\": [{\"License\": \"Unspecified\", \"License URL\": \"\"}], \"Creators\": [\"Tsinghua University\", \"Stanford University\"], \"Countries\": [\"China\", \"United States of America\"], \"License Verified By\": \"Vivek Sharma\", \"Video Hours\": 213.2, \"Taken Down\": \"False\", \"Video Sources\": \"undisclosed web\", \"Task Categories\": [\"Video Captioning\"], \"Inferred Metadata\": {\"GitHub License\": \"\", \"GitHub Stars (June 2024)\": \"\", \"GitHub Topics\": \"\", \"Github Date\": \"\", \"HF Config\": \"\", \"HF Config License\": \"\", \"HF Dataset\": \"\", \"HF Date\": \"\", \"HF Downloads (June 2024)\": \"\", \"HF Likes (June 2024)\": \"\", \"HF Yaml License\": \"\", \"PwC Date\": \"\", \"PwC Description\": \"No code available yet.\", \"PwC License Name\": \"\", \"PwC License URL\": \"\", \"S2 Citation Count (June 2024)\": \"\", \"S2 Date\": \"\"}, \"Bibtex\": \"@Article{Zeng2016TitleGF,\\n author = {Kuo-Hao Zeng and Tseng-Hung Chen and Juan Carlos Niebles and Min Sun},\\n booktitle = {European Conference on Computer Vision},\\n pages = {609-625},\\n title = {Title Generation for User Generated Videos},\\n year = {2016}\\n}\\n\", \"License Use (DataProvenance)\": \"unspecified\", \"License Attribution (DataProvenance)\": 0, \"License Share Alike (DataProvenance)\": 0, \"License Use (DataProvenance IgnoreOpenAI)\": \"unspecified\", \"License Attribution (DataProvenance IgnoreOpenAI)\": 0, \"License Share Alike (DataProvenance IgnoreOpenAI)\": 0, \"License Type\": \"Unspecified\"}, {\"Unique Dataset Identifier\": \"narrated-instruction-vids\", \"Collection\": \"narrated-instruction-vids\", \"Collection URL\": \"https://arxiv.org/abs/1506.09215\", \"Dataset Name\": \"Narrated Instruction Videos\", \"Paper Title\": \"Narrated Instruction Videos\", \"Paper URL\": \"https://arxiv.org/abs/1506.09215\", \"GitHub URL\": \"https://github.com/jalayrac/instructionVideos\", \"Hugging Face URL\": \"\", \"Papers with Code URL\": \"https://paperswithcode.com/dataset/youtube-inria-instructional\", \"ArXiv URL\": \"https://arxiv.org/abs/1506.09215\", \"Semantic Scholar Corpus ID\": 2617244, \"Year Released\": \"2016\", \"Text Sources\": [\"youtube\"], \"Licenses\": [{\"License\": \"MIT License\", \"License URL\": \"https://github.com/jalayrac/instructionVideos?tab=readme-ov-file#license\"}], \"Creators\": [\"CNRS\", \"Ecole Normale Sup\\u00e9rieure\", \"Inria\", \"International Institute of Information Technology - Hyderabad\"], \"Countries\": [\"United States of America\", \"India\"], \"License Verified By\": \"Vivek Sharma\", \"Video Hours\": 7.0, \"Taken Down\": \"False\", \"Video Sources\": \"youtube\", \"Task Categories\": [\"Video Captioning\"], \"Inferred Metadata\": {\"GitHub License\": \"MIT License\", \"GitHub Stars (June 2024)\": 19, \"GitHub Topics\": [], \"Github Date\": \"\", \"HF Config\": \"\", \"HF Config License\": \"\", \"HF Dataset\": \"\", \"HF Date\": \"\", \"HF Downloads (June 2024)\": \"\", \"HF Likes (June 2024)\": \"\", \"HF Yaml License\": \"\", \"PwC Date\": \"\", \"PwC Description\": \"We address the problem of automatically learning the main steps to complete a certain task, such as changing a car tire, from a set of narrated instruction videos. The contributions of this paper are three-fold. First, we develop a new unsupervised learning approach that takes advantage of the complementary nature of the input video and the associated narration. The method solves two clustering problems, one in text and one in video, applied one after each other and linked by joint constraints to obtain a single coherent sequence of steps in both modalities. Second, we collect and annotate a new challenging dataset of real-world instruction videos from the Internet. The dataset contains about 800,000 frames for five different tasks (How to : change a car tire, perform CardioPulmonary resuscitation (CPR), jump cars, repot a plant and make coffee) that include complex interactions between people and objects, and are captured in a variety of indoor and outdoor settings. Third, we experimentally demonstrate that the proposed method can automatically discover, in an unsupervised manner , the main steps to achieve the task and locate the steps in the input videos.\\n\\nThis video presents our results of automatically discovering the scenario for the two following task : changing a tire and performing CardioPulmonary Resuscitation (CPR). At the bottom of the videos, there are three bars. The first one corresponds to our ground truth annotation. The second one corresponds to our time interval prediction in video. Finally the third one corresponds to the constraints that we obtain from the text domain. On the right, there is a list of label. They corresponds to the label recovered by our NLP method in an unsupervised manner.\", \"PwC License Name\": \"Unspecified\", \"PwC License URL\": null, \"S2 Citation Count (June 2024)\": \"\", \"S2 Date\": \"\"}, \"Bibtex\": \"@Article{Alayrac2015UnsupervisedLF,\\n author = {Jean-Baptiste Alayrac and Piotr Bojanowski and Nishant Agrawal and Josef Sivic and I. Laptev and Simon Lacoste-Julien},\\n booktitle = {Computer Vision and Pattern Recognition},\\n journal = {2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},\\n pages = {4575-4583},\\n title = {Unsupervised Learning from Narrated Instruction Videos},\\n year = {2015}\\n}\\n\", \"License Use (DataProvenance)\": \"commercial\", \"License Attribution (DataProvenance)\": 1, \"License Share Alike (DataProvenance)\": 0, \"License Use (DataProvenance IgnoreOpenAI)\": \"commercial\", \"License Attribution (DataProvenance IgnoreOpenAI)\": 1, \"License Share Alike (DataProvenance IgnoreOpenAI)\": 0, \"License Type\": \"Commercial\"}, {\"Unique Dataset Identifier\": \"mpii-cooking2\", \"Collection\": \"mpii-cooking2\", \"Collection URL\": \"https://arxiv.org/pdf/1502.06648.pdf\", \"Dataset Name\": \"MPII Cooking 2\", \"Paper Title\": \"MPII Cooking 2\", \"Paper URL\": \"https://arxiv.org/pdf/1502.06648.pdf\", \"GitHub URL\": \"\", \"Hugging Face URL\": \"\", \"Papers with Code URL\": \"https://paperswithcode.com/dataset/mpii-cooking-2-dataset\", \"ArXiv URL\": \"https://arxiv.org/pdf/1502.06648.pdf\", \"Semantic Scholar Corpus ID\": 14036544, \"Year Released\": \"2016\", \"Text Sources\": [\"human\"], \"Licenses\": [{\"License\": \"Custom\", \"License URL\": \"https://www.mpi-inf.mpg.de/departments/computer-vision-and-machine-learning/research/human-activity-recognition/mpii-cooking-2-dataset/\"}], \"Creators\": [\"Max Planck Institute for Informatics\", \"Saarland University\"], \"Countries\": [\"Germany\"], \"License Verified By\": \"Vivek Sharma\", \"Video Hours\": 27.0, \"Taken Down\": \"False\", \"Video Sources\": \"human\", \"Task Categories\": [\"Temporal Action Detection\"], \"Inferred Metadata\": {\"GitHub License\": \"\", \"GitHub Stars (June 2024)\": \"\", \"GitHub Topics\": \"\", \"Github Date\": \"\", \"HF Config\": \"\", \"HF Config License\": \"\", \"HF Dataset\": \"\", \"HF Date\": \"\", \"HF Downloads (June 2024)\": \"\", \"HF Likes (June 2024)\": \"\", \"HF Yaml License\": \"\", \"PwC Date\": \"\", \"PwC Description\": \"A dataset which provides detailed annotations for activity recognition.\", \"PwC License Name\": \"Unspecified\", \"PwC License URL\": null, \"S2 Citation Count (June 2024)\": \"\", \"S2 Date\": \"\"}, \"Bibtex\": \"@Article{Rohrbach2015RecognizingFA,\\n author = {Marcus Rohrbach and Anna Rohrbach and Michaela Regneri and S. Amin and Mykhaylo Andriluka and Manfred Pinkal and B. Schiele},\\n booktitle = {International Journal of Computer Vision},\\n journal = {International Journal of Computer Vision},\\n pages = {346 - 373},\\n title = {Recognizing Fine-Grained and Composite Activities Using Hand-Centric Features and Script Data},\\n volume = {119},\\n year = {2015}\\n}\\n\", \"License Use (DataProvenance)\": \"academic-only\", \"License Attribution (DataProvenance)\": 1, \"License Share Alike (DataProvenance)\": 0, \"License Use (DataProvenance IgnoreOpenAI)\": \"academic-only\", \"License Attribution (DataProvenance IgnoreOpenAI)\": 1, \"License Share Alike (DataProvenance IgnoreOpenAI)\": 0, \"License Type\": \"Non-Commercial/Academic\"}, {\"Unique Dataset Identifier\": \"tgif\", \"Collection\": \"tgif\", \"Collection URL\": \"https://arxiv.org/abs/1604.02748\", \"Dataset Name\": \"TGIF\", \"Paper Title\": \"TGIF\", \"Paper URL\": \"https://arxiv.org/abs/1604.02748\", \"GitHub URL\": \"https://github.com/raingo/TGIF-Release\", \"Hugging Face URL\": \"https://huggingface.co/datasets/HuggingFaceM4/TGIF\", \"Papers with Code URL\": \"https://paperswithcode.com/dataset/tgif\", \"ArXiv URL\": \"https://arxiv.org/abs/1604.02748\", \"Semantic Scholar Corpus ID\": 6262415, \"Year Released\": \"2016\", \"Text Sources\": [\"tumblr\"], \"Licenses\": [{\"License\": \"Custom\", \"License URL\": \"https://github.com/raingo/TGIF-Release\"}], \"Creators\": [\"University of Rochester\", \"Yahoo! Inc.\", \"AiCure\"], \"Countries\": [\"United States of America\"], \"License Verified By\": \"Vivek Sharma\", \"Video Hours\": 86.1, \"Taken Down\": \"False\", \"Video Sources\": \"Other\", \"Task Categories\": [\"Video Captioning\"], \"Inferred Metadata\": {\"GitHub License\": \"BSD 3-Clause License\", \"GitHub Stars (June 2024)\": 111, \"GitHub Topics\": [], \"Github Date\": \"\", \"HF Config\": \"all\", \"HF Config License\": \"\", \"HF Dataset\": \"HuggingFaceM4/TGIF\", \"HF Date\": \"2022-05-17\", \"HF Downloads (June 2024)\": 9, \"HF Likes (June 2024)\": 11, \"HF Yaml License\": \"Unspecified\", \"PwC Date\": \"2016-04-10\", \"PwC Description\": \"The Tumblr GIF (TGIF) dataset contains 100K animated GIFs and 120K sentences describing visual content of the animated GIFs. The animated GIFs have been collected from Tumblr, from randomly selected posts published between May and June of 2015. The dataset provides the URLs of animated GIFs. The sentences are collected via crowdsourcing, with a carefully designed annotation interface that ensures high quality dataset. There is one sentence per animated GIF for the training and validation splits, and three sentences per GIF for the test split. The dataset can be used to evaluate animated GIF/video description techniques.\", \"PwC License Name\": \"Academic Research Purposes Only\", \"PwC License URL\": \"https://github.com/raingo/TGIF-Release#license\", \"S2 Citation Count (June 2024)\": \"\", \"S2 Date\": \"\"}, \"Bibtex\": \"@Article{Li2016TGIFAN,\\n author = {Yuncheng Li and Yale Song and Liangliang Cao and Joel R. Tetreault and Larry Goldberg and A. Jaimes and Jiebo Luo},\\n booktitle = {Computer Vision and Pattern Recognition},\\n journal = {2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},\\n pages = {4641-4650},\\n title = {TGIF: A New Dataset and Benchmark on Animated GIF Description},\\n year = {2016}\\n}\\n\", \"License Use (DataProvenance)\": \"commercial\", \"License Attribution (DataProvenance)\": 1, \"License Share Alike (DataProvenance)\": 0, \"License Use (DataProvenance IgnoreOpenAI)\": \"commercial\", \"License Attribution (DataProvenance IgnoreOpenAI)\": 1, \"License Share Alike (DataProvenance IgnoreOpenAI)\": 0, \"License Type\": \"Commercial\"}, {\"Unique Dataset Identifier\": \"ntu-rgbd\", \"Collection\": \"ntu-rgbd\", \"Collection URL\": \"https://arxiv.org/pdf/1604.02808.pdf\", \"Dataset Name\": \"NTU RGB+D: A Large Scale Dataset for 3D Human Activity Analysis (CVPR 2016, TPAMI 2019)\", \"Paper Title\": \"NTU RGB+D: A Large Scale Dataset for 3D Human Activity Analysis (CVPR 2016, TPAMI 2019)\", \"Paper URL\": \"https://arxiv.org/pdf/1604.02808.pdf\", \"GitHub URL\": \"https://github.com/shahroudy/NTURGB-D\", \"Hugging Face URL\": \"\", \"Papers with Code URL\": \"https://paperswithcode.com/paper/ntu-rgbd-a-large-scale-dataset-for-3d-human\", \"ArXiv URL\": \"https://arxiv.org/pdf/1604.02808.pdf\", \"Semantic Scholar Corpus ID\": 15928602, \"Year Released\": \"2016\", \"Text Sources\": [\"crowdsourced\"], \"Licenses\": [{\"License\": \"Custom\", \"License URL\": \"https://rose1.ntu.edu.sg/dataset/actionRecognition/\"}], \"Creators\": [\"Nanyang Technological University\"], \"Countries\": [\"Singapore\"], \"License Verified By\": \"Vivek Sharma\", \"Video Hours\": 74.1, \"Taken Down\": \"False\", \"Video Sources\": \"crowdsourced\", \"Task Categories\": [\"Video Classification\"], \"Inferred Metadata\": {\"GitHub License\": \"\", \"GitHub Stars (June 2024)\": 715, \"GitHub Topics\": [], \"Github Date\": \"\", \"HF Config\": \"\", \"HF Config License\": \"\", \"HF Dataset\": \"\", \"HF Date\": \"\", \"HF Downloads (June 2024)\": \"\", \"HF Likes (June 2024)\": \"\", \"HF Yaml License\": \"\", \"PwC Date\": \"\", \"PwC Description\": \"#4 best model for Skeleton Based Action Recognition on Varying-view RGB-D Action-Skeleton (Accuracy (CS) metric)\", \"PwC License Name\": \"\", \"PwC License URL\": \"\", \"S2 Citation Count (June 2024)\": \"\", \"S2 Date\": \"\"}, \"Bibtex\": \"@Article{Shahroudy2016NTURA,\\n author = {Amir Shahroudy and Jun Liu and T. Ng and G. Wang},\\n booktitle = {Computer Vision and Pattern Recognition},\\n journal = {2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},\\n pages = {1010-1019},\\n title = {NTU RGB+D: A Large Scale Dataset for 3D Human Activity Analysis},\\n year = {2016}\\n}\\n\", \"License Use (DataProvenance)\": \"academic-only\", \"License Attribution (DataProvenance)\": 1, \"License Share Alike (DataProvenance)\": 0, \"License Use (DataProvenance IgnoreOpenAI)\": \"academic-only\", \"License Attribution (DataProvenance IgnoreOpenAI)\": 1, \"License Share Alike (DataProvenance IgnoreOpenAI)\": 0, \"License Type\": \"Non-Commercial/Academic\"}, {\"Unique Dataset Identifier\": \"charades\", \"Collection\": \"charades\", \"Collection URL\": \"https://arxiv.org/abs/1604.01753\", \"Dataset Name\": \"Charades\", \"Paper Title\": \"Charades\", \"Paper URL\": \"https://arxiv.org/abs/1604.01753\", \"GitHub URL\": \"\", \"Hugging Face URL\": \"https://huggingface.co/datasets/HuggingFaceM4/charades\", \"Papers with Code URL\": \"https://paperswithcode.com/dataset/charades\", \"ArXiv URL\": \"https://arxiv.org/abs/1604.01753\", \"Semantic Scholar Corpus ID\": 18061547, \"Year Released\": \"2016\", \"Text Sources\": [\"human\"], \"Licenses\": [{\"License\": \"Custom\", \"License URL\": \"https://huggingface.co/datasets/HuggingFaceM4/charades#licensing-information\"}], \"Creators\": [\"Carnegie Mellon University\", \"Inria\", \"University of Washington\", \"Allen Institute for AI\"], \"Countries\": [\"United States of America\", \"France\"], \"License Verified By\": \"Vivek Sharma\", \"Video Hours\": 82.3, \"Taken Down\": \"False\", \"Video Sources\": \"human\", \"Task Categories\": [\"Video Captioning\"], \"Inferred Metadata\": {\"GitHub License\": \"\", \"GitHub Stars (June 2024)\": \"\", \"GitHub Topics\": \"\", \"Github Date\": \"\", \"HF Config\": \"default\", \"HF Config License\": \"\", \"HF Dataset\": \"HuggingFaceM4/charades\", \"HF Date\": \"2022-05-11\", \"HF Downloads (June 2024)\": 7, \"HF Likes (June 2024)\": 2, \"HF Yaml License\": \"Unspecified\", \"PwC Date\": \"2016-01-01\", \"PwC Description\": \"The Charades dataset is composed of 9,848 videos of daily indoors activities with an average length of 30 seconds, involving interactions with 46 objects classes in 15 types of indoor scenes and containing a vocabulary of 30 verbs leading to 157 action classes. Each video in this dataset is annotated by multiple free-text descriptions, action labels, action intervals and classes of interacting objects. 267 different users were presented with a sentence, which includes objects and actions from a fixed vocabulary, and they recorded a video acting out the sentence. In total, the dataset contains 66,500 temporal annotations for 157 action classes, 41,104 labels for 46 object classes, and 27,847 textual descriptions of the videos. In the standard split there are7,986 training video and 1,863 validation video.\", \"PwC License Name\": \"Non Commercial\", \"PwC License URL\": \"http://vuchallenge.org/license-charades.txt\", \"S2 Citation Count (June 2024)\": \"\", \"S2 Date\": \"\"}, \"Bibtex\": \"@Article{Sigurdsson2016HollywoodIH,\\n author = {Gunnar A. Sigurdsson and G\\u00fcl Varol and X. Wang and Ali Farhadi and I. Laptev and A. Gupta},\\n booktitle = {European Conference on Computer Vision},\\n pages = {510-526},\\n title = {Hollywood in Homes: Crowdsourcing Data Collection for Activity Understanding},\\n year = {2016}\\n}\\n\", \"License Use (DataProvenance)\": \"commercial\", \"License Attribution (DataProvenance)\": 1, \"License Share Alike (DataProvenance)\": 1, \"License Use (DataProvenance IgnoreOpenAI)\": \"commercial\", \"License Attribution (DataProvenance IgnoreOpenAI)\": 1, \"License Share Alike (DataProvenance IgnoreOpenAI)\": 1, \"License Type\": \"Commercial\"}, {\"Unique Dataset Identifier\": \"mars\", \"Collection\": \"mars\", \"Collection URL\": \"https://link.springer.com/content/pdf/10.1007/978-3-319-46466-4_52.pdf\", \"Dataset Name\": \"Mars\", \"Paper Title\": \"Mars\", \"Paper URL\": \"https://link.springer.com/content/pdf/10.1007/978-3-319-46466-4_52.pdf\", \"GitHub URL\": \"\", \"Hugging Face URL\": \"\", \"Papers with Code URL\": \"https://paperswithcode.com/dataset/mars\", \"ArXiv URL\": \"https://link.springer.com/content/pdf/10.1007/978-3-319-46466-4_52.pdf\", \"Semantic Scholar Corpus ID\": 2214158, \"Year Released\": \"2016\", \"Text Sources\": [\"human\"], \"Licenses\": [{\"License\": \"Unspecified\", \"License URL\": \"\"}], \"Creators\": [\"Microsoft\", \"Tsinghua University\", \"The University of Texas at San Antonio\", \"Peking University\"], \"Countries\": [\"China\"], \"License Verified By\": \"Vivek Sharma\", \"Video Hours\": 0.24, \"Taken Down\": \"False\", \"Video Sources\": \"human\", \"Task Categories\": [\"Video Segmentation\", \"Video Classification\"], \"Inferred Metadata\": {\"GitHub License\": \"\", \"GitHub Stars (June 2024)\": \"\", \"GitHub Topics\": \"\", \"Github Date\": \"\", \"HF Config\": \"\", \"HF Config License\": \"\", \"HF Dataset\": \"\", \"HF Date\": \"\", \"HF Downloads (June 2024)\": \"\", \"HF Likes (June 2024)\": \"\", \"HF Yaml License\": \"\", \"PwC Date\": \"2016-01-01\", \"PwC Description\": \"MARS (Motion Analysis and Re-identification Set) is a large scale video based person reidentification dataset, an extension of the Market-1501 dataset. It has been collected from six near-synchronized cameras. It consists of 1,261 different pedestrians, who are captured by at least 2 cameras. The variations in poses, colors and illuminations of pedestrians, as well as the poor image quality, make it very difficult to yield high matching accuracy. Moreover, the dataset contains 3,248 distractors in order to make it more realistic. Deformable Part Model and GMMCP tracker were used to automatically generate the tracklets (mostly 25-50 frames long).\", \"PwC License Name\": \"Unspecified\", \"PwC License URL\": null, \"S2 Citation Count (June 2024)\": \"\", \"S2 Date\": \"\"}, \"Bibtex\": \"@Article{Zheng2016MARSAV,\\n author = {Liang Zheng and Zhi Bie and Yifan Sun and Jingdong Wang and Chi Su and Shengjin Wang and Q. Tian},\\n booktitle = {European Conference on Computer Vision},\\n pages = {868-884},\\n title = {MARS: A Video Benchmark for Large-Scale Person Re-Identification},\\n year = {2016}\\n}\\n\", \"License Use (DataProvenance)\": \"unspecified\", \"License Attribution (DataProvenance)\": 0, \"License Share Alike (DataProvenance)\": 0, \"License Use (DataProvenance IgnoreOpenAI)\": \"unspecified\", \"License Attribution (DataProvenance IgnoreOpenAI)\": 0, \"License Share Alike (DataProvenance IgnoreOpenAI)\": 0, \"License Type\": \"Unspecified\"}, {\"Unique Dataset Identifier\": \"youtube-8m\", \"Collection\": \"youtube-8m\", \"Collection URL\": \"https://arxiv.org/abs/1609.08675\", \"Dataset Name\": \"Youtube-8M: A Large-Scale Video Classification Benchmark\", \"Paper Title\": \"Youtube-8M: A Large-Scale Video Classification Benchmark\", \"Paper URL\": \"https://arxiv.org/abs/1609.08675\", \"GitHub URL\": \"\", \"Hugging Face URL\": \"\", \"Papers with Code URL\": \"https://paperswithcode.com/dataset/youtube-8m\", \"ArXiv URL\": \"https://arxiv.org/abs/1609.08675\", \"Semantic Scholar Corpus ID\": 11241677, \"Year Released\": \"2016\", \"Text Sources\": [\"youtube\"], \"Licenses\": [{\"License\": \"Unspecified\", \"License URL\": \"\"}], \"Creators\": [\"Google Research\"], \"Countries\": [\"United States of America\"], \"License Verified By\": \"Vivek Sharma\", \"Video Hours\": 350000.0, \"Taken Down\": \"False\", \"Video Sources\": \"youtube\", \"Task Categories\": [\"Video Classification\"], \"Inferred Metadata\": {\"GitHub License\": \"\", \"GitHub Stars (June 2024)\": \"\", \"GitHub Topics\": \"\", \"Github Date\": \"\", \"HF Config\": \"\", \"HF Config License\": \"\", \"HF Dataset\": \"\", \"HF Date\": \"\", \"HF Downloads (June 2024)\": \"\", \"HF Likes (June 2024)\": \"\", \"HF Yaml License\": \"\", \"PwC Date\": \"2016-01-01\", \"PwC Description\": \"The YouTube-8M dataset is a large scale video dataset, which includes more than 7 million videos with 4716 classes labeled by the annotation system. The dataset consists of three parts: training set, validate set, and test set. In the training set, each class contains at least 100 training videos. Features of these videos are extracted by the state-of-the-art popular pre-trained models and released for public use. Each video contains audio and visual modality. Based on the visual information, videos are divided into 24 topics, such as sports, game, arts & entertainment, etc\", \"PwC License Name\": \"CC BY 4.0\", \"PwC License URL\": \"https://creativecommons.org/licenses/by/4.0/\", \"S2 Citation Count (June 2024)\": \"\", \"S2 Date\": \"\"}, \"Bibtex\": \"@Article{Abu-El-Haija2016YouTube8MAL,\\n author = {Sami Abu-El-Haija and Nisarg Kothari and Joonseok Lee and A. Natsev and G. Toderici and Balakrishnan Varadarajan and Sudheendra Vijayanarasimhan},\\n booktitle = {arXiv.org},\\n journal = {ArXiv},\\n title = {YouTube-8M: A Large-Scale Video Classification Benchmark},\\n volume = {abs/1609.08675},\\n year = {2016}\\n}\\n\", \"License Use (DataProvenance)\": \"unspecified\", \"License Attribution (DataProvenance)\": 0, \"License Share Alike (DataProvenance)\": 0, \"License Use (DataProvenance IgnoreOpenAI)\": \"unspecified\", \"License Attribution (DataProvenance IgnoreOpenAI)\": 0, \"License Share Alike (DataProvenance IgnoreOpenAI)\": 0, \"License Type\": \"Unspecified\"}, {\"Unique Dataset Identifier\": \"pku-mmd-dataset\", \"Collection\": \"pku-mmd-dataset\", \"Collection URL\": \"https://arxiv.org/abs/1703.07475\", \"Dataset Name\": \"PKU-MMD: A Large Scale Benchmark for Continuous Multi-Modal Human Action Understanding (ACM Multimedia Workshop)\", \"Paper Title\": \"PKU-MMD: A Large Scale Benchmark for Continuous Multi-Modal Human Action Understanding (ACM Multimedia Workshop)\", \"Paper URL\": \"https://arxiv.org/abs/1703.07475\", \"GitHub URL\": \"https://struct002.github.io/PKUMMD/\", \"Hugging Face URL\": \"\", \"Papers with Code URL\": \"https://paperswithcode.com/paper/pku-mmd-a-large-scale-benchmark-for\", \"ArXiv URL\": \"https://arxiv.org/abs/1703.07475\", \"Semantic Scholar Corpus ID\": 1904265, \"Year Released\": \"2017\", \"Text Sources\": [\"crowdsourced\"], \"Licenses\": [{\"License\": \"Unspecified\", \"License URL\": \"\"}], \"Creators\": [\"Microsoft Research\", \"Peking University\"], \"Countries\": [\"China\"], \"License Verified By\": \"Vivek Sharma\", \"Video Hours\": 50.0, \"Taken Down\": \"False\", \"Video Sources\": \"crowdsourced\", \"Task Categories\": [\"Video Classification\"], \"Inferred Metadata\": {\"GitHub License\": \"\", \"GitHub Stars (June 2024)\": \"\", \"GitHub Topics\": \"\", \"Github Date\": \"\", \"HF Config\": \"\", \"HF Config License\": \"\", \"HF Dataset\": \"\", \"HF Date\": \"\", \"HF Downloads (June 2024)\": \"\", \"HF Likes (June 2024)\": \"\", \"HF Yaml License\": \"\", \"PwC Date\": \"\", \"PwC Description\": \"No code available yet.\", \"PwC License Name\": \"\", \"PwC License URL\": \"\", \"S2 Citation Count (June 2024)\": \"\", \"S2 Date\": \"\"}, \"Bibtex\": \"@Article{Liu2017PKUMMDAL,\\n author = {Chunhui Liu and Yueyu Hu and Yanghao Li and Sijie Song and Jiaying Liu},\\n booktitle = {arXiv.org},\\n journal = {ArXiv},\\n title = {PKU-MMD: A Large Scale Benchmark for Continuous Multi-Modal Human Action Understanding},\\n volume = {abs/1703.07475},\\n year = {2017}\\n}\\n\", \"License Use (DataProvenance)\": \"unspecified\", \"License Attribution (DataProvenance)\": 0, \"License Share Alike (DataProvenance)\": 0, \"License Use (DataProvenance IgnoreOpenAI)\": \"unspecified\", \"License Attribution (DataProvenance IgnoreOpenAI)\": 0, \"License Share Alike (DataProvenance IgnoreOpenAI)\": 0, \"License Type\": \"Unspecified\"}, {\"Unique Dataset Identifier\": \"mpii-md\", \"Collection\": \"mpii-md\", \"Collection URL\": \"https://arxiv.org/pdf/1501.02530.pdf\", \"Dataset Name\": \"MPII-MD: A Dataset for Movie Description\", \"Paper Title\": \"MPII-MD: A Dataset for Movie Description\", \"Paper URL\": \"https://arxiv.org/pdf/1501.02530.pdf\", \"GitHub URL\": \"\", \"Hugging Face URL\": \"\", \"Papers with Code URL\": \"\", \"ArXiv URL\": \"https://arxiv.org/pdf/1501.02530.pdf\", \"Semantic Scholar Corpus ID\": 15184723, \"Year Released\": \"2017\", \"Text Sources\": [\"movies\"], \"Licenses\": [{\"License\": \"Unspecified\", \"License URL\": \"\"}], \"Creators\": [\"Max Planck Institute for Informatics\"], \"Countries\": [\"Germany\"], \"License Verified By\": \"Vivek Sharma\", \"Video Hours\": 56.5, \"Taken Down\": \"False\", \"Video Sources\": \"movies\", \"Task Categories\": [\"Video Captioning\"], \"Inferred Metadata\": {\"GitHub License\": \"\", \"GitHub Stars (June 2024)\": \"\", \"GitHub Topics\": \"\", \"Github Date\": \"\", \"HF Config\": \"\", \"HF Config License\": \"\", \"HF Dataset\": \"\", \"HF Date\": \"\", \"HF Downloads (June 2024)\": \"\", \"HF Likes (June 2024)\": \"\", \"HF Yaml License\": \"\", \"PwC Date\": \"\", \"PwC Description\": \"\", \"PwC License Name\": \"\", \"PwC License URL\": \"\", \"S2 Citation Count (June 2024)\": \"\", \"S2 Date\": \"\"}, \"Bibtex\": \"@Article{Rohrbach2015ADF,\\n author = {Anna Rohrbach and Marcus Rohrbach and Niket Tandon and B. Schiele},\\n booktitle = {Computer Vision and Pattern Recognition},\\n journal = {2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},\\n pages = {3202-3212},\\n title = {A dataset for Movie Description},\\n year = {2015}\\n}\\n\", \"License Use (DataProvenance)\": \"unspecified\", \"License Attribution (DataProvenance)\": 0, \"License Share Alike (DataProvenance)\": 0, \"License Use (DataProvenance IgnoreOpenAI)\": \"unspecified\", \"License Attribution (DataProvenance IgnoreOpenAI)\": 0, \"License Share Alike (DataProvenance IgnoreOpenAI)\": 0, \"License Type\": \"Unspecified\"}, {\"Unique Dataset Identifier\": \"kinetics-400\", \"Collection\": \"kinetics-400\", \"Collection URL\": \"https://arxiv.org/abs/1705.06950\", \"Dataset Name\": \"Kinetics 400\", \"Paper Title\": \"Kinetics 400\", \"Paper URL\": \"https://arxiv.org/abs/1705.06950\", \"GitHub URL\": \"https://github.com/cvdfoundation/kinetics-dataset\", \"Hugging Face URL\": \"\", \"Papers with Code URL\": \"https://paperswithcode.com/dataset/kinetics\", \"ArXiv URL\": \"https://arxiv.org/abs/1705.06950\", \"Semantic Scholar Corpus ID\": 27300853, \"Year Released\": \"2017\", \"Text Sources\": [\"human\"], \"Licenses\": [{\"License\": \"Unspecified\", \"License URL\": \"\"}], \"Creators\": [\"DeepMind\"], \"Countries\": [\"United Kingdom\"], \"License Verified By\": \"Vivek Sharma\", \"Video Hours\": 850.68, \"Taken Down\": \"False\", \"Video Sources\": \"human\", \"Task Categories\": [\"Video Classification\"], \"Inferred Metadata\": {\"GitHub License\": \"\", \"GitHub Stars (June 2024)\": 704, \"GitHub Topics\": [], \"Github Date\": \"\", \"HF Config\": \"\", \"HF Config License\": \"\", \"HF Dataset\": \"\", \"HF Date\": \"\", \"HF Downloads (June 2024)\": \"\", \"HF Likes (June 2024)\": \"\", \"HF Yaml License\": \"\", \"PwC Date\": \"2017-05-19\", \"PwC Description\": \"The Kinetics dataset is a large-scale, high-quality dataset for human action recognition in videos. The dataset consists of around 500,000 video clips covering 600 human action classes with at least 600 video clips for each action class. Each video clip lasts around 10 seconds and is labeled with a single action class. The videos are collected from YouTube.\", \"PwC License Name\": \"CC BY 4.0\", \"PwC License URL\": \"https://creativecommons.org/licenses/by/4.0/\", \"S2 Citation Count (June 2024)\": \"\", \"S2 Date\": \"\"}, \"Bibtex\": \"@Article{Kay2017TheKH,\\n author = {W. Kay and Jo\\u00e3o Carreira and K. Simonyan and Brian Zhang and Chloe Hillier and Sudheendra Vijayanarasimhan and Fabio Viola and Tim Green and T. Back and A. Natsev and Mustafa Suleyman and Andrew Zisserman},\\n booktitle = {arXiv.org},\\n journal = {ArXiv},\\n title = {The Kinetics Human Action Video Dataset},\\n volume = {abs/1705.06950},\\n year = {2017}\\n}\\n\", \"License Use (DataProvenance)\": \"unspecified\", \"License Attribution (DataProvenance)\": 0, \"License Share Alike (DataProvenance)\": 0, \"License Use (DataProvenance IgnoreOpenAI)\": \"unspecified\", \"License Attribution (DataProvenance IgnoreOpenAI)\": 0, \"License Share Alike (DataProvenance IgnoreOpenAI)\": 0, \"License Type\": \"Unspecified\"}, {\"Unique Dataset Identifier\": \"multi-thumos-challenge\", \"Collection\": \"multi-thumos-challenge\", \"Collection URL\": \"https://arxiv.org/pdf/1507.05738v3.pdf\", \"Dataset Name\": \"MultiTHUMOS: Every Moment Counts: Dense Detailed Labeling of Actions in Complex Videos (IJCV 2017)\", \"Paper Title\": \"MultiTHUMOS: Every Moment Counts: Dense Detailed Labeling of Actions in Complex Videos (IJCV 2017)\", \"Paper URL\": \"https://arxiv.org/pdf/1507.05738v3.pdf\", \"GitHub URL\": \"\", \"Hugging Face URL\": \"\", \"Papers with Code URL\": \"https://paperswithcode.com/dataset/multithumos\", \"ArXiv URL\": \"https://arxiv.org/pdf/1507.05738v3.pdf\", \"Semantic Scholar Corpus ID\": 3337929, \"Year Released\": \"2017\", \"Text Sources\": [\"youtube\"], \"Licenses\": [{\"License\": \"CC BY 4.0\", \"License URL\": \"https://ai.stanford.edu/~syyeung/resources/multithumos.zip\"}], \"Creators\": [\"Stanford University\", \"Carnegie Mellon University\", \"Simon Fraser University\"], \"Countries\": [\"United States of America\", \"Canada\"], \"License Verified By\": \"Vivek Sharma\", \"Video Hours\": 30.0, \"Taken Down\": \"False\", \"Video Sources\": \"youtube\", \"Task Categories\": [\"Video Classification\"], \"Inferred Metadata\": {\"GitHub License\": \"\", \"GitHub Stars (June 2024)\": \"\", \"GitHub Topics\": \"\", \"Github Date\": \"\", \"HF Config\": \"\", \"HF Config License\": \"\", \"HF Dataset\": \"\", \"HF Date\": \"\", \"HF Downloads (June 2024)\": \"\", \"HF Likes (June 2024)\": \"\", \"HF Yaml License\": \"\", \"PwC Date\": \"2018-01-01\", \"PwC Description\": \"The MultiTHUMOS dataset contains dense, multilabel, frame-level action annotations for 30 hours across 400 videos in the THUMOS'14 action detection dataset. It consists of 38,690 annotations of 65 action classes, with an average of 1.5 labels per frame and 10.5 action classes per video.\", \"PwC License Name\": \"CC BY 4.0\", \"PwC License URL\": null, \"S2 Citation Count (June 2024)\": \"\", \"S2 Date\": \"\"}, \"Bibtex\": \"@Article{Yeung2015EveryMC,\\n author = {Serena Yeung and Olga Russakovsky and Ning Jin and Mykhaylo Andriluka and Greg Mori and Li Fei-Fei},\\n booktitle = {International Journal of Computer Vision},\\n journal = {International Journal of Computer Vision},\\n pages = {375 - 389},\\n title = {Every Moment Counts: Dense Detailed Labeling of Actions in Complex Videos},\\n volume = {126},\\n year = {2015}\\n}\\n\", \"License Use (DataProvenance)\": \"commercial\", \"License Attribution (DataProvenance)\": 1, \"License Share Alike (DataProvenance)\": 0, \"License Use (DataProvenance IgnoreOpenAI)\": \"commercial\", \"License Attribution (DataProvenance IgnoreOpenAI)\": 1, \"License Share Alike (DataProvenance IgnoreOpenAI)\": 0, \"License Type\": \"Commercial\"}, {\"Unique Dataset Identifier\": \"ava\", \"Collection\": \"ava\", \"Collection URL\": \"https://arxiv.org/pdf/1705.08421\", \"Dataset Name\": \"AVA\", \"Paper Title\": \"AVA\", \"Paper URL\": \"https://arxiv.org/pdf/1705.08421\", \"GitHub URL\": \"\", \"Hugging Face URL\": \"\", \"Papers with Code URL\": \"https://paperswithcode.com/paper/ava-a-video-dataset-of-spatio-temporally\", \"ArXiv URL\": \"https://arxiv.org/pdf/1705.08421\", \"Semantic Scholar Corpus ID\": 688013, \"Year Released\": \"2017\", \"Text Sources\": [\"youtube\"], \"Licenses\": [{\"License\": \"CC BY 4.0\", \"License URL\": \"https://research.google.com/ava/\"}], \"Creators\": [\"Google Research\"], \"Countries\": [\"United States of America\"], \"License Verified By\": \"Vivek Sharma\", \"Video Hours\": 107.5, \"Taken Down\": \"False\", \"Video Sources\": \"youtube\", \"Task Categories\": [\"Temporal Localization\", \"Video Classification\"], \"Inferred Metadata\": {\"GitHub License\": \"\", \"GitHub Stars (June 2024)\": \"\", \"GitHub Topics\": \"\", \"Github Date\": \"\", \"HF Config\": \"\", \"HF Config License\": \"\", \"HF Dataset\": \"\", \"HF Date\": \"\", \"HF Downloads (June 2024)\": \"\", \"HF Likes (June 2024)\": \"\", \"HF Yaml License\": \"\", \"PwC Date\": \"\", \"PwC Description\": \"#6 best model for Action Detection on UCF101-24 (Frame-mAP 0.5 metric)\", \"PwC License Name\": \"\", \"PwC License URL\": \"\", \"S2 Citation Count (June 2024)\": \"\", \"S2 Date\": \"\"}, \"Bibtex\": \"@Article{Gu2017AVAAV,\\n author = {Chunhui Gu and Chen Sun and Sudheendra Vijayanarasimhan and C. Pantofaru and David A. Ross and G. Toderici and Yeqing Li and Susanna Ricco and R. Sukthankar and C. Schmid and J. Malik},\\n booktitle = {2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition},\\n journal = {2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition},\\n pages = {6047-6056},\\n title = {AVA: A Video Dataset of Spatio-Temporally Localized Atomic Visual Actions},\\n year = {2017}\\n}\\n\", \"License Use (DataProvenance)\": \"commercial\", \"License Attribution (DataProvenance)\": 1, \"License Share Alike (DataProvenance)\": 0, \"License Use (DataProvenance IgnoreOpenAI)\": \"commercial\", \"License Attribution (DataProvenance IgnoreOpenAI)\": 1, \"License Share Alike (DataProvenance IgnoreOpenAI)\": 0, \"License Type\": \"Commercial\"}, {\"Unique Dataset Identifier\": \"20bn-something\", \"Collection\": \"20bn-something\", \"Collection URL\": \"https://arxiv.org/abs/1706.04261\", \"Dataset Name\": \"20BN-SOMETHING-SOMETHING: The \\\"something something\\\" video database for learning and evaluating visual common sense\", \"Paper Title\": \"20BN-SOMETHING-SOMETHING: The \\\"something something\\\" video database for learning and evaluating visual common sense\", \"Paper URL\": \"https://arxiv.org/abs/1706.04261\", \"GitHub URL\": \"\", \"Hugging Face URL\": \"\", \"Papers with Code URL\": \"https://paperswithcode.com/dataset/something-something-v2\", \"ArXiv URL\": \"https://arxiv.org/abs/1706.04261\", \"Semantic Scholar Corpus ID\": 834612, \"Year Released\": \"2017\", \"Text Sources\": [\"crowdsourced\"], \"Licenses\": [{\"License\": \"Custom\", \"License URL\": \"https://developer.qualcomm.com/software/ai-datasets/something-something\"}], \"Creators\": [\"Twenty Billion Neurons GmbH\"], \"Countries\": [\"Germany\"], \"License Verified By\": \"Vivek Sharma\", \"Video Hours\": 121.46, \"Taken Down\": \"False\", \"Video Sources\": \"crowdsourced\", \"Task Categories\": [\"Video Classification\"], \"Inferred Metadata\": {\"GitHub License\": \"\", \"GitHub Stars (June 2024)\": \"\", \"GitHub Topics\": \"\", \"Github Date\": \"\", \"HF Config\": \"\", \"HF Config License\": \"\", \"HF Dataset\": \"\", \"HF Date\": \"\", \"HF Downloads (June 2024)\": \"\", \"HF Likes (June 2024)\": \"\", \"HF Yaml License\": \"\", \"PwC Date\": \"2017-01-01\", \"PwC Description\": \"The 20BN-SOMETHING-SOMETHING V2 dataset is a large collection of labeled video clips that show humans performing pre-defined basic actions with everyday objects. The dataset was created by a large number of crowd workers. It allows machine learning models to develop fine-grained understanding of basic actions that occur in the physical world. It contains 220,847 videos, with 168,913 in the training set, 24,777 in the validation set and 27,157 in the test set. There are 174 labels.\\n\\nSource\\n\\nImage Source\", \"PwC License Name\": \"Various\", \"PwC License URL\": \"https://20bn.com/licensing/datasets/academic\", \"S2 Citation Count (June 2024)\": \"\", \"S2 Date\": \"\"}, \"Bibtex\": \"@Article{Goyal2017TheS,\\n author = {Raghav Goyal and Samira Ebrahimi Kahou and Vincent Michalski and Joanna Materzynska and S. Westphal and Heuna Kim and V. Haenel and Ingo Fr\\u00fcnd and P. Yianilos and Moritz Mueller-Freitag and F. Hoppe and Christian Thurau and Ingo Bax and R. Memisevic},\\n booktitle = {IEEE International Conference on Computer Vision},\\n journal = {2017 IEEE International Conference on Computer Vision (ICCV)},\\n pages = {5843-5851},\\n title = {The \\u201cSomething Something\\u201d Video Database for Learning and Evaluating Visual Common Sense},\\n year = {2017}\\n}\\n\", \"License Use (DataProvenance)\": \"commercial\", \"License Attribution (DataProvenance)\": 1, \"License Share Alike (DataProvenance)\": 1, \"License Use (DataProvenance IgnoreOpenAI)\": \"commercial\", \"License Attribution (DataProvenance IgnoreOpenAI)\": 1, \"License Share Alike (DataProvenance IgnoreOpenAI)\": 1, \"License Type\": \"Commercial\"}, {\"Unique Dataset Identifier\": \"davis\", \"Collection\": \"davis\", \"Collection URL\": \"https://ieeexplore.ieee.org/document/7780454\", \"Dataset Name\": \"Davis\", \"Paper Title\": \"Davis\", \"Paper URL\": \"https://ieeexplore.ieee.org/document/7780454\", \"GitHub URL\": \"https://github.com/fperazzi/davis\", \"Hugging Face URL\": \"\", \"Papers with Code URL\": \"https://paperswithcode.com/paper/a-benchmark-dataset-and-evaluation\", \"ArXiv URL\": \"https://ieeexplore.ieee.org/document/7780454\", \"Semantic Scholar Corpus ID\": 3619941, \"Year Released\": \"2017\", \"Text Sources\": [\"undisclosed web\"], \"Licenses\": [{\"License\": \"Custom\", \"License URL\": \"https://github.com/fperazzi/davis/blob/main/LICENSE\"}], \"Creators\": [\"ETH Z\\u00fcrich\", \"Disney Research\"], \"Countries\": [\"Switzerland\"], \"License Verified By\": \"Vivek Sharma\", \"Video Hours\": 0.04, \"Taken Down\": \"False\", \"Video Sources\": \"undisclosed web\", \"Task Categories\": [\"Video Segmentation\", \"Video Classification\"], \"Inferred Metadata\": {\"GitHub License\": \"\", \"GitHub Stars (June 2024)\": 6, \"GitHub Topics\": [], \"Github Date\": \"\", \"HF Config\": \"\", \"HF Config License\": \"\", \"HF Dataset\": \"\", \"HF Date\": \"\", \"HF Downloads (June 2024)\": \"\", \"HF Likes (June 2024)\": \"\", \"HF Yaml License\": \"\", \"PwC Date\": \"\", \"PwC Description\": \"Implemented in one code library.\", \"PwC License Name\": \"\", \"PwC License URL\": \"\", \"S2 Citation Count (June 2024)\": \"\", \"S2 Date\": \"\"}, \"Bibtex\": \"@Article{Pont-Tuset2017The2D,\\n author = {J. Pont-Tuset and Federico Perazzi and Sergi Caelles and Pablo Arbel\\u00e1ez and A. Sorkine-Hornung and L. Gool},\\n booktitle = {arXiv.org},\\n journal = {ArXiv},\\n title = {The 2017 DAVIS Challenge on Video Object Segmentation},\\n volume = {abs/1704.00675},\\n year = {2017}\\n}\\n\", \"License Use (DataProvenance)\": \"commercial\", \"License Attribution (DataProvenance)\": 1, \"License Share Alike (DataProvenance)\": 0, \"License Use (DataProvenance IgnoreOpenAI)\": \"commercial\", \"License Attribution (DataProvenance IgnoreOpenAI)\": 1, \"License Share Alike (DataProvenance IgnoreOpenAI)\": 0, \"License Type\": \"Commercial\"}, {\"Unique Dataset Identifier\": \"qfvs\", \"Collection\": \"qfvs\", \"Collection URL\": \"https://arxiv.org/abs/1707.04960\", \"Dataset Name\": \"QFVS: Query-Focused Video Summarization: Dataset, Evaluation, and A Memory Network Based Approach (CVPR 2017)\", \"Paper Title\": \"QFVS: Query-Focused Video Summarization: Dataset, Evaluation, and A Memory Network Based Approach (CVPR 2017)\", \"Paper URL\": \"https://arxiv.org/abs/1707.04960\", \"GitHub URL\": \"\", \"Hugging Face URL\": \"\", \"Papers with Code URL\": \"https://paperswithcode.com/dataset/query-focused-video-summarization-dataset\", \"ArXiv URL\": \"https://arxiv.org/abs/1707.04960\", \"Semantic Scholar Corpus ID\": 2774608, \"Year Released\": \"2017\", \"Text Sources\": [\"crowdsourced\"], \"Licenses\": [{\"License\": \"Unspecified\", \"License URL\": \"\"}], \"Creators\": [\"University of Central Florida\", \"University of Alabama\"], \"Countries\": [\"United States of America\"], \"License Verified By\": \"Vivek Sharma\", \"Video Hours\": 20.0, \"Taken Down\": \"False\", \"Video Sources\": \"crowdsourced\", \"Task Categories\": [\"Video Summarization\"], \"Inferred Metadata\": {\"GitHub License\": \"\", \"GitHub Stars (June 2024)\": \"\", \"GitHub Topics\": \"\", \"Github Date\": \"\", \"HF Config\": \"\", \"HF Config License\": \"\", \"HF Dataset\": \"\", \"HF Date\": \"\", \"HF Downloads (June 2024)\": \"\", \"HF Likes (June 2024)\": \"\", \"HF Yaml License\": \"\", \"PwC Date\": \"\", \"PwC Description\": \"Collects dense per-video-shot concept annotations.\", \"PwC License Name\": \"Unspecified\", \"PwC License URL\": null, \"S2 Citation Count (June 2024)\": \"\", \"S2 Date\": \"\"}, \"Bibtex\": \"@Article{Sharghi2017QueryFocusedVS,\\n author = {Aidean Sharghi and Jacob S. Laurel and Boqing Gong},\\n booktitle = {Computer Vision and Pattern Recognition},\\n journal = {2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},\\n pages = {2127-2136},\\n title = {Query-Focused Video Summarization: Dataset, Evaluation, and a Memory Network Based Approach},\\n year = {2017}\\n}\\n\", \"License Use (DataProvenance)\": \"unspecified\", \"License Attribution (DataProvenance)\": 0, \"License Share Alike (DataProvenance)\": 0, \"License Use (DataProvenance IgnoreOpenAI)\": \"unspecified\", \"License Attribution (DataProvenance IgnoreOpenAI)\": 0, \"License Share Alike (DataProvenance IgnoreOpenAI)\": 0, \"License Type\": \"Unspecified\"}, {\"Unique Dataset Identifier\": \"imagenet-vid\", \"Collection\": \"imagenet-vid\", \"Collection URL\": \"https://link.springer.com/article/10.1007/s11263-015-0816-y?sa_campaign=email/event/articleAuthor/onlineFirst#\", \"Dataset Name\": \"ImageNet VID\", \"Paper Title\": \"ImageNet VID\", \"Paper URL\": \"https://link.springer.com/article/10.1007/s11263-015-0816-y?sa_campaign=email/event/articleAuthor/onlineFirst#\", \"GitHub URL\": \"\", \"Hugging Face URL\": \"\", \"Papers with Code URL\": \"https://paperswithcode.com/sota/video-object-detection-on-imagenet-vid\", \"ArXiv URL\": \"https://link.springer.com/article/10.1007/s11263-015-0816-y?sa_campaign=email/event/articleAuthor/onlineFirst#\", \"Semantic Scholar Corpus ID\": 2930547, \"Year Released\": \"2017\", \"Text Sources\": [\"undisclosed web\"], \"Licenses\": [{\"License\": \"CC BY-NC 4.0\", \"License URL\": \"https://www.image-net.org/challenges/LSVRC/2017/index.php\"}], \"Creators\": [\"Stanford University\"], \"Countries\": [\"United States of America\"], \"License Verified By\": \"Vivek Sharma\", \"Video Hours\": 9.26, \"Taken Down\": \"False\", \"Video Sources\": \"undisclosed web\", \"Task Categories\": [\"Video Classification\"], \"Inferred Metadata\": {\"GitHub License\": \"\", \"GitHub Stars (June 2024)\": \"\", \"GitHub Topics\": \"\", \"Github Date\": \"\", \"HF Config\": \"\", \"HF Config License\": \"\", \"HF Dataset\": \"\", \"HF Date\": \"\", \"HF Downloads (June 2024)\": \"\", \"HF Likes (June 2024)\": \"\", \"HF Yaml License\": \"\", \"PwC Date\": \"\", \"PwC Description\": \"The current state-of-the-art on ImageNet VID is DiffusionVID (Swin-B). See a full comparison of 31 papers with code.\", \"PwC License Name\": \"\", \"PwC License URL\": \"\", \"S2 Citation Count (June 2024)\": \"\", \"S2 Date\": \"\"}, \"Bibtex\": \"@Article{Russakovsky2014ImageNetLS,\\n author = {Olga Russakovsky and Jia Deng and Hao Su and J. Krause and S. Satheesh and Sean Ma and Zhiheng Huang and A. Karpathy and A. Khosla and Michael S. Bernstein and A. Berg and Li Fei-Fei},\\n booktitle = {International Journal of Computer Vision},\\n journal = {International Journal of Computer Vision},\\n pages = {211 - 252},\\n title = {ImageNet Large Scale Visual Recognition Challenge},\\n volume = {115},\\n year = {2014}\\n}\\n\", \"License Use (DataProvenance)\": \"non-commercial\", \"License Attribution (DataProvenance)\": 1, \"License Share Alike (DataProvenance)\": 0, \"License Use (DataProvenance IgnoreOpenAI)\": \"non-commercial\", \"License Attribution (DataProvenance IgnoreOpenAI)\": 1, \"License Share Alike (DataProvenance IgnoreOpenAI)\": 0, \"License Type\": \"Non-Commercial/Academic\"}, {\"Unique Dataset Identifier\": \"youcook-2\", \"Collection\": \"youcook-2\", \"Collection URL\": \"https://arxiv.org/abs/1703.09788\", \"Dataset Name\": \"YouCook2\", \"Paper Title\": \"YouCook2\", \"Paper URL\": \"https://arxiv.org/abs/1703.09788\", \"GitHub URL\": \"\", \"Hugging Face URL\": \"\", \"Papers with Code URL\": \"https://paperswithcode.com/dataset/youcook2\", \"ArXiv URL\": \"https://arxiv.org/abs/1703.09788\", \"Semantic Scholar Corpus ID\": 19713015, \"Year Released\": \"2017\", \"Text Sources\": [\"youtube\"], \"Licenses\": [{\"License\": \"MIT License\", \"License URL\": \"http://youcook2.eecs.umich.edu/static/YouCookII/LICENSE_YOUCOOK2.txt\"}], \"Creators\": [\"University of Rochester\", \"University of Michigan\"], \"Countries\": [\"United States of America\"], \"License Verified By\": \"Vivek Sharma\", \"Video Hours\": 175.6, \"Taken Down\": \"False\", \"Video Sources\": \"youtube\", \"Task Categories\": [\"Video Captioning\"], \"Inferred Metadata\": {\"GitHub License\": \"\", \"GitHub Stars (June 2024)\": \"\", \"GitHub Topics\": \"\", \"Github Date\": \"\", \"HF Config\": \"\", \"HF Config License\": \"\", \"HF Dataset\": \"\", \"HF Date\": \"\", \"HF Downloads (June 2024)\": \"\", \"HF Likes (June 2024)\": \"\", \"HF Yaml License\": \"\", \"PwC Date\": \"2018-01-01\", \"PwC Description\": \"YouCook2 is the largest task-oriented, instructional video dataset in the vision community. It contains 2000 long untrimmed videos from 89 cooking recipes; on average, each distinct recipe has 22 videos. The procedure steps for each video are annotated with temporal boundaries and described by imperative English sentences (see the example below). The videos were downloaded from YouTube and are all in the third-person viewpoint. All the videos are unconstrained and can be performed by individual persons at their houses with unfixed cameras. YouCook2 contains rich recipe types and various cooking styles from all over the world.\", \"PwC License Name\": \"Various\", \"PwC License URL\": \"http://youcook2.eecs.umich.edu/download\", \"S2 Citation Count (June 2024)\": \"\", \"S2 Date\": \"\"}, \"Bibtex\": \"@Article{Zhou2017TowardsAL,\\n author = {Luowei Zhou and Chenliang Xu and Jason J. Corso},\\n booktitle = {AAAI Conference on Artificial Intelligence},\\n pages = {7590-7598},\\n title = {Towards Automatic Learning of Procedures From Web Instructional Videos},\\n year = {2017}\\n}\\n\", \"License Use (DataProvenance)\": \"commercial\", \"License Attribution (DataProvenance)\": 1, \"License Share Alike (DataProvenance)\": 0, \"License Use (DataProvenance IgnoreOpenAI)\": \"commercial\", \"License Attribution (DataProvenance IgnoreOpenAI)\": 1, \"License Share Alike (DataProvenance IgnoreOpenAI)\": 0, \"License Type\": \"Commercial\"}, {\"Unique Dataset Identifier\": \"lsmdc\", \"Collection\": \"lsmdc\", \"Collection URL\": \"https://arxiv.org/pdf/1605.03705\", \"Dataset Name\": \"LSMDC\", \"Paper Title\": \"LSMDC\", \"Paper URL\": \"https://arxiv.org/pdf/1605.03705\", \"GitHub URL\": \"\", \"Hugging Face URL\": \"\", \"Papers with Code URL\": \"https://paperswithcode.com/paper/movie-description\", \"ArXiv URL\": \"https://arxiv.org/pdf/1605.03705\", \"Semantic Scholar Corpus ID\": 18217052, \"Year Released\": \"2017\", \"Text Sources\": [\"movies\"], \"Licenses\": [{\"License\": \"Custom\", \"License URL\": \"https://datasets.d2.mpi-inf.mpg.de/movieDescription/protected/lsmdc2016/README.txt\"}], \"Creators\": [\"UC Berkeley\", \"Disney Research\", \"Universite de Montreal\", \"Polytechnique Montr\\u00e9al\", \"Universit\\u00e9 de Sherbrooke\", \"Twitter\"], \"Countries\": [\"United States of America\", \"Canada\"], \"License Verified By\": \"Vivek Sharma\", \"Video Hours\": 158.0, \"Taken Down\": \"False\", \"Video Sources\": \"movies\", \"Task Categories\": [\"Video Summarization\", \"Misc\", \"Video Classification\"], \"Inferred Metadata\": {\"GitHub License\": \"\", \"GitHub Stars (June 2024)\": \"\", \"GitHub Topics\": \"\", \"Github Date\": \"\", \"HF Config\": \"\", \"HF Config License\": \"\", \"HF Dataset\": \"\", \"HF Date\": \"\", \"HF Downloads (June 2024)\": \"\", \"HF Likes (June 2024)\": \"\", \"HF Yaml License\": \"\", \"PwC Date\": \"\", \"PwC Description\": \"No code available yet.\", \"PwC License Name\": \"\", \"PwC License URL\": \"\", \"S2 Citation Count (June 2024)\": \"\", \"S2 Date\": \"\"}, \"Bibtex\": \"@Article{Rohrbach2016MovieD,\\n author = {Anna Rohrbach and Atousa Torabi and Marcus Rohrbach and Niket Tandon and C. Pal and H. Larochelle and Aaron C. Courville and B. Schiele},\\n booktitle = {International Journal of Computer Vision},\\n journal = {International Journal of Computer Vision},\\n pages = {94 - 120},\\n title = {Movie Description},\\n volume = {123},\\n year = {2016}\\n}\\n\", \"License Use (DataProvenance)\": \"academic-only\", \"License Attribution (DataProvenance)\": 1, \"License Share Alike (DataProvenance)\": 0, \"License Use (DataProvenance IgnoreOpenAI)\": \"academic-only\", \"License Attribution (DataProvenance IgnoreOpenAI)\": 1, \"License Share Alike (DataProvenance IgnoreOpenAI)\": 0, \"License Type\": \"Non-Commercial/Academic\"}, {\"Unique Dataset Identifier\": \"voxceleb\", \"Collection\": \"voxceleb\", \"Collection URL\": \"https://arxiv.org/abs/1706.08612\", \"Dataset Name\": \"VoxCeleb\", \"Paper Title\": \"VoxCeleb\", \"Paper URL\": \"https://arxiv.org/abs/1706.08612\", \"GitHub URL\": \"https://github.com/a-nagrani/VGGVox\", \"Hugging Face URL\": \"\", \"Papers with Code URL\": \"https://cs.paperswithcode.com/paper/voxceleb-a-large-scale-speaker-identification\", \"ArXiv URL\": \"https://arxiv.org/abs/1706.08612\", \"Semantic Scholar Corpus ID\": 10475843, \"Year Released\": \"2017\", \"Text Sources\": [\"youtube\"], \"Licenses\": [{\"License\": \"Custom\", \"License URL\": \"https://www.robots.ox.ac.uk/~vgg/data/voxceleb/\"}], \"Creators\": [\"University of Oxford\"], \"Countries\": [\"United Kingdom\", \"South Korea\"], \"License Verified By\": \"Vivek Sharma\", \"Video Hours\": 2000.0, \"Taken Down\": \"False\", \"Video Sources\": \"youtube\", \"Task Categories\": [\"Video Classification\"], \"Inferred Metadata\": {\"GitHub License\": \"\", \"GitHub Stars (June 2024)\": 366, \"GitHub Topics\": [], \"Github Date\": \"\", \"HF Config\": \"\", \"HF Config License\": \"\", \"HF Dataset\": \"\", \"HF Date\": \"\", \"HF Downloads (June 2024)\": \"\", \"HF Likes (June 2024)\": \"\", \"HF Yaml License\": \"\", \"PwC Date\": \"\", \"PwC Description\": \"Implemented in 8 code libraries.\", \"PwC License Name\": \"\", \"PwC License URL\": \"\", \"S2 Citation Count (June 2024)\": \"\", \"S2 Date\": \"\"}, \"Bibtex\": \"@Article{Nagrani2017VoxCelebAL,\\n author = {Arsha Nagrani and Joon Son Chung and Andrew Zisserman},\\n booktitle = {Interspeech},\\n pages = {2616-2620},\\n title = {VoxCeleb: A Large-Scale Speaker Identification Dataset},\\n year = {2017}\\n}\\n\", \"License Use (DataProvenance)\": \"academic-only\", \"License Attribution (DataProvenance)\": 1, \"License Share Alike (DataProvenance)\": 0, \"License Use (DataProvenance IgnoreOpenAI)\": \"academic-only\", \"License Attribution (DataProvenance IgnoreOpenAI)\": 1, \"License Share Alike (DataProvenance IgnoreOpenAI)\": 0, \"License Type\": \"Non-Commercial/Academic\"}, {\"Unique Dataset Identifier\": \"epic-kitchenes\", \"Collection\": \"epic-kitchenes\", \"Collection URL\": \"https://arxiv.org/abs/1804.02748\", \"Dataset Name\": \"EPIC-KITCHENS\", \"Paper Title\": \"EPIC-KITCHENS\", \"Paper URL\": \"https://arxiv.org/abs/1804.02748\", \"GitHub URL\": \"https://github.com/epic-kitchens\", \"Hugging Face URL\": \"\", \"Papers with Code URL\": \"https://paperswithcode.com/dataset/epic-kitchens-100\", \"ArXiv URL\": \"https://arxiv.org/abs/1804.02748\", \"Semantic Scholar Corpus ID\": 4710439, \"Year Released\": \"2018\", \"Text Sources\": [\"human\"], \"Licenses\": [{\"License\": \"CC BY-NC 4.0\", \"License URL\": \"https://epic-kitchens.github.io/2024\"}], \"Creators\": [\"University of Toronto\", \"University of Bristol\", \"University of Catania\"], \"Countries\": [\"United Kingdom\", \"Canada\", \"Spain\"], \"License Verified By\": \"Vivek Sharma\", \"Video Hours\": 100.0, \"Taken Down\": \"False\", \"Video Sources\": \"human\", \"Task Categories\": [\"Video Captioning\"], \"Inferred Metadata\": {\"GitHub License\": \"\", \"GitHub Stars (June 2024)\": 0, \"GitHub Topics\": [\"\"], \"Github Date\": \"\", \"HF Config\": \"\", \"HF Config License\": \"\", \"HF Dataset\": \"\", \"HF Date\": \"\", \"HF Downloads (June 2024)\": \"\", \"HF Likes (June 2024)\": \"\", \"HF Yaml License\": \"\", \"PwC Date\": \"2020-06-23\", \"PwC Description\": \"This paper introduces the pipeline to scale the largest dataset in egocentric vision EPIC-KITCHENS. The effort culminates in EPIC-KITCHENS-100, a collection of 100 hours, 20M frames, 90K actions in 700 variable-length videos, capturing long-term unscripted activities in 45 environments, using head-mounted cameras. Compared to its previous version (EPIC-KITCHENS-55), EPIC-KITCHENS-100 has been annotated using a novel pipeline that allows denser (54% more actions per minute) and more complete annotations of fine-grained actions (+128% more action segments). This collection also enables evaluating the \\\"test of time\\\" - i.e. whether models trained on data collected in 2018 can generalise to new footage collected under the same hypotheses albeit \\\"two years on\\\".\\nThe dataset is aligned with 6 challenges: action recognition (full and weak supervision), action detection, action anticipation, cross-modal retrieval (from captions), as well as unsupervised domain adaptation for action recognition. For each challenge, we define the task, provide baselines and evaluation metrics.\", \"PwC License Name\": \"CC BY-NC 4.0\", \"PwC License URL\": \"https://epic-kitchens.github.io/2021\", \"S2 Citation Count (June 2024)\": \"\", \"S2 Date\": \"\"}, \"Bibtex\": \"@Article{Damen2018ScalingEV,\\n author = {D. Damen and Hazel Doughty and G. Farinella and S. Fidler and Antonino Furnari and E. Kazakos and D. Moltisanti and Jonathan Munro and Toby Perrett and Will Price and Michael Wray},\\n booktitle = {arXiv.org},\\n journal = {ArXiv},\\n title = {Scaling Egocentric Vision: The EPIC-KITCHENS Dataset},\\n volume = {abs/1804.02748},\\n year = {2018}\\n}\\n\", \"License Use (DataProvenance)\": \"non-commercial\", \"License Attribution (DataProvenance)\": 1, \"License Share Alike (DataProvenance)\": 0, \"License Use (DataProvenance IgnoreOpenAI)\": \"non-commercial\", \"License Attribution (DataProvenance IgnoreOpenAI)\": 1, \"License Share Alike (DataProvenance IgnoreOpenAI)\": 0, \"License Type\": \"Non-Commercial/Academic\"}, {\"Unique Dataset Identifier\": \"didemo\", \"Collection\": \"didemo\", \"Collection URL\": \"https://paperswithcode.com/dataset/didemo\", \"Dataset Name\": \"DiDeMo: Localizing Moments in Video with Temporal Language (EMNLP 2018)\", \"Paper Title\": \"DiDeMo: Localizing Moments in Video with Temporal Language (EMNLP 2018)\", \"Paper URL\": \"https://paperswithcode.com/dataset/didemo\", \"GitHub URL\": \"https://github.com/LisaAnne/TemporalLanguageRelease\", \"Hugging Face URL\": \"\", \"Papers with Code URL\": \"\", \"ArXiv URL\": \"https://paperswithcode.com/dataset/didemo\", \"Semantic Scholar Corpus ID\": 52164739, \"Year Released\": \"2018\", \"Text Sources\": [\"flickr\"], \"Licenses\": [{\"License\": \"BSD 2-Clause License\", \"License URL\": \"https://github.com/LisaAnne/TemporalLanguageRelease\"}], \"Creators\": [\"UC Berkeley\"], \"Countries\": [\"United States of America\"], \"License Verified By\": \"Vivek Sharma\", \"Video Hours\": 275.0, \"Taken Down\": \"False\", \"Video Sources\": \"flickr\", \"Task Categories\": [\"Temporal Action Localization\"], \"Inferred Metadata\": {\"GitHub License\": \"\", \"GitHub Stars (June 2024)\": 40, \"GitHub Topics\": [], \"Github Date\": \"\", \"HF Config\": \"\", \"HF Config License\": \"\", \"HF Dataset\": \"\", \"HF Date\": \"\", \"HF Downloads (June 2024)\": \"\", \"HF Likes (June 2024)\": \"\", \"HF Yaml License\": \"\", \"PwC Date\": \"\", \"PwC Description\": \"\", \"PwC License Name\": \"\", \"PwC License URL\": \"\", \"S2 Citation Count (June 2024)\": \"\", \"S2 Date\": \"\"}, \"Bibtex\": \"@Article{Hendricks2018LocalizingMI,\\n author = {Lisa Anne Hendricks and Oliver Wang and Eli Shechtman and Josef Sivic and Trevor Darrell and Bryan C. Russell},\\n booktitle = {Conference on Empirical Methods in Natural Language Processing},\\n journal = {ArXiv},\\n title = {Localizing Moments in Video with Temporal Language},\\n volume = {abs/1809.01337},\\n year = {2018}\\n}\\n\", \"License Use (DataProvenance)\": \"commercial\", \"License Attribution (DataProvenance)\": 1, \"License Share Alike (DataProvenance)\": 0, \"License Use (DataProvenance IgnoreOpenAI)\": \"commercial\", \"License Attribution (DataProvenance IgnoreOpenAI)\": 1, \"License Share Alike (DataProvenance IgnoreOpenAI)\": 0, \"License Type\": \"Commercial\"}, {\"Unique Dataset Identifier\": \"how2\", \"Collection\": \"how2\", \"Collection URL\": \"https://arxiv.org/abs/1811.00347\", \"Dataset Name\": \"How2: A Large-scale Dataset for Multimodal Language Understanding (NeurIPS 2018)\", \"Paper Title\": \"How2: A Large-scale Dataset for Multimodal Language Understanding (NeurIPS 2018)\", \"Paper URL\": \"https://arxiv.org/abs/1811.00347\", \"GitHub URL\": \"https://github.com/srvk/how2-dataset\", \"Hugging Face URL\": \"\", \"Papers with Code URL\": \"\", \"ArXiv URL\": \"https://arxiv.org/abs/1811.00347\", \"Semantic Scholar Corpus ID\": 53186236, \"Year Released\": \"2018\", \"Text Sources\": [\"youtube\"], \"Licenses\": [{\"License\": \"Various\", \"License URL\": \"https://github.com/srvk/how2-dataset?tab=readme-ov-file#how2-license\"}], \"Creators\": [\"Carnegie Mellon University\"], \"Countries\": [\"United States of America\"], \"License Verified By\": \"Vivek Sharma\", \"Video Hours\": 2300.0, \"Taken Down\": \"False\", \"Video Sources\": \"youtube\", \"Task Categories\": [\"Video Captioning\"], \"Inferred Metadata\": {\"GitHub License\": \"\", \"GitHub Stars (June 2024)\": 150, \"GitHub Topics\": [\"corpus\", \"dataset\", \"how2-dataset\", \"language\", \"machine-translation\", \"multimodality\", \"speech-recognition\", \"video\"], \"Github Date\": \"\", \"HF Config\": \"\", \"HF Config License\": \"\", \"HF Dataset\": \"\", \"HF Date\": \"\", \"HF Downloads (June 2024)\": \"\", \"HF Likes (June 2024)\": \"\", \"HF Yaml License\": \"\", \"PwC Date\": \"\", \"PwC Description\": \"\", \"PwC License Name\": \"\", \"PwC License URL\": \"\", \"S2 Citation Count (June 2024)\": \"\", \"S2 Date\": \"\"}, \"Bibtex\": \"@Article{Sanabria2018How2AL,\\n author = {Ramon Sanabria and Ozan Caglayan and Shruti Palaskar and Desmond Elliott and Lo\\u00efc Barrault and Lucia Specia and Florian Metze},\\n booktitle = {Neural Information Processing Systems},\\n journal = {ArXiv},\\n title = {How2: A Large-scale Dataset for Multimodal Language Understanding},\\n volume = {abs/1811.00347},\\n year = {2018}\\n}\\n\", \"License Use (DataProvenance)\": \"academic-only\", \"License Attribution (DataProvenance)\": 1, \"License Share Alike (DataProvenance)\": 1, \"License Use (DataProvenance IgnoreOpenAI)\": \"academic-only\", \"License Attribution (DataProvenance IgnoreOpenAI)\": 1, \"License Share Alike (DataProvenance IgnoreOpenAI)\": 1, \"License Type\": \"Non-Commercial/Academic\"}, {\"Unique Dataset Identifier\": \"charades-ego\", \"Collection\": \"charades-ego\", \"Collection URL\": \"https://arxiv.org/abs/1804.09627\", \"Dataset Name\": \"Charades-Ego: Actor and Observer: Joint Modeling of First and Third-Person Videos (CVPR 2018)\", \"Paper Title\": \"Charades-Ego: Actor and Observer: Joint Modeling of First and Third-Person Videos (CVPR 2018)\", \"Paper URL\": \"https://arxiv.org/abs/1804.09627\", \"GitHub URL\": \"https://github.com/gsig/actor-observer\", \"Hugging Face URL\": \"\", \"Papers with Code URL\": \"https://paperswithcode.com/search?q_meta=&q_type=&q=Actor+and+Observer%3A+Joint+Modeling+of+First+and+Third-Person+Videos\", \"ArXiv URL\": \"https://arxiv.org/abs/1804.09627\", \"Semantic Scholar Corpus ID\": 4562167, \"Year Released\": \"2018\", \"Text Sources\": [\"human\"], \"Licenses\": [{\"License\": \"Custom\", \"License URL\": \"https://prior.allenai.org/projects/data/charades-ego/license.txt\"}], \"Creators\": [\"Allen Institute for AI\"], \"Countries\": [\"United States of America\"], \"License Verified By\": \"Vivek Sharma\", \"Video Hours\": 69.33, \"Taken Down\": \"False\", \"Video Sources\": \"human\", \"Task Categories\": [\"Video Classification\"], \"Inferred Metadata\": {\"GitHub License\": \"GNU General Public License v3.0\", \"GitHub Stars (June 2024)\": 75, \"GitHub Topics\": [], \"Github Date\": \"\", \"HF Config\": \"\", \"HF Config License\": \"\", \"HF Dataset\": \"\", \"HF Date\": \"\", \"HF Downloads (June 2024)\": \"\", \"HF Likes (June 2024)\": \"\", \"HF Yaml License\": \"\", \"PwC Date\": \"\", \"PwC Description\": \"\", \"PwC License Name\": \"\", \"PwC License URL\": \"\", \"S2 Citation Count (June 2024)\": \"\", \"S2 Date\": \"\"}, \"Bibtex\": \"@Book{Sigurdsson2018ActorAO,\\n author = {Gunnar A. Sigurdsson and A. Gupta and C. Schmid and Ali Farhadi and Alahari Karteek},\\n booktitle = {2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition},\\n journal = {2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition},\\n pages = {7396-7404},\\n title = {Actor and Observer: Joint Modeling of First and Third-Person Videos},\\n year = {2018}\\n}\\n\", \"License Use (DataProvenance)\": \"academic-only\", \"License Attribution (DataProvenance)\": 1, \"License Share Alike (DataProvenance)\": 0, \"License Use (DataProvenance IgnoreOpenAI)\": \"academic-only\", \"License Attribution (DataProvenance IgnoreOpenAI)\": 1, \"License Share Alike (DataProvenance IgnoreOpenAI)\": 0, \"License Type\": \"Non-Commercial/Academic\"}, {\"Unique Dataset Identifier\": \"soa-dataset\", \"Collection\": \"soa-dataset\", \"Collection URL\": \"https://arxiv.org/pdf/1904.11451\", \"Dataset Name\": \"Scenes-objects-actions: A multi-task, multi-label video dataset\", \"Paper Title\": \"Scenes-objects-actions: A multi-task, multi-label video dataset\", \"Paper URL\": \"https://arxiv.org/pdf/1904.11451\", \"GitHub URL\": \"\", \"Hugging Face URL\": \"\", \"Papers with Code URL\": \"https://paperswithcode.com/paper/scenes-objects-actions-a-multi-task-multi\", \"ArXiv URL\": \"https://arxiv.org/pdf/1904.11451\", \"Semantic Scholar Corpus ID\": 52968009, \"Year Released\": \"2018\", \"Text Sources\": [\"facebook\"], \"Licenses\": [{\"License\": \"Unspecified\", \"License URL\": \"\"}], \"Creators\": [\"Meta\"], \"Countries\": [\"United States of America\"], \"License Verified By\": \"Vivek Sharma\", \"Video Hours\": 1561.1, \"Taken Down\": \"False\", \"Video Sources\": \"Other\", \"Task Categories\": [\"Video Classification\"], \"Inferred Metadata\": {\"GitHub License\": \"\", \"GitHub Stars (June 2024)\": \"\", \"GitHub Topics\": \"\", \"Github Date\": \"\", \"HF Config\": \"\", \"HF Config License\": \"\", \"HF Dataset\": \"\", \"HF Date\": \"\", \"HF Downloads (June 2024)\": \"\", \"HF Likes (June 2024)\": \"\", \"HF Yaml License\": \"\", \"PwC Date\": \"\", \"PwC Description\": \"No code available yet.\", \"PwC License Name\": \"\", \"PwC License URL\": \"\", \"S2 Citation Count (June 2024)\": \"\", \"S2 Date\": \"\"}, \"Bibtex\": \"@Article{Ray2018ScenesObjectsActionsAM,\\n author = {Jamie Ray and Heng Wang and Du Tran and Yufei Wang and Matt Feiszli and L. Torresani and Manohar Paluri},\\n booktitle = {European Conference on Computer Vision},\\n pages = {660-676},\\n title = {Scenes-Objects-Actions: A Multi-task, Multi-label Video Dataset},\\n year = {2018}\\n}\\n\", \"License Use (DataProvenance)\": \"unspecified\", \"License Attribution (DataProvenance)\": 0, \"License Share Alike (DataProvenance)\": 0, \"License Use (DataProvenance IgnoreOpenAI)\": \"unspecified\", \"License Attribution (DataProvenance IgnoreOpenAI)\": 0, \"License Share Alike (DataProvenance IgnoreOpenAI)\": 0, \"License Type\": \"Unspecified\"}, {\"Unique Dataset Identifier\": \"vlog-vids\", \"Collection\": \"vlog-vids\", \"Collection URL\": \"https://arxiv.org/abs/1712.02310\", \"Dataset Name\": \"VLOG: From Lifestyle Vlogs to Everyday Interactions (CVPR 2018)\", \"Paper Title\": \"VLOG: From Lifestyle Vlogs to Everyday Interactions (CVPR 2018)\", \"Paper URL\": \"https://arxiv.org/abs/1712.02310\", \"GitHub URL\": \"\", \"Hugging Face URL\": \"\", \"Papers with Code URL\": \"https://paperswithcode.com/dataset/vlog-dataset\", \"ArXiv URL\": \"https://arxiv.org/abs/1712.02310\", \"Semantic Scholar Corpus ID\": 22264672, \"Year Released\": \"2018\", \"Text Sources\": [\"youtube\"], \"Licenses\": [{\"License\": \"Custom\", \"License URL\": \"https://web.eecs.umich.edu/~fouhey/2017/VLOG/index.html\"}], \"Creators\": [\"UC Berkeley\"], \"Countries\": [\"United States of America\"], \"License Verified By\": \"Vivek Sharma\", \"Video Hours\": 336.0, \"Taken Down\": \"False\", \"Video Sources\": \"youtube\", \"Task Categories\": [\"Video Classification\"], \"Inferred Metadata\": {\"GitHub License\": \"\", \"GitHub Stars (June 2024)\": \"\", \"GitHub Topics\": \"\", \"Github Date\": \"\", \"HF Config\": \"\", \"HF Config License\": \"\", \"HF Dataset\": \"\", \"HF Date\": \"\", \"HF Downloads (June 2024)\": \"\", \"HF Likes (June 2024)\": \"\", \"HF Yaml License\": \"\", \"PwC Date\": \"\", \"PwC Description\": \"A large collection of interaction-rich video data which are annotated and analyzed.\", \"PwC License Name\": \"Unspecified\", \"PwC License URL\": null, \"S2 Citation Count (June 2024)\": \"\", \"S2 Date\": \"\"}, \"Bibtex\": \"@Article{Fouhey2017FromLV,\\n author = {D. Fouhey and Weicheng Kuo and Alexei A. Efros and Jitendra Malik},\\n booktitle = {2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition},\\n journal = {2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition},\\n pages = {4991-5000},\\n title = {From Lifestyle Vlogs to Everyday Interactions},\\n year = {2017}\\n}\\n\", \"License Use (DataProvenance)\": \"academic-only\", \"License Attribution (DataProvenance)\": 1, \"License Share Alike (DataProvenance)\": 0, \"License Use (DataProvenance IgnoreOpenAI)\": \"academic-only\", \"License Attribution (DataProvenance IgnoreOpenAI)\": 1, \"License Share Alike (DataProvenance IgnoreOpenAI)\": 0, \"License Type\": \"Non-Commercial/Academic\"}, {\"Unique Dataset Identifier\": \"moviegraphs\", \"Collection\": \"moviegraphs\", \"Collection URL\": \"https://arxiv.org/pdf/1712.06761\", \"Dataset Name\": \"MovieGraphs\", \"Paper Title\": \"MovieGraphs\", \"Paper URL\": \"https://arxiv.org/pdf/1712.06761\", \"GitHub URL\": \"\", \"Hugging Face URL\": \"\", \"Papers with Code URL\": \"https://paperswithcode.com/dataset/moviegraphs\", \"ArXiv URL\": \"https://arxiv.org/pdf/1712.06761\", \"Semantic Scholar Corpus ID\": 4856028, \"Year Released\": \"2018\", \"Text Sources\": [\"movies\"], \"Licenses\": [{\"License\": \"Custom\", \"License URL\": \"http://moviegraphs.cs.toronto.edu/download.html\"}], \"Creators\": [\"Vector Institute for Artificial Intelligence\", \"Montreal Institute of Learning Algorithms (Mila)\", \"University of Toronto\"], \"Countries\": [\"Canada\"], \"License Verified By\": \"Vivek Sharma\", \"Video Hours\": 93.9, \"Taken Down\": \"True\", \"Video Sources\": \"movies\", \"Task Categories\": [\"Misc (video retrieval\", \"interaction understanding via ordering\", \"reason prediction)\"], \"Inferred Metadata\": {\"GitHub License\": \"\", \"GitHub Stars (June 2024)\": \"\", \"GitHub Topics\": \"\", \"Github Date\": \"\", \"HF Config\": \"\", \"HF Config License\": \"\", \"HF Dataset\": \"\", \"HF Date\": \"\", \"HF Downloads (June 2024)\": \"\", \"HF Likes (June 2024)\": \"\", \"HF Yaml License\": \"\", \"PwC Date\": \"\", \"PwC Description\": \"Provides detailed, graph-based annotations of social situations depicted in movie clips. Each graph consists of several types of nodes, to capture who is present in the clip, their emotional and physical attributes, their relationships (i.e., parent/child), and the interactions between them. Most interactions are associated with topics that provide additional details, and reasons that give motivations for actions.\", \"PwC License Name\": \"Academic Research Purposes Only\", \"PwC License URL\": \"https://docs.google.com/forms/d/e/1FAIpQLScytuCn4kRBKFPPei0t01Sfadpu8Qh5i9fFvfODWAAJGyEs7g/viewform\", \"S2 Citation Count (June 2024)\": \"\", \"S2 Date\": \"\"}, \"Bibtex\": \"@Article{Vicol2017MovieGraphsTU,\\n author = {Paul Vicol and Makarand Tapaswi and Llu\\u00eds Castrej\\u00f3n and S. Fidler},\\n booktitle = {2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition},\\n journal = {2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition},\\n pages = {8581-8590},\\n title = {MovieGraphs: Towards Understanding Human-Centric Situations from Videos},\\n year = {2017}\\n}\\n\", \"License Use (DataProvenance)\": \"academic-only\", \"License Attribution (DataProvenance)\": 1, \"License Share Alike (DataProvenance)\": 0, \"License Use (DataProvenance IgnoreOpenAI)\": \"academic-only\", \"License Attribution (DataProvenance IgnoreOpenAI)\": 1, \"License Share Alike (DataProvenance IgnoreOpenAI)\": 0, \"License Type\": \"Non-Commercial/Academic\"}, {\"Unique Dataset Identifier\": \"kinetics-600\", \"Collection\": \"kinetics-600\", \"Collection URL\": \"https://arxiv.org/abs/1808.01340\", \"Dataset Name\": \"Kinetics 600\", \"Paper Title\": \"Kinetics 600\", \"Paper URL\": \"https://arxiv.org/abs/1808.01340\", \"GitHub URL\": \"https://github.com/cvdfoundation/kinetics-dataset\", \"Hugging Face URL\": \"\", \"Papers with Code URL\": \"https://paperswithcode.com/dataset/kinetics-600\", \"ArXiv URL\": \"https://arxiv.org/abs/1808.01340\", \"Semantic Scholar Corpus ID\": 51927456, \"Year Released\": \"2018\", \"Text Sources\": [\"youtube\"], \"Licenses\": [{\"License\": \"Unspecified\", \"License URL\": \"\"}], \"Creators\": [\"DeepMind\"], \"Countries\": [\"United Kingdom\"], \"License Verified By\": \"Vivek Sharma\", \"Video Hours\": 1376.52, \"Taken Down\": \"False\", \"Video Sources\": \"youtube\", \"Task Categories\": [\"Video Classification\"], \"Inferred Metadata\": {\"GitHub License\": \"\", \"GitHub Stars (June 2024)\": 704, \"GitHub Topics\": [], \"Github Date\": \"\", \"HF Config\": \"\", \"HF Config License\": \"\", \"HF Dataset\": \"\", \"HF Date\": \"\", \"HF Downloads (June 2024)\": \"\", \"HF Likes (June 2024)\": \"\", \"HF Yaml License\": \"\", \"PwC Date\": \"2018-01-01\", \"PwC Description\": \"The Kinetics-600 is a large-scale action recognition dataset which consists of around 480K videos from 600 action categories. The 480K videos are divided into 390K, 30K, 60K for training, validation and test sets, respectively. Each video in the dataset is a 10-second clip of action moment annotated from raw YouTube video. It is an extensions of the Kinetics-400 dataset.\", \"PwC License Name\": \"CC BY 4.0\", \"PwC License URL\": \"https://creativecommons.org/licenses/by/4.0/\", \"S2 Citation Count (June 2024)\": \"\", \"S2 Date\": \"\"}, \"Bibtex\": \"@Article{Carreira2018ASN,\\n author = {Jo\\u00e3o Carreira and Eric Noland and Andras Banki-Horvath and Chloe Hillier and Andrew Zisserman},\\n booktitle = {arXiv.org},\\n journal = {ArXiv},\\n title = {A Short Note about Kinetics-600},\\n volume = {abs/1808.01340},\\n year = {2018}\\n}\\n\", \"License Use (DataProvenance)\": \"unspecified\", \"License Attribution (DataProvenance)\": 0, \"License Share Alike (DataProvenance)\": 0, \"License Use (DataProvenance IgnoreOpenAI)\": \"unspecified\", \"License Attribution (DataProvenance IgnoreOpenAI)\": 0, \"License Share Alike (DataProvenance IgnoreOpenAI)\": 0, \"License Type\": \"Unspecified\"}, {\"Unique Dataset Identifier\": \"howto100m\", \"Collection\": \"howto100m\", \"Collection URL\": \"https://arxiv.org/abs/1906.03327\", \"Dataset Name\": \"HowTo100M\", \"Paper Title\": \"HowTo100M\", \"Paper URL\": \"https://arxiv.org/abs/1906.03327\", \"GitHub URL\": \"https://github.com/antoine77340/howto100m\", \"Hugging Face URL\": \"\", \"Papers with Code URL\": \"https://paperswithcode.com/dataset/howto100m\", \"ArXiv URL\": \"https://arxiv.org/abs/1906.03327\", \"Semantic Scholar Corpus ID\": 182952863, \"Year Released\": \"2019\", \"Text Sources\": [\"youtube\"], \"Licenses\": [{\"License\": \"Unspecified\", \"License URL\": \"\"}], \"Creators\": [\"Ecole Normale Sup\\u00e9rieure\", \"Inria\", \"CIIRC\", \"Czech Technical University\"], \"Countries\": [\"France\", \"Czech Republic\"], \"License Verified By\": \"Vivek Sharma\", \"Video Hours\": 134472.0, \"Taken Down\": \"False\", \"Video Sources\": \"youtube\", \"Task Categories\": [\"Video Captioning\"], \"Inferred Metadata\": {\"GitHub License\": \"Apache License 2.0\", \"GitHub Stars (June 2024)\": 242, \"GitHub Topics\": [], \"Github Date\": \"\", \"HF Config\": \"\", \"HF Config License\": \"\", \"HF Dataset\": \"\", \"HF Date\": \"\", \"HF Downloads (June 2024)\": \"\", \"HF Likes (June 2024)\": \"\", \"HF Yaml License\": \"\", \"PwC Date\": \"2019-06-07\", \"PwC Description\": \"HowTo100M is a large-scale dataset of narrated videos with an emphasis on instructional videos where content creators teach complex tasks with an explicit intention of explaining the visual content on screen. HowTo100M features a total of:\\n\\n\\n136M video clips with captions sourced from 1.2M Youtube videos (15 years of video)\\n23k activities from domains such as cooking, hand crafting, personal care, gardening or fitness\\n\\nEach video is associated with a narration available as subtitles automatically downloaded from Youtube.\", \"PwC License Name\": \"Various\", \"PwC License URL\": \"https://www.di.ens.fr/willow/research/howto100m/\", \"S2 Citation Count (June 2024)\": \"\", \"S2 Date\": \"\"}, \"Bibtex\": \"@Article{Miech2019HowTo100MLA,\\n author = {Antoine Miech and Dimitri Zhukov and Jean-Baptiste Alayrac and Makarand Tapaswi and I. Laptev and Josef Sivic},\\n booktitle = {IEEE International Conference on Computer Vision},\\n journal = {2019 IEEE/CVF International Conference on Computer Vision (ICCV)},\\n pages = {2630-2640},\\n title = {HowTo100M: Learning a Text-Video Embedding by Watching Hundred Million Narrated Video Clips},\\n year = {2019}\\n}\\n\", \"License Use (DataProvenance)\": \"unspecified\", \"License Attribution (DataProvenance)\": 0, \"License Share Alike (DataProvenance)\": 0, \"License Use (DataProvenance IgnoreOpenAI)\": \"unspecified\", \"License Attribution (DataProvenance IgnoreOpenAI)\": 0, \"License Share Alike (DataProvenance IgnoreOpenAI)\": 0, \"License Type\": \"Unspecified\"}, {\"Unique Dataset Identifier\": \"coin-dataset\", \"Collection\": \"coin-dataset\", \"Collection URL\": \"https://arxiv.org/abs/1903.02874\", \"Dataset Name\": \"COIN: A Large-scale Dataset for Comprehensive Instructional Video Analysis (CVPR 2019)\", \"Paper Title\": \"COIN: A Large-scale Dataset for Comprehensive Instructional Video Analysis (CVPR 2019)\", \"Paper URL\": \"https://arxiv.org/abs/1903.02874\", \"GitHub URL\": \"https://github.com/coin-dataset/annotations\", \"Hugging Face URL\": \"\", \"Papers with Code URL\": \"https://paperswithcode.com/dataset/coin#:~:text=The%20COIN%20dataset%20(a%20large,are%20all%20collected%20from%20YouTube.\", \"ArXiv URL\": \"https://arxiv.org/abs/1903.02874\", \"Semantic Scholar Corpus ID\": 71147568, \"Year Released\": \"2019\", \"Text Sources\": [\"youtube\"], \"Licenses\": [{\"License\": \"Custom\", \"License URL\": \"https://github.com/coin-dataset/annotations?tab=readme-ov-file#license\"}], \"Creators\": [\"Tsinghua University\", \"Meitu Inc.\"], \"Countries\": [\"China\"], \"License Verified By\": \"Vivek Sharma\", \"Video Hours\": 476.0, \"Taken Down\": \"False\", \"Video Sources\": \"youtube\", \"Task Categories\": [\"Video Classification\"], \"Inferred Metadata\": {\"GitHub License\": \"\", \"GitHub Stars (June 2024)\": 100, \"GitHub Topics\": [], \"Github Date\": \"\", \"HF Config\": \"\", \"HF Config License\": \"\", \"HF Dataset\": \"\", \"HF Date\": \"\", \"HF Downloads (June 2024)\": \"\", \"HF Likes (June 2024)\": \"\", \"HF Yaml License\": \"\", \"PwC Date\": \"\", \"PwC Description\": \"The COIN dataset (a large-scale dataset for COmprehensive INstructional video analysis) consists of 11,827 videos related to 180 different tasks in 12 domains (e.g., vehicles, gadgets, etc.) related to our daily life. The videos are all collected from YouTube. The average length of a video is 2.36 minutes. Each video is labelled with 3.91 step segments, where each segment lasts 14.91 seconds on average. In total, the dataset contains videos of 476 hours, with 46,354 annotated segments.\", \"PwC License Name\": \"Various\", \"PwC License URL\": \"https://coin-dataset.github.io/\", \"S2 Citation Count (June 2024)\": \"\", \"S2 Date\": \"\"}, \"Bibtex\": \"@Article{Tang2019COINAL,\\n author = {Yansong Tang and Dajun Ding and Yongming Rao and Yu Zheng and Danyang Zhang and Lili Zhao and Jiwen Lu and Jie Zhou},\\n booktitle = {Computer Vision and Pattern Recognition},\\n journal = {2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},\\n pages = {1207-1216},\\n title = {COIN: A Large-Scale Dataset for Comprehensive Instructional Video Analysis},\\n year = {2019}\\n}\\n\", \"License Use (DataProvenance)\": \"academic-only\", \"License Attribution (DataProvenance)\": 1, \"License Share Alike (DataProvenance)\": 0, \"License Use (DataProvenance IgnoreOpenAI)\": \"academic-only\", \"License Attribution (DataProvenance IgnoreOpenAI)\": 1, \"License Share Alike (DataProvenance IgnoreOpenAI)\": 0, \"License Type\": \"Non-Commercial/Academic\"}, {\"Unique Dataset Identifier\": \"20BN-jester\", \"Collection\": \"20BN-jester\", \"Collection URL\": \"https://openaccess.thecvf.com/content_ICCVW_2019/papers/HANDS/Materzynska_The_Jester_Dataset_A_Large-Scale_Video_Dataset_of_Human_Gestures_ICCVW_2019_paper.pdf\", \"Dataset Name\": \"20BN-jester: The Jester Dataset: A Large-Scale Video Dataset of Human Gestures (ICCVW 2019)\", \"Paper Title\": \"20BN-jester: The Jester Dataset: A Large-Scale Video Dataset of Human Gestures (ICCVW 2019)\", \"Paper URL\": \"https://openaccess.thecvf.com/content_ICCVW_2019/papers/HANDS/Materzynska_The_Jester_Dataset_A_Large-Scale_Video_Dataset_of_Human_Gestures_ICCVW_2019_paper.pdf\", \"GitHub URL\": \"\", \"Hugging Face URL\": \"\", \"Papers with Code URL\": \"\", \"ArXiv URL\": \"https://openaccess.thecvf.com/content_ICCVW_2019/papers/HANDS/Materzynska_The_Jester_Dataset_A_Large-Scale_Video_Dataset_of_Human_Gestures_ICCVW_2019_paper.pdf\", \"Semantic Scholar Corpus ID\": 208010438, \"Year Released\": \"2019\", \"Text Sources\": [\"crowdsourced\"], \"Licenses\": [{\"License\": \"Custom\", \"License URL\": \"https://developer.qualcomm.com/software/ai-datasets/jester\"}], \"Creators\": [\"Twenty Billion Neurons GmbH\"], \"Countries\": [\"Germany\"], \"License Verified By\": \"Vivek Sharma\", \"Video Hours\": 13.0, \"Taken Down\": \"False\", \"Video Sources\": \"crowdsourced\", \"Task Categories\": [\"Video Classification\"], \"Inferred Metadata\": {\"GitHub License\": \"\", \"GitHub Stars (June 2024)\": \"\", \"GitHub Topics\": \"\", \"Github Date\": \"\", \"HF Config\": \"\", \"HF Config License\": \"\", \"HF Dataset\": \"\", \"HF Date\": \"\", \"HF Downloads (June 2024)\": \"\", \"HF Likes (June 2024)\": \"\", \"HF Yaml License\": \"\", \"PwC Date\": \"\", \"PwC Description\": \"\", \"PwC License Name\": \"\", \"PwC License URL\": \"\", \"S2 Citation Count (June 2024)\": \"\", \"S2 Date\": \"\"}, \"Bibtex\": \"@Article{Materzynska2019TheJD,\\n author = {Joanna Materzynska and Guillaume Berger and Ingo Bax and R. Memisevic},\\n booktitle = {2019 IEEE/CVF International Conference on Computer Vision Workshop (ICCVW)},\\n journal = {2019 IEEE/CVF International Conference on Computer Vision Workshop (ICCVW)},\\n pages = {2874-2882},\\n title = {The Jester Dataset: A Large-Scale Video Dataset of Human Gestures},\\n year = {2019}\\n}\\n\", \"License Use (DataProvenance)\": \"commercial\", \"License Attribution (DataProvenance)\": 1, \"License Share Alike (DataProvenance)\": 1, \"License Use (DataProvenance IgnoreOpenAI)\": \"commercial\", \"License Attribution (DataProvenance IgnoreOpenAI)\": 1, \"License Share Alike (DataProvenance IgnoreOpenAI)\": 1, \"License Type\": \"Commercial\"}, {\"Unique Dataset Identifier\": \"moments-in-time-dataset\", \"Collection\": \"moments-in-time-dataset\", \"Collection URL\": \"https://arxiv.org/pdf/1801.03150.pdf\", \"Dataset Name\": \"Moments in Time Dataset: one million videos for event understanding (TPAMI 2019)\", \"Paper Title\": \"Moments in Time Dataset: one million videos for event understanding (TPAMI 2019)\", \"Paper URL\": \"https://arxiv.org/pdf/1801.03150.pdf\", \"GitHub URL\": \"\", \"Hugging Face URL\": \"\", \"Papers with Code URL\": \"\", \"ArXiv URL\": \"https://arxiv.org/pdf/1801.03150.pdf\", \"Semantic Scholar Corpus ID\": 11868155, \"Year Released\": \"2019\", \"Text Sources\": [\"youtube\", \"flickr\", \"vine\", \"metacafe\", \"peeks\", \"vimeo\", \"videoblocks\", \"bing videos\", \"giphy\", \"weather channel\", \"getty-images\"], \"Licenses\": [{\"License\": \"Custom\", \"License URL\": \"http://moments.csail.mit.edu/\"}], \"Creators\": [\"Massachusetts Institute of Technology\"], \"Countries\": [\"United States of America\"], \"License Verified By\": \"Vivek Sharma\", \"Video Hours\": 833.0, \"Taken Down\": \"False\", \"Video Sources\": \"Other\", \"Task Categories\": [\"Video Classification\"], \"Inferred Metadata\": {\"GitHub License\": \"\", \"GitHub Stars (June 2024)\": \"\", \"GitHub Topics\": \"\", \"Github Date\": \"\", \"HF Config\": \"\", \"HF Config License\": \"\", \"HF Dataset\": \"\", \"HF Date\": \"\", \"HF Downloads (June 2024)\": \"\", \"HF Likes (June 2024)\": \"\", \"HF Yaml License\": \"\", \"PwC Date\": \"\", \"PwC Description\": \"\", \"PwC License Name\": \"\", \"PwC License URL\": \"\", \"S2 Citation Count (June 2024)\": \"\", \"S2 Date\": \"\"}, \"Bibtex\": \"@Article{Monfort2018MomentsIT,\\n author = {Mathew Monfort and Bolei Zhou and Sarah Adel Bargal and A. Andonian and Tom Yan and K. Ramakrishnan and L. Brown and Quanfu Fan and Dan Gutfreund and Carl Vondrick and A. Oliva},\\n booktitle = {IEEE Transactions on Pattern Analysis and Machine Intelligence},\\n journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},\\n pages = {502-508},\\n title = {Moments in Time Dataset: One Million Videos for Event Understanding},\\n volume = {42},\\n year = {2018}\\n}\\n\", \"License Use (DataProvenance)\": \"academic-only\", \"License Attribution (DataProvenance)\": 1, \"License Share Alike (DataProvenance)\": 0, \"License Use (DataProvenance IgnoreOpenAI)\": \"academic-only\", \"License Attribution (DataProvenance IgnoreOpenAI)\": 1, \"License Share Alike (DataProvenance IgnoreOpenAI)\": 0, \"License Type\": \"Non-Commercial/Academic\"}, {\"Unique Dataset Identifier\": \"mmact\", \"Collection\": \"mmact\", \"Collection URL\": \"https://ieeexplore.ieee.org/document/9009579\", \"Dataset Name\": \"MMAct\", \"Paper Title\": \"MMAct\", \"Paper URL\": \"https://ieeexplore.ieee.org/document/9009579\", \"GitHub URL\": \"\", \"Hugging Face URL\": \"\", \"Papers with Code URL\": \"https://paperswithcode.com/dataset/mmact\", \"ArXiv URL\": \"https://ieeexplore.ieee.org/document/9009579\", \"Semantic Scholar Corpus ID\": 207980205, \"Year Released\": \"2019\", \"Text Sources\": [\"human\"], \"Licenses\": [{\"License\": \"Custom\", \"License URL\": \"https://mmact19.github.io/2019/\"}], \"Creators\": [\"The Hong Kong University of Science and Technology\", \"Alibaba Group\"], \"Countries\": [\"Hong Kong\", \"China\"], \"License Verified By\": \"Vivek Sharma\", \"Video Hours\": 100.0, \"Taken Down\": \"False\", \"Video Sources\": \"human\", \"Task Categories\": [\"Temporal Localization\", \"Action Recognition\", \"Spatial-Temporal Action Localization\"], \"Inferred Metadata\": {\"GitHub License\": \"\", \"GitHub Stars (June 2024)\": \"\", \"GitHub Topics\": \"\", \"Github Date\": \"\", \"HF Config\": \"\", \"HF Config License\": \"\", \"HF Dataset\": \"\", \"HF Date\": \"\", \"HF Downloads (June 2024)\": \"\", \"HF Likes (June 2024)\": \"\", \"HF Yaml License\": \"\", \"PwC Date\": \"\", \"PwC Description\": \"MMAct is a large-scale dataset for multi/cross modal action understanding. This dataset has been recorded from 20 distinct subjects with seven different types of modalities: RGB videos, keypoints, acceleration, gyroscope, orientation, Wi-Fi and pressure signal. The dataset consists of more than 36k video clips for 37 action classes covering a wide range of daily life activities such as desktop-related and check-in-based ones in four different distinct scenarios.\", \"PwC License Name\": \"Unspecified\", \"PwC License URL\": null, \"S2 Citation Count (June 2024)\": \"\", \"S2 Date\": \"\"}, \"Bibtex\": \"@Article{Kong2019MMActAL,\\n author = {Quan Kong and Ziming Wu and Ziwei Deng and Martin Klinkigt and Bin Tong and Tomokazu Murakami},\\n booktitle = {IEEE International Conference on Computer Vision},\\n journal = {2019 IEEE/CVF International Conference on Computer Vision (ICCV)},\\n pages = {8657-8666},\\n title = {MMAct: A Large-Scale Dataset for Cross Modal Human Action Understanding},\\n year = {2019}\\n}\\n\", \"License Use (DataProvenance)\": \"academic-only\", \"License Attribution (DataProvenance)\": 1, \"License Share Alike (DataProvenance)\": 0, \"License Use (DataProvenance IgnoreOpenAI)\": \"academic-only\", \"License Attribution (DataProvenance IgnoreOpenAI)\": 1, \"License Share Alike (DataProvenance IgnoreOpenAI)\": 0, \"License Type\": \"Non-Commercial/Academic\"}, {\"Unique Dataset Identifier\": \"vatex\", \"Collection\": \"vatex\", \"Collection URL\": \"https://arxiv.org/abs/1904.03493\", \"Dataset Name\": \"VaTeX\", \"Paper Title\": \"VaTeX\", \"Paper URL\": \"https://arxiv.org/abs/1904.03493\", \"GitHub URL\": \"\", \"Hugging Face URL\": \"https://huggingface.co/datasets/HuggingFaceM4/vatex\", \"Papers with Code URL\": \"https://paperswithcode.com/dataset/vatex\", \"ArXiv URL\": \"https://arxiv.org/abs/1904.03493\", \"Semantic Scholar Corpus ID\": 102352148, \"Year Released\": \"2019\", \"Text Sources\": [\"youtube\"], \"Licenses\": [{\"License\": \"CC BY 4.0\", \"License URL\": \"https://eric-xw.github.io/vatex-website/index.html\"}], \"Creators\": [\"ByteDance AI Lab\", \"UC Santa Barbara\"], \"Countries\": [\"China\", \"United States of America\"], \"License Verified By\": \"Vivek Sharma\", \"Video Hours\": 114.58, \"Taken Down\": \"False\", \"Video Sources\": \"youtube\", \"Task Categories\": [\"Video Captioning\"], \"Inferred Metadata\": {\"GitHub License\": \"\", \"GitHub Stars (June 2024)\": \"\", \"GitHub Topics\": \"\", \"Github Date\": \"\", \"HF Config\": \"v1.1\", \"HF Config License\": \"\", \"HF Dataset\": \"HuggingFaceM4/vatex\", \"HF Date\": \"2022-05-13\", \"HF Downloads (June 2024)\": 112, \"HF Likes (June 2024)\": 4, \"HF Yaml License\": \"\", \"PwC Date\": \"2019-01-01\", \"PwC Description\": \"VATEX is multilingual, large, linguistically complex, and diverse dataset in terms of both video and natural language descriptions. It has two tasks for video-and-language research: (1) Multilingual Video Captioning, aimed at describing a video in various languages with a compact unified captioning model, and (2) Video-guided Machine Translation, to translate a source language description into the target language using the video information as additional spatiotemporal context.\", \"PwC License Name\": \"CC BY 4.0\", \"PwC License URL\": \"https://creativecommons.org/licenses/by/4.0/\", \"S2 Citation Count (June 2024)\": \"\", \"S2 Date\": \"\"}, \"Bibtex\": \"@Article{Wang2019VaTeXAL,\\n author = {Xin Eric Wang and Jiawei Wu and Junkun Chen and Lei Li and Yuan-fang Wang and William Yang Wang},\\n booktitle = {IEEE International Conference on Computer Vision},\\n journal = {2019 IEEE/CVF International Conference on Computer Vision (ICCV)},\\n pages = {4580-4590},\\n title = {VaTeX: A Large-Scale, High-Quality Multilingual Dataset for Video-and-Language Research},\\n year = {2019}\\n}\\n\", \"License Use (DataProvenance)\": \"commercial\", \"License Attribution (DataProvenance)\": 1, \"License Share Alike (DataProvenance)\": 0, \"License Use (DataProvenance IgnoreOpenAI)\": \"commercial\", \"License Attribution (DataProvenance IgnoreOpenAI)\": 1, \"License Share Alike (DataProvenance IgnoreOpenAI)\": 0, \"License Type\": \"Commercial\"}, {\"Unique Dataset Identifier\": \"moments-in-time-dataset\", \"Collection\": \"moments-in-time-dataset\", \"Collection URL\": \"https://arxiv.org/pdf/1801.03150.pdf\", \"Dataset Name\": \"Moments in Time Dataset: one million videos for event understanding (TPAMI 2019)\", \"Paper Title\": \"Moments in Time Dataset: one million videos for event understanding (TPAMI 2019)\", \"Paper URL\": \"https://arxiv.org/pdf/1801.03150.pdf\", \"GitHub URL\": \"\", \"Hugging Face URL\": \"\", \"Papers with Code URL\": \"\", \"ArXiv URL\": \"https://arxiv.org/pdf/1801.03150.pdf\", \"Semantic Scholar Corpus ID\": 11868155, \"Year Released\": \"2019\", \"Text Sources\": [\"youtube\", \"flickr\", \"vine\", \"metacafe\", \"peeks\", \"vimeo\", \"videoblocks\", \"bing videos\", \"giphy\", \"weather channel\", \"getty-images\"], \"Licenses\": [{\"License\": \"Custom\", \"License URL\": \"http://moments.csail.mit.edu/\"}], \"Creators\": [\"Massachusetts Institute of Technology\"], \"Countries\": [\"United States of America\"], \"License Verified By\": \"Vivek Sharma\", \"Video Hours\": 833.0, \"Taken Down\": \"False\", \"Video Sources\": \"Other\", \"Task Categories\": [\"Video Classification\"], \"Inferred Metadata\": {\"GitHub License\": \"\", \"GitHub Stars (June 2024)\": \"\", \"GitHub Topics\": \"\", \"Github Date\": \"\", \"HF Config\": \"\", \"HF Config License\": \"\", \"HF Dataset\": \"\", \"HF Date\": \"\", \"HF Downloads (June 2024)\": \"\", \"HF Likes (June 2024)\": \"\", \"HF Yaml License\": \"\", \"PwC Date\": \"\", \"PwC Description\": \"\", \"PwC License Name\": \"\", \"PwC License URL\": \"\", \"S2 Citation Count (June 2024)\": \"\", \"S2 Date\": \"\"}, \"Bibtex\": \"@Article{Monfort2018MomentsIT,\\n author = {Mathew Monfort and Bolei Zhou and Sarah Adel Bargal and A. Andonian and Tom Yan and K. Ramakrishnan and L. Brown and Quanfu Fan and Dan Gutfreund and Carl Vondrick and A. Oliva},\\n booktitle = {IEEE Transactions on Pattern Analysis and Machine Intelligence},\\n journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},\\n pages = {502-508},\\n title = {Moments in Time Dataset: One Million Videos for Event Understanding},\\n volume = {42},\\n year = {2018}\\n}\\n\", \"License Use (DataProvenance)\": \"academic-only\", \"License Attribution (DataProvenance)\": 1, \"License Share Alike (DataProvenance)\": 0, \"License Use (DataProvenance IgnoreOpenAI)\": \"academic-only\", \"License Attribution (DataProvenance IgnoreOpenAI)\": 1, \"License Share Alike (DataProvenance IgnoreOpenAI)\": 0, \"License Type\": \"Non-Commercial/Academic\"}, {\"Unique Dataset Identifier\": \"toyota-smarthome\", \"Collection\": \"toyota-smarthome\", \"Collection URL\": \"https://openaccess.thecvf.com/content_ICCV_2019/papers/Das_Toyota_Smarthome_Real-World_Activities_of_Daily_Living_ICCV_2019_paper.pdf\", \"Dataset Name\": \"Toyota Smarthome: Real-World Activities of Daily Living (ICCV 2019)\", \"Paper Title\": \"Toyota Smarthome: Real-World Activities of Daily Living (ICCV 2019)\", \"Paper URL\": \"https://openaccess.thecvf.com/content_ICCV_2019/papers/Das_Toyota_Smarthome_Real-World_Activities_of_Daily_Living_ICCV_2019_paper.pdf\", \"GitHub URL\": \"\", \"Hugging Face URL\": \"\", \"Papers with Code URL\": \"\", \"ArXiv URL\": \"https://openaccess.thecvf.com/content_ICCV_2019/papers/Das_Toyota_Smarthome_Real-World_Activities_of_Daily_Living_ICCV_2019_paper.pdf\", \"Semantic Scholar Corpus ID\": 207971208, \"Year Released\": \"2019\", \"Text Sources\": [\"human\"], \"Licenses\": [{\"License\": \"Custom\", \"License URL\": \"https://project.inria.fr/toyotasmarthome/files/2020/12/License_v2.pdf\"}], \"Creators\": [\"Toyota\"], \"Countries\": [\"France\"], \"License Verified By\": \"Vivek Sharma\", \"Video Hours\": 268.58, \"Taken Down\": \"False\", \"Video Sources\": \"human\", \"Task Categories\": [\"Video Classification\"], \"Inferred Metadata\": {\"GitHub License\": \"\", \"GitHub Stars (June 2024)\": \"\", \"GitHub Topics\": \"\", \"Github Date\": \"\", \"HF Config\": \"\", \"HF Config License\": \"\", \"HF Dataset\": \"\", \"HF Date\": \"\", \"HF Downloads (June 2024)\": \"\", \"HF Likes (June 2024)\": \"\", \"HF Yaml License\": \"\", \"PwC Date\": \"\", \"PwC Description\": \"\", \"PwC License Name\": \"\", \"PwC License URL\": \"\", \"S2 Citation Count (June 2024)\": \"\", \"S2 Date\": \"\"}, \"Bibtex\": \"@Article{Das2019ToyotaSR,\\n author = {Srijan Das and Rui Dai and Michal Koperski and Luca Minciullo and L. Garattoni and F. Br\\u00e9mond and G. Francesca},\\n booktitle = {IEEE International Conference on Computer Vision},\\n journal = {2019 IEEE/CVF International Conference on Computer Vision (ICCV)},\\n pages = {833-842},\\n title = {Toyota Smarthome: Real-World Activities of Daily Living},\\n year = {2019}\\n}\\n\", \"License Use (DataProvenance)\": \"academic-only\", \"License Attribution (DataProvenance)\": 1, \"License Share Alike (DataProvenance)\": 0, \"License Use (DataProvenance IgnoreOpenAI)\": \"academic-only\", \"License Attribution (DataProvenance IgnoreOpenAI)\": 1, \"License Share Alike (DataProvenance IgnoreOpenAI)\": 0, \"License Type\": \"Non-Commercial/Academic\"}, {\"Unique Dataset Identifier\": \"moments-in-time-dataset\", \"Collection\": \"moments-in-time-dataset\", \"Collection URL\": \"https://arxiv.org/pdf/1801.03150.pdf\", \"Dataset Name\": \"Moments in Time Dataset: one million videos for event understanding (TPAMI 2019)\", \"Paper Title\": \"Moments in Time Dataset: one million videos for event understanding (TPAMI 2019)\", \"Paper URL\": \"https://arxiv.org/pdf/1801.03150.pdf\", \"GitHub URL\": \"\", \"Hugging Face URL\": \"\", \"Papers with Code URL\": \"\", \"ArXiv URL\": \"https://arxiv.org/pdf/1801.03150.pdf\", \"Semantic Scholar Corpus ID\": 11868155, \"Year Released\": \"2019\", \"Text Sources\": [\"youtube\", \"flickr\", \"vine\", \"metacafe\", \"peeks\", \"vimeo\", \"videoblocks\", \"bing videos\", \"giphy\", \"weather channel\", \"getty-images\"], \"Licenses\": [{\"License\": \"Custom\", \"License URL\": \"http://moments.csail.mit.edu/\"}], \"Creators\": [\"Massachusetts Institute of Technology\"], \"Countries\": [\"United States of America\"], \"License Verified By\": \"Vivek Sharma\", \"Video Hours\": 833.0, \"Taken Down\": \"False\", \"Video Sources\": \"Other\", \"Task Categories\": [\"Video Classification\"], \"Inferred Metadata\": {\"GitHub License\": \"\", \"GitHub Stars (June 2024)\": \"\", \"GitHub Topics\": \"\", \"Github Date\": \"\", \"HF Config\": \"\", \"HF Config License\": \"\", \"HF Dataset\": \"\", \"HF Date\": \"\", \"HF Downloads (June 2024)\": \"\", \"HF Likes (June 2024)\": \"\", \"HF Yaml License\": \"\", \"PwC Date\": \"\", \"PwC Description\": \"\", \"PwC License Name\": \"\", \"PwC License URL\": \"\", \"S2 Citation Count (June 2024)\": \"\", \"S2 Date\": \"\"}, \"Bibtex\": \"@Article{Monfort2018MomentsIT,\\n author = {Mathew Monfort and Bolei Zhou and Sarah Adel Bargal and A. Andonian and Tom Yan and K. Ramakrishnan and L. Brown and Quanfu Fan and Dan Gutfreund and Carl Vondrick and A. Oliva},\\n booktitle = {IEEE Transactions on Pattern Analysis and Machine Intelligence},\\n journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},\\n pages = {502-508},\\n title = {Moments in Time Dataset: One Million Videos for Event Understanding},\\n volume = {42},\\n year = {2018}\\n}\\n\", \"License Use (DataProvenance)\": \"academic-only\", \"License Attribution (DataProvenance)\": 1, \"License Share Alike (DataProvenance)\": 0, \"License Use (DataProvenance IgnoreOpenAI)\": \"academic-only\", \"License Attribution (DataProvenance IgnoreOpenAI)\": 1, \"License Share Alike (DataProvenance IgnoreOpenAI)\": 0, \"License Type\": \"Non-Commercial/Academic\"}, {\"Unique Dataset Identifier\": \"trecvid\", \"Collection\": \"trecvid\", \"Collection URL\": \"https://arxiv.org/abs/2009.09984\", \"Dataset Name\": \"TRECVID\", \"Paper Title\": \"TRECVID\", \"Paper URL\": \"https://arxiv.org/abs/2009.09984\", \"GitHub URL\": \"\", \"Hugging Face URL\": \"\", \"Papers with Code URL\": \"https://paperswithcode.com/paper/trecvid-2019-an-evaluation-campaign-to/review/\", \"ArXiv URL\": \"https://arxiv.org/abs/2009.09984\", \"Semantic Scholar Corpus ID\": 212694843, \"Year Released\": \"2019\", \"Text Sources\": [\"undisclosed web\", \"bbc\"], \"Licenses\": [{\"License\": \"CC BY-NC-SA 4.0\", \"License URL\": \"https://trecvid.nist.gov/\"}], \"Creators\": [\"National Institute of Standards and Technology\"], \"Countries\": [\"United States of America\"], \"License Verified By\": \"Vivek Sharma\", \"Video Hours\": 1000.0, \"Taken Down\": \"False\", \"Video Sources\": \"undisclosed web\", \"Task Categories\": [\"Video Captioning\", \"Video Classification\"], \"Inferred Metadata\": {\"GitHub License\": \"\", \"GitHub Stars (June 2024)\": \"\", \"GitHub Topics\": \"\", \"Github Date\": \"\", \"HF Config\": \"\", \"HF Config License\": \"\", \"HF Dataset\": \"\", \"HF Date\": \"\", \"HF Downloads (June 2024)\": \"\", \"HF Likes (June 2024)\": \"\", \"HF Yaml License\": \"\", \"PwC Date\": \"\", \"PwC Description\": \"Paper tables with annotated results for TRECVID 2019: An Evaluation Campaign to Benchmark Video Activity Detection, Video Captioning and Matching, and Video Search & Retrieval\", \"PwC License Name\": \"\", \"PwC License URL\": \"\", \"S2 Citation Count (June 2024)\": \"\", \"S2 Date\": \"\"}, \"Bibtex\": \"@Article{Awad2019TRECVID2A,\\n author = {G. Awad and A. Butt and Keith Curtis and Yooyoung Lee and Jonathan G. Fiscus and A. Godil and Andrew Delgado and Jesse Zhang and Eliot Godard and Lukas L. Diduch and A. Smeaton and Yyette Graham and Wessel Kraaij and G. Qu\\u00e9not},\\n booktitle = {TREC Video Retrieval Evaluation},\\n journal = {ArXiv},\\n title = {TRECVID 2019: An evaluation campaign to benchmark Video Activity Detection, Video Captioning and Matching, and Video Search & retrieval},\\n volume = {abs/2009.09984},\\n year = {2019}\\n}\\n\", \"License Use (DataProvenance)\": \"non-commercial\", \"License Attribution (DataProvenance)\": 1, \"License Share Alike (DataProvenance)\": 1, \"License Use (DataProvenance IgnoreOpenAI)\": \"non-commercial\", \"License Attribution (DataProvenance IgnoreOpenAI)\": 1, \"License Share Alike (DataProvenance IgnoreOpenAI)\": 1, \"License Type\": \"Non-Commercial/Academic\"}, {\"Unique Dataset Identifier\": \"moments-in-time-dataset\", \"Collection\": \"moments-in-time-dataset\", \"Collection URL\": \"https://arxiv.org/pdf/1801.03150.pdf\", \"Dataset Name\": \"Moments in Time Dataset: one million videos for event understanding (TPAMI 2019)\", \"Paper Title\": \"Moments in Time Dataset: one million videos for event understanding (TPAMI 2019)\", \"Paper URL\": \"https://arxiv.org/pdf/1801.03150.pdf\", \"GitHub URL\": \"\", \"Hugging Face URL\": \"\", \"Papers with Code URL\": \"\", \"ArXiv URL\": \"https://arxiv.org/pdf/1801.03150.pdf\", \"Semantic Scholar Corpus ID\": 11868155, \"Year Released\": \"2019\", \"Text Sources\": [\"youtube\", \"flickr\", \"vine\", \"metacafe\", \"peeks\", \"vimeo\", \"videoblocks\", \"bing videos\", \"giphy\", \"weather channel\", \"getty-images\"], \"Licenses\": [{\"License\": \"Custom\", \"License URL\": \"http://moments.csail.mit.edu/\"}], \"Creators\": [\"Massachusetts Institute of Technology\"], \"Countries\": [\"United States of America\"], \"License Verified By\": \"Vivek Sharma\", \"Video Hours\": 833.0, \"Taken Down\": \"False\", \"Video Sources\": \"youtube\", \"Task Categories\": [\"Video Classification\"], \"Inferred Metadata\": {\"GitHub License\": \"\", \"GitHub Stars (June 2024)\": \"\", \"GitHub Topics\": \"\", \"Github Date\": \"\", \"HF Config\": \"\", \"HF Config License\": \"\", \"HF Dataset\": \"\", \"HF Date\": \"\", \"HF Downloads (June 2024)\": \"\", \"HF Likes (June 2024)\": \"\", \"HF Yaml License\": \"\", \"PwC Date\": \"\", \"PwC Description\": \"\", \"PwC License Name\": \"\", \"PwC License URL\": \"\", \"S2 Citation Count (June 2024)\": \"\", \"S2 Date\": \"\"}, \"Bibtex\": \"@Article{Monfort2018MomentsIT,\\n author = {Mathew Monfort and Bolei Zhou and Sarah Adel Bargal and A. Andonian and Tom Yan and K. Ramakrishnan and L. Brown and Quanfu Fan and Dan Gutfreund and Carl Vondrick and A. Oliva},\\n booktitle = {IEEE Transactions on Pattern Analysis and Machine Intelligence},\\n journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},\\n pages = {502-508},\\n title = {Moments in Time Dataset: One Million Videos for Event Understanding},\\n volume = {42},\\n year = {2018}\\n}\\n\", \"License Use (DataProvenance)\": \"academic-only\", \"License Attribution (DataProvenance)\": 1, \"License Share Alike (DataProvenance)\": 0, \"License Use (DataProvenance IgnoreOpenAI)\": \"academic-only\", \"License Attribution (DataProvenance IgnoreOpenAI)\": 1, \"License Share Alike (DataProvenance IgnoreOpenAI)\": 0, \"License Type\": \"Non-Commercial/Academic\"}, {\"Unique Dataset Identifier\": \"moments-in-time-dataset\", \"Collection\": \"moments-in-time-dataset\", \"Collection URL\": \"https://arxiv.org/pdf/1801.03150.pdf\", \"Dataset Name\": \"Moments in Time Dataset: one million videos for event understanding (TPAMI 2019)\", \"Paper Title\": \"Moments in Time Dataset: one million videos for event understanding (TPAMI 2019)\", \"Paper URL\": \"https://arxiv.org/pdf/1801.03150.pdf\", \"GitHub URL\": \"\", \"Hugging Face URL\": \"\", \"Papers with Code URL\": \"\", \"ArXiv URL\": \"https://arxiv.org/pdf/1801.03150.pdf\", \"Semantic Scholar Corpus ID\": 11868155, \"Year Released\": \"2019\", \"Text Sources\": [\"youtube\", \"flickr\", \"vine\", \"metacafe\", \"peeks\", \"vimeo\", \"videoblocks\", \"bing videos\", \"giphy\", \"weather channel\", \"getty-images\"], \"Licenses\": [{\"License\": \"Custom\", \"License URL\": \"http://moments.csail.mit.edu/\"}], \"Creators\": [\"Massachusetts Institute of Technology\"], \"Countries\": [\"United States of America\"], \"License Verified By\": \"Vivek Sharma\", \"Video Hours\": 833.0, \"Taken Down\": \"False\", \"Video Sources\": \"flickr\", \"Task Categories\": [\"Video Classification\"], \"Inferred Metadata\": {\"GitHub License\": \"\", \"GitHub Stars (June 2024)\": \"\", \"GitHub Topics\": \"\", \"Github Date\": \"\", \"HF Config\": \"\", \"HF Config License\": \"\", \"HF Dataset\": \"\", \"HF Date\": \"\", \"HF Downloads (June 2024)\": \"\", \"HF Likes (June 2024)\": \"\", \"HF Yaml License\": \"\", \"PwC Date\": \"\", \"PwC Description\": \"\", \"PwC License Name\": \"\", \"PwC License URL\": \"\", \"S2 Citation Count (June 2024)\": \"\", \"S2 Date\": \"\"}, \"Bibtex\": \"@Article{Monfort2018MomentsIT,\\n author = {Mathew Monfort and Bolei Zhou and Sarah Adel Bargal and A. Andonian and Tom Yan and K. Ramakrishnan and L. Brown and Quanfu Fan and Dan Gutfreund and Carl Vondrick and A. Oliva},\\n booktitle = {IEEE Transactions on Pattern Analysis and Machine Intelligence},\\n journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},\\n pages = {502-508},\\n title = {Moments in Time Dataset: One Million Videos for Event Understanding},\\n volume = {42},\\n year = {2018}\\n}\\n\", \"License Use (DataProvenance)\": \"academic-only\", \"License Attribution (DataProvenance)\": 1, \"License Share Alike (DataProvenance)\": 0, \"License Use (DataProvenance IgnoreOpenAI)\": \"academic-only\", \"License Attribution (DataProvenance IgnoreOpenAI)\": 1, \"License Share Alike (DataProvenance IgnoreOpenAI)\": 0, \"License Type\": \"Non-Commercial/Academic\"}, {\"Unique Dataset Identifier\": \"moments-in-time-dataset\", \"Collection\": \"moments-in-time-dataset\", \"Collection URL\": \"https://arxiv.org/pdf/1801.03150.pdf\", \"Dataset Name\": \"Moments in Time Dataset: one million videos for event understanding (TPAMI 2019)\", \"Paper Title\": \"Moments in Time Dataset: one million videos for event understanding (TPAMI 2019)\", \"Paper URL\": \"https://arxiv.org/pdf/1801.03150.pdf\", \"GitHub URL\": \"\", \"Hugging Face URL\": \"\", \"Papers with Code URL\": \"\", \"ArXiv URL\": \"https://arxiv.org/pdf/1801.03150.pdf\", \"Semantic Scholar Corpus ID\": 11868155, \"Year Released\": \"2019\", \"Text Sources\": [\"youtube\", \"flickr\", \"vine\", \"metacafe\", \"peeks\", \"vimeo\", \"videoblocks\", \"bing videos\", \"giphy\", \"weather channel\", \"getty-images\"], \"Licenses\": [{\"License\": \"Custom\", \"License URL\": \"http://moments.csail.mit.edu/\"}], \"Creators\": [\"Massachusetts Institute of Technology\"], \"Countries\": [\"United States of America\"], \"License Verified By\": \"Vivek Sharma\", \"Video Hours\": 833.0, \"Taken Down\": \"False\", \"Video Sources\": \"Other\", \"Task Categories\": [\"Video Classification\"], \"Inferred Metadata\": {\"GitHub License\": \"\", \"GitHub Stars (June 2024)\": \"\", \"GitHub Topics\": \"\", \"Github Date\": \"\", \"HF Config\": \"\", \"HF Config License\": \"\", \"HF Dataset\": \"\", \"HF Date\": \"\", \"HF Downloads (June 2024)\": \"\", \"HF Likes (June 2024)\": \"\", \"HF Yaml License\": \"\", \"PwC Date\": \"\", \"PwC Description\": \"\", \"PwC License Name\": \"\", \"PwC License URL\": \"\", \"S2 Citation Count (June 2024)\": \"\", \"S2 Date\": \"\"}, \"Bibtex\": \"@Article{Monfort2018MomentsIT,\\n author = {Mathew Monfort and Bolei Zhou and Sarah Adel Bargal and A. Andonian and Tom Yan and K. Ramakrishnan and L. Brown and Quanfu Fan and Dan Gutfreund and Carl Vondrick and A. Oliva},\\n booktitle = {IEEE Transactions on Pattern Analysis and Machine Intelligence},\\n journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},\\n pages = {502-508},\\n title = {Moments in Time Dataset: One Million Videos for Event Understanding},\\n volume = {42},\\n year = {2018}\\n}\\n\", \"License Use (DataProvenance)\": \"academic-only\", \"License Attribution (DataProvenance)\": 1, \"License Share Alike (DataProvenance)\": 0, \"License Use (DataProvenance IgnoreOpenAI)\": \"academic-only\", \"License Attribution (DataProvenance IgnoreOpenAI)\": 1, \"License Share Alike (DataProvenance IgnoreOpenAI)\": 0, \"License Type\": \"Non-Commercial/Academic\"}, {\"Unique Dataset Identifier\": \"crosstask\", \"Collection\": \"crosstask\", \"Collection URL\": \"https://arxiv.org/abs/1903.08225\", \"Dataset Name\": \"CrossTask: weakly supervised learning from instructional videos (CVPR 2019)\", \"Paper Title\": \"CrossTask: weakly supervised learning from instructional videos (CVPR 2019)\", \"Paper URL\": \"https://arxiv.org/abs/1903.08225\", \"GitHub URL\": \"https://github.com/DmZhukov/CrossTask\", \"Hugging Face URL\": \"\", \"Papers with Code URL\": \"https://paperswithcode.com/dataset/crosstask\", \"ArXiv URL\": \"https://arxiv.org/abs/1903.08225\", \"Semantic Scholar Corpus ID\": 84187266, \"Year Released\": \"2019\", \"Text Sources\": [\"youtube\"], \"Licenses\": [{\"License\": \"Unspecified\", \"License URL\": \"\"}], \"Creators\": [\"Ecole Normale Sup\\u00e9rieure\", \"Inria\", \"CIIRC\", \"Czech Technical University\", \"University of Michigan\"], \"Countries\": [\"France\", \"Turkey\", \"United States of America\", \"Czech Republic\"], \"License Verified By\": \"Vivek Sharma\", \"Video Hours\": 376.0, \"Taken Down\": \"False\", \"Video Sources\": \"youtube\", \"Task Categories\": [\"Temporal Action Localization\"], \"Inferred Metadata\": {\"GitHub License\": \"BSD 3-Clause License\", \"GitHub Stars (June 2024)\": 83, \"GitHub Topics\": [], \"Github Date\": \"\", \"HF Config\": \"\", \"HF Config License\": \"\", \"HF Dataset\": \"\", \"HF Date\": \"\", \"HF Downloads (June 2024)\": \"\", \"HF Likes (June 2024)\": \"\", \"HF Yaml License\": \"\", \"PwC Date\": \"2019-01-01\", \"PwC Description\": \"CrossTask dataset contains instructional videos, collected for 83 different tasks. For each task an ordered list of steps with manual descriptions is provided. The dataset is divided in two parts: 18 primary and 65 related tasks. Videos for the primary tasks are collected manually and provided with annotations for temporal step boundaries. Videos for the related tasks are collected automatically and don't have annotations.\", \"PwC License Name\": \"Unspecified\", \"PwC License URL\": null, \"S2 Citation Count (June 2024)\": \"\", \"S2 Date\": \"\"}, \"Bibtex\": \"@Article{Zhukov2019CrossTaskWS,\\n author = {Dimitri Zhukov and Jean-Baptiste Alayrac and R. G. Cinbis and D. Fouhey and I. Laptev and Josef Sivic},\\n booktitle = {Computer Vision and Pattern Recognition},\\n journal = {2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},\\n pages = {3532-3540},\\n title = {Cross-Task Weakly Supervised Learning From Instructional Videos},\\n year = {2019}\\n}\\n\", \"License Use (DataProvenance)\": \"unspecified\", \"License Attribution (DataProvenance)\": 0, \"License Share Alike (DataProvenance)\": 0, \"License Use (DataProvenance IgnoreOpenAI)\": \"unspecified\", \"License Attribution (DataProvenance IgnoreOpenAI)\": 0, \"License Share Alike (DataProvenance IgnoreOpenAI)\": 0, \"License Type\": \"Unspecified\"}, {\"Unique Dataset Identifier\": \"moments-in-time-dataset\", \"Collection\": \"moments-in-time-dataset\", \"Collection URL\": \"https://arxiv.org/pdf/1801.03150.pdf\", \"Dataset Name\": \"Moments in Time Dataset: one million videos for event understanding (TPAMI 2019)\", \"Paper Title\": \"Moments in Time Dataset: one million videos for event understanding (TPAMI 2019)\", \"Paper URL\": \"https://arxiv.org/pdf/1801.03150.pdf\", \"GitHub URL\": \"\", \"Hugging Face URL\": \"\", \"Papers with Code URL\": \"\", \"ArXiv URL\": \"https://arxiv.org/pdf/1801.03150.pdf\", \"Semantic Scholar Corpus ID\": 11868155, \"Year Released\": \"2019\", \"Text Sources\": [\"youtube\", \"flickr\", \"vine\", \"metacafe\", \"peeks\", \"vimeo\", \"videoblocks\", \"bing videos\", \"giphy\", \"weather channel\", \"getty-images\"], \"Licenses\": [{\"License\": \"Custom\", \"License URL\": \"http://moments.csail.mit.edu/\"}], \"Creators\": [\"Massachusetts Institute of Technology\"], \"Countries\": [\"United States of America\"], \"License Verified By\": \"Vivek Sharma\", \"Video Hours\": 833.0, \"Taken Down\": \"False\", \"Video Sources\": \"Other\", \"Task Categories\": [\"Video Classification\"], \"Inferred Metadata\": {\"GitHub License\": \"\", \"GitHub Stars (June 2024)\": \"\", \"GitHub Topics\": \"\", \"Github Date\": \"\", \"HF Config\": \"\", \"HF Config License\": \"\", \"HF Dataset\": \"\", \"HF Date\": \"\", \"HF Downloads (June 2024)\": \"\", \"HF Likes (June 2024)\": \"\", \"HF Yaml License\": \"\", \"PwC Date\": \"\", \"PwC Description\": \"\", \"PwC License Name\": \"\", \"PwC License URL\": \"\", \"S2 Citation Count (June 2024)\": \"\", \"S2 Date\": \"\"}, \"Bibtex\": \"@Article{Monfort2018MomentsIT,\\n author = {Mathew Monfort and Bolei Zhou and Sarah Adel Bargal and A. Andonian and Tom Yan and K. Ramakrishnan and L. Brown and Quanfu Fan and Dan Gutfreund and Carl Vondrick and A. Oliva},\\n booktitle = {IEEE Transactions on Pattern Analysis and Machine Intelligence},\\n journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},\\n pages = {502-508},\\n title = {Moments in Time Dataset: One Million Videos for Event Understanding},\\n volume = {42},\\n year = {2018}\\n}\\n\", \"License Use (DataProvenance)\": \"academic-only\", \"License Attribution (DataProvenance)\": 1, \"License Share Alike (DataProvenance)\": 0, \"License Use (DataProvenance IgnoreOpenAI)\": \"academic-only\", \"License Attribution (DataProvenance IgnoreOpenAI)\": 1, \"License Share Alike (DataProvenance IgnoreOpenAI)\": 0, \"License Type\": \"Non-Commercial/Academic\"}, {\"Unique Dataset Identifier\": \"moments-in-time-dataset\", \"Collection\": \"moments-in-time-dataset\", \"Collection URL\": \"https://arxiv.org/pdf/1801.03150.pdf\", \"Dataset Name\": \"Moments in Time Dataset: one million videos for event understanding (TPAMI 2019)\", \"Paper Title\": \"Moments in Time Dataset: one million videos for event understanding (TPAMI 2019)\", \"Paper URL\": \"https://arxiv.org/pdf/1801.03150.pdf\", \"GitHub URL\": \"\", \"Hugging Face URL\": \"\", \"Papers with Code URL\": \"\", \"ArXiv URL\": \"https://arxiv.org/pdf/1801.03150.pdf\", \"Semantic Scholar Corpus ID\": 11868155, \"Year Released\": \"2019\", \"Text Sources\": [\"youtube\", \"flickr\", \"vine\", \"metacafe\", \"peeks\", \"vimeo\", \"videoblocks\", \"bing videos\", \"giphy\", \"weather channel\", \"getty-images\"], \"Licenses\": [{\"License\": \"Custom\", \"License URL\": \"http://moments.csail.mit.edu/\"}], \"Creators\": [\"Massachusetts Institute of Technology\"], \"Countries\": [\"United States of America\"], \"License Verified By\": \"Vivek Sharma\", \"Video Hours\": 833.0, \"Taken Down\": \"False\", \"Video Sources\": \"Other\", \"Task Categories\": [\"Video Classification\"], \"Inferred Metadata\": {\"GitHub License\": \"\", \"GitHub Stars (June 2024)\": \"\", \"GitHub Topics\": \"\", \"Github Date\": \"\", \"HF Config\": \"\", \"HF Config License\": \"\", \"HF Dataset\": \"\", \"HF Date\": \"\", \"HF Downloads (June 2024)\": \"\", \"HF Likes (June 2024)\": \"\", \"HF Yaml License\": \"\", \"PwC Date\": \"\", \"PwC Description\": \"\", \"PwC License Name\": \"\", \"PwC License URL\": \"\", \"S2 Citation Count (June 2024)\": \"\", \"S2 Date\": \"\"}, \"Bibtex\": \"@Article{Monfort2018MomentsIT,\\n author = {Mathew Monfort and Bolei Zhou and Sarah Adel Bargal and A. Andonian and Tom Yan and K. Ramakrishnan and L. Brown and Quanfu Fan and Dan Gutfreund and Carl Vondrick and A. Oliva},\\n booktitle = {IEEE Transactions on Pattern Analysis and Machine Intelligence},\\n journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},\\n pages = {502-508},\\n title = {Moments in Time Dataset: One Million Videos for Event Understanding},\\n volume = {42},\\n year = {2018}\\n}\\n\", \"License Use (DataProvenance)\": \"academic-only\", \"License Attribution (DataProvenance)\": 1, \"License Share Alike (DataProvenance)\": 0, \"License Use (DataProvenance IgnoreOpenAI)\": \"academic-only\", \"License Attribution (DataProvenance IgnoreOpenAI)\": 1, \"License Share Alike (DataProvenance IgnoreOpenAI)\": 0, \"License Type\": \"Non-Commercial/Academic\"}, {\"Unique Dataset Identifier\": \"moments-in-time-dataset\", \"Collection\": \"moments-in-time-dataset\", \"Collection URL\": \"https://arxiv.org/pdf/1801.03150.pdf\", \"Dataset Name\": \"Moments in Time Dataset: one million videos for event understanding (TPAMI 2019)\", \"Paper Title\": \"Moments in Time Dataset: one million videos for event understanding (TPAMI 2019)\", \"Paper URL\": \"https://arxiv.org/pdf/1801.03150.pdf\", \"GitHub URL\": \"\", \"Hugging Face URL\": \"\", \"Papers with Code URL\": \"\", \"ArXiv URL\": \"https://arxiv.org/pdf/1801.03150.pdf\", \"Semantic Scholar Corpus ID\": 11868155, \"Year Released\": \"2019\", \"Text Sources\": [\"youtube\", \"flickr\", \"vine\", \"metacafe\", \"peeks\", \"vimeo\", \"videoblocks\", \"bing videos\", \"giphy\", \"weather channel\", \"getty-images\"], \"Licenses\": [{\"License\": \"Custom\", \"License URL\": \"http://moments.csail.mit.edu/\"}], \"Creators\": [\"Massachusetts Institute of Technology\"], \"Countries\": [\"United States of America\"], \"License Verified By\": \"Vivek Sharma\", \"Video Hours\": 833.0, \"Taken Down\": \"False\", \"Video Sources\": \"Other\", \"Task Categories\": [\"Video Classification\"], \"Inferred Metadata\": {\"GitHub License\": \"\", \"GitHub Stars (June 2024)\": \"\", \"GitHub Topics\": \"\", \"Github Date\": \"\", \"HF Config\": \"\", \"HF Config License\": \"\", \"HF Dataset\": \"\", \"HF Date\": \"\", \"HF Downloads (June 2024)\": \"\", \"HF Likes (June 2024)\": \"\", \"HF Yaml License\": \"\", \"PwC Date\": \"\", \"PwC Description\": \"\", \"PwC License Name\": \"\", \"PwC License URL\": \"\", \"S2 Citation Count (June 2024)\": \"\", \"S2 Date\": \"\"}, \"Bibtex\": \"@Article{Monfort2018MomentsIT,\\n author = {Mathew Monfort and Bolei Zhou and Sarah Adel Bargal and A. Andonian and Tom Yan and K. Ramakrishnan and L. Brown and Quanfu Fan and Dan Gutfreund and Carl Vondrick and A. Oliva},\\n booktitle = {IEEE Transactions on Pattern Analysis and Machine Intelligence},\\n journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},\\n pages = {502-508},\\n title = {Moments in Time Dataset: One Million Videos for Event Understanding},\\n volume = {42},\\n year = {2018}\\n}\\n\", \"License Use (DataProvenance)\": \"academic-only\", \"License Attribution (DataProvenance)\": 1, \"License Share Alike (DataProvenance)\": 0, \"License Use (DataProvenance IgnoreOpenAI)\": \"academic-only\", \"License Attribution (DataProvenance IgnoreOpenAI)\": 1, \"License Share Alike (DataProvenance IgnoreOpenAI)\": 0, \"License Type\": \"Non-Commercial/Academic\"}, {\"Unique Dataset Identifier\": \"moments-in-time-dataset\", \"Collection\": \"moments-in-time-dataset\", \"Collection URL\": \"https://arxiv.org/pdf/1801.03150.pdf\", \"Dataset Name\": \"Moments in Time Dataset: one million videos for event understanding (TPAMI 2019)\", \"Paper Title\": \"Moments in Time Dataset: one million videos for event understanding (TPAMI 2019)\", \"Paper URL\": \"https://arxiv.org/pdf/1801.03150.pdf\", \"GitHub URL\": \"\", \"Hugging Face URL\": \"\", \"Papers with Code URL\": \"\", \"ArXiv URL\": \"https://arxiv.org/pdf/1801.03150.pdf\", \"Semantic Scholar Corpus ID\": 11868155, \"Year Released\": \"2019\", \"Text Sources\": [\"youtube\", \"flickr\", \"vine\", \"metacafe\", \"peeks\", \"vimeo\", \"videoblocks\", \"bing videos\", \"giphy\", \"weather channel\", \"getty-images\"], \"Licenses\": [{\"License\": \"Custom\", \"License URL\": \"http://moments.csail.mit.edu/\"}], \"Creators\": [\"Massachusetts Institute of Technology\"], \"Countries\": [\"United States of America\"], \"License Verified By\": \"Vivek Sharma\", \"Video Hours\": 833.0, \"Taken Down\": \"False\", \"Video Sources\": \"Other\", \"Task Categories\": [\"Video Classification\"], \"Inferred Metadata\": {\"GitHub License\": \"\", \"GitHub Stars (June 2024)\": \"\", \"GitHub Topics\": \"\", \"Github Date\": \"\", \"HF Config\": \"\", \"HF Config License\": \"\", \"HF Dataset\": \"\", \"HF Date\": \"\", \"HF Downloads (June 2024)\": \"\", \"HF Likes (June 2024)\": \"\", \"HF Yaml License\": \"\", \"PwC Date\": \"\", \"PwC Description\": \"\", \"PwC License Name\": \"\", \"PwC License URL\": \"\", \"S2 Citation Count (June 2024)\": \"\", \"S2 Date\": \"\"}, \"Bibtex\": \"@Article{Monfort2018MomentsIT,\\n author = {Mathew Monfort and Bolei Zhou and Sarah Adel Bargal and A. Andonian and Tom Yan and K. Ramakrishnan and L. Brown and Quanfu Fan and Dan Gutfreund and Carl Vondrick and A. Oliva},\\n booktitle = {IEEE Transactions on Pattern Analysis and Machine Intelligence},\\n journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},\\n pages = {502-508},\\n title = {Moments in Time Dataset: One Million Videos for Event Understanding},\\n volume = {42},\\n year = {2018}\\n}\\n\", \"License Use (DataProvenance)\": \"academic-only\", \"License Attribution (DataProvenance)\": 1, \"License Share Alike (DataProvenance)\": 0, \"License Use (DataProvenance IgnoreOpenAI)\": \"academic-only\", \"License Attribution (DataProvenance IgnoreOpenAI)\": 1, \"License Share Alike (DataProvenance IgnoreOpenAI)\": 0, \"License Type\": \"Non-Commercial/Academic\"}, {\"Unique Dataset Identifier\": \"msa\", \"Collection\": \"msa\", \"Collection URL\": \"https://arxiv.org/abs/1910.11009\", \"Dataset Name\": \"MSA\", \"Paper Title\": \"MSA\", \"Paper URL\": \"https://arxiv.org/abs/1910.11009\", \"GitHub URL\": \"https://github.com/ycxioooong/MovieSynopsisAssociation\", \"Hugging Face URL\": \"\", \"Papers with Code URL\": \"https://paperswithcode.com/paper/a-graph-based-framework-to-bridge-movies-and-1\", \"ArXiv URL\": \"https://arxiv.org/abs/1910.11009\", \"Semantic Scholar Corpus ID\": 204852218, \"Year Released\": \"2019\", \"Text Sources\": [\"movies\"], \"Licenses\": [{\"License\": \"Unspecified\", \"License URL\": \"\"}], \"Creators\": [\"The Chinese University of Hong Kong\", \"UC Berkeley\"], \"Countries\": [\"China\", \"United States of America\"], \"License Verified By\": \"Vivek Sharma\", \"Video Hours\": 516.0, \"Taken Down\": \"False\", \"Video Sources\": \"movies\", \"Task Categories\": [\"Video Summarization\"], \"Inferred Metadata\": {\"GitHub License\": \"\", \"GitHub Stars (June 2024)\": 52, \"GitHub Topics\": [], \"Github Date\": \"\", \"HF Config\": \"\", \"HF Config License\": \"\", \"HF Dataset\": \"\", \"HF Date\": \"\", \"HF Downloads (June 2024)\": \"\", \"HF Likes (June 2024)\": \"\", \"HF Yaml License\": \"\", \"PwC Date\": \"\", \"PwC Description\": \"No code available yet.\", \"PwC License Name\": \"\", \"PwC License URL\": \"\", \"S2 Citation Count (June 2024)\": \"\", \"S2 Date\": \"\"}, \"Bibtex\": \"@Article{Xiong2019AGF,\\n author = {Yu Xiong and Qingqiu Huang and Lingfeng Guo and Hang Zhou and Bolei Zhou and Dahua Lin},\\n booktitle = {IEEE International Conference on Computer Vision},\\n journal = {2019 IEEE/CVF International Conference on Computer Vision (ICCV)},\\n pages = {4591-4600},\\n title = {A Graph-Based Framework to Bridge Movies and Synopses},\\n year = {2019}\\n}\\n\", \"License Use (DataProvenance)\": \"unspecified\", \"License Attribution (DataProvenance)\": 0, \"License Share Alike (DataProvenance)\": 0, \"License Use (DataProvenance IgnoreOpenAI)\": \"unspecified\", \"License Attribution (DataProvenance IgnoreOpenAI)\": 0, \"License Share Alike (DataProvenance IgnoreOpenAI)\": 0, \"License Type\": \"Unspecified\"}, {\"Unique Dataset Identifier\": \"moments-in-time-dataset\", \"Collection\": \"moments-in-time-dataset\", \"Collection URL\": \"https://arxiv.org/pdf/1801.03150.pdf\", \"Dataset Name\": \"Moments in Time Dataset: one million videos for event understanding (TPAMI 2019)\", \"Paper Title\": \"Moments in Time Dataset: one million videos for event understanding (TPAMI 2019)\", \"Paper URL\": \"https://arxiv.org/pdf/1801.03150.pdf\", \"GitHub URL\": \"\", \"Hugging Face URL\": \"\", \"Papers with Code URL\": \"\", \"ArXiv URL\": \"https://arxiv.org/pdf/1801.03150.pdf\", \"Semantic Scholar Corpus ID\": 11868155, \"Year Released\": \"2019\", \"Text Sources\": [\"youtube\", \"flickr\", \"vine\", \"metacafe\", \"peeks\", \"vimeo\", \"videoblocks\", \"bing videos\", \"giphy\", \"weather channel\", \"getty-images\"], \"Licenses\": [{\"License\": \"Custom\", \"License URL\": \"http://moments.csail.mit.edu/\"}], \"Creators\": [\"Massachusetts Institute of Technology\"], \"Countries\": [\"United States of America\"], \"License Verified By\": \"Vivek Sharma\", \"Video Hours\": 833.0, \"Taken Down\": \"False\", \"Video Sources\": \"Other\", \"Task Categories\": [\"Video Classification\"], \"Inferred Metadata\": {\"GitHub License\": \"\", \"GitHub Stars (June 2024)\": \"\", \"GitHub Topics\": \"\", \"Github Date\": \"\", \"HF Config\": \"\", \"HF Config License\": \"\", \"HF Dataset\": \"\", \"HF Date\": \"\", \"HF Downloads (June 2024)\": \"\", \"HF Likes (June 2024)\": \"\", \"HF Yaml License\": \"\", \"PwC Date\": \"\", \"PwC Description\": \"\", \"PwC License Name\": \"\", \"PwC License URL\": \"\", \"S2 Citation Count (June 2024)\": \"\", \"S2 Date\": \"\"}, \"Bibtex\": \"@Article{Monfort2018MomentsIT,\\n author = {Mathew Monfort and Bolei Zhou and Sarah Adel Bargal and A. Andonian and Tom Yan and K. Ramakrishnan and L. Brown and Quanfu Fan and Dan Gutfreund and Carl Vondrick and A. Oliva},\\n booktitle = {IEEE Transactions on Pattern Analysis and Machine Intelligence},\\n journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},\\n pages = {502-508},\\n title = {Moments in Time Dataset: One Million Videos for Event Understanding},\\n volume = {42},\\n year = {2018}\\n}\\n\", \"License Use (DataProvenance)\": \"academic-only\", \"License Attribution (DataProvenance)\": 1, \"License Share Alike (DataProvenance)\": 0, \"License Use (DataProvenance IgnoreOpenAI)\": \"academic-only\", \"License Attribution (DataProvenance IgnoreOpenAI)\": 1, \"License Share Alike (DataProvenance IgnoreOpenAI)\": 0, \"License Type\": \"Non-Commercial/Academic\"}, {\"Unique Dataset Identifier\": \"trecvid\", \"Collection\": \"trecvid\", \"Collection URL\": \"https://arxiv.org/abs/2009.09984\", \"Dataset Name\": \"TRECVID\", \"Paper Title\": \"TRECVID\", \"Paper URL\": \"https://arxiv.org/abs/2009.09984\", \"GitHub URL\": \"\", \"Hugging Face URL\": \"\", \"Papers with Code URL\": \"https://paperswithcode.com/paper/trecvid-2019-an-evaluation-campaign-to/review/\", \"ArXiv URL\": \"https://arxiv.org/abs/2009.09984\", \"Semantic Scholar Corpus ID\": 212694843, \"Year Released\": \"2019\", \"Text Sources\": [\"undisclosed web\", \"bbc\"], \"Licenses\": [{\"License\": \"CC BY-NC-SA 4.0\", \"License URL\": \"https://trecvid.nist.gov/\"}], \"Creators\": [\"National Institute of Standards and Technology\"], \"Countries\": [\"United States of America\"], \"License Verified By\": \"Vivek Sharma\", \"Video Hours\": 1000.0, \"Taken Down\": \"False\", \"Video Sources\": \"Other\", \"Task Categories\": [\"Video Captioning\", \"Video Classification\"], \"Inferred Metadata\": {\"GitHub License\": \"\", \"GitHub Stars (June 2024)\": \"\", \"GitHub Topics\": \"\", \"Github Date\": \"\", \"HF Config\": \"\", \"HF Config License\": \"\", \"HF Dataset\": \"\", \"HF Date\": \"\", \"HF Downloads (June 2024)\": \"\", \"HF Likes (June 2024)\": \"\", \"HF Yaml License\": \"\", \"PwC Date\": \"\", \"PwC Description\": \"Paper tables with annotated results for TRECVID 2019: An Evaluation Campaign to Benchmark Video Activity Detection, Video Captioning and Matching, and Video Search & Retrieval\", \"PwC License Name\": \"\", \"PwC License URL\": \"\", \"S2 Citation Count (June 2024)\": \"\", \"S2 Date\": \"\"}, \"Bibtex\": \"@Article{Awad2019TRECVID2A,\\n author = {G. Awad and A. Butt and Keith Curtis and Yooyoung Lee and Jonathan G. Fiscus and A. Godil and Andrew Delgado and Jesse Zhang and Eliot Godard and Lukas L. Diduch and A. Smeaton and Yyette Graham and Wessel Kraaij and G. Qu\\u00e9not},\\n booktitle = {TREC Video Retrieval Evaluation},\\n journal = {ArXiv},\\n title = {TRECVID 2019: An evaluation campaign to benchmark Video Activity Detection, Video Captioning and Matching, and Video Search & retrieval},\\n volume = {abs/2009.09984},\\n year = {2019}\\n}\\n\", \"License Use (DataProvenance)\": \"non-commercial\", \"License Attribution (DataProvenance)\": 1, \"License Share Alike (DataProvenance)\": 1, \"License Use (DataProvenance IgnoreOpenAI)\": \"non-commercial\", \"License Attribution (DataProvenance IgnoreOpenAI)\": 1, \"License Share Alike (DataProvenance IgnoreOpenAI)\": 1, \"License Type\": \"Non-Commercial/Academic\"}, {\"Unique Dataset Identifier\": \"ava-dataset\", \"Collection\": \"ava-dataset\", \"Collection URL\": \"https://arxiv.org/abs/1901.01342\", \"Dataset Name\": \"AVA Active Speaker\", \"Paper Title\": \"AVA Active Speaker\", \"Paper URL\": \"https://arxiv.org/abs/1901.01342\", \"GitHub URL\": \"https://github.com/cvdfoundation/ava-dataset\", \"Hugging Face URL\": \"\", \"Papers with Code URL\": \"https://paperswithcode.com/dataset/ava-activespeaker\", \"ArXiv URL\": \"https://arxiv.org/abs/1901.01342\", \"Semantic Scholar Corpus ID\": 216211909, \"Year Released\": \"2019\", \"Text Sources\": [\"Not Prohibited\"], \"Licenses\": [{\"License\": \"CC BY 4.0\", \"License URL\": \"https://research.google.com/ava/download.html#ava_active_speaker_download\"}], \"Creators\": [\"Google Research\"], \"Countries\": [\"United States of America\"], \"License Verified By\": \"Vivek Sharma\", \"Video Hours\": 38.5, \"Taken Down\": \"False\", \"Video Sources\": \"Other\", \"Task Categories\": [\"Video Classification\"], \"Inferred Metadata\": {\"GitHub License\": \"\", \"GitHub Stars (June 2024)\": 305, \"GitHub Topics\": [], \"Github Date\": \"\", \"HF Config\": \"\", \"HF Config License\": \"\", \"HF Dataset\": \"\", \"HF Date\": \"\", \"HF Downloads (June 2024)\": \"\", \"HF Likes (June 2024)\": \"\", \"HF Yaml License\": \"\", \"PwC Date\": \"\", \"PwC Description\": \"Contains temporally labeled face tracks in video, where each face instance is labeled as speaking or not, and whether the speech is audible. This dataset contains about 3.65 million human labeled frames or about 38.5 hours of face tracks, and the corresponding audio.\", \"PwC License Name\": \"Unspecified\", \"PwC License URL\": null, \"S2 Citation Count (June 2024)\": \"\", \"S2 Date\": \"\"}, \"Bibtex\": \"@Article{Roth2019AvaAS,\\n author = {Joseph Roth and Sourish Chaudhuri and Ondrej Klejch and Radhika Marvin and Andrew C. Gallagher and Liat Kaver and S. Ramaswamy and Arkadiusz Stopczynski and C. Schmid and Zhonghua Xi and C. Pantofaru},\\n booktitle = {IEEE International Conference on Acoustics, Speech, and Signal Processing},\\n journal = {ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},\\n pages = {4492-4496},\\n title = {Ava Active Speaker: An Audio-Visual Dataset for Active Speaker Detection},\\n year = {2019}\\n}\\n\", \"License Use (DataProvenance)\": \"commercial\", \"License Attribution (DataProvenance)\": 1, \"License Share Alike (DataProvenance)\": 0, \"License Use (DataProvenance IgnoreOpenAI)\": \"commercial\", \"License Attribution (DataProvenance IgnoreOpenAI)\": 1, \"License Share Alike (DataProvenance IgnoreOpenAI)\": 0, \"License Type\": \"Commercial\"}, {\"Unique Dataset Identifier\": \"hacs-dataset\", \"Collection\": \"hacs-dataset\", \"Collection URL\": \"https://arxiv.org/abs/1712.09374\", \"Dataset Name\": \"HACS: Human Action Clips and Segments Dataset for Recognition and Temporal Localization\", \"Paper Title\": \"HACS: Human Action Clips and Segments Dataset for Recognition and Temporal Localization\", \"Paper URL\": \"https://arxiv.org/abs/1712.09374\", \"GitHub URL\": \"https://github.com/hangzhaomit/HACS-dataset\", \"Hugging Face URL\": \"\", \"Papers with Code URL\": \"https://paperswithcode.com/dataset/hacs\", \"ArXiv URL\": \"https://arxiv.org/abs/1712.09374\", \"Semantic Scholar Corpus ID\": 68049510, \"Year Released\": \"2019\", \"Text Sources\": [\"youtube\"], \"Licenses\": [{\"License\": \"Custom\", \"License URL\": \"https://github.com/hangzhaomit/HACS-dataset?tab=readme-ov-file#request-testing-videos-and-missing-videos-new\"}], \"Creators\": [\"Massachusetts Institute of Technology\", \"Dartmouth University\", \"UIUC\"], \"Countries\": [\"United States of America\"], \"License Verified By\": \"Vivek Sharma\", \"Video Hours\": 833.0, \"Taken Down\": \"False\", \"Video Sources\": \"youtube\", \"Task Categories\": [\"Video Classification\"], \"Inferred Metadata\": {\"GitHub License\": \"BSD 3-Clause License\", \"GitHub Stars (June 2024)\": 184, \"GitHub Topics\": [], \"Github Date\": \"\", \"HF Config\": \"\", \"HF Config License\": \"\", \"HF Dataset\": \"\", \"HF Date\": \"\", \"HF Downloads (June 2024)\": \"\", \"HF Likes (June 2024)\": \"\", \"HF Yaml License\": \"\", \"PwC Date\": \"\", \"PwC Description\": \"HACS is a dataset for human action recognition. It uses a taxonomy of 200 action classes, which is identical to that of the ActivityNet-v1.3 dataset. It has 504K videos retrieved from YouTube. Each one is strictly shorter than 4 minutes, and the average length is 2.6 minutes. A total of 1.5M clips of 2-second duration are sparsely sampled by methods based on both uniform randomness and consensus/disagreement of image classifiers. 0.6M and 0.9M clips are annotated as positive and negative samples, respectively.\\n\\nAuthors split the collection into training, validation and testing sets of size 1.4M, 50K and 50K clips, which are sampled\\nfrom 492K, 6K and 6K videos, respectively.\", \"PwC License Name\": \"BSD 3-Clause License\", \"PwC License URL\": \"https://github.com/hangzhaomit/HACS-dataset/blob/master/LICENSE\", \"S2 Citation Count (June 2024)\": \"\", \"S2 Date\": \"\"}, \"Bibtex\": \"@Article{Zhao2017HACSHA,\\n author = {Hang Zhao and A. Torralba and L. Torresani and Zhicheng Yan},\\n booktitle = {IEEE International Conference on Computer Vision},\\n journal = {2019 IEEE/CVF International Conference on Computer Vision (ICCV)},\\n pages = {8667-8677},\\n title = {HACS: Human Action Clips and Segments Dataset for Recognition and Temporal Localization},\\n year = {2017}\\n}\\n\", \"License Use (DataProvenance)\": \"academic-only\", \"License Attribution (DataProvenance)\": 1, \"License Share Alike (DataProvenance)\": 0, \"License Use (DataProvenance IgnoreOpenAI)\": \"academic-only\", \"License Attribution (DataProvenance IgnoreOpenAI)\": 1, \"License Share Alike (DataProvenance IgnoreOpenAI)\": 0, \"License Type\": \"Non-Commercial/Academic\"}, {\"Unique Dataset Identifier\": \"lsmdc-ordering\", \"Collection\": \"lsmdc-ordering\", \"Collection URL\": \"https://arxiv.org/pdf/2004.02205\", \"Dataset Name\": \"LSMDC Ordering\", \"Paper Title\": \"LSMDC Ordering\", \"Paper URL\": \"https://arxiv.org/pdf/2004.02205\", \"GitHub URL\": \"https://github.com/vivoutlaw/TCBP\", \"Hugging Face URL\": \"\", \"Papers with Code URL\": \"https://paperswithcode.com/paper/deep-multimodal-feature-encoding-for-video\", \"ArXiv URL\": \"https://arxiv.org/pdf/2004.02205\", \"Semantic Scholar Corpus ID\": 214802821, \"Year Released\": \"2020\", \"Text Sources\": [\"movies\"], \"Licenses\": [{\"License\": \"MIT License\", \"License URL\": \"https://github.com/vivoutlaw/tcbp/blob/master/LICENSE\"}], \"Creators\": [\"University of Toronto\", \"Karlsruhe Institute of Technology\", \"Inria\", \"Massachusetts Institute of Technology\"], \"Countries\": [\"Germany\", \"Canada\", \"United States of America\", \"France\"], \"License Verified By\": \"Vivek Sharma\", \"Video Hours\": 158.0, \"Taken Down\": \"False\", \"Video Sources\": \"movies\", \"Task Categories\": [\"Video Classification\"], \"Inferred Metadata\": {\"GitHub License\": \"MIT License\", \"GitHub Stars (June 2024)\": 11, \"GitHub Topics\": [], \"Github Date\": \"\", \"HF Config\": \"\", \"HF Config License\": \"\", \"HF Dataset\": \"\", \"HF Date\": \"\", \"HF Downloads (June 2024)\": \"\", \"HF Likes (June 2024)\": \"\", \"HF Yaml License\": \"\", \"PwC Date\": \"\", \"PwC Description\": \"1 code implementation in PyTorch. True understanding of videos comes from a joint analysis of all its modalities: the video frames, the audio track, and any accompanying text such as closed captions. We present a way to learn a compact multimodal feature representation that encodes all these modalities. Our model parameters are learned through a proxy task of inferring the temporal ordering of a set of unordered videos in a timeline. To this end, we create a new multimodal dataset for temporal ordering that consists of approximately 30K scenes (2-6 clips per scene) based on the \\\"Large Scale Movie Description Challenge\\\". We analyze and evaluate the individual and joint modalities on three challenging tasks: (i) inferring the temporal ordering of a set of videos; and (ii) action recognition. We demonstrate empirically that multimodal representations are indeed complementary, and can play a key role in improving the performance of many applications.\", \"PwC License Name\": \"\", \"PwC License URL\": \"\", \"S2 Citation Count (June 2024)\": \"\", \"S2 Date\": \"\"}, \"Bibtex\": \"@Article{Sharma2020DeepMF,\\n author = {Vivek Sharma and Makarand Tapaswi and R. Stiefelhagen},\\n booktitle = {arXiv.org},\\n journal = {ArXiv},\\n title = {Deep Multimodal Feature Encoding for Video Ordering},\\n volume = {abs/2004.02205},\\n year = {2020}\\n}\\n\", \"License Use (DataProvenance)\": \"commercial\", \"License Attribution (DataProvenance)\": 1, \"License Share Alike (DataProvenance)\": 0, \"License Use (DataProvenance IgnoreOpenAI)\": \"commercial\", \"License Attribution (DataProvenance IgnoreOpenAI)\": 1, \"License Share Alike (DataProvenance IgnoreOpenAI)\": 0, \"License Type\": \"Commercial\"}, {\"Unique Dataset Identifier\": \"oops-dataset\", \"Collection\": \"oops-dataset\", \"Collection URL\": \"https://openaccess.thecvf.com/content_CVPR_2020/papers/Epstein_Oops_Predicting_Unintentional_Action_in_Video_CVPR_2020_paper.pdf\", \"Dataset Name\": \"Oops!: Predicting Unintentional Action in Video (CVPR 2020)\", \"Paper Title\": \"Oops!: Predicting Unintentional Action in Video (CVPR 2020)\", \"Paper URL\": \"https://openaccess.thecvf.com/content_CVPR_2020/papers/Epstein_Oops_Predicting_Unintentional_Action_in_Video_CVPR_2020_paper.pdf\", \"GitHub URL\": \"https://github.com/cvlab-columbia/oops\", \"Hugging Face URL\": \"\", \"Papers with Code URL\": \"https://paperswithcode.com/paper/oops-predicting-unintentional-action-in-video\", \"ArXiv URL\": \"https://openaccess.thecvf.com/content_CVPR_2020/papers/Epstein_Oops_Predicting_Unintentional_Action_in_Video_CVPR_2020_paper.pdf\", \"Semantic Scholar Corpus ID\": 208291335, \"Year Released\": \"2020\", \"Text Sources\": [\"youtube\"], \"Licenses\": [{\"License\": \"CC BY-NC-SA 4.0\", \"License URL\": \"https://oops.cs.columbia.edu/data/\"}], \"Creators\": [\"Columbia University\"], \"Countries\": [\"United States of America\"], \"License Verified By\": \"Vivek Sharma\", \"Video Hours\": 50.0, \"Taken Down\": \"False\", \"Video Sources\": \"youtube\", \"Task Categories\": [\"Video Classification\"], \"Inferred Metadata\": {\"GitHub License\": \"MIT License\", \"GitHub Stars (June 2024)\": 77, \"GitHub Topics\": [], \"Github Date\": \"\", \"HF Config\": \"\", \"HF Config License\": \"\", \"HF Dataset\": \"\", \"HF Date\": \"\", \"HF Downloads (June 2024)\": \"\", \"HF Likes (June 2024)\": \"\", \"HF Yaml License\": \"\", \"PwC Date\": \"\", \"PwC Description\": \"Implemented in one code library.\", \"PwC License Name\": \"\", \"PwC License URL\": \"\", \"S2 Citation Count (June 2024)\": \"\", \"S2 Date\": \"\"}, \"Bibtex\": \"@Article{Epstein2019OopsPU,\\n author = {Dave Epstein and Boyuan Chen and Carl Vondrick},\\n booktitle = {Computer Vision and Pattern Recognition},\\n journal = {2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},\\n pages = {916-926},\\n title = {Oops! Predicting Unintentional Action in Video},\\n year = {2019}\\n}\\n\", \"License Use (DataProvenance)\": \"non-commercial\", \"License Attribution (DataProvenance)\": 1, \"License Share Alike (DataProvenance)\": 1, \"License Use (DataProvenance IgnoreOpenAI)\": \"non-commercial\", \"License Attribution (DataProvenance IgnoreOpenAI)\": 1, \"License Share Alike (DataProvenance IgnoreOpenAI)\": 1, \"License Type\": \"Non-Commercial/Academic\"}, {\"Unique Dataset Identifier\": \"eev-dataset\", \"Collection\": \"eev-dataset\", \"Collection URL\": \"https://arxiv.org/abs/2001.05488\", \"Dataset Name\": \"EEV: A Large-Scale Dataset for Studying Evoked Expressions from Video\", \"Paper Title\": \"EEV: A Large-Scale Dataset for Studying Evoked Expressions from Video\", \"Paper URL\": \"https://arxiv.org/abs/2001.05488\", \"GitHub URL\": \"https://github.com/google-research-datasets/eev\", \"Hugging Face URL\": \"\", \"Papers with Code URL\": \"\", \"ArXiv URL\": \"https://arxiv.org/abs/2001.05488\", \"Semantic Scholar Corpus ID\": 210701992, \"Year Released\": \"2020\", \"Text Sources\": [\"undisclosed web\"], \"Licenses\": [{\"License\": \"CC BY 4.0\", \"License URL\": \"https://github.com/google-research-datasets/eev?tab=readme-ov-file#license\"}], \"Creators\": [\"Google Research\", \"California Institute of Technology\"], \"Countries\": [\"United States of America\"], \"License Verified By\": \"Vivek Sharma\", \"Video Hours\": 370.0, \"Taken Down\": \"False\", \"Video Sources\": \"undisclosed web\", \"Task Categories\": [\"Video Classification\"], \"Inferred Metadata\": {\"GitHub License\": \"\", \"GitHub Stars (June 2024)\": 34, \"GitHub Topics\": [], \"Github Date\": \"\", \"HF Config\": \"\", \"HF Config License\": \"\", \"HF Dataset\": \"\", \"HF Date\": \"\", \"HF Downloads (June 2024)\": \"\", \"HF Likes (June 2024)\": \"\", \"HF Yaml License\": \"\", \"PwC Date\": \"\", \"PwC Description\": \"\", \"PwC License Name\": \"\", \"PwC License URL\": \"\", \"S2 Citation Count (June 2024)\": \"\", \"S2 Date\": \"\"}, \"Bibtex\": \"@Article{Sun2020EEVDP,\\n author = {Jennifer J. Sun and Ting Liu and Alan S. Cowen and Florian Schroff and Hartwig Adam and Gautam Prasad},\\n booktitle = {arXiv.org},\\n journal = {ArXiv},\\n title = {EEV Dataset: Predicting Expressions Evoked by Diverse Videos},\\n volume = {abs/2001.05488},\\n year = {2020}\\n}\\n\", \"License Use (DataProvenance)\": \"commercial\", \"License Attribution (DataProvenance)\": 1, \"License Share Alike (DataProvenance)\": 0, \"License Use (DataProvenance IgnoreOpenAI)\": \"commercial\", \"License Attribution (DataProvenance IgnoreOpenAI)\": 1, \"License Share Alike (DataProvenance IgnoreOpenAI)\": 0, \"License Type\": \"Commercial\"}, {\"Unique Dataset Identifier\": \"omnisource-web-dataset\", \"Collection\": \"omnisource-web-dataset\", \"Collection URL\": \"https://arxiv.org/abs/2003.13042\", \"Dataset Name\": \"OmniSource Web Dataset\", \"Paper Title\": \"OmniSource Web Dataset\", \"Paper URL\": \"https://arxiv.org/abs/2003.13042\", \"GitHub URL\": \"https://github.com/open-mmlab/mmaction\", \"Hugging Face URL\": \"\", \"Papers with Code URL\": \"https://paperswithcode.com/paper/omni-sourced-webly-supervised-learning-for#code\", \"ArXiv URL\": \"https://arxiv.org/abs/2003.13042\", \"Semantic Scholar Corpus ID\": 214714240, \"Year Released\": \"2020\", \"Text Sources\": [\"google videos\", \"instagram\", \"youtube\"], \"Licenses\": [{\"License\": \"Apache License 2.0\", \"License URL\": \"https://github.com/open-mmlab/mmaction?tab=readme-ov-file#license\"}], \"Creators\": [\"The Chinese University of Hong Kong\"], \"Countries\": [\"China\"], \"License Verified By\": \"Vivek Sharma\", \"Video Hours\": 13333.0, \"Taken Down\": \"False\", \"Video Sources\": \"youtube\", \"Task Categories\": [\"Video Classification\"], \"Inferred Metadata\": {\"GitHub License\": \"Apache License 2.0\", \"GitHub Stars (June 2024)\": 1856, \"GitHub Topics\": [\"action-detection\", \"action-recognition\", \"pytorch\", \"spatial-temporal-action-detection\", \"temporal-action-detection\", \"temporal-action-localization\", \"video-understanding\"], \"Github Date\": \"\", \"HF Config\": \"\", \"HF Config License\": \"\", \"HF Dataset\": \"\", \"HF Date\": \"\", \"HF Downloads (June 2024)\": \"\", \"HF Likes (June 2024)\": \"\", \"HF Yaml License\": \"\", \"PwC Date\": \"\", \"PwC Description\": \"3 code implementations in PyTorch. We introduce OmniSource, a novel framework for leveraging web data to train video recognition models. OmniSource overcomes the barriers between data formats, such as images, short videos, and long untrimmed videos for webly-supervised learning. First, data samples with multiple formats, curated by task-specific data collection and automatically filtered by a teacher model, are transformed into a unified form. Then a joint-training strategy is proposed to deal with the domain gaps between multiple data sources and formats in webly-supervised learning. Several good practices, including data balancing, resampling, and cross-dataset mixup are adopted in joint training. Experiments show that by utilizing data from multiple sources and formats, OmniSource is more data-efficient in training. With only 3.5M images and 800K minutes videos crawled from the internet without human labeling (less than 2% of prior works), our models learned with OmniSource improve Top-1 accuracy of 2D- and 3D-ConvNet baseline models by 3.0% and 3.9%, respectively, on the Kinetics-400 benchmark. With OmniSource, we establish new records with different pretraining strategies for video recognition. Our best models achieve 80.4%, 80.5%, and 83.6 Top-1 accuracies on the Kinetics-400 benchmark respectively for training-from-scratch, ImageNet pre-training and IG-65M pre-training.\", \"PwC License Name\": \"\", \"PwC License URL\": \"\", \"S2 Citation Count (June 2024)\": \"\", \"S2 Date\": \"\"}, \"Bibtex\": \"@Article{Duan2020OmnisourcedWL,\\n author = {Haodong Duan and Yue Zhao and Yuanjun Xiong and Wentao Liu and Dahua Lin},\\n booktitle = {European Conference on Computer Vision},\\n journal = {ArXiv},\\n title = {Omni-sourced Webly-supervised Learning for Video Recognition},\\n volume = {abs/2003.13042},\\n year = {2020}\\n}\\n\", \"License Use (DataProvenance)\": \"commercial\", \"License Attribution (DataProvenance)\": 1, \"License Share Alike (DataProvenance)\": 0, \"License Use (DataProvenance IgnoreOpenAI)\": \"commercial\", \"License Attribution (DataProvenance IgnoreOpenAI)\": 1, \"License Share Alike (DataProvenance IgnoreOpenAI)\": 0, \"License Type\": \"Commercial\"}, {\"Unique Dataset Identifier\": \"omnisource-web-dataset\", \"Collection\": \"omnisource-web-dataset\", \"Collection URL\": \"https://arxiv.org/abs/2003.13042\", \"Dataset Name\": \"OmniSource Web Dataset\", \"Paper Title\": \"OmniSource Web Dataset\", \"Paper URL\": \"https://arxiv.org/abs/2003.13042\", \"GitHub URL\": \"https://github.com/open-mmlab/mmaction\", \"Hugging Face URL\": \"\", \"Papers with Code URL\": \"https://paperswithcode.com/paper/omni-sourced-webly-supervised-learning-for#code\", \"ArXiv URL\": \"https://arxiv.org/abs/2003.13042\", \"Semantic Scholar Corpus ID\": 214714240, \"Year Released\": \"2020\", \"Text Sources\": [\"google videos\", \"instagram\", \"youtube\"], \"Licenses\": [{\"License\": \"Apache License 2.0\", \"License URL\": \"https://github.com/open-mmlab/mmaction?tab=readme-ov-file#license\"}], \"Creators\": [\"The Chinese University of Hong Kong\"], \"Countries\": [\"China\"], \"License Verified By\": \"Vivek Sharma\", \"Video Hours\": 13333.0, \"Taken Down\": \"False\", \"Video Sources\": \"Other\", \"Task Categories\": [\"Video Classification\"], \"Inferred Metadata\": {\"GitHub License\": \"Apache License 2.0\", \"GitHub Stars (June 2024)\": 1856, \"GitHub Topics\": [\"action-detection\", \"action-recognition\", \"pytorch\", \"spatial-temporal-action-detection\", \"temporal-action-detection\", \"temporal-action-localization\", \"video-understanding\"], \"Github Date\": \"\", \"HF Config\": \"\", \"HF Config License\": \"\", \"HF Dataset\": \"\", \"HF Date\": \"\", \"HF Downloads (June 2024)\": \"\", \"HF Likes (June 2024)\": \"\", \"HF Yaml License\": \"\", \"PwC Date\": \"\", \"PwC Description\": \"3 code implementations in PyTorch. We introduce OmniSource, a novel framework for leveraging web data to train video recognition models. OmniSource overcomes the barriers between data formats, such as images, short videos, and long untrimmed videos for webly-supervised learning. First, data samples with multiple formats, curated by task-specific data collection and automatically filtered by a teacher model, are transformed into a unified form. Then a joint-training strategy is proposed to deal with the domain gaps between multiple data sources and formats in webly-supervised learning. Several good practices, including data balancing, resampling, and cross-dataset mixup are adopted in joint training. Experiments show that by utilizing data from multiple sources and formats, OmniSource is more data-efficient in training. With only 3.5M images and 800K minutes videos crawled from the internet without human labeling (less than 2% of prior works), our models learned with OmniSource improve Top-1 accuracy of 2D- and 3D-ConvNet baseline models by 3.0% and 3.9%, respectively, on the Kinetics-400 benchmark. With OmniSource, we establish new records with different pretraining strategies for video recognition. Our best models achieve 80.4%, 80.5%, and 83.6 Top-1 accuracies on the Kinetics-400 benchmark respectively for training-from-scratch, ImageNet pre-training and IG-65M pre-training.\", \"PwC License Name\": \"\", \"PwC License URL\": \"\", \"S2 Citation Count (June 2024)\": \"\", \"S2 Date\": \"\"}, \"Bibtex\": \"@Article{Duan2020OmnisourcedWL,\\n author = {Haodong Duan and Yue Zhao and Yuanjun Xiong and Wentao Liu and Dahua Lin},\\n booktitle = {European Conference on Computer Vision},\\n journal = {ArXiv},\\n title = {Omni-sourced Webly-supervised Learning for Video Recognition},\\n volume = {abs/2003.13042},\\n year = {2020}\\n}\\n\", \"License Use (DataProvenance)\": \"commercial\", \"License Attribution (DataProvenance)\": 1, \"License Share Alike (DataProvenance)\": 0, \"License Use (DataProvenance IgnoreOpenAI)\": \"commercial\", \"License Attribution (DataProvenance IgnoreOpenAI)\": 1, \"License Share Alike (DataProvenance IgnoreOpenAI)\": 0, \"License Type\": \"Commercial\"}, {\"Unique Dataset Identifier\": \"omnisource-web-dataset\", \"Collection\": \"omnisource-web-dataset\", \"Collection URL\": \"https://arxiv.org/abs/2003.13042\", \"Dataset Name\": \"OmniSource Web Dataset\", \"Paper Title\": \"OmniSource Web Dataset\", \"Paper URL\": \"https://arxiv.org/abs/2003.13042\", \"GitHub URL\": \"https://github.com/open-mmlab/mmaction\", \"Hugging Face URL\": \"\", \"Papers with Code URL\": \"https://paperswithcode.com/paper/omni-sourced-webly-supervised-learning-for#code\", \"ArXiv URL\": \"https://arxiv.org/abs/2003.13042\", \"Semantic Scholar Corpus ID\": 214714240, \"Year Released\": \"2020\", \"Text Sources\": [\"google videos\", \"instagram\", \"youtube\"], \"Licenses\": [{\"License\": \"Apache License 2.0\", \"License URL\": \"https://github.com/open-mmlab/mmaction?tab=readme-ov-file#license\"}], \"Creators\": [\"The Chinese University of Hong Kong\"], \"Countries\": [\"China\"], \"License Verified By\": \"Vivek Sharma\", \"Video Hours\": 13333.0, \"Taken Down\": \"False\", \"Video Sources\": \"Other\", \"Task Categories\": [\"Video Classification\"], \"Inferred Metadata\": {\"GitHub License\": \"Apache License 2.0\", \"GitHub Stars (June 2024)\": 1856, \"GitHub Topics\": [\"action-detection\", \"action-recognition\", \"pytorch\", \"spatial-temporal-action-detection\", \"temporal-action-detection\", \"temporal-action-localization\", \"video-understanding\"], \"Github Date\": \"\", \"HF Config\": \"\", \"HF Config License\": \"\", \"HF Dataset\": \"\", \"HF Date\": \"\", \"HF Downloads (June 2024)\": \"\", \"HF Likes (June 2024)\": \"\", \"HF Yaml License\": \"\", \"PwC Date\": \"\", \"PwC Description\": \"3 code implementations in PyTorch. We introduce OmniSource, a novel framework for leveraging web data to train video recognition models. OmniSource overcomes the barriers between data formats, such as images, short videos, and long untrimmed videos for webly-supervised learning. First, data samples with multiple formats, curated by task-specific data collection and automatically filtered by a teacher model, are transformed into a unified form. Then a joint-training strategy is proposed to deal with the domain gaps between multiple data sources and formats in webly-supervised learning. Several good practices, including data balancing, resampling, and cross-dataset mixup are adopted in joint training. Experiments show that by utilizing data from multiple sources and formats, OmniSource is more data-efficient in training. With only 3.5M images and 800K minutes videos crawled from the internet without human labeling (less than 2% of prior works), our models learned with OmniSource improve Top-1 accuracy of 2D- and 3D-ConvNet baseline models by 3.0% and 3.9%, respectively, on the Kinetics-400 benchmark. With OmniSource, we establish new records with different pretraining strategies for video recognition. Our best models achieve 80.4%, 80.5%, and 83.6 Top-1 accuracies on the Kinetics-400 benchmark respectively for training-from-scratch, ImageNet pre-training and IG-65M pre-training.\", \"PwC License Name\": \"\", \"PwC License URL\": \"\", \"S2 Citation Count (June 2024)\": \"\", \"S2 Date\": \"\"}, \"Bibtex\": \"@Article{Duan2020OmnisourcedWL,\\n author = {Haodong Duan and Yue Zhao and Yuanjun Xiong and Wentao Liu and Dahua Lin},\\n booktitle = {European Conference on Computer Vision},\\n journal = {ArXiv},\\n title = {Omni-sourced Webly-supervised Learning for Video Recognition},\\n volume = {abs/2003.13042},\\n year = {2020}\\n}\\n\", \"License Use (DataProvenance)\": \"commercial\", \"License Attribution (DataProvenance)\": 1, \"License Share Alike (DataProvenance)\": 0, \"License Use (DataProvenance IgnoreOpenAI)\": \"commercial\", \"License Attribution (DataProvenance IgnoreOpenAI)\": 1, \"License Share Alike (DataProvenance IgnoreOpenAI)\": 0, \"License Type\": \"Commercial\"}, {\"Unique Dataset Identifier\": \"finegym-dataset\", \"Collection\": \"finegym-dataset\", \"Collection URL\": \"https://arxiv.org/abs/2004.06704\", \"Dataset Name\": \"FineGym: A Hierarchical Video Dataset for Fine-grained Action Understanding (CVPR 2020)\", \"Paper Title\": \"FineGym: A Hierarchical Video Dataset for Fine-grained Action Understanding (CVPR 2020)\", \"Paper URL\": \"https://arxiv.org/abs/2004.06704\", \"GitHub URL\": \"https://github.com/SDOlivia/FineGym/\", \"Hugging Face URL\": \"\", \"Papers with Code URL\": \"https://paperswithcode.com/dataset/finegym\", \"ArXiv URL\": \"https://arxiv.org/abs/2004.06704\", \"Semantic Scholar Corpus ID\": 215754360, \"Year Released\": \"2020\", \"Text Sources\": [\"undisclosed web\"], \"Licenses\": [{\"License\": \"CC BY-NC 4.0\", \"License URL\": \"https://sdolivia.github.io/FineGym/\"}], \"Creators\": [\"The Chinese University of Hong Kong\"], \"Countries\": [\"China\"], \"License Verified By\": \"Vivek Sharma\", \"Video Hours\": 708.0, \"Taken Down\": \"False\", \"Video Sources\": \"undisclosed web\", \"Task Categories\": [\"Video Classification\"], \"Inferred Metadata\": {\"GitHub License\": \"\", \"GitHub Stars (June 2024)\": 124, \"GitHub Topics\": [], \"Github Date\": \"\", \"HF Config\": \"\", \"HF Config License\": \"\", \"HF Dataset\": \"\", \"HF Date\": \"\", \"HF Downloads (June 2024)\": \"\", \"HF Likes (June 2024)\": \"\", \"HF Yaml License\": \"\", \"PwC Date\": \"2020-04-14\", \"PwC Description\": \"FineGym is an action recognition dataset build on top of gymnasium videos. Compared to existing action recognition datasets, FineGym is distinguished in richness, quality, and diversity. In particular, it provides temporal annotations at both action and sub-action levels with a three-level semantic hierarchy. For example, a \\\"balance beam\\\" event will be annotated as a sequence of elementary sub-actions derived from five sets: \\\"leap-jumphop\\\", \\\"beam-turns\\\", \\\"flight-salto\\\", \\\"flight-handspring\\\", and \\\"dismount\\\", where the sub-action in each set will be further annotated with finely defined class labels. This new level of granularity presents significant challenges for action recognition, e.g. how to parse the temporal structures from a coherent action, and how to distinguish between subtly different action classes.\", \"PwC License Name\": \"CC BY-NC 4.0\", \"PwC License URL\": \"https://creativecommons.org/licenses/by-nc/4.0/\", \"S2 Citation Count (June 2024)\": \"\", \"S2 Date\": \"\"}, \"Bibtex\": \"@Article{Shao2020FineGymAH,\\n author = {Dian Shao and Yue Zhao and Bo Dai and Dahua Lin},\\n booktitle = {Computer Vision and Pattern Recognition},\\n journal = {2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},\\n pages = {2613-2622},\\n title = {FineGym: A Hierarchical Video Dataset for Fine-Grained Action Understanding},\\n year = {2020}\\n}\\n\", \"License Use (DataProvenance)\": \"non-commercial\", \"License Attribution (DataProvenance)\": 1, \"License Share Alike (DataProvenance)\": 0, \"License Use (DataProvenance IgnoreOpenAI)\": \"non-commercial\", \"License Attribution (DataProvenance IgnoreOpenAI)\": 1, \"License Share Alike (DataProvenance IgnoreOpenAI)\": 0, \"License Type\": \"Non-Commercial/Academic\"}, {\"Unique Dataset Identifier\": \"condensed-movies\", \"Collection\": \"condensed-movies\", \"Collection URL\": \"https://arxiv.org/pdf/2005.04208\", \"Dataset Name\": \"Condensed Movies\", \"Paper Title\": \"Condensed Movies\", \"Paper URL\": \"https://arxiv.org/pdf/2005.04208\", \"GitHub URL\": \"https://github.com/m-bain/CondensedMovies\", \"Hugging Face URL\": \"\", \"Papers with Code URL\": \"https://paperswithcode.com/dataset/condensed-movies\", \"ArXiv URL\": \"https://arxiv.org/pdf/2005.04208\", \"Semantic Scholar Corpus ID\": 218571391, \"Year Released\": \"2020\", \"Text Sources\": [\"movies\"], \"Licenses\": [{\"License\": \"CC BY 4.0\", \"License URL\": \"https://www.robots.ox.ac.uk/~vgg/data/condensed-movies/#download\"}], \"Creators\": [\"University of Oxford\"], \"Countries\": [\"United Kingdom\"], \"License Verified By\": \"Vivek Sharma\", \"Video Hours\": 1270.0, \"Taken Down\": \"False\", \"Video Sources\": \"movies\", \"Task Categories\": [\"Video Summarization\"], \"Inferred Metadata\": {\"GitHub License\": \"\", \"GitHub Stars (June 2024)\": 153, \"GitHub Topics\": [\"dataset\", \"precomputed-features\", \"retrieval\", \"source-videos\", \"video-text-retrieval\"], \"Github Date\": \"\", \"HF Config\": \"\", \"HF Config License\": \"\", \"HF Dataset\": \"\", \"HF Date\": \"\", \"HF Downloads (June 2024)\": \"\", \"HF Likes (June 2024)\": \"\", \"HF Yaml License\": \"\", \"PwC Date\": \"2020-05-08\", \"PwC Description\": \"A large-scale video dataset, featuring clips from movies with detailed captions.\", \"PwC License Name\": \"Unspecified\", \"PwC License URL\": null, \"S2 Citation Count (June 2024)\": \"\", \"S2 Date\": \"\"}, \"Bibtex\": \"@Article{Bain2020CondensedMS,\\n author = {Max Bain and Arsha Nagrani and A. Brown and Andrew Zisserman},\\n booktitle = {Asian Conference on Computer Vision},\\n journal = {ArXiv},\\n title = {Condensed Movies: Story Based Retrieval with Contextual Embeddings},\\n volume = {abs/2005.04208},\\n year = {2020}\\n}\\n\", \"License Use (DataProvenance)\": \"commercial\", \"License Attribution (DataProvenance)\": 1, \"License Share Alike (DataProvenance)\": 0, \"License Use (DataProvenance IgnoreOpenAI)\": \"commercial\", \"License Attribution (DataProvenance IgnoreOpenAI)\": 1, \"License Share Alike (DataProvenance IgnoreOpenAI)\": 0, \"License Type\": \"Commercial\"}, {\"Unique Dataset Identifier\": \"haa500-dataset\", \"Collection\": \"haa500-dataset\", \"Collection URL\": \"https://arxiv.org/abs/2009.05224\", \"Dataset Name\": \"HAA500: Human-Centric Atomic Action Dataset with Curated Videos (ICCV 2021)\", \"Paper Title\": \"HAA500: Human-Centric Atomic Action Dataset with Curated Videos (ICCV 2021)\", \"Paper URL\": \"https://arxiv.org/abs/2009.05224\", \"GitHub URL\": \"\", \"Hugging Face URL\": \"\", \"Papers with Code URL\": \"https://paperswithcode.com/paper/haa500-human-centric-atomic-action-dataset\", \"ArXiv URL\": \"https://arxiv.org/abs/2009.05224\", \"Semantic Scholar Corpus ID\": 221640805, \"Year Released\": \"2020\", \"Text Sources\": [\"human\"], \"Licenses\": [{\"License\": \"Unspecified\", \"License URL\": \"\"}], \"Creators\": [\"Carnegie Mellon University\", \"The Hong Kong University of Science and Technology\", \"Princeton University\", \"Kuaishou Technology\"], \"Countries\": [\"China\", \"United States of America\"], \"License Verified By\": \"Vivek Sharma\", \"Video Hours\": 5.48, \"Taken Down\": \"False\", \"Video Sources\": \"human\", \"Task Categories\": [\"Video Classification\"], \"Inferred Metadata\": {\"GitHub License\": \"\", \"GitHub Stars (June 2024)\": \"\", \"GitHub Topics\": \"\", \"Github Date\": \"\", \"HF Config\": \"\", \"HF Config License\": \"\", \"HF Dataset\": \"\", \"HF Date\": \"\", \"HF Downloads (June 2024)\": \"\", \"HF Likes (June 2024)\": \"\", \"HF Yaml License\": \"\", \"PwC Date\": \"\", \"PwC Description\": \"\\ud83c\\udfc6 SOTA for Action Recognition on HAA500 (Top-1 (%) metric)\", \"PwC License Name\": \"\", \"PwC License URL\": \"\", \"S2 Citation Count (June 2024)\": \"\", \"S2 Date\": \"\"}, \"Bibtex\": \"@Article{Chung2020HAA500HA,\\n author = {Jihoon Chung and Cheng-hsin Wuu and Hsuan-ru Yang and Yu-Wing Tai and Chi-Keung Tang},\\n booktitle = {IEEE International Conference on Computer Vision},\\n journal = {2021 IEEE/CVF International Conference on Computer Vision (ICCV)},\\n pages = {13445-13454},\\n title = {HAA500: Human-Centric Atomic Action Dataset with Curated Videos},\\n year = {2020}\\n}\\n\", \"License Use (DataProvenance)\": \"unspecified\", \"License Attribution (DataProvenance)\": 0, \"License Share Alike (DataProvenance)\": 0, \"License Use (DataProvenance IgnoreOpenAI)\": \"unspecified\", \"License Attribution (DataProvenance IgnoreOpenAI)\": 0, \"License Share Alike (DataProvenance IgnoreOpenAI)\": 0, \"License Type\": \"Unspecified\"}, {\"Unique Dataset Identifier\": \"movie-net\", \"Collection\": \"movie-net\", \"Collection URL\": \"https://arxiv.org/abs/2007.10937\", \"Dataset Name\": \"MovieNet\", \"Paper Title\": \"MovieNet\", \"Paper URL\": \"https://arxiv.org/abs/2007.10937\", \"GitHub URL\": \"https://github.com/movienet/movienet-tools\", \"Hugging Face URL\": \"\", \"Papers with Code URL\": \"https://paperswithcode.com/dataset/movienet\", \"ArXiv URL\": \"https://arxiv.org/abs/2007.10937\", \"Semantic Scholar Corpus ID\": 220665753, \"Year Released\": \"2020\", \"Text Sources\": [\"movies\"], \"Licenses\": [{\"License\": \"Unspecified\", \"License URL\": \"\"}], \"Creators\": [\"The Chinese University of Hong Kong\"], \"Countries\": [\"China\"], \"License Verified By\": \"Vivek Sharma\", \"Video Hours\": 3000.0, \"Taken Down\": \"False\", \"Video Sources\": \"movies\", \"Task Categories\": [\"Video Summarization\", \"Misc\"], \"Inferred Metadata\": {\"GitHub License\": \"\", \"GitHub Stars (June 2024)\": 256, \"GitHub Topics\": [\"action-recognition\", \"computer-vision\", \"cross-modality\", \"deep-learning\", \"movie\", \"person-analysis\", \"shot-detection\", \"video-understanding\", \"vision-language\"], \"Github Date\": \"\", \"HF Config\": \"\", \"HF Config License\": \"\", \"HF Dataset\": \"\", \"HF Date\": \"\", \"HF Downloads (June 2024)\": \"\", \"HF Likes (June 2024)\": \"\", \"HF Yaml License\": \"\", \"PwC Date\": \"2020-07-21\", \"PwC Description\": \"MovieNet is a holistic dataset for movie understanding. MovieNet contains 1,100 movies with a large amount of multi-modal data, e.g. trailers, photos, plot descriptions, etc.. Besides, different aspects of manual annotations are provided in MovieNet, including 1.1M characters with bounding boxes and identities, 42K scene boundaries, 2.5K aligned description sentences, 65K tags of place and action, and 92 K tags of cinematic style.\", \"PwC License Name\": \"Unspecified\", \"PwC License URL\": null, \"S2 Citation Count (June 2024)\": \"\", \"S2 Date\": \"\"}, \"Bibtex\": \"@Article{Huang2020MovieNetAH,\\n author = {Qingqiu Huang and Yu Xiong and Anyi Rao and Jiaze Wang and Dahua Lin},\\n booktitle = {European Conference on Computer Vision},\\n pages = {709-727},\\n title = {MovieNet: A Holistic Dataset for Movie Understanding},\\n year = {2020}\\n}\\n\", \"License Use (DataProvenance)\": \"unspecified\", \"License Attribution (DataProvenance)\": 0, \"License Share Alike (DataProvenance)\": 0, \"License Use (DataProvenance IgnoreOpenAI)\": \"unspecified\", \"License Attribution (DataProvenance IgnoreOpenAI)\": 0, \"License Share Alike (DataProvenance IgnoreOpenAI)\": 0, \"License Type\": \"Unspecified\"}, {\"Unique Dataset Identifier\": \"titan\", \"Collection\": \"titan\", \"Collection URL\": \"https://openaccess.thecvf.com/content_CVPR_2020/papers/Malla_TITAN_Future_Forecast_Using_Action_Priors_CVPR_2020_paper.pdf\", \"Dataset Name\": \"TITAN: Future Forecast using Action Priors (CVPR 2020)\", \"Paper Title\": \"TITAN: Future Forecast using Action Priors (CVPR 2020)\", \"Paper URL\": \"https://openaccess.thecvf.com/content_CVPR_2020/papers/Malla_TITAN_Future_Forecast_Using_Action_Priors_CVPR_2020_paper.pdf\", \"GitHub URL\": \"\", \"Hugging Face URL\": \"\", \"Papers with Code URL\": \"https://paperswithcode.com/paper/titan-future-forecast-using-action-priors\", \"ArXiv URL\": \"https://openaccess.thecvf.com/content_CVPR_2020/papers/Malla_TITAN_Future_Forecast_Using_Action_Priors_CVPR_2020_paper.pdf\", \"Semantic Scholar Corpus ID\": 214727763, \"Year Released\": \"2020\", \"Text Sources\": [\"crowdsourced\"], \"Licenses\": [{\"License\": \"Non Commercial\", \"License URL\": \"https://usa.honda-ri.com/titan\"}], \"Creators\": [\"Honda Research Institute\"], \"Countries\": [\"United States of America\"], \"License Verified By\": \"Vivek Sharma\", \"Video Hours\": 2.91, \"Taken Down\": \"False\", \"Video Sources\": \"crowdsourced\", \"Task Categories\": [\"Video Classification\"], \"Inferred Metadata\": {\"GitHub License\": \"\", \"GitHub Stars (June 2024)\": \"\", \"GitHub Topics\": \"\", \"Github Date\": \"\", \"HF Config\": \"\", \"HF Config License\": \"\", \"HF Dataset\": \"\", \"HF Date\": \"\", \"HF Downloads (June 2024)\": \"\", \"HF Likes (June 2024)\": \"\", \"HF Yaml License\": \"\", \"PwC Date\": \"\", \"PwC Description\": \"We consider the problem of predicting the future trajectory of scene agents from egocentric views obtained from a moving platform. This problem is important in a variety of domains, particularly for autonomous systems making reactive or strategic decisions in navigation. In an attempt to address this problem, we introduce TITAN (Trajectory Inference using Targeted Action priors Network), a new model that incorporates prior positions, actions, and context to forecast future trajectory of agents and future ego-motion. In the absence of an appropriate dataset for this task, we created the TITAN dataset that consists of 700 labeled video-clips (with odometry) captured from a moving vehicle on highly interactive urban traffic scenes in Tokyo. Our dataset includes 50 labels including vehicle states and actions, pedestrian age groups, and targeted pedestrian action attributes that are organized hierarchically corresponding to atomic, simple/complex-contextual, transportive, and communicative actions. To evaluate our model, we conducted extensive experiments on the TITAN dataset, revealing significant performance improvement against baselines and state-of-the-art algorithms. We also report promising results from our Agent Importance Mechanism (AIM), a module which provides insight into assessment of perceived risk by calculating the relative influence of each agent on the future ego-trajectory. The dataset is available at https://usa.honda-ri.com/titan\", \"PwC License Name\": \"\", \"PwC License URL\": \"\", \"S2 Citation Count (June 2024)\": \"\", \"S2 Date\": \"\"}, \"Bibtex\": \"@Article{Malla2020TITANFF,\\n author = {Srikanth Malla and B. Dariush and Chiho Choi},\\n booktitle = {Computer Vision and Pattern Recognition},\\n journal = {2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},\\n pages = {11183-11193},\\n title = {TITAN: Future Forecast Using Action Priors},\\n year = {2020}\\n}\\n\", \"License Use (DataProvenance)\": \"non-commercial\", \"License Attribution (DataProvenance)\": 0, \"License Share Alike (DataProvenance)\": 0, \"License Use (DataProvenance IgnoreOpenAI)\": \"non-commercial\", \"License Attribution (DataProvenance IgnoreOpenAI)\": 0, \"License Share Alike (DataProvenance IgnoreOpenAI)\": 0, \"License Type\": \"Non-Commercial/Academic\"}, {\"Unique Dataset Identifier\": \"100doh\", \"Collection\": \"100doh\", \"Collection URL\": \"https://arxiv.org/abs/2006.06669\", \"Dataset Name\": \"100DOH: Understanding Human Hands in Contact at Internet Scale (CVPR 2020)\", \"Paper Title\": \"100DOH: Understanding Human Hands in Contact at Internet Scale (CVPR 2020)\", \"Paper URL\": \"https://arxiv.org/abs/2006.06669\", \"GitHub URL\": \"https://github.com/ddshan/hand_object_detector\", \"Hugging Face URL\": \"\", \"Papers with Code URL\": \"https://paperswithcode.com/paper/understanding-human-hands-in-contact-at-1\", \"ArXiv URL\": \"https://arxiv.org/abs/2006.06669\", \"Semantic Scholar Corpus ID\": 215413188, \"Year Released\": \"2020\", \"Text Sources\": [\"youtube\"], \"Licenses\": [{\"License\": \"Custom\", \"License URL\": \"https://fouheylab.eecs.umich.edu/~dandans/projects/100DOH/download.html\"}], \"Creators\": [\"University of Michigan\", \"Johns Hopkins University\"], \"Countries\": [\"United States of America\"], \"License Verified By\": \"Vivek Sharma\", \"Video Hours\": 4577.3, \"Taken Down\": \"False\", \"Video Sources\": \"youtube\", \"Task Categories\": [\"Misc (Hand/Object Detection)\"], \"Inferred Metadata\": {\"GitHub License\": \"MIT License\", \"GitHub Stars (June 2024)\": 213, \"GitHub Topics\": [\"cvpr2020\", \"dataset\", \"handobjectdetection\", \"interactiondetection\"], \"Github Date\": \"\", \"HF Config\": \"\", \"HF Config License\": \"\", \"HF Dataset\": \"\", \"HF Date\": \"\", \"HF Downloads (June 2024)\": \"\", \"HF Likes (June 2024)\": \"\", \"HF Yaml License\": \"\", \"PwC Date\": \"\", \"PwC Description\": \"Implemented in one code library.\", \"PwC License Name\": \"\", \"PwC License URL\": \"\", \"S2 Citation Count (June 2024)\": \"\", \"S2 Date\": \"\"}, \"Bibtex\": \"@Article{Shan2020UnderstandingHH,\\n author = {Dandan Shan and Jiaqi Geng and Michelle Shu and D. Fouhey},\\n booktitle = {Computer Vision and Pattern Recognition},\\n journal = {2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},\\n pages = {9866-9875},\\n title = {Understanding Human Hands in Contact at Internet Scale},\\n year = {2020}\\n}\\n\", \"License Use (DataProvenance)\": \"academic-only\", \"License Attribution (DataProvenance)\": 1, \"License Share Alike (DataProvenance)\": 0, \"License Use (DataProvenance IgnoreOpenAI)\": \"academic-only\", \"License Attribution (DataProvenance IgnoreOpenAI)\": 1, \"License Share Alike (DataProvenance IgnoreOpenAI)\": 0, \"License Type\": \"Non-Commercial/Academic\"}, {\"Unique Dataset Identifier\": \"kinetics-700\", \"Collection\": \"kinetics-700\", \"Collection URL\": \"https://arxiv.org/pdf/2010.10864.pdf\", \"Dataset Name\": \"Kinetics-700\", \"Paper Title\": \"Kinetics-700\", \"Paper URL\": \"https://arxiv.org/pdf/2010.10864.pdf\", \"GitHub URL\": \"\", \"Hugging Face URL\": \"\", \"Papers with Code URL\": \"https://paperswithcode.com/dataset/kinetics-700\", \"ArXiv URL\": \"https://arxiv.org/pdf/2010.10864.pdf\", \"Semantic Scholar Corpus ID\": 196831809, \"Year Released\": \"2020\", \"Text Sources\": [\"youtube\"], \"Licenses\": [{\"License\": \"Unspecified\", \"License URL\": \"\"}], \"Creators\": [\"DeepMind\"], \"Countries\": [\"United Kingdom\"], \"License Verified By\": \"Vivek Sharma\", \"Video Hours\": 1805.56, \"Taken Down\": \"False\", \"Video Sources\": \"youtube\", \"Task Categories\": [\"Video Classification\"], \"Inferred Metadata\": {\"GitHub License\": \"\", \"GitHub Stars (June 2024)\": \"\", \"GitHub Topics\": \"\", \"Github Date\": \"\", \"HF Config\": \"\", \"HF Config License\": \"\", \"HF Dataset\": \"\", \"HF Date\": \"\", \"HF Downloads (June 2024)\": \"\", \"HF Likes (June 2024)\": \"\", \"HF Yaml License\": \"\", \"PwC Date\": \"2019-07-15\", \"PwC Description\": \"Kinetics-700 is a video dataset of 650,000 clips that covers 700 human action classes. The videos include human-object interactions such as playing instruments, as well as human-human interactions such as shaking hands and hugging. Each action class has at least 700 video clips. Each clip is annotated with an action class and lasts approximately 10 seconds.\", \"PwC License Name\": \"CC BY 4.0\", \"PwC License URL\": \"https://creativecommons.org/licenses/by/4.0/\", \"S2 Citation Count (June 2024)\": \"\", \"S2 Date\": \"\"}, \"Bibtex\": \"@Article{Carreira2019ASN,\\n author = {Jo\\u00e3o Carreira and Eric Noland and Chloe Hillier and Andrew Zisserman},\\n booktitle = {arXiv.org},\\n journal = {ArXiv},\\n title = {A Short Note on the Kinetics-700 Human Action Dataset},\\n volume = {abs/1907.06987},\\n year = {2019}\\n}\\n\", \"License Use (DataProvenance)\": \"unspecified\", \"License Attribution (DataProvenance)\": 0, \"License Share Alike (DataProvenance)\": 0, \"License Use (DataProvenance IgnoreOpenAI)\": \"unspecified\", \"License Attribution (DataProvenance IgnoreOpenAI)\": 0, \"License Share Alike (DataProvenance IgnoreOpenAI)\": 0, \"License Type\": \"Unspecified\"}, {\"Unique Dataset Identifier\": \"rare-act-dataset\", \"Collection\": \"rare-act-dataset\", \"Collection URL\": \"https://arxiv.org/pdf/2008.01018.pdf\", \"Dataset Name\": \"RareAct: A video dataset of unusual interactions\", \"Paper Title\": \"RareAct: A video dataset of unusual interactions\", \"Paper URL\": \"https://arxiv.org/pdf/2008.01018.pdf\", \"GitHub URL\": \"https://github.com/antoine77340/RareAct\", \"Hugging Face URL\": \"\", \"Papers with Code URL\": \"https://paperswithcode.com/dataset/rareact\", \"ArXiv URL\": \"https://arxiv.org/pdf/2008.01018.pdf\", \"Semantic Scholar Corpus ID\": 220936243, \"Year Released\": \"2020\", \"Text Sources\": [\"undisclosed web\"], \"Licenses\": [{\"License\": \"Unspecified\", \"License URL\": \"\"}], \"Creators\": [\"University of Oxford\", \"Ecole Normale Sup\\u00e9rieure\", \"Inria\", \"CIIRC\", \"Czech Technical University\"], \"Countries\": [\"United Kingdom\", \"France\", \"Czech Republic\"], \"License Verified By\": \"Vivek Sharma\", \"Video Hours\": 21.13, \"Taken Down\": \"False\", \"Video Sources\": \"undisclosed web\", \"Task Categories\": [\"Video Classification\"], \"Inferred Metadata\": {\"GitHub License\": \"Apache License 2.0\", \"GitHub Stars (June 2024)\": 32, \"GitHub Topics\": [], \"Github Date\": \"\", \"HF Config\": \"\", \"HF Config License\": \"\", \"HF Dataset\": \"\", \"HF Date\": \"\", \"HF Downloads (June 2024)\": \"\", \"HF Likes (June 2024)\": \"\", \"HF Yaml License\": \"\", \"PwC Date\": \"\", \"PwC Description\": \"RareAct is a video dataset of unusual actions, including actions like \\u201cblend phone\\u201d, \\u201ccut keyboard\\u201d and \\u201cmicrowave shoes\\u201d. It aims at evaluating the zero-shot and few-shot compositionality of action recognition models for unlikely compositions of common action verbs and object nouns. It contains 122 different actions which were obtained by combining verbs and nouns rarely co-occurring together in the large-scale textual corpus from HowTo100M, but that frequently appear separately.\", \"PwC License Name\": \"Unspecified\", \"PwC License URL\": null, \"S2 Citation Count (June 2024)\": \"\", \"S2 Date\": \"\"}, \"Bibtex\": \"@Article{Miech2020RareActAV,\\n author = {Antoine Miech and Jean-Baptiste Alayrac and I. Laptev and Josef Sivic and Andrew Zisserman},\\n booktitle = {arXiv.org},\\n journal = {ArXiv},\\n title = {RareAct: A video dataset of unusual interactions},\\n volume = {abs/2008.01018},\\n year = {2020}\\n}\\n\", \"License Use (DataProvenance)\": \"unspecified\", \"License Attribution (DataProvenance)\": 0, \"License Share Alike (DataProvenance)\": 0, \"License Use (DataProvenance IgnoreOpenAI)\": \"unspecified\", \"License Attribution (DataProvenance IgnoreOpenAI)\": 0, \"License Share Alike (DataProvenance IgnoreOpenAI)\": 0, \"License Type\": \"Unspecified\"}, {\"Unique Dataset Identifier\": \"violin\", \"Collection\": \"violin\", \"Collection URL\": \"https://arxiv.org/abs/2003.11618\", \"Dataset Name\": \"VIOLIN\", \"Paper Title\": \"VIOLIN\", \"Paper URL\": \"https://arxiv.org/abs/2003.11618\", \"GitHub URL\": \"https://github.com/jimmy646/violin\", \"Hugging Face URL\": \"\", \"Papers with Code URL\": \"https://paperswithcode.com/dataset/violin\", \"ArXiv URL\": \"https://arxiv.org/abs/2003.11618\", \"Semantic Scholar Corpus ID\": 214668012, \"Year Released\": \"2020\", \"Text Sources\": [\"youtube\"], \"Licenses\": [{\"License\": \"Unspecified\", \"License URL\": \"\"}], \"Creators\": [\"Microsoft\"], \"Countries\": [\"United States of America\"], \"License Verified By\": \"Vivek Sharma\", \"Video Hours\": 582.0, \"Taken Down\": \"False\", \"Video Sources\": \"youtube\", \"Task Categories\": [\"Video Captioning\"], \"Inferred Metadata\": {\"GitHub License\": \"MIT License\", \"GitHub Stars (June 2024)\": 156, \"GitHub Topics\": [], \"Github Date\": \"\", \"HF Config\": \"\", \"HF Config License\": \"\", \"HF Dataset\": \"\", \"HF Date\": \"\", \"HF Downloads (June 2024)\": \"\", \"HF Likes (June 2024)\": \"\", \"HF Yaml License\": \"\", \"PwC Date\": \"\", \"PwC Description\": \"Video-and-Language Inference is the task of joint multimodal understanding of video and text. Given a video clip with aligned subtitles as premise, paired with a natural language hypothesis based on the video content, a model needs to infer whether the hypothesis is entailed or contradicted by the given video clip. The Violin dataset is a dataset for this task which consists of 95,322 video-hypothesis pairs from 15,887 video clips, spanning over 582 hours of video. These video clips contain rich content with diverse temporal dynamics, event shifts, and people interactions, collected from two sources: (i) popular TV shows, and (ii) movie clips from YouTube channels.\", \"PwC License Name\": \"Unspecified\", \"PwC License URL\": null, \"S2 Citation Count (June 2024)\": \"\", \"S2 Date\": \"\"}, \"Bibtex\": \"@Article{Liu2020ViolinAL,\\n author = {J. Liu and Wenhu Chen and Yu Cheng and Zhe Gan and Licheng Yu and Yiming Yang and Jingjing Liu},\\n booktitle = {Computer Vision and Pattern Recognition},\\n journal = {2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},\\n pages = {10897-10907},\\n title = {Violin: A Large-Scale Dataset for Video-and-Language Inference},\\n year = {2020}\\n}\\n\", \"License Use (DataProvenance)\": \"unspecified\", \"License Attribution (DataProvenance)\": 0, \"License Share Alike (DataProvenance)\": 0, \"License Use (DataProvenance IgnoreOpenAI)\": \"unspecified\", \"License Attribution (DataProvenance IgnoreOpenAI)\": 0, \"License Share Alike (DataProvenance IgnoreOpenAI)\": 0, \"License Type\": \"Unspecified\"}, {\"Unique Dataset Identifier\": \"lemma-dataset\", \"Collection\": \"lemma-dataset\", \"Collection URL\": \"https://arxiv.org/abs/2007.15781\", \"Dataset Name\": \"LEMMA: A Multi-view Dataset for LEarning Multi-agent Multi-task Activities (ECCV 2020)\", \"Paper Title\": \"LEMMA: A Multi-view Dataset for LEarning Multi-agent Multi-task Activities (ECCV 2020)\", \"Paper URL\": \"https://arxiv.org/abs/2007.15781\", \"GitHub URL\": \"https://github.com/Buzz-Beater/LEMMA\", \"Hugging Face URL\": \"\", \"Papers with Code URL\": \"https://paperswithcode.com/dataset/lemma\", \"ArXiv URL\": \"https://arxiv.org/abs/2007.15781\", \"Semantic Scholar Corpus ID\": 220634784, \"Year Released\": \"2020\", \"Text Sources\": [\"crowdsourced\", \"human\"], \"Licenses\": [{\"License\": \"Unspecified\", \"License URL\": \"\"}], \"Creators\": [\"UCLA\"], \"Countries\": [\"United States of America\"], \"License Verified By\": \"Vivek Sharma\", \"Video Hours\": 10.8, \"Taken Down\": \"False\", \"Video Sources\": \"crowdsourced\", \"Task Categories\": [\"Video Classification\"], \"Inferred Metadata\": {\"GitHub License\": \"\", \"GitHub Stars (June 2024)\": 27, \"GitHub Topics\": [], \"Github Date\": \"\", \"HF Config\": \"\", \"HF Config License\": \"\", \"HF Dataset\": \"\", \"HF Date\": \"\", \"HF Downloads (June 2024)\": \"\", \"HF Likes (June 2024)\": \"\", \"HF Yaml License\": \"\", \"PwC Date\": \"\", \"PwC Description\": \"The LEMMA dataset aims to explore the essence of complex human activities in a goal-directed, multi-agent, multi-task setting with ground-truth labels of compositional atomic-actions and their associated tasks. By quantifying the scenarios to up to two multi-step tasks with two agents, the authors strive to address human multi-task and multi-agent interactions in four scenarios: single-agent single-task (1 x 1), single-agent multi-task (1 x 2), multi-agent single-task (2 x 1), and multi-agent multi-task (2 x 2). Task instructions are only given to one agent in the 2 x 1 setting to resemble the robot-helping scenario, hoping that the learned perception models could be applied in robotic tasks (especially in HRI) in the near future.\\n\\nBoth the third-person views (TPVs) and the first-person views (FPVs) were recorded to account for different perspectives of the same activities. The authors densely annotate atomic-actions (in the form of compositional verb-noun pairs) and tasks of each atomic-action, as well as the spatial location of each participating agent (bounding boxes) to facilitate the learning of multi-agent multi-task task scheduling and assignment.\", \"PwC License Name\": \"Unspecified\", \"PwC License URL\": null, \"S2 Citation Count (June 2024)\": \"\", \"S2 Date\": \"\"}, \"Bibtex\": \"@Article{Jia2020LEMMAAM,\\n author = {Baoxiong Jia and Yixin Chen and Siyuan Huang and Yixin Zhu and Song-Chun Zhu},\\n booktitle = {European Conference on Computer Vision},\\n journal = {ArXiv},\\n title = {LEMMA: A Multi-view Dataset for Learning Multi-agent Multi-task Activities},\\n volume = {abs/2007.15781},\\n year = {2020}\\n}\\n\", \"License Use (DataProvenance)\": \"unspecified\", \"License Attribution (DataProvenance)\": 0, \"License Share Alike (DataProvenance)\": 0, \"License Use (DataProvenance IgnoreOpenAI)\": \"unspecified\", \"License Attribution (DataProvenance IgnoreOpenAI)\": 0, \"License Share Alike (DataProvenance IgnoreOpenAI)\": 0, \"License Type\": \"Unspecified\"}, {\"Unique Dataset Identifier\": \"lemma-dataset\", \"Collection\": \"lemma-dataset\", \"Collection URL\": \"https://arxiv.org/abs/2007.15781\", \"Dataset Name\": \"LEMMA: A Multi-view Dataset for LEarning Multi-agent Multi-task Activities (ECCV 2020)\", \"Paper Title\": \"LEMMA: A Multi-view Dataset for LEarning Multi-agent Multi-task Activities (ECCV 2020)\", \"Paper URL\": \"https://arxiv.org/abs/2007.15781\", \"GitHub URL\": \"https://github.com/Buzz-Beater/LEMMA\", \"Hugging Face URL\": \"\", \"Papers with Code URL\": \"https://paperswithcode.com/dataset/lemma\", \"ArXiv URL\": \"https://arxiv.org/abs/2007.15781\", \"Semantic Scholar Corpus ID\": 220634784, \"Year Released\": \"2020\", \"Text Sources\": [\"crowdsourced\", \"human\"], \"Licenses\": [{\"License\": \"Unspecified\", \"License URL\": \"\"}], \"Creators\": [\"UCLA\"], \"Countries\": [\"United States of America\"], \"License Verified By\": \"Vivek Sharma\", \"Video Hours\": 10.8, \"Taken Down\": \"False\", \"Video Sources\": \"human\", \"Task Categories\": [\"Video Classification\"], \"Inferred Metadata\": {\"GitHub License\": \"\", \"GitHub Stars (June 2024)\": 27, \"GitHub Topics\": [], \"Github Date\": \"\", \"HF Config\": \"\", \"HF Config License\": \"\", \"HF Dataset\": \"\", \"HF Date\": \"\", \"HF Downloads (June 2024)\": \"\", \"HF Likes (June 2024)\": \"\", \"HF Yaml License\": \"\", \"PwC Date\": \"\", \"PwC Description\": \"The LEMMA dataset aims to explore the essence of complex human activities in a goal-directed, multi-agent, multi-task setting with ground-truth labels of compositional atomic-actions and their associated tasks. By quantifying the scenarios to up to two multi-step tasks with two agents, the authors strive to address human multi-task and multi-agent interactions in four scenarios: single-agent single-task (1 x 1), single-agent multi-task (1 x 2), multi-agent single-task (2 x 1), and multi-agent multi-task (2 x 2). Task instructions are only given to one agent in the 2 x 1 setting to resemble the robot-helping scenario, hoping that the learned perception models could be applied in robotic tasks (especially in HRI) in the near future.\\n\\nBoth the third-person views (TPVs) and the first-person views (FPVs) were recorded to account for different perspectives of the same activities. The authors densely annotate atomic-actions (in the form of compositional verb-noun pairs) and tasks of each atomic-action, as well as the spatial location of each participating agent (bounding boxes) to facilitate the learning of multi-agent multi-task task scheduling and assignment.\", \"PwC License Name\": \"Unspecified\", \"PwC License URL\": null, \"S2 Citation Count (June 2024)\": \"\", \"S2 Date\": \"\"}, \"Bibtex\": \"@Article{Jia2020LEMMAAM,\\n author = {Baoxiong Jia and Yixin Chen and Siyuan Huang and Yixin Zhu and Song-Chun Zhu},\\n booktitle = {European Conference on Computer Vision},\\n journal = {ArXiv},\\n title = {LEMMA: A Multi-view Dataset for Learning Multi-agent Multi-task Activities},\\n volume = {abs/2007.15781},\\n year = {2020}\\n}\\n\", \"License Use (DataProvenance)\": \"unspecified\", \"License Attribution (DataProvenance)\": 0, \"License Share Alike (DataProvenance)\": 0, \"License Use (DataProvenance IgnoreOpenAI)\": \"unspecified\", \"License Attribution (DataProvenance IgnoreOpenAI)\": 0, \"License Share Alike (DataProvenance IgnoreOpenAI)\": 0, \"License Type\": \"Unspecified\"}, {\"Unique Dataset Identifier\": \"hyu-vids\", \"Collection\": \"hyu-vids\", \"Collection URL\": \"https://arxiv.org/abs/1904.11451\", \"Dataset Name\": \"HVU: Large Scale Holistic Video Understanding (ECCV 2020)\", \"Paper Title\": \"HVU: Large Scale Holistic Video Understanding (ECCV 2020)\", \"Paper URL\": \"https://arxiv.org/abs/1904.11451\", \"GitHub URL\": \"https://github.com/holistic-video-understanding/HVU-Dataset\", \"Hugging Face URL\": \"\", \"Papers with Code URL\": \"https://paperswithcode.com/paper/holistic-large-scale-video-understanding\", \"ArXiv URL\": \"https://arxiv.org/abs/1904.11451\", \"Semantic Scholar Corpus ID\": 131777079, \"Year Released\": \"2020\", \"Text Sources\": [\"youtube\"], \"Licenses\": [{\"License\": \"Custom\", \"License URL\": \"https://github.com/holistic-video-understanding/HVU-Dataset\"}], \"Creators\": [\"Karlsruhe Institute of Technology\", \"ETH Z\\u00fcrich\", \"KU Leuven\", \"University of Bonn\", \"Sensifai\"], \"Countries\": [\"Switzerland\", \"Belgium\", \"Germany\"], \"License Verified By\": \"Vivek Sharma\", \"Video Hours\": 96166.67, \"Taken Down\": \"False\", \"Video Sources\": \"youtube\", \"Task Categories\": [\"Video Classification\"], \"Inferred Metadata\": {\"GitHub License\": \"\", \"GitHub Stars (June 2024)\": 70, \"GitHub Topics\": [], \"Github Date\": \"\", \"HF Config\": \"\", \"HF Config License\": \"\", \"HF Dataset\": \"\", \"HF Date\": \"\", \"HF Downloads (June 2024)\": \"\", \"HF Likes (June 2024)\": \"\", \"HF Yaml License\": \"\", \"PwC Date\": \"\", \"PwC Description\": \"#11 best model for Action Recognition on UCF101 (3-fold Accuracy metric)\", \"PwC License Name\": \"\", \"PwC License URL\": \"\", \"S2 Citation Count (June 2024)\": \"\", \"S2 Date\": \"\"}, \"Bibtex\": \"@Article{Diba2019HolisticLS,\\n author = {Ali Diba and Mohsen Fayyaz and Vivek Sharma and Manohar Paluri and Juergen Gall and R. Stiefelhagen and L. Gool},\\n booktitle = {arXiv.org},\\n journal = {ArXiv},\\n title = {Holistic Large Scale Video Understanding},\\n volume = {abs/1904.11451},\\n year = {2019}\\n}\\n\", \"License Use (DataProvenance)\": \"academic-only\", \"License Attribution (DataProvenance)\": 1, \"License Share Alike (DataProvenance)\": 0, \"License Use (DataProvenance IgnoreOpenAI)\": \"academic-only\", \"License Attribution (DataProvenance IgnoreOpenAI)\": 1, \"License Share Alike (DataProvenance IgnoreOpenAI)\": 0, \"License Type\": \"Non-Commercial/Academic\"}, {\"Unique Dataset Identifier\": \"tiny-virat\", \"Collection\": \"tiny-virat\", \"Collection URL\": \"https://arxiv.org/abs/2007.07355\", \"Dataset Name\": \"TinyVIRAT: Low-resolution Video Action Recognition\", \"Paper Title\": \"TinyVIRAT: Low-resolution Video Action Recognition\", \"Paper URL\": \"https://arxiv.org/abs/2007.07355\", \"GitHub URL\": \"https://github.com/UgurDemir/Tiny-VIRAT\", \"Hugging Face URL\": \"\", \"Papers with Code URL\": \"https://paperswithcode.com/dataset/tinyvirat\", \"ArXiv URL\": \"https://arxiv.org/abs/2007.07355\", \"Semantic Scholar Corpus ID\": 220525685, \"Year Released\": \"2020\", \"Text Sources\": [\"crowdsourced\"], \"Licenses\": [{\"License\": \"Unspecified\", \"License URL\": \"\"}], \"Creators\": [\"University of Central Florida\"], \"Countries\": [\"United States of America\"], \"License Verified By\": \"Vivek Sharma\", \"Video Hours\": 10.83, \"Taken Down\": \"False\", \"Video Sources\": \"crowdsourced\", \"Task Categories\": [\"Video Classification\"], \"Inferred Metadata\": {\"GitHub License\": \"\", \"GitHub Stars (June 2024)\": 16, \"GitHub Topics\": [], \"Github Date\": \"\", \"HF Config\": \"\", \"HF Config License\": \"\", \"HF Dataset\": \"\", \"HF Date\": \"\", \"HF Downloads (June 2024)\": \"\", \"HF Likes (June 2024)\": \"\", \"HF Yaml License\": \"\", \"PwC Date\": \"\", \"PwC Description\": \"TinyVIRAT contains natural low-resolution activities. The actions in TinyVIRAT videos have multiple labels and they are extracted from surveillance videos which makes them realistic and more challenging.\", \"PwC License Name\": \"Unspecified\", \"PwC License URL\": null, \"S2 Citation Count (June 2024)\": \"\", \"S2 Date\": \"\"}, \"Bibtex\": \"@Article{Demir2020TinyVIRATLV,\\n author = {Ugur Demir and Y. Rawat and M. Shah},\\n booktitle = {International Conference on Pattern Recognition},\\n journal = {2020 25th International Conference on Pattern Recognition (ICPR)},\\n pages = {7387-7394},\\n title = {TinyVIRAT: Low-resolution Video Action Recognition},\\n year = {2020}\\n}\\n\", \"License Use (DataProvenance)\": \"unspecified\", \"License Attribution (DataProvenance)\": 0, \"License Share Alike (DataProvenance)\": 0, \"License Use (DataProvenance IgnoreOpenAI)\": \"unspecified\", \"License Attribution (DataProvenance IgnoreOpenAI)\": 0, \"License Share Alike (DataProvenance IgnoreOpenAI)\": 0, \"License Type\": \"Unspecified\"}, {\"Unique Dataset Identifier\": \"moviescenes\", \"Collection\": \"moviescenes\", \"Collection URL\": \"https://arxiv.org/abs/2004.02678\", \"Dataset Name\": \"MovieScenes\", \"Paper Title\": \"MovieScenes\", \"Paper URL\": \"https://arxiv.org/abs/2004.02678\", \"GitHub URL\": \"https://github.com/AnyiRao/SceneSeg\", \"Hugging Face URL\": \"\", \"Papers with Code URL\": \"https://paperswithcode.com/paper/a-local-to-global-approach-to-multi-modal\", \"ArXiv URL\": \"https://arxiv.org/abs/2004.02678\", \"Semantic Scholar Corpus ID\": 214802984, \"Year Released\": \"2020\", \"Text Sources\": [\"movies\"], \"Licenses\": [{\"License\": \"Unspecified\", \"License URL\": \"\"}], \"Creators\": [\"The Chinese University of Hong Kong\", \"UC Berkeley\"], \"Countries\": [\"China\", \"United States of America\"], \"License Verified By\": \"Vivek Sharma\", \"Video Hours\": 250.0, \"Taken Down\": \"False\", \"Video Sources\": \"movies\", \"Task Categories\": [\"Misc (Scene Segmentation)\"], \"Inferred Metadata\": {\"GitHub License\": \"\", \"GitHub Stars (June 2024)\": 213, \"GitHub Topics\": [\"boundary-detection\", \"scene\", \"segmentation\", \"video-analysis\"], \"Github Date\": \"\", \"HF Config\": \"\", \"HF Config License\": \"\", \"HF Dataset\": \"\", \"HF Date\": \"\", \"HF Downloads (June 2024)\": \"\", \"HF Likes (June 2024)\": \"\", \"HF Yaml License\": \"\", \"PwC Date\": \"\", \"PwC Description\": \"4 code implementations in PyTorch and TensorFlow. Scene, as the crucial unit of storytelling in movies, contains complex activities of actors and their interactions in a physical environment. Identifying the composition of scenes serves as a critical step towards semantic understanding of movies. This is very challenging -- compared to the videos studied in conventional vision problems, e.g. action recognition, as scenes in movies usually contain much richer temporal structures and more complex semantic information. Towards this goal, we scale up the scene segmentation task by building a large-scale video dataset MovieScenes, which contains 21K annotated scene segments from 150 movies. We further propose a local-to-global scene segmentation framework, which integrates multi-modal information across three levels, i.e. clip, segment, and movie. This framework is able to distill complex semantics from hierarchical temporal structures over a long movie, providing top-down guidance for scene segmentation. Our experiments show that the proposed network is able to segment a movie into scenes with high accuracy, consistently outperforming previous methods. We also found that pretraining on our MovieScenes can bring significant improvements to the existing approaches.\", \"PwC License Name\": \"\", \"PwC License URL\": \"\", \"S2 Citation Count (June 2024)\": \"\", \"S2 Date\": \"\"}, \"Bibtex\": \"@Article{Rao2020ALA,\\n author = {Anyi Rao and Linning Xu and Yu Xiong and Guodong Xu and Qingqiu Huang and Bolei Zhou and Dahua Lin},\\n booktitle = {Computer Vision and Pattern Recognition},\\n journal = {2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},\\n pages = {10143-10152},\\n title = {A Local-to-Global Approach to Multi-Modal Movie Scene Segmentation},\\n year = {2020}\\n}\\n\", \"License Use (DataProvenance)\": \"unspecified\", \"License Attribution (DataProvenance)\": 0, \"License Share Alike (DataProvenance)\": 0, \"License Use (DataProvenance IgnoreOpenAI)\": \"unspecified\", \"License Attribution (DataProvenance IgnoreOpenAI)\": 0, \"License Share Alike (DataProvenance IgnoreOpenAI)\": 0, \"License Type\": \"Unspecified\"}, {\"Unique Dataset Identifier\": \"spoken-moments\", \"Collection\": \"spoken-moments\", \"Collection URL\": \"https://arxiv.org/abs/2105.04489\", \"Dataset Name\": \"Spoken Moments\", \"Paper Title\": \"Spoken Moments\", \"Paper URL\": \"https://arxiv.org/abs/2105.04489\", \"GitHub URL\": \"\", \"Hugging Face URL\": \"\", \"Papers with Code URL\": \"https://paperswithcode.com/paper/spoken-moments-learning-joint-audio-visual\", \"ArXiv URL\": \"https://arxiv.org/abs/2105.04489\", \"Semantic Scholar Corpus ID\": 233992735, \"Year Released\": \"2021\", \"Text Sources\": [\"youtube\", \"flickr\", \"vine\", \"metacafe\", \"peeks\", \"vimeo\", \"videoblocks\", \"bing videos\", \"giphy\", \"weather channel\", \"getty-images\"], \"Licenses\": [{\"License\": \"Custom\", \"License URL\": \"https://docs.google.com/forms/d/e/1FAIpQLSdUOScon6uWkslgcA2-BCkAhMp86SihgKtMqX53zRf-E2fIHw/formResponse\"}], \"Creators\": [\"Massachusetts Institute of Technology\", \"IBM\", \"The University of Texas at Austin\"], \"Countries\": [\"United States of America\"], \"License Verified By\": \"Vivek Sharma\", \"Video Hours\": 416.67, \"Taken Down\": \"False\", \"Video Sources\": \"Other\", \"Task Categories\": [\"Video Captioning\"], \"Inferred Metadata\": {\"GitHub License\": \"\", \"GitHub Stars (June 2024)\": \"\", \"GitHub Topics\": \"\", \"Github Date\": \"\", \"HF Config\": \"\", \"HF Config License\": \"\", \"HF Dataset\": \"\", \"HF Date\": \"\", \"HF Downloads (June 2024)\": \"\", \"HF Likes (June 2024)\": \"\", \"HF Yaml License\": \"\", \"PwC Date\": \"\", \"PwC Description\": \"No code available yet.\", \"PwC License Name\": \"\", \"PwC License URL\": \"\", \"S2 Citation Count (June 2024)\": \"\", \"S2 Date\": \"\"}, \"Bibtex\": \"@Article{Monfort2021SpokenML,\\n author = {Mathew Monfort and SouYoung Jin},\\n booktitle = {Computer Vision and Pattern Recognition},\\n journal = {2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},\\n pages = {14866-14876},\\n title = {Spoken Moments: Learning Joint Audio-Visual Representations from Video Descriptions},\\n year = {2021}\\n}\\n\", \"License Use (DataProvenance)\": \"commercial\", \"License Attribution (DataProvenance)\": 0, \"License Share Alike (DataProvenance)\": 0, \"License Use (DataProvenance IgnoreOpenAI)\": \"commercial\", \"License Attribution (DataProvenance IgnoreOpenAI)\": 0, \"License Share Alike (DataProvenance IgnoreOpenAI)\": 0, \"License Type\": \"Commercial\"}, {\"Unique Dataset Identifier\": \"spoken-moments\", \"Collection\": \"spoken-moments\", \"Collection URL\": \"https://arxiv.org/abs/2105.04489\", \"Dataset Name\": \"Spoken Moments\", \"Paper Title\": \"Spoken Moments\", \"Paper URL\": \"https://arxiv.org/abs/2105.04489\", \"GitHub URL\": \"\", \"Hugging Face URL\": \"\", \"Papers with Code URL\": \"https://paperswithcode.com/paper/spoken-moments-learning-joint-audio-visual\", \"ArXiv URL\": \"https://arxiv.org/abs/2105.04489\", \"Semantic Scholar Corpus ID\": 233992735, \"Year Released\": \"2021\", \"Text Sources\": [\"youtube\", \"flickr\", \"vine\", \"metacafe\", \"peeks\", \"vimeo\", \"videoblocks\", \"bing videos\", \"giphy\", \"weather channel\", \"getty-images\"], \"Licenses\": [{\"License\": \"Custom\", \"License URL\": \"https://docs.google.com/forms/d/e/1FAIpQLSdUOScon6uWkslgcA2-BCkAhMp86SihgKtMqX53zRf-E2fIHw/formResponse\"}], \"Creators\": [\"Massachusetts Institute of Technology\", \"IBM\", \"The University of Texas at Austin\"], \"Countries\": [\"United States of America\"], \"License Verified By\": \"Vivek Sharma\", \"Video Hours\": 416.67, \"Taken Down\": \"False\", \"Video Sources\": \"Other\", \"Task Categories\": [\"Video Captioning\"], \"Inferred Metadata\": {\"GitHub License\": \"\", \"GitHub Stars (June 2024)\": \"\", \"GitHub Topics\": \"\", \"Github Date\": \"\", \"HF Config\": \"\", \"HF Config License\": \"\", \"HF Dataset\": \"\", \"HF Date\": \"\", \"HF Downloads (June 2024)\": \"\", \"HF Likes (June 2024)\": \"\", \"HF Yaml License\": \"\", \"PwC Date\": \"\", \"PwC Description\": \"No code available yet.\", \"PwC License Name\": \"\", \"PwC License URL\": \"\", \"S2 Citation Count (June 2024)\": \"\", \"S2 Date\": \"\"}, \"Bibtex\": \"@Article{Monfort2021SpokenML,\\n author = {Mathew Monfort and SouYoung Jin},\\n booktitle = {Computer Vision and Pattern Recognition},\\n journal = {2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},\\n pages = {14866-14876},\\n title = {Spoken Moments: Learning Joint Audio-Visual Representations from Video Descriptions},\\n year = {2021}\\n}\\n\", \"License Use (DataProvenance)\": \"commercial\", \"License Attribution (DataProvenance)\": 0, \"License Share Alike (DataProvenance)\": 0, \"License Use (DataProvenance IgnoreOpenAI)\": \"commercial\", \"License Attribution (DataProvenance IgnoreOpenAI)\": 0, \"License Share Alike (DataProvenance IgnoreOpenAI)\": 0, \"License Type\": \"Commercial\"}, {\"Unique Dataset Identifier\": \"spoken-moments\", \"Collection\": \"spoken-moments\", \"Collection URL\": \"https://arxiv.org/abs/2105.04489\", \"Dataset Name\": \"Spoken Moments\", \"Paper Title\": \"Spoken Moments\", \"Paper URL\": \"https://arxiv.org/abs/2105.04489\", \"GitHub URL\": \"\", \"Hugging Face URL\": \"\", \"Papers with Code URL\": \"https://paperswithcode.com/paper/spoken-moments-learning-joint-audio-visual\", \"ArXiv URL\": \"https://arxiv.org/abs/2105.04489\", \"Semantic Scholar Corpus ID\": 233992735, \"Year Released\": \"2021\", \"Text Sources\": [\"youtube\", \"flickr\", \"vine\", \"metacafe\", \"peeks\", \"vimeo\", \"videoblocks\", \"bing videos\", \"giphy\", \"weather channel\", \"getty-images\"], \"Licenses\": [{\"License\": \"Custom\", \"License URL\": \"https://docs.google.com/forms/d/e/1FAIpQLSdUOScon6uWkslgcA2-BCkAhMp86SihgKtMqX53zRf-E2fIHw/formResponse\"}], \"Creators\": [\"Massachusetts Institute of Technology\", \"IBM\", \"The University of Texas at Austin\"], \"Countries\": [\"United States of America\"], \"License Verified By\": \"Vivek Sharma\", \"Video Hours\": 416.67, \"Taken Down\": \"False\", \"Video Sources\": \"Other\", \"Task Categories\": [\"Video Captioning\"], \"Inferred Metadata\": {\"GitHub License\": \"\", \"GitHub Stars (June 2024)\": \"\", \"GitHub Topics\": \"\", \"Github Date\": \"\", \"HF Config\": \"\", \"HF Config License\": \"\", \"HF Dataset\": \"\", \"HF Date\": \"\", \"HF Downloads (June 2024)\": \"\", \"HF Likes (June 2024)\": \"\", \"HF Yaml License\": \"\", \"PwC Date\": \"\", \"PwC Description\": \"No code available yet.\", \"PwC License Name\": \"\", \"PwC License URL\": \"\", \"S2 Citation Count (June 2024)\": \"\", \"S2 Date\": \"\"}, \"Bibtex\": \"@Article{Monfort2021SpokenML,\\n author = {Mathew Monfort and SouYoung Jin},\\n booktitle = {Computer Vision and Pattern Recognition},\\n journal = {2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},\\n pages = {14866-14876},\\n title = {Spoken Moments: Learning Joint Audio-Visual Representations from Video Descriptions},\\n year = {2021}\\n}\\n\", \"License Use (DataProvenance)\": \"commercial\", \"License Attribution (DataProvenance)\": 0, \"License Share Alike (DataProvenance)\": 0, \"License Use (DataProvenance IgnoreOpenAI)\": \"commercial\", \"License Attribution (DataProvenance IgnoreOpenAI)\": 0, \"License Share Alike (DataProvenance IgnoreOpenAI)\": 0, \"License Type\": \"Commercial\"}, {\"Unique Dataset Identifier\": \"spoken-moments\", \"Collection\": \"spoken-moments\", \"Collection URL\": \"https://arxiv.org/abs/2105.04489\", \"Dataset Name\": \"Spoken Moments\", \"Paper Title\": \"Spoken Moments\", \"Paper URL\": \"https://arxiv.org/abs/2105.04489\", \"GitHub URL\": \"\", \"Hugging Face URL\": \"\", \"Papers with Code URL\": \"https://paperswithcode.com/paper/spoken-moments-learning-joint-audio-visual\", \"ArXiv URL\": \"https://arxiv.org/abs/2105.04489\", \"Semantic Scholar Corpus ID\": 233992735, \"Year Released\": \"2021\", \"Text Sources\": [\"youtube\", \"flickr\", \"vine\", \"metacafe\", \"peeks\", \"vimeo\", \"videoblocks\", \"bing videos\", \"giphy\", \"weather channel\", \"getty-images\"], \"Licenses\": [{\"License\": \"Custom\", \"License URL\": \"https://docs.google.com/forms/d/e/1FAIpQLSdUOScon6uWkslgcA2-BCkAhMp86SihgKtMqX53zRf-E2fIHw/formResponse\"}], \"Creators\": [\"Massachusetts Institute of Technology\", \"IBM\", \"The University of Texas at Austin\"], \"Countries\": [\"United States of America\"], \"License Verified By\": \"Vivek Sharma\", \"Video Hours\": 416.67, \"Taken Down\": \"False\", \"Video Sources\": \"Other\", \"Task Categories\": [\"Video Captioning\"], \"Inferred Metadata\": {\"GitHub License\": \"\", \"GitHub Stars (June 2024)\": \"\", \"GitHub Topics\": \"\", \"Github Date\": \"\", \"HF Config\": \"\", \"HF Config License\": \"\", \"HF Dataset\": \"\", \"HF Date\": \"\", \"HF Downloads (June 2024)\": \"\", \"HF Likes (June 2024)\": \"\", \"HF Yaml License\": \"\", \"PwC Date\": \"\", \"PwC Description\": \"No code available yet.\", \"PwC License Name\": \"\", \"PwC License URL\": \"\", \"S2 Citation Count (June 2024)\": \"\", \"S2 Date\": \"\"}, \"Bibtex\": \"@Article{Monfort2021SpokenML,\\n author = {Mathew Monfort and SouYoung Jin},\\n booktitle = {Computer Vision and Pattern Recognition},\\n journal = {2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},\\n pages = {14866-14876},\\n title = {Spoken Moments: Learning Joint Audio-Visual Representations from Video Descriptions},\\n year = {2021}\\n}\\n\", \"License Use (DataProvenance)\": \"commercial\", \"License Attribution (DataProvenance)\": 0, \"License Share Alike (DataProvenance)\": 0, \"License Use (DataProvenance IgnoreOpenAI)\": \"commercial\", \"License Attribution (DataProvenance IgnoreOpenAI)\": 0, \"License Share Alike (DataProvenance IgnoreOpenAI)\": 0, \"License Type\": \"Commercial\"}, {\"Unique Dataset Identifier\": \"spoken-moments\", \"Collection\": \"spoken-moments\", \"Collection URL\": \"https://arxiv.org/abs/2105.04489\", \"Dataset Name\": \"Spoken Moments\", \"Paper Title\": \"Spoken Moments\", \"Paper URL\": \"https://arxiv.org/abs/2105.04489\", \"GitHub URL\": \"\", \"Hugging Face URL\": \"\", \"Papers with Code URL\": \"https://paperswithcode.com/paper/spoken-moments-learning-joint-audio-visual\", \"ArXiv URL\": \"https://arxiv.org/abs/2105.04489\", \"Semantic Scholar Corpus ID\": 233992735, \"Year Released\": \"2021\", \"Text Sources\": [\"youtube\", \"flickr\", \"vine\", \"metacafe\", \"peeks\", \"vimeo\", \"videoblocks\", \"bing videos\", \"giphy\", \"weather channel\", \"getty-images\"], \"Licenses\": [{\"License\": \"Custom\", \"License URL\": \"https://docs.google.com/forms/d/e/1FAIpQLSdUOScon6uWkslgcA2-BCkAhMp86SihgKtMqX53zRf-E2fIHw/formResponse\"}], \"Creators\": [\"Massachusetts Institute of Technology\", \"IBM\", \"The University of Texas at Austin\"], \"Countries\": [\"United States of America\"], \"License Verified By\": \"Vivek Sharma\", \"Video Hours\": 416.67, \"Taken Down\": \"False\", \"Video Sources\": \"Other\", \"Task Categories\": [\"Video Captioning\"], \"Inferred Metadata\": {\"GitHub License\": \"\", \"GitHub Stars (June 2024)\": \"\", \"GitHub Topics\": \"\", \"Github Date\": \"\", \"HF Config\": \"\", \"HF Config License\": \"\", \"HF Dataset\": \"\", \"HF Date\": \"\", \"HF Downloads (June 2024)\": \"\", \"HF Likes (June 2024)\": \"\", \"HF Yaml License\": \"\", \"PwC Date\": \"\", \"PwC Description\": \"No code available yet.\", \"PwC License Name\": \"\", \"PwC License URL\": \"\", \"S2 Citation Count (June 2024)\": \"\", \"S2 Date\": \"\"}, \"Bibtex\": \"@Article{Monfort2021SpokenML,\\n author = {Mathew Monfort and SouYoung Jin},\\n booktitle = {Computer Vision and Pattern Recognition},\\n journal = {2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},\\n pages = {14866-14876},\\n title = {Spoken Moments: Learning Joint Audio-Visual Representations from Video Descriptions},\\n year = {2021}\\n}\\n\", \"License Use (DataProvenance)\": \"commercial\", \"License Attribution (DataProvenance)\": 0, \"License Share Alike (DataProvenance)\": 0, \"License Use (DataProvenance IgnoreOpenAI)\": \"commercial\", \"License Attribution (DataProvenance IgnoreOpenAI)\": 0, \"License Share Alike (DataProvenance IgnoreOpenAI)\": 0, \"License Type\": \"Commercial\"}, {\"Unique Dataset Identifier\": \"spoken-moments\", \"Collection\": \"spoken-moments\", \"Collection URL\": \"https://arxiv.org/abs/2105.04489\", \"Dataset Name\": \"Spoken Moments\", \"Paper Title\": \"Spoken Moments\", \"Paper URL\": \"https://arxiv.org/abs/2105.04489\", \"GitHub URL\": \"\", \"Hugging Face URL\": \"\", \"Papers with Code URL\": \"https://paperswithcode.com/paper/spoken-moments-learning-joint-audio-visual\", \"ArXiv URL\": \"https://arxiv.org/abs/2105.04489\", \"Semantic Scholar Corpus ID\": 233992735, \"Year Released\": \"2021\", \"Text Sources\": [\"youtube\", \"flickr\", \"vine\", \"metacafe\", \"peeks\", \"vimeo\", \"videoblocks\", \"bing videos\", \"giphy\", \"weather channel\", \"getty-images\"], \"Licenses\": [{\"License\": \"Custom\", \"License URL\": \"https://docs.google.com/forms/d/e/1FAIpQLSdUOScon6uWkslgcA2-BCkAhMp86SihgKtMqX53zRf-E2fIHw/formResponse\"}], \"Creators\": [\"Massachusetts Institute of Technology\", \"IBM\", \"The University of Texas at Austin\"], \"Countries\": [\"United States of America\"], \"License Verified By\": \"Vivek Sharma\", \"Video Hours\": 416.67, \"Taken Down\": \"False\", \"Video Sources\": \"Other\", \"Task Categories\": [\"Video Captioning\"], \"Inferred Metadata\": {\"GitHub License\": \"\", \"GitHub Stars (June 2024)\": \"\", \"GitHub Topics\": \"\", \"Github Date\": \"\", \"HF Config\": \"\", \"HF Config License\": \"\", \"HF Dataset\": \"\", \"HF Date\": \"\", \"HF Downloads (June 2024)\": \"\", \"HF Likes (June 2024)\": \"\", \"HF Yaml License\": \"\", \"PwC Date\": \"\", \"PwC Description\": \"No code available yet.\", \"PwC License Name\": \"\", \"PwC License URL\": \"\", \"S2 Citation Count (June 2024)\": \"\", \"S2 Date\": \"\"}, \"Bibtex\": \"@Article{Monfort2021SpokenML,\\n author = {Mathew Monfort and SouYoung Jin},\\n booktitle = {Computer Vision and Pattern Recognition},\\n journal = {2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},\\n pages = {14866-14876},\\n title = {Spoken Moments: Learning Joint Audio-Visual Representations from Video Descriptions},\\n year = {2021}\\n}\\n\", \"License Use (DataProvenance)\": \"commercial\", \"License Attribution (DataProvenance)\": 0, \"License Share Alike (DataProvenance)\": 0, \"License Use (DataProvenance IgnoreOpenAI)\": \"commercial\", \"License Attribution (DataProvenance IgnoreOpenAI)\": 0, \"License Share Alike (DataProvenance IgnoreOpenAI)\": 0, \"License Type\": \"Commercial\"}, {\"Unique Dataset Identifier\": \"spoken-moments\", \"Collection\": \"spoken-moments\", \"Collection URL\": \"https://arxiv.org/abs/2105.04489\", \"Dataset Name\": \"Spoken Moments\", \"Paper Title\": \"Spoken Moments\", \"Paper URL\": \"https://arxiv.org/abs/2105.04489\", \"GitHub URL\": \"\", \"Hugging Face URL\": \"\", \"Papers with Code URL\": \"https://paperswithcode.com/paper/spoken-moments-learning-joint-audio-visual\", \"ArXiv URL\": \"https://arxiv.org/abs/2105.04489\", \"Semantic Scholar Corpus ID\": 233992735, \"Year Released\": \"2021\", \"Text Sources\": [\"youtube\", \"flickr\", \"vine\", \"metacafe\", \"peeks\", \"vimeo\", \"videoblocks\", \"bing videos\", \"giphy\", \"weather channel\", \"getty-images\"], \"Licenses\": [{\"License\": \"Custom\", \"License URL\": \"https://docs.google.com/forms/d/e/1FAIpQLSdUOScon6uWkslgcA2-BCkAhMp86SihgKtMqX53zRf-E2fIHw/formResponse\"}], \"Creators\": [\"Massachusetts Institute of Technology\", \"IBM\", \"The University of Texas at Austin\"], \"Countries\": [\"United States of America\"], \"License Verified By\": \"Vivek Sharma\", \"Video Hours\": 416.67, \"Taken Down\": \"False\", \"Video Sources\": \"Other\", \"Task Categories\": [\"Video Captioning\"], \"Inferred Metadata\": {\"GitHub License\": \"\", \"GitHub Stars (June 2024)\": \"\", \"GitHub Topics\": \"\", \"Github Date\": \"\", \"HF Config\": \"\", \"HF Config License\": \"\", \"HF Dataset\": \"\", \"HF Date\": \"\", \"HF Downloads (June 2024)\": \"\", \"HF Likes (June 2024)\": \"\", \"HF Yaml License\": \"\", \"PwC Date\": \"\", \"PwC Description\": \"No code available yet.\", \"PwC License Name\": \"\", \"PwC License URL\": \"\", \"S2 Citation Count (June 2024)\": \"\", \"S2 Date\": \"\"}, \"Bibtex\": \"@Article{Monfort2021SpokenML,\\n author = {Mathew Monfort and SouYoung Jin},\\n booktitle = {Computer Vision and Pattern Recognition},\\n journal = {2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},\\n pages = {14866-14876},\\n title = {Spoken Moments: Learning Joint Audio-Visual Representations from Video Descriptions},\\n year = {2021}\\n}\\n\", \"License Use (DataProvenance)\": \"commercial\", \"License Attribution (DataProvenance)\": 0, \"License Share Alike (DataProvenance)\": 0, \"License Use (DataProvenance IgnoreOpenAI)\": \"commercial\", \"License Attribution (DataProvenance IgnoreOpenAI)\": 0, \"License Share Alike (DataProvenance IgnoreOpenAI)\": 0, \"License Type\": \"Commercial\"}, {\"Unique Dataset Identifier\": \"spoken-moments\", \"Collection\": \"spoken-moments\", \"Collection URL\": \"https://arxiv.org/abs/2105.04489\", \"Dataset Name\": \"Spoken Moments\", \"Paper Title\": \"Spoken Moments\", \"Paper URL\": \"https://arxiv.org/abs/2105.04489\", \"GitHub URL\": \"\", \"Hugging Face URL\": \"\", \"Papers with Code URL\": \"https://paperswithcode.com/paper/spoken-moments-learning-joint-audio-visual\", \"ArXiv URL\": \"https://arxiv.org/abs/2105.04489\", \"Semantic Scholar Corpus ID\": 233992735, \"Year Released\": \"2021\", \"Text Sources\": [\"youtube\", \"flickr\", \"vine\", \"metacafe\", \"peeks\", \"vimeo\", \"videoblocks\", \"bing videos\", \"giphy\", \"weather channel\", \"getty-images\"], \"Licenses\": [{\"License\": \"Custom\", \"License URL\": \"https://docs.google.com/forms/d/e/1FAIpQLSdUOScon6uWkslgcA2-BCkAhMp86SihgKtMqX53zRf-E2fIHw/formResponse\"}], \"Creators\": [\"Massachusetts Institute of Technology\", \"IBM\", \"The University of Texas at Austin\"], \"Countries\": [\"United States of America\"], \"License Verified By\": \"Vivek Sharma\", \"Video Hours\": 416.67, \"Taken Down\": \"False\", \"Video Sources\": \"Other\", \"Task Categories\": [\"Video Captioning\"], \"Inferred Metadata\": {\"GitHub License\": \"\", \"GitHub Stars (June 2024)\": \"\", \"GitHub Topics\": \"\", \"Github Date\": \"\", \"HF Config\": \"\", \"HF Config License\": \"\", \"HF Dataset\": \"\", \"HF Date\": \"\", \"HF Downloads (June 2024)\": \"\", \"HF Likes (June 2024)\": \"\", \"HF Yaml License\": \"\", \"PwC Date\": \"\", \"PwC Description\": \"No code available yet.\", \"PwC License Name\": \"\", \"PwC License URL\": \"\", \"S2 Citation Count (June 2024)\": \"\", \"S2 Date\": \"\"}, \"Bibtex\": \"@Article{Monfort2021SpokenML,\\n author = {Mathew Monfort and SouYoung Jin},\\n booktitle = {Computer Vision and Pattern Recognition},\\n journal = {2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},\\n pages = {14866-14876},\\n title = {Spoken Moments: Learning Joint Audio-Visual Representations from Video Descriptions},\\n year = {2021}\\n}\\n\", \"License Use (DataProvenance)\": \"commercial\", \"License Attribution (DataProvenance)\": 0, \"License Share Alike (DataProvenance)\": 0, \"License Use (DataProvenance IgnoreOpenAI)\": \"commercial\", \"License Attribution (DataProvenance IgnoreOpenAI)\": 0, \"License Share Alike (DataProvenance IgnoreOpenAI)\": 0, \"License Type\": \"Commercial\"}, {\"Unique Dataset Identifier\": \"homage\", \"Collection\": \"homage\", \"Collection URL\": \"https://arxiv.org/abs/2105.05226\", \"Dataset Name\": \"HOMAGE: Home Action Genome: Cooperative Compositional Action Understanding (CVPR 2021)\", \"Paper Title\": \"HOMAGE: Home Action Genome: Cooperative Compositional Action Understanding (CVPR 2021)\", \"Paper URL\": \"https://arxiv.org/abs/2105.05226\", \"GitHub URL\": \"https://github.com/nishantrai18/homage\", \"Hugging Face URL\": \"\", \"Papers with Code URL\": \"https://paperswithcode.com/paper/home-action-genome-cooperative-compositional\", \"ArXiv URL\": \"https://arxiv.org/abs/2105.05226\", \"Semantic Scholar Corpus ID\": 234357543, \"Year Released\": \"2021\", \"Text Sources\": [\"human\"], \"Licenses\": [{\"License\": \"Unspecified\", \"License URL\": \"\"}], \"Creators\": [\"Stanford University\", \"Panasonic Corporation\"], \"Countries\": [\"United States of America\"], \"License Verified By\": \"Vivek Sharma\", \"Video Hours\": 30.0, \"Taken Down\": \"False\", \"Video Sources\": \"human\", \"Task Categories\": [\"Video Classification\", \"Temporal Action Segmentation\"], \"Inferred Metadata\": {\"GitHub License\": \"MIT License\", \"GitHub Stars (June 2024)\": 17, \"GitHub Topics\": [], \"Github Date\": \"\", \"HF Config\": \"\", \"HF Config License\": \"\", \"HF Dataset\": \"\", \"HF Date\": \"\", \"HF Downloads (June 2024)\": \"\", \"HF Likes (June 2024)\": \"\", \"HF Yaml License\": \"\", \"PwC Date\": \"\", \"PwC Description\": \"\\ud83c\\udfc6 SOTA for Video Classification on Home Action Genome (Accuracy (%) metric)\", \"PwC License Name\": \"\", \"PwC License URL\": \"\", \"S2 Citation Count (June 2024)\": \"\", \"S2 Date\": \"\"}, \"Bibtex\": \"@Article{Rai2021HomeAG,\\n author = {Nishant Rai and Haofeng Chen and Jingwei Ji and Rishi Desai and K. Kozuka and Shun Ishizaka and E. Adeli and Juan Carlos Niebles},\\n booktitle = {Computer Vision and Pattern Recognition},\\n journal = {2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},\\n pages = {11179-11188},\\n title = {Home Action Genome: Cooperative Compositional Action Understanding},\\n year = {2021}\\n}\\n\", \"License Use (DataProvenance)\": \"unspecified\", \"License Attribution (DataProvenance)\": 0, \"License Share Alike (DataProvenance)\": 0, \"License Use (DataProvenance IgnoreOpenAI)\": \"unspecified\", \"License Attribution (DataProvenance IgnoreOpenAI)\": 0, \"License Share Alike (DataProvenance IgnoreOpenAI)\": 0, \"License Type\": \"Unspecified\"}, {\"Unique Dataset Identifier\": \"queryd\", \"Collection\": \"queryd\", \"Collection URL\": \"https://arxiv.org/abs/2011.11071\", \"Dataset Name\": \"QuerYD\", \"Paper Title\": \"QuerYD\", \"Paper URL\": \"https://arxiv.org/abs/2011.11071\", \"GitHub URL\": \"\", \"Hugging Face URL\": \"\", \"Papers with Code URL\": \"https://paperswithcode.com/dataset/queryd\", \"ArXiv URL\": \"https://arxiv.org/abs/2011.11071\", \"Semantic Scholar Corpus ID\": 261006321, \"Year Released\": \"2021\", \"Text Sources\": [\"youtube\", \"youdescribe\"], \"Licenses\": [{\"License\": \"Unspecified\", \"License URL\": \"\"}], \"Creators\": [\"University of Oxford\"], \"Countries\": [\"United Kingdom\"], \"License Verified By\": \"Vivek Sharma\", \"Video Hours\": 207.0, \"Taken Down\": \"False\", \"Video Sources\": \"Other\", \"Task Categories\": [\"Video Captioning\"], \"Inferred Metadata\": {\"GitHub License\": \"\", \"GitHub Stars (June 2024)\": \"\", \"GitHub Topics\": \"\", \"Github Date\": \"\", \"HF Config\": \"\", \"HF Config License\": \"\", \"HF Dataset\": \"\", \"HF Date\": \"\", \"HF Downloads (June 2024)\": \"\", \"HF Likes (June 2024)\": \"\", \"HF Yaml License\": \"\", \"PwC Date\": \"\", \"PwC Description\": \"A large-scale dataset for retrieval and event localisation in video. A unique feature of the dataset is the availability of two audio tracks for each video: the original audio, and a high-quality spoken description of the visual content.\", \"PwC License Name\": \"Unspecified\", \"PwC License URL\": null, \"S2 Citation Count (June 2024)\": \"\", \"S2 Date\": \"\"}, \"Bibtex\": \"@Article{Oncescu2021QUERYDAV,\\n author = {Andreea-Maria Oncescu and Jo\\u00e3o F. Henriques and Yang Liu and Andrew Zisserman and Samuel Albanie},\\n booktitle = {IEEE International Conference on Acoustics, Speech, and Signal Processing},\\n journal = {ICASSP 2021 - 2021 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},\\n pages = {2265-2269},\\n title = {QUERYD: A Video Dataset with High-Quality Text and Audio Narrations},\\n year = {2021}\\n}\\n\", \"License Use (DataProvenance)\": \"unspecified\", \"License Attribution (DataProvenance)\": 0, \"License Share Alike (DataProvenance)\": 0, \"License Use (DataProvenance IgnoreOpenAI)\": \"unspecified\", \"License Attribution (DataProvenance IgnoreOpenAI)\": 0, \"License Share Alike (DataProvenance IgnoreOpenAI)\": 0, \"License Type\": \"Unspecified\"}, {\"Unique Dataset Identifier\": \"queryd\", \"Collection\": \"queryd\", \"Collection URL\": \"https://arxiv.org/abs/2011.11071\", \"Dataset Name\": \"QuerYD\", \"Paper Title\": \"QuerYD\", \"Paper URL\": \"https://arxiv.org/abs/2011.11071\", \"GitHub URL\": \"\", \"Hugging Face URL\": \"\", \"Papers with Code URL\": \"https://paperswithcode.com/dataset/queryd\", \"ArXiv URL\": \"https://arxiv.org/abs/2011.11071\", \"Semantic Scholar Corpus ID\": 261006321, \"Year Released\": \"2021\", \"Text Sources\": [\"youtube\", \"youdescribe\"], \"Licenses\": [{\"License\": \"Unspecified\", \"License URL\": \"\"}], \"Creators\": [\"University of Oxford\"], \"Countries\": [\"United Kingdom\"], \"License Verified By\": \"Vivek Sharma\", \"Video Hours\": 207.0, \"Taken Down\": \"False\", \"Video Sources\": \"youtube\", \"Task Categories\": [\"Video Captioning\"], \"Inferred Metadata\": {\"GitHub License\": \"\", \"GitHub Stars (June 2024)\": \"\", \"GitHub Topics\": \"\", \"Github Date\": \"\", \"HF Config\": \"\", \"HF Config License\": \"\", \"HF Dataset\": \"\", \"HF Date\": \"\", \"HF Downloads (June 2024)\": \"\", \"HF Likes (June 2024)\": \"\", \"HF Yaml License\": \"\", \"PwC Date\": \"\", \"PwC Description\": \"A large-scale dataset for retrieval and event localisation in video. A unique feature of the dataset is the availability of two audio tracks for each video: the original audio, and a high-quality spoken description of the visual content.\", \"PwC License Name\": \"Unspecified\", \"PwC License URL\": null, \"S2 Citation Count (June 2024)\": \"\", \"S2 Date\": \"\"}, \"Bibtex\": \"@Article{Oncescu2021QUERYDAV,\\n author = {Andreea-Maria Oncescu and Jo\\u00e3o F. Henriques and Yang Liu and Andrew Zisserman and Samuel Albanie},\\n booktitle = {IEEE International Conference on Acoustics, Speech, and Signal Processing},\\n journal = {ICASSP 2021 - 2021 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},\\n pages = {2265-2269},\\n title = {QUERYD: A Video Dataset with High-Quality Text and Audio Narrations},\\n year = {2021}\\n}\\n\", \"License Use (DataProvenance)\": \"unspecified\", \"License Attribution (DataProvenance)\": 0, \"License Share Alike (DataProvenance)\": 0, \"License Use (DataProvenance IgnoreOpenAI)\": \"unspecified\", \"License Attribution (DataProvenance IgnoreOpenAI)\": 0, \"License Share Alike (DataProvenance IgnoreOpenAI)\": 0, \"License Type\": \"Unspecified\"}, {\"Unique Dataset Identifier\": \"videolt-dataset\", \"Collection\": \"videolt-dataset\", \"Collection URL\": \"https://arxiv.org/abs/2105.02668\", \"Dataset Name\": \"VideoLT: Large-scale Long-tailed Video Recognition\", \"Paper Title\": \"VideoLT: Large-scale Long-tailed Video Recognition\", \"Paper URL\": \"https://arxiv.org/abs/2105.02668\", \"GitHub URL\": \"https://github.com/17Skye17/VideoLT\", \"Hugging Face URL\": \"\", \"Papers with Code URL\": \"https://paperswithcode.com/dataset/videolt\", \"ArXiv URL\": \"https://arxiv.org/abs/2105.02668\", \"Semantic Scholar Corpus ID\": 233864776, \"Year Released\": \"2021\", \"Text Sources\": [\"youtube\"], \"Licenses\": [{\"License\": \"Non Commercial\", \"License URL\": \"https://github.com/17Skye17/VideoLT?tab=readme-ov-file#data-preparation\"}], \"Creators\": [\"Fudan University\", \"Shanghai Collaborative Innovation Center of Intelligent Visual Computing\", \"Inception Institute of Artificial Intelligence\", \"University of Maryland\"], \"Countries\": [\"China\", \"UAE\"], \"License Verified By\": \"Vivek Sharma\", \"Video Hours\": 13664.96, \"Taken Down\": \"False\", \"Video Sources\": \"youtube\", \"Task Categories\": [\"Video Classification\"], \"Inferred Metadata\": {\"GitHub License\": \"\", \"GitHub Stars (June 2024)\": 33, \"GitHub Topics\": [\"long-tailed-recognition\", \"video-classification\", \"video-dataset\"], \"Github Date\": \"\", \"HF Config\": \"\", \"HF Config License\": \"\", \"HF Dataset\": \"\", \"HF Date\": \"\", \"HF Downloads (June 2024)\": \"\", \"HF Likes (June 2024)\": \"\", \"HF Yaml License\": \"\", \"PwC Date\": \"2021-05-06\", \"PwC Description\": \"VideoLT is a large-scale long-tailed video recognition dataset that contains 256,218 untrimmed videos, annotated into 1,004 classes with a long-tailed distribution.\", \"PwC License Name\": \"Unspecified\", \"PwC License URL\": null, \"S2 Citation Count (June 2024)\": \"\", \"S2 Date\": \"\"}, \"Bibtex\": \"@Article{Zhang2021VideoLTLL,\\n author = {Xing Zhang and Zuxuan Wu and Zejia Weng and H. Fu and Jingjing Chen and Yu-Gang Jiang and Larry S. Davis},\\n booktitle = {IEEE International Conference on Computer Vision},\\n journal = {2021 IEEE/CVF International Conference on Computer Vision (ICCV)},\\n pages = {7940-7949},\\n title = {VideoLT: Large-scale Long-tailed Video Recognition},\\n year = {2021}\\n}\\n\", \"License Use (DataProvenance)\": \"non-commercial\", \"License Attribution (DataProvenance)\": 0, \"License Share Alike (DataProvenance)\": 0, \"License Use (DataProvenance IgnoreOpenAI)\": \"non-commercial\", \"License Attribution (DataProvenance IgnoreOpenAI)\": 0, \"License Share Alike (DataProvenance IgnoreOpenAI)\": 0, \"License Type\": \"Non-Commercial/Academic\"}, {\"Unique Dataset Identifier\": \"spoken-moments\", \"Collection\": \"spoken-moments\", \"Collection URL\": \"https://arxiv.org/abs/2105.04489\", \"Dataset Name\": \"Spoken Moments\", \"Paper Title\": \"Spoken Moments\", \"Paper URL\": \"https://arxiv.org/abs/2105.04489\", \"GitHub URL\": \"\", \"Hugging Face URL\": \"\", \"Papers with Code URL\": \"https://paperswithcode.com/paper/spoken-moments-learning-joint-audio-visual\", \"ArXiv URL\": \"https://arxiv.org/abs/2105.04489\", \"Semantic Scholar Corpus ID\": 233992735, \"Year Released\": \"2021\", \"Text Sources\": [\"youtube\", \"flickr\", \"vine\", \"metacafe\", \"peeks\", \"vimeo\", \"videoblocks\", \"bing videos\", \"giphy\", \"weather channel\", \"getty-images\"], \"Licenses\": [{\"License\": \"Custom\", \"License URL\": \"https://docs.google.com/forms/d/e/1FAIpQLSdUOScon6uWkslgcA2-BCkAhMp86SihgKtMqX53zRf-E2fIHw/formResponse\"}], \"Creators\": [\"Massachusetts Institute of Technology\", \"IBM\", \"The University of Texas at Austin\"], \"Countries\": [\"United States of America\"], \"License Verified By\": \"Vivek Sharma\", \"Video Hours\": 416.67, \"Taken Down\": \"False\", \"Video Sources\": \"Other\", \"Task Categories\": [\"Video Captioning\"], \"Inferred Metadata\": {\"GitHub License\": \"\", \"GitHub Stars (June 2024)\": \"\", \"GitHub Topics\": \"\", \"Github Date\": \"\", \"HF Config\": \"\", \"HF Config License\": \"\", \"HF Dataset\": \"\", \"HF Date\": \"\", \"HF Downloads (June 2024)\": \"\", \"HF Likes (June 2024)\": \"\", \"HF Yaml License\": \"\", \"PwC Date\": \"\", \"PwC Description\": \"No code available yet.\", \"PwC License Name\": \"\", \"PwC License URL\": \"\", \"S2 Citation Count (June 2024)\": \"\", \"S2 Date\": \"\"}, \"Bibtex\": \"@Article{Monfort2021SpokenML,\\n author = {Mathew Monfort and SouYoung Jin},\\n booktitle = {Computer Vision and Pattern Recognition},\\n journal = {2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},\\n pages = {14866-14876},\\n title = {Spoken Moments: Learning Joint Audio-Visual Representations from Video Descriptions},\\n year = {2021}\\n}\\n\", \"License Use (DataProvenance)\": \"commercial\", \"License Attribution (DataProvenance)\": 0, \"License Share Alike (DataProvenance)\": 0, \"License Use (DataProvenance IgnoreOpenAI)\": \"commercial\", \"License Attribution (DataProvenance IgnoreOpenAI)\": 0, \"License Share Alike (DataProvenance IgnoreOpenAI)\": 0, \"License Type\": \"Commercial\"}, {\"Unique Dataset Identifier\": \"spoken-moments\", \"Collection\": \"spoken-moments\", \"Collection URL\": \"https://arxiv.org/abs/2105.04489\", \"Dataset Name\": \"Spoken Moments\", \"Paper Title\": \"Spoken Moments\", \"Paper URL\": \"https://arxiv.org/abs/2105.04489\", \"GitHub URL\": \"\", \"Hugging Face URL\": \"\", \"Papers with Code URL\": \"https://paperswithcode.com/paper/spoken-moments-learning-joint-audio-visual\", \"ArXiv URL\": \"https://arxiv.org/abs/2105.04489\", \"Semantic Scholar Corpus ID\": 233992735, \"Year Released\": \"2021\", \"Text Sources\": [\"youtube\", \"flickr\", \"vine\", \"metacafe\", \"peeks\", \"vimeo\", \"videoblocks\", \"bing videos\", \"giphy\", \"weather channel\", \"getty-images\"], \"Licenses\": [{\"License\": \"Custom\", \"License URL\": \"https://docs.google.com/forms/d/e/1FAIpQLSdUOScon6uWkslgcA2-BCkAhMp86SihgKtMqX53zRf-E2fIHw/formResponse\"}], \"Creators\": [\"Massachusetts Institute of Technology\", \"IBM\", \"The University of Texas at Austin\"], \"Countries\": [\"United States of America\"], \"License Verified By\": \"Vivek Sharma\", \"Video Hours\": 416.67, \"Taken Down\": \"False\", \"Video Sources\": \"flickr\", \"Task Categories\": [\"Video Captioning\"], \"Inferred Metadata\": {\"GitHub License\": \"\", \"GitHub Stars (June 2024)\": \"\", \"GitHub Topics\": \"\", \"Github Date\": \"\", \"HF Config\": \"\", \"HF Config License\": \"\", \"HF Dataset\": \"\", \"HF Date\": \"\", \"HF Downloads (June 2024)\": \"\", \"HF Likes (June 2024)\": \"\", \"HF Yaml License\": \"\", \"PwC Date\": \"\", \"PwC Description\": \"No code available yet.\", \"PwC License Name\": \"\", \"PwC License URL\": \"\", \"S2 Citation Count (June 2024)\": \"\", \"S2 Date\": \"\"}, \"Bibtex\": \"@Article{Monfort2021SpokenML,\\n author = {Mathew Monfort and SouYoung Jin},\\n booktitle = {Computer Vision and Pattern Recognition},\\n journal = {2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},\\n pages = {14866-14876},\\n title = {Spoken Moments: Learning Joint Audio-Visual Representations from Video Descriptions},\\n year = {2021}\\n}\\n\", \"License Use (DataProvenance)\": \"commercial\", \"License Attribution (DataProvenance)\": 0, \"License Share Alike (DataProvenance)\": 0, \"License Use (DataProvenance IgnoreOpenAI)\": \"commercial\", \"License Attribution (DataProvenance IgnoreOpenAI)\": 0, \"License Share Alike (DataProvenance IgnoreOpenAI)\": 0, \"License Type\": \"Commercial\"}, {\"Unique Dataset Identifier\": \"hd-vila-100m\", \"Collection\": \"hd-vila-100m\", \"Collection URL\": \"https://arxiv.org/abs/2111.10337\", \"Dataset Name\": \"Advancing High-Resolution Video-Language Representation with Large-Scale Video Transcriptions\", \"Paper Title\": \"Advancing High-Resolution Video-Language Representation with Large-Scale Video Transcriptions\", \"Paper URL\": \"https://arxiv.org/abs/2111.10337\", \"GitHub URL\": \"https://github.com/microsoft/XPretrain/blob/main/hd-vila-100m/README.md\", \"Hugging Face URL\": \"\", \"Papers with Code URL\": \"https://paperswithcode.com/paper/advancing-high-resolution-video-language/review/\", \"ArXiv URL\": \"https://arxiv.org/abs/2111.10337\", \"Semantic Scholar Corpus ID\": 244462849, \"Year Released\": \"2021\", \"Text Sources\": [\"youtube\"], \"Licenses\": [{\"License\": \"Custom\", \"License URL\": \"https://github.com/microsoft/XPretrain/blob/main/hd-vila-100m/README.md\"}], \"Creators\": [\"Microsoft\"], \"Countries\": [\"United States of America\"], \"License Verified By\": \"Vivek Sharma\", \"Video Hours\": 371.5, \"Taken Down\": \"False\", \"Video Sources\": \"youtube\", \"Task Categories\": [\"Video Captioning\"], \"Inferred Metadata\": {\"GitHub License\": \"\", \"GitHub Stars (June 2024)\": 0, \"GitHub Topics\": [\"\"], \"Github Date\": \"\", \"HF Config\": \"\", \"HF Config License\": \"\", \"HF Dataset\": \"\", \"HF Date\": \"\", \"HF Downloads (June 2024)\": \"\", \"HF Likes (June 2024)\": \"\", \"HF Yaml License\": \"\", \"PwC Date\": \"\", \"PwC Description\": \"Paper tables with annotated results for Advancing High-Resolution Video-Language Representation with Large-Scale Video Transcriptions\", \"PwC License Name\": \"\", \"PwC License URL\": \"\", \"S2 Citation Count (June 2024)\": \"\", \"S2 Date\": \"\"}, \"Bibtex\": \"@Article{Xue2021AdvancingHV,\\n author = {Hongwei Xue and Tiankai Hang and Yanhong Zeng and Yuchong Sun and Bei Liu and Huan Yang and Jianlong Fu and B. Guo},\\n booktitle = {Computer Vision and Pattern Recognition},\\n journal = {2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},\\n pages = {5026-5035},\\n title = {Advancing High-Resolution Video-Language Representation with Large-Scale Video Transcriptions},\\n year = {2021}\\n}\\n\", \"License Use (DataProvenance)\": \"academic-only\", \"License Attribution (DataProvenance)\": 1, \"License Share Alike (DataProvenance)\": 0, \"License Use (DataProvenance IgnoreOpenAI)\": \"academic-only\", \"License Attribution (DataProvenance IgnoreOpenAI)\": 1, \"License Share Alike (DataProvenance IgnoreOpenAI)\": 0, \"License Type\": \"Non-Commercial/Academic\"}, {\"Unique Dataset Identifier\": \"YT-Temporal-180m\", \"Collection\": \"YT-Temporal-180m\", \"Collection URL\": \"https://arxiv.org/pdf/2106.02636\", \"Dataset Name\": \"MERLOT: Multimodal Neural Script Knowledge Models\", \"Paper Title\": \"MERLOT: Multimodal Neural Script Knowledge Models\", \"Paper URL\": \"https://arxiv.org/pdf/2106.02636\", \"GitHub URL\": \"https://github.com/rowanz/merlot\", \"Hugging Face URL\": \"https://huggingface.co/datasets/HuggingFaceM4/yttemporal180m\", \"Papers with Code URL\": \"https://paperswithcode.com/paper/merlot-multimodal-neural-script-knowledge\", \"ArXiv URL\": \"https://arxiv.org/pdf/2106.02636\", \"Semantic Scholar Corpus ID\": 235352775, \"Year Released\": \"2021\", \"Text Sources\": [\"youtube\"], \"Licenses\": [{\"License\": \"MIT License\", \"License URL\": \"https://github.com/rowanz/merlot/blob/main/LICENSE\"}], \"Creators\": [\"University of Washington\", \"Allen Institute for AI\"], \"Countries\": [\"United States of America\"], \"License Verified By\": \"Vivek Sharma\", \"Video Hours\": 1515.0, \"Taken Down\": \"False\", \"Video Sources\": \"youtube\", \"Task Categories\": [\"Video Q&A\"], \"Inferred Metadata\": null, \"Bibtex\": null, \"License Use (DataProvenance)\": \"commercial\", \"License Attribution (DataProvenance)\": 1, \"License Share Alike (DataProvenance)\": 0, \"License Use (DataProvenance IgnoreOpenAI)\": \"commercial\", \"License Attribution (DataProvenance IgnoreOpenAI)\": 1, \"License Share Alike (DataProvenance IgnoreOpenAI)\": 0, \"License Type\": \"Commercial\"}, {\"Unique Dataset Identifier\": \"mimetics-dataset\", \"Collection\": \"mimetics-dataset\", \"Collection URL\": \"https://arxiv.org/abs/1912.07249\", \"Dataset Name\": \"Mimetics: Towards Understanding Human Actions Out of Context\", \"Paper Title\": \"Mimetics: Towards Understanding Human Actions Out of Context\", \"Paper URL\": \"https://arxiv.org/abs/1912.07249\", \"GitHub URL\": \"\", \"Hugging Face URL\": \"\", \"Papers with Code URL\": \"https://paperswithcode.com/paper/mimetics-towards-understanding-human-actions\", \"ArXiv URL\": \"https://arxiv.org/abs/1912.07249\", \"Semantic Scholar Corpus ID\": 209376248, \"Year Released\": \"2021\", \"Text Sources\": [\"youtube\"], \"Licenses\": [{\"License\": \"Unspecified\", \"License URL\": \"\"}], \"Creators\": [\"Naver\"], \"Countries\": [\"South Korea\"], \"License Verified By\": \"Vivek Sharma\", \"Video Hours\": 0.99, \"Taken Down\": \"False\", \"Video Sources\": \"youtube\", \"Task Categories\": [\"Video Classification\"], \"Inferred Metadata\": {\"GitHub License\": \"\", \"GitHub Stars (June 2024)\": \"\", \"GitHub Topics\": \"\", \"Github Date\": \"\", \"HF Config\": \"\", \"HF Config License\": \"\", \"HF Dataset\": \"\", \"HF Date\": \"\", \"HF Downloads (June 2024)\": \"\", \"HF Likes (June 2024)\": \"\", \"HF Yaml License\": \"\", \"PwC Date\": \"\", \"PwC Description\": \"No code available yet.\", \"PwC License Name\": \"\", \"PwC License URL\": \"\", \"S2 Citation Count (June 2024)\": \"\", \"S2 Date\": \"\"}, \"Bibtex\": \"@Article{Weinzaepfel2019MimeticsTU,\\n author = {Philippe Weinzaepfel and Gr\\u00e9gory Rogez},\\n booktitle = {International Journal of Computer Vision},\\n journal = {International Journal of Computer Vision},\\n pages = {1675 - 1690},\\n title = {Mimetics: Towards Understanding Human Actions Out of Context},\\n volume = {129},\\n year = {2019}\\n}\\n\", \"License Use (DataProvenance)\": \"unspecified\", \"License Attribution (DataProvenance)\": 0, \"License Share Alike (DataProvenance)\": 0, \"License Use (DataProvenance IgnoreOpenAI)\": \"unspecified\", \"License Attribution (DataProvenance IgnoreOpenAI)\": 0, \"License Share Alike (DataProvenance IgnoreOpenAI)\": 0, \"License Type\": \"Unspecified\"}, {\"Unique Dataset Identifier\": \"apes\", \"Collection\": \"apes\", \"Collection URL\": \"https://arxiv.org/pdf/2106.01667\", \"Dataset Name\": \"Apes\", \"Paper Title\": \"Apes\", \"Paper URL\": \"https://arxiv.org/pdf/2106.01667\", \"GitHub URL\": \"https://github.com/fuankarion/audiovisual-person-search\", \"Hugging Face URL\": \"\", \"Papers with Code URL\": \"https://paperswithcode.com/paper/apes-audiovisual-person-search-in-untrimmed/review/\", \"ArXiv URL\": \"https://arxiv.org/pdf/2106.01667\", \"Semantic Scholar Corpus ID\": 235313698, \"Year Released\": \"2021\", \"Text Sources\": [\"movies\"], \"Licenses\": [{\"License\": \"Unspecified\", \"License URL\": \"\"}], \"Creators\": [\"Universidad de los Andes\", \"Adobe Research\", \"King Abdullah University of Science and Technology (KAUST)\"], \"Countries\": [\"Chile\", \"United States of America\", \"Saudi Arabia\"], \"License Verified By\": \"Vivek Sharma\", \"Video Hours\": 36.0, \"Taken Down\": \"False\", \"Video Sources\": \"movies\", \"Task Categories\": [\"Video Classification\"], \"Inferred Metadata\": {\"GitHub License\": \"\", \"GitHub Stars (June 2024)\": 4, \"GitHub Topics\": [], \"Github Date\": \"\", \"HF Config\": \"\", \"HF Config License\": \"\", \"HF Dataset\": \"\", \"HF Date\": \"\", \"HF Downloads (June 2024)\": \"\", \"HF Likes (June 2024)\": \"\", \"HF Yaml License\": \"\", \"PwC Date\": \"\", \"PwC Description\": \"Paper tables with annotated results for APES: Audiovisual Person Search in Untrimmed Video\", \"PwC License Name\": \"\", \"PwC License URL\": \"\", \"S2 Citation Count (June 2024)\": \"\", \"S2 Date\": \"\"}, \"Bibtex\": \"@Article{Alcazar2021APESAP,\\n author = {Juan Leon Alcazar and Long Mai and Federico Perazzi and Joon-Young Lee and Pablo Arbel\\u00e1ez and Bernard Ghanem and Fabian Caba Heilbron},\\n booktitle = {2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)},\\n journal = {2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)},\\n pages = {1720-1729},\\n title = {APES: Audiovisual Person Search in Untrimmed Video},\\n year = {2021}\\n}\\n\", \"License Use (DataProvenance)\": \"unspecified\", \"License Attribution (DataProvenance)\": 0, \"License Share Alike (DataProvenance)\": 0, \"License Use (DataProvenance IgnoreOpenAI)\": \"unspecified\", \"License Attribution (DataProvenance IgnoreOpenAI)\": 0, \"License Share Alike (DataProvenance IgnoreOpenAI)\": 0, \"License Type\": \"Unspecified\"}, {\"Unique Dataset Identifier\": \"WebVid\", \"Collection\": \"WebVid\", \"Collection URL\": \"https://arxiv.org/abs/2104.00650\", \"Dataset Name\": \"WebVid\", \"Paper Title\": \"WebVid\", \"Paper URL\": \"https://arxiv.org/abs/2104.00650\", \"GitHub URL\": \"https://github.com/m-bain/webvid\", \"Hugging Face URL\": \"https://huggingface.co/datasets/TempoFunk/webvid-10M\", \"Papers with Code URL\": \"https://paperswithcode.com/dataset/webvid\", \"ArXiv URL\": \"https://arxiv.org/abs/2104.00650\", \"Semantic Scholar Corpus ID\": 232478955, \"Year Released\": \"2021\", \"Text Sources\": [\"undisclosed web\"], \"Licenses\": [{\"License\": \"Custom\", \"License URL\": \"https://github.com/m-bain/webvid/blob/main/TERMS.md\"}], \"Creators\": [\"University of Oxford\", \"CNRS\"], \"Countries\": [\"United Kingdom\", \"France\"], \"License Verified By\": \"Vivek Sharma\", \"Video Hours\": 13000.0, \"Taken Down\": \"True\", \"Video Sources\": \"undisclosed web\", \"Task Categories\": [\"Video Captioning\"], \"Inferred Metadata\": {\"GitHub License\": \"\", \"GitHub Stars (June 2024)\": 528, \"GitHub Topics\": [], \"Github Date\": \"\", \"HF Config\": \"default\", \"HF Config License\": \"\", \"HF Dataset\": \"TempoFunk/webvid-10M\", \"HF Date\": \"2023-06-16\", \"HF Downloads (June 2024)\": 360, \"HF Likes (June 2024)\": 28, \"HF Yaml License\": \"GNU General Public License v3.0\", \"PwC Date\": \"2021-04-01\", \"PwC Description\": \"WebVid contains 10 million video clips with captions, sourced from the web. The videos are diverse and rich in their content.\\n\\nBoth the full 10M set and a 2.5M subset is available for download:\\nhttps://github.com/m-bain/webvid-dataset\", \"PwC License Name\": \"Various\", \"PwC License URL\": \"https://github.com/m-bain/webvid/blob/main/TERMS.md\", \"S2 Citation Count (June 2024)\": \"\", \"S2 Date\": \"\"}, \"Bibtex\": \"@Article{Bain2021FrozenIT,\\n author = {Max Bain and Arsha Nagrani and G\\u00fcl Varol and Andrew Zisserman},\\n booktitle = {IEEE International Conference on Computer Vision},\\n journal = {2021 IEEE/CVF International Conference on Computer Vision (ICCV)},\\n pages = {1708-1718},\\n title = {Frozen in Time: A Joint Video and Image Encoder for End-to-End Retrieval},\\n year = {2021}\\n}\\n\", \"License Use (DataProvenance)\": \"academic-only\", \"License Attribution (DataProvenance)\": 0, \"License Share Alike (DataProvenance)\": 0, \"License Use (DataProvenance IgnoreOpenAI)\": \"academic-only\", \"License Attribution (DataProvenance IgnoreOpenAI)\": 0, \"License Share Alike (DataProvenance IgnoreOpenAI)\": 0, \"License Type\": \"Non-Commercial/Academic\"}, {\"Unique Dataset Identifier\": \"multi-moments-in-time-dataset\", \"Collection\": \"multi-moments-in-time-dataset\", \"Collection URL\": \"https://arxiv.org/pdf/1911.00232.pdf\", \"Dataset Name\": \"Multi-Moments in Time: Learning and Interpreting Models for Multi-Action Video Understanding\", \"Paper Title\": \"Multi-Moments in Time: Learning and Interpreting Models for Multi-Action Video Understanding\", \"Paper URL\": \"https://arxiv.org/pdf/1911.00232.pdf\", \"GitHub URL\": \"\", \"Hugging Face URL\": \"\", \"Papers with Code URL\": \"\", \"ArXiv URL\": \"https://arxiv.org/pdf/1911.00232.pdf\", \"Semantic Scholar Corpus ID\": 207780280, \"Year Released\": \"2021\", \"Text Sources\": [\"crowdsourced\", \"undisclosed web\"], \"Licenses\": [{\"License\": \"Unspecified\", \"License URL\": \"\"}], \"Creators\": [\"Massachusetts Institute of Technology\"], \"Countries\": [\"United States of America\"], \"License Verified By\": \"Vivek Sharma\", \"Video Hours\": 833.0, \"Taken Down\": \"False\", \"Video Sources\": \"crowdsourced\", \"Task Categories\": [\"Video Classification\"], \"Inferred Metadata\": {\"GitHub License\": \"\", \"GitHub Stars (June 2024)\": \"\", \"GitHub Topics\": \"\", \"Github Date\": \"\", \"HF Config\": \"\", \"HF Config License\": \"\", \"HF Dataset\": \"\", \"HF Date\": \"\", \"HF Downloads (June 2024)\": \"\", \"HF Likes (June 2024)\": \"\", \"HF Yaml License\": \"\", \"PwC Date\": \"\", \"PwC Description\": \"\", \"PwC License Name\": \"\", \"PwC License URL\": \"\", \"S2 Citation Count (June 2024)\": \"\", \"S2 Date\": \"\"}, \"Bibtex\": \"@Article{Monfort2019MultiMomentsIT,\\n author = {Mathew Monfort and K. Ramakrishnan and A. Andonian and Barry A. McNamara and A. Lascelles and Bowen Pan and Quanfu Fan and Dan Gutfreund and R. Feris and A. Oliva},\\n booktitle = {IEEE Transactions on Pattern Analysis and Machine Intelligence},\\n journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},\\n pages = {1-1},\\n title = {Multi-Moments in Time: Learning and Interpreting Models for Multi-Action Video Understanding},\\n volume = {PP},\\n year = {2019}\\n}\\n\", \"License Use (DataProvenance)\": \"unspecified\", \"License Attribution (DataProvenance)\": 0, \"License Share Alike (DataProvenance)\": 0, \"License Use (DataProvenance IgnoreOpenAI)\": \"unspecified\", \"License Attribution (DataProvenance IgnoreOpenAI)\": 0, \"License Share Alike (DataProvenance IgnoreOpenAI)\": 0, \"License Type\": \"Unspecified\"}, {\"Unique Dataset Identifier\": \"spoken-moments\", \"Collection\": \"spoken-moments\", \"Collection URL\": \"https://arxiv.org/abs/2105.04489\", \"Dataset Name\": \"Spoken Moments\", \"Paper Title\": \"Spoken Moments\", \"Paper URL\": \"https://arxiv.org/abs/2105.04489\", \"GitHub URL\": \"\", \"Hugging Face URL\": \"\", \"Papers with Code URL\": \"https://paperswithcode.com/paper/spoken-moments-learning-joint-audio-visual\", \"ArXiv URL\": \"https://arxiv.org/abs/2105.04489\", \"Semantic Scholar Corpus ID\": 233992735, \"Year Released\": \"2021\", \"Text Sources\": [\"youtube\", \"flickr\", \"vine\", \"metacafe\", \"peeks\", \"vimeo\", \"videoblocks\", \"bing videos\", \"giphy\", \"weather channel\", \"getty-images\"], \"Licenses\": [{\"License\": \"Custom\", \"License URL\": \"https://docs.google.com/forms/d/e/1FAIpQLSdUOScon6uWkslgcA2-BCkAhMp86SihgKtMqX53zRf-E2fIHw/formResponse\"}], \"Creators\": [\"Massachusetts Institute of Technology\", \"IBM\", \"The University of Texas at Austin\"], \"Countries\": [\"United States of America\"], \"License Verified By\": \"Vivek Sharma\", \"Video Hours\": 416.67, \"Taken Down\": \"False\", \"Video Sources\": \"youtube\", \"Task Categories\": [\"Video Captioning\"], \"Inferred Metadata\": {\"GitHub License\": \"\", \"GitHub Stars (June 2024)\": \"\", \"GitHub Topics\": \"\", \"Github Date\": \"\", \"HF Config\": \"\", \"HF Config License\": \"\", \"HF Dataset\": \"\", \"HF Date\": \"\", \"HF Downloads (June 2024)\": \"\", \"HF Likes (June 2024)\": \"\", \"HF Yaml License\": \"\", \"PwC Date\": \"\", \"PwC Description\": \"No code available yet.\", \"PwC License Name\": \"\", \"PwC License URL\": \"\", \"S2 Citation Count (June 2024)\": \"\", \"S2 Date\": \"\"}, \"Bibtex\": \"@Article{Monfort2021SpokenML,\\n author = {Mathew Monfort and SouYoung Jin},\\n booktitle = {Computer Vision and Pattern Recognition},\\n journal = {2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},\\n pages = {14866-14876},\\n title = {Spoken Moments: Learning Joint Audio-Visual Representations from Video Descriptions},\\n year = {2021}\\n}\\n\", \"License Use (DataProvenance)\": \"commercial\", \"License Attribution (DataProvenance)\": 0, \"License Share Alike (DataProvenance)\": 0, \"License Use (DataProvenance IgnoreOpenAI)\": \"commercial\", \"License Attribution (DataProvenance IgnoreOpenAI)\": 0, \"License Share Alike (DataProvenance IgnoreOpenAI)\": 0, \"License Type\": \"Commercial\"}, {\"Unique Dataset Identifier\": \"uav-human\", \"Collection\": \"uav-human\", \"Collection URL\": \"https://arxiv.org/abs/2104.00946\", \"Dataset Name\": \"UAV-Human: A Large Benchmark for Human Behavior Understanding with Unmanned Aerial Vehicles\", \"Paper Title\": \"UAV-Human: A Large Benchmark for Human Behavior Understanding with Unmanned Aerial Vehicles\", \"Paper URL\": \"https://arxiv.org/abs/2104.00946\", \"GitHub URL\": \"https://github.com/sutdcv/UAV-Human\", \"Hugging Face URL\": \"\", \"Papers with Code URL\": \"https://paperswithcode.com/paper/uav-human-a-large-benchmark-for-human\", \"ArXiv URL\": \"https://arxiv.org/abs/2104.00946\", \"Semantic Scholar Corpus ID\": 233004700, \"Year Released\": \"2021\", \"Text Sources\": [\"human\"], \"Licenses\": [{\"License\": \"Custom\", \"License URL\": \"https://sutdcv.github.io/uav-human-web/\"}], \"Creators\": [\"Shandong University\", \"Singapore University of Technology and Design\"], \"Countries\": [\"Singapore\", \"China\"], \"License Verified By\": \"Vivek Sharma\", \"Video Hours\": 18.34, \"Taken Down\": \"False\", \"Video Sources\": \"human\", \"Task Categories\": [\"Video Classification\"], \"Inferred Metadata\": {\"GitHub License\": \"\", \"GitHub Stars (June 2024)\": 180, \"GitHub Topics\": [\"action-recognition\", \"dataset\", \"uav\"], \"Github Date\": \"\", \"HF Config\": \"\", \"HF Config License\": \"\", \"HF Dataset\": \"\", \"HF Date\": \"\", \"HF Downloads (June 2024)\": \"\", \"HF Likes (June 2024)\": \"\", \"HF Yaml License\": \"\", \"PwC Date\": \"\", \"PwC Description\": \"Implemented in 2 code libraries.\", \"PwC License Name\": \"\", \"PwC License URL\": \"\", \"S2 Citation Count (June 2024)\": \"\", \"S2 Date\": \"\"}, \"Bibtex\": \"@Article{Li2021UAVHumanAL,\\n author = {Tianjiao Li and Jun Liu and Wei Zhang and Yun Ni and Wenqian Wang and Zhiheng Li},\\n booktitle = {Computer Vision and Pattern Recognition},\\n journal = {2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},\\n pages = {16261-16270},\\n title = {UAV-Human: A Large Benchmark for Human Behavior Understanding with Unmanned Aerial Vehicles},\\n year = {2021}\\n}\\n\", \"License Use (DataProvenance)\": \"academic-only\", \"License Attribution (DataProvenance)\": 1, \"License Share Alike (DataProvenance)\": 0, \"License Use (DataProvenance IgnoreOpenAI)\": \"academic-only\", \"License Attribution (DataProvenance IgnoreOpenAI)\": 1, \"License Share Alike (DataProvenance IgnoreOpenAI)\": 0, \"License Type\": \"Non-Commercial/Academic\"}, {\"Unique Dataset Identifier\": \"multi-moments-in-time-dataset\", \"Collection\": \"multi-moments-in-time-dataset\", \"Collection URL\": \"https://arxiv.org/pdf/1911.00232.pdf\", \"Dataset Name\": \"Multi-Moments in Time: Learning and Interpreting Models for Multi-Action Video Understanding\", \"Paper Title\": \"Multi-Moments in Time: Learning and Interpreting Models for Multi-Action Video Understanding\", \"Paper URL\": \"https://arxiv.org/pdf/1911.00232.pdf\", \"GitHub URL\": \"\", \"Hugging Face URL\": \"\", \"Papers with Code URL\": \"\", \"ArXiv URL\": \"https://arxiv.org/pdf/1911.00232.pdf\", \"Semantic Scholar Corpus ID\": 207780280, \"Year Released\": \"2021\", \"Text Sources\": [\"crowdsourced\", \"undisclosed web\"], \"Licenses\": [{\"License\": \"Unspecified\", \"License URL\": \"\"}], \"Creators\": [\"Massachusetts Institute of Technology\"], \"Countries\": [\"United States of America\"], \"License Verified By\": \"Vivek Sharma\", \"Video Hours\": 833.0, \"Taken Down\": \"False\", \"Video Sources\": \"undisclosed web\", \"Task Categories\": [\"Video Classification\"], \"Inferred Metadata\": {\"GitHub License\": \"\", \"GitHub Stars (June 2024)\": \"\", \"GitHub Topics\": \"\", \"Github Date\": \"\", \"HF Config\": \"\", \"HF Config License\": \"\", \"HF Dataset\": \"\", \"HF Date\": \"\", \"HF Downloads (June 2024)\": \"\", \"HF Likes (June 2024)\": \"\", \"HF Yaml License\": \"\", \"PwC Date\": \"\", \"PwC Description\": \"\", \"PwC License Name\": \"\", \"PwC License URL\": \"\", \"S2 Citation Count (June 2024)\": \"\", \"S2 Date\": \"\"}, \"Bibtex\": \"@Article{Monfort2019MultiMomentsIT,\\n author = {Mathew Monfort and K. Ramakrishnan and A. Andonian and Barry A. McNamara and A. Lascelles and Bowen Pan and Quanfu Fan and Dan Gutfreund and R. Feris and A. Oliva},\\n booktitle = {IEEE Transactions on Pattern Analysis and Machine Intelligence},\\n journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},\\n pages = {1-1},\\n title = {Multi-Moments in Time: Learning and Interpreting Models for Multi-Action Video Understanding},\\n volume = {PP},\\n year = {2019}\\n}\\n\", \"License Use (DataProvenance)\": \"unspecified\", \"License Attribution (DataProvenance)\": 0, \"License Share Alike (DataProvenance)\": 0, \"License Use (DataProvenance IgnoreOpenAI)\": \"unspecified\", \"License Attribution (DataProvenance IgnoreOpenAI)\": 0, \"License Share Alike (DataProvenance IgnoreOpenAI)\": 0, \"License Type\": \"Unspecified\"}, {\"Unique Dataset Identifier\": \"YT-Temporal-1B\", \"Collection\": \"YT-Temporal-1B\", \"Collection URL\": \"https://arxiv.org/pdf/2201.02639\", \"Dataset Name\": \"MERLOT Reserve: Multimodal Neural Script Knowledge through Vision and Language and Sound\", \"Paper Title\": \"MERLOT Reserve: Multimodal Neural Script Knowledge through Vision and Language and Sound\", \"Paper URL\": \"https://arxiv.org/pdf/2201.02639\", \"GitHub URL\": \"\", \"Hugging Face URL\": \"\", \"Papers with Code URL\": \"https://paperswithcode.com/paper/merlot-reserve-neural-script-knowledge\", \"ArXiv URL\": \"https://arxiv.org/pdf/2201.02639\", \"Semantic Scholar Corpus ID\": 245837609, \"Year Released\": \"2022\", \"Text Sources\": [\"youtube\"], \"Licenses\": [{\"License\": \"MIT License\", \"License URL\": \"https://github.com/rowanz/merlot_reserve/blob/main/LICENSE\"}], \"Creators\": [\"University of Washington\", \"Allen Institute for AI\", \"University of Edinburgh\"], \"Countries\": [\"United States of America\", \"Scotland\"], \"License Verified By\": \"Vivek Sharma\", \"Video Hours\": 55555.0, \"Taken Down\": \"False\", \"Video Sources\": \"youtube\", \"Task Categories\": [\"Video Q&A\"], \"Inferred Metadata\": null, \"Bibtex\": null, \"License Use (DataProvenance)\": \"commercial\", \"License Attribution (DataProvenance)\": 1, \"License Share Alike (DataProvenance)\": 0, \"License Use (DataProvenance IgnoreOpenAI)\": \"commercial\", \"License Attribution (DataProvenance IgnoreOpenAI)\": 1, \"License Share Alike (DataProvenance IgnoreOpenAI)\": 0, \"License Type\": \"Commercial\"}, {\"Unique Dataset Identifier\": \"mad\", \"Collection\": \"mad\", \"Collection URL\": \"https://arxiv.org/abs/2112.00431\", \"Dataset Name\": \"MAD: A Scalable Dataset for Language Grounding in Videos from Movie Audio Descriptions\", \"Paper Title\": \"MAD: A Scalable Dataset for Language Grounding in Videos from Movie Audio Descriptions\", \"Paper URL\": \"https://arxiv.org/abs/2112.00431\", \"GitHub URL\": \"https://github.com/Soldelli/MAD\", \"Hugging Face URL\": \"\", \"Papers with Code URL\": \"https://paperswithcode.com/dataset/mad\", \"ArXiv URL\": \"https://arxiv.org/abs/2112.00431\", \"Semantic Scholar Corpus ID\": 244773187, \"Year Released\": \"2022\", \"Text Sources\": [\"undisclosed web\"], \"Licenses\": [{\"License\": \"Custom\", \"License URL\": \"https://docs.google.com/forms/d/e/1FAIpQLSdtUV3uweS0u7AHAMIJAL_dRRdZ5MHpJS3fdZVbhnVt-Yb4NA/viewform\"}], \"Creators\": [\"King Abdullah University of Science and Technology (KAUST)\"], \"Countries\": [\"Saudi Arabia\"], \"License Verified By\": \"Vivek Sharma\", \"Video Hours\": 1207.3, \"Taken Down\": \"False\", \"Video Sources\": \"undisclosed web\", \"Task Categories\": [\"Video Captioning\"], \"Inferred Metadata\": {\"GitHub License\": \"MIT License\", \"GitHub Stars (June 2024)\": 138, \"GitHub Topics\": [], \"Github Date\": \"\", \"HF Config\": \"\", \"HF Config License\": \"\", \"HF Dataset\": \"\", \"HF Date\": \"\", \"HF Downloads (June 2024)\": \"\", \"HF Likes (June 2024)\": \"\", \"HF Yaml License\": \"\", \"PwC Date\": \"2021-12-01\", \"PwC Description\": \"MAD (Movie Audio Descriptions) is an automatically curated large-scale dataset for the task of natural language grounding in videos or natural language moment retrieval.\\nMAD exploits available audio descriptions of mainstream movies. Such audio descriptions are redacted for visually impaired audiences and are therefore highly descriptive of the visual content being displayed. \\nMAD contains over 384,000 natural language sentences grounded in over 1,200 hours of video, and provides a unique setup for video grounding as the visual stream is truly untrimmed with an average video duration of 110 minutes. 2 orders of magnitude longer than legacy datasets. \\n\\nTake a look at the paper for additional information.\\n\\nFrom the authors on availability: \\\"Due to copyright constraints, MAD\\u2019s videos will not be publicly released. However, we will provide all necessary features for our experiments\\u2019 reproducibility and promote future research in this direction\\\"\", \"PwC License Name\": \"Unspecified\", \"PwC License URL\": null, \"S2 Citation Count (June 2024)\": \"\", \"S2 Date\": \"\"}, \"Bibtex\": \"@Article{Soldan2021MADAS,\\n author = {Mattia Soldan and A. Pardo and Juan Le'on Alc'azar and Fabian Caba Heilbron and Chen Zhao and Silvio Giancola and Bernard Ghanem},\\n booktitle = {Computer Vision and Pattern Recognition},\\n journal = {2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},\\n pages = {5016-5025},\\n title = {MAD: A Scalable Dataset for Language Grounding in Videos from Movie Audio Descriptions},\\n year = {2021}\\n}\\n\", \"License Use (DataProvenance)\": \"academic-only\", \"License Attribution (DataProvenance)\": 1, \"License Share Alike (DataProvenance)\": 0, \"License Use (DataProvenance IgnoreOpenAI)\": \"academic-only\", \"License Attribution (DataProvenance IgnoreOpenAI)\": 1, \"License Share Alike (DataProvenance IgnoreOpenAI)\": 0, \"License Type\": \"Non-Commercial/Academic\"}, {\"Unique Dataset Identifier\": \"ferv39k-dataset\", \"Collection\": \"ferv39k-dataset\", \"Collection URL\": \"https://arxiv.org/abs/2203.09463\", \"Dataset Name\": \"FERV39k: A Large-Scale Multi-Scene Dataset for Facial Expression Recognition in Videos (CVPR 2022)\", \"Paper Title\": \"FERV39k: A Large-Scale Multi-Scene Dataset for Facial Expression Recognition in Videos (CVPR 2022)\", \"Paper URL\": \"https://arxiv.org/abs/2203.09463\", \"GitHub URL\": \"https://github.com/wangyanckxx/FERV39k\", \"Hugging Face URL\": \"\", \"Papers with Code URL\": \"https://paperswithcode.com/paper/ferv39k-a-large-scale-multi-scene-dataset-for\", \"ArXiv URL\": \"https://arxiv.org/abs/2203.09463\", \"Semantic Scholar Corpus ID\": 247518747, \"Year Released\": \"2022\", \"Text Sources\": [\"undisclosed web\"], \"Licenses\": [{\"License\": \"CC BY-NC 4.0\", \"License URL\": \"https://wangyanckxx.github.io/Proj_CVPR2022_FERV39k.html\"}], \"Creators\": [\"Fudan University\"], \"Countries\": [\"China\"], \"License Verified By\": \"Vivek Sharma\", \"Video Hours\": 16.47, \"Taken Down\": \"False\", \"Video Sources\": \"undisclosed web\", \"Task Categories\": [\"Video Classification\"], \"Inferred Metadata\": {\"GitHub License\": \"\", \"GitHub Stars (June 2024)\": 52, \"GitHub Topics\": [], \"Github Date\": \"\", \"HF Config\": \"\", \"HF Config License\": \"\", \"HF Dataset\": \"\", \"HF Date\": \"\", \"HF Downloads (June 2024)\": \"\", \"HF Likes (June 2024)\": \"\", \"HF Yaml License\": \"\", \"PwC Date\": \"\", \"PwC Description\": \"No code available yet.\", \"PwC License Name\": \"\", \"PwC License URL\": \"\", \"S2 Citation Count (June 2024)\": \"\", \"S2 Date\": \"\"}, \"Bibtex\": \"@Article{Wang2022FERV39kAL,\\n author = {Yan Wang and Yixuan Sun and Yiwen Huang and Zhongying Liu and Shuyong Gao and Wei Zhang and Weifeng Ge and Wenqiang Zhang},\\n booktitle = {Computer Vision and Pattern Recognition},\\n journal = {2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},\\n pages = {20890-20899},\\n title = {FERV39k: A Large-Scale Multi-Scene Dataset for Facial Expression Recognition in Videos},\\n year = {2022}\\n}\\n\", \"License Use (DataProvenance)\": \"non-commercial\", \"License Attribution (DataProvenance)\": 1, \"License Share Alike (DataProvenance)\": 0, \"License Use (DataProvenance IgnoreOpenAI)\": \"non-commercial\", \"License Attribution (DataProvenance IgnoreOpenAI)\": 1, \"License Share Alike (DataProvenance IgnoreOpenAI)\": 0, \"License Type\": \"Non-Commercial/Academic\"}, {\"Unique Dataset Identifier\": \"cacd\", \"Collection\": \"cacd\", \"Collection URL\": \"https://openaccess.thecvf.com/content/CVPR2022W/L3D-IVU/papers/Xiang_CDAD_A_Common_Daily_Action_Dataset_With_Collected_Hard_Negative_CVPRW_2022_paper.pdf\", \"Dataset Name\": \"CDAD: A Common Daily Action Dataset with Collected Hard Negative Samples (CVPR 2022)\", \"Paper Title\": \"CDAD: A Common Daily Action Dataset with Collected Hard Negative Samples (CVPR 2022)\", \"Paper URL\": \"https://openaccess.thecvf.com/content/CVPR2022W/L3D-IVU/papers/Xiang_CDAD_A_Common_Daily_Action_Dataset_With_Collected_Hard_Negative_CVPRW_2022_paper.pdf\", \"GitHub URL\": \"https://github.com/MartinXM/CDAD\", \"Hugging Face URL\": \"\", \"Papers with Code URL\": \"\", \"ArXiv URL\": \"https://openaccess.thecvf.com/content/CVPR2022W/L3D-IVU/papers/Xiang_CDAD_A_Common_Daily_Action_Dataset_With_Collected_Hard_Negative_CVPRW_2022_paper.pdf\", \"Semantic Scholar Corpus ID\": 251035434, \"Year Released\": \"2022\", \"Text Sources\": [\"crowdsourced\"], \"Licenses\": [{\"License\": \"Unspecified\", \"License URL\": \"\"}], \"Creators\": [\"The Hong Kong Polytechnic University\", \"Alibaba Group\"], \"Countries\": [\"China\"], \"License Verified By\": \"Vivek Sharma\", \"Video Hours\": 215.0, \"Taken Down\": \"False\", \"Video Sources\": \"crowdsourced\", \"Task Categories\": [\"Video Classification\"], \"Inferred Metadata\": {\"GitHub License\": \"\", \"GitHub Stars (June 2024)\": 6, \"GitHub Topics\": [], \"Github Date\": \"\", \"HF Config\": \"\", \"HF Config License\": \"\", \"HF Dataset\": \"\", \"HF Date\": \"\", \"HF Downloads (June 2024)\": \"\", \"HF Likes (June 2024)\": \"\", \"HF Yaml License\": \"\", \"PwC Date\": \"\", \"PwC Description\": \"\", \"PwC License Name\": \"\", \"PwC License URL\": \"\", \"S2 Citation Count (June 2024)\": \"\", \"S2 Date\": \"\"}, \"Bibtex\": \"@Article{Xiang2022CDADAC,\\n author = {Wangmeng Xiang and C. Li and Ke Li and Biao Wang and Xiangpei Hua and Lei Zhang},\\n booktitle = {2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)},\\n journal = {2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)},\\n pages = {3920-3929},\\n title = {CDAD: A Common Daily Action Dataset with Collected Hard Negative Samples},\\n year = {2022}\\n}\\n\", \"License Use (DataProvenance)\": \"unspecified\", \"License Attribution (DataProvenance)\": 0, \"License Share Alike (DataProvenance)\": 0, \"License Use (DataProvenance IgnoreOpenAI)\": \"unspecified\", \"License Attribution (DataProvenance IgnoreOpenAI)\": 0, \"License Share Alike (DataProvenance IgnoreOpenAI)\": 0, \"License Type\": \"Unspecified\"}, {\"Unique Dataset Identifier\": \"ego-4d\", \"Collection\": \"ego-4d\", \"Collection URL\": \"https://arxiv.org/abs/2110.07058\", \"Dataset Name\": \"Ego4D: Around the World in 3,000 Hours of Egocentric Video\", \"Paper Title\": \"Ego4D: Around the World in 3,000 Hours of Egocentric Video\", \"Paper URL\": \"https://arxiv.org/abs/2110.07058\", \"GitHub URL\": \"https://github.com/EGO4D/forecasting\", \"Hugging Face URL\": \"\", \"Papers with Code URL\": \"https://paperswithcode.com/paper/ego4d-around-the-world-in-3000-hours-of\", \"ArXiv URL\": \"https://arxiv.org/abs/2110.07058\", \"Semantic Scholar Corpus ID\": 238856888, \"Year Released\": \"2022\", \"Text Sources\": [\"human\"], \"Licenses\": [{\"License\": \"Custom\", \"License URL\": \"https://ego4d-data.org/pdfs/Ego4D-Licenses-Draft.pdf\"}], \"Creators\": [\"Facebook AI Research\"], \"Countries\": [\"United States of America\"], \"License Verified By\": \"Vivek Sharma\", \"Video Hours\": 3670.0, \"Taken Down\": \"False\", \"Video Sources\": \"human\", \"Task Categories\": [\"Video Classification\", \"Temporal Action Segmentation\", \"Video Captioning\", \"Misc\"], \"Inferred Metadata\": {\"GitHub License\": \"MIT License\", \"GitHub Stars (June 2024)\": 63, \"GitHub Topics\": [], \"Github Date\": \"\", \"HF Config\": \"\", \"HF Config License\": \"\", \"HF Dataset\": \"\", \"HF Date\": \"\", \"HF Downloads (June 2024)\": \"\", \"HF Likes (June 2024)\": \"\", \"HF Yaml License\": \"\", \"PwC Date\": \"\", \"PwC Description\": \"Implemented in 8 code libraries.\", \"PwC License Name\": \"\", \"PwC License URL\": \"\", \"S2 Citation Count (June 2024)\": \"\", \"S2 Date\": \"\"}, \"Bibtex\": \"@Article{Grauman2021Ego4DAT,\\n author = {K. Grauman and Andrew Westbury and Eugene Byrne and Zachary Chavis and Antonino Furnari and Rohit Girdhar and Jackson Hamburger and Hao Jiang and Miao Liu and Xingyu Liu and Miguel Martin and Tushar Nagarajan and Ilija Radosavovic and Santhosh K. Ramakrishnan and Fiona Ryan and J. Sharma and Michael Wray and Mengmeng Xu and Eric Z. Xu and Chen Zhao and Siddhant Bansal and Dhruv Batra and Vincent Cartillier and S. Crane and Tien Do and Morrie Doulaty and Akshay Erapalli and Christoph Feichtenhofer and A. Fragomeni and Qichen Fu and Christian Fuegen and A. Gebreselasie and Cristina Gonz\\u00e1lez and James M. Hillis and Xuhua Huang and Yifei Huang and Wenqi Jia and Weslie Khoo and J. Kol\\u00e1r and Satwik Kottur and Anurag Kumar and F. Landini and Chao Li and Yanghao Li and Zhenqiang Li and K. Mangalam and Raghava Modhugu and Jonathan Munro and Tullie Murrell and Takumi Nishiyasu and Will Price and Paola Ruiz Puentes and Merey Ramazanova and Leda Sari and K. Somasundaram and Audrey Southerland and Yusuke Sugano and Ruijie Tao and Minh Vo and Yuchen Wang and Xindi Wu and Takuma Yagi and Yunyi Zhu and P. Arbel\\u00e1ez and David J. Crandall and D. Damen and G. Farinella and Bernard Ghanem and V. Ithapu and C. V. Jawahar and H. Joo and Kris Kitani and Haizhou Li and Richard A. Newcombe and A. Oliva and H. Park and James M. Rehg and Yoichi Sato and Jianbo Shi and Mike Zheng Shou and A. Torralba and L. Torresani and Mingfei Yan and J. Malik},\\n booktitle = {Computer Vision and Pattern Recognition},\\n journal = {2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},\\n pages = {18973-18990},\\n title = {Ego4D: Around the World in 3,000 Hours of Egocentric Video},\\n year = {2021}\\n}\\n\", \"License Use (DataProvenance)\": \"commercial\", \"License Attribution (DataProvenance)\": 1, \"License Share Alike (DataProvenance)\": 0, \"License Use (DataProvenance IgnoreOpenAI)\": \"commercial\", \"License Attribution (DataProvenance IgnoreOpenAI)\": 1, \"License Share Alike (DataProvenance IgnoreOpenAI)\": 0, \"License Type\": \"Commercial\"}, {\"Unique Dataset Identifier\": \"project-aria-digital-twin-dataset\", \"Collection\": \"project-aria-digital-twin-dataset\", \"Collection URL\": \"https://arxiv.org/abs/2306.06362\", \"Dataset Name\": \"Aria Digital Twin\", \"Paper Title\": \"Aria Digital Twin\", \"Paper URL\": \"https://arxiv.org/abs/2306.06362\", \"GitHub URL\": \"https://github.com/facebookresearch/projectaria_tools\", \"Hugging Face URL\": \"\", \"Papers with Code URL\": \"https://paperswithcode.com/paper/aria-digital-twin-a-new-benchmark-dataset-for\", \"ArXiv URL\": \"https://arxiv.org/abs/2306.06362\", \"Semantic Scholar Corpus ID\": 261243365, \"Year Released\": \"2023\", \"Text Sources\": [\"human\"], \"Licenses\": [{\"License\": \"Apache License 2.0\", \"License URL\": \"https://github.com/facebookresearch/projectaria_tools/blob/main/LICENSE\"}], \"Creators\": [\"Meta\"], \"Countries\": [\"United States of America\"], \"License Verified By\": \"Vivek Sharma\", \"Video Hours\": 6.6, \"Taken Down\": \"False\", \"Video Sources\": \"human\", \"Task Categories\": [\"Video Object Detection\", \"Video Segmentation\"], \"Inferred Metadata\": {\"GitHub License\": \"Apache License 2.0\", \"GitHub Stars (June 2024)\": 361, \"GitHub Topics\": [], \"Github Date\": \"\", \"HF Config\": \"\", \"HF Config License\": \"\", \"HF Dataset\": \"\", \"HF Date\": \"\", \"HF Downloads (June 2024)\": \"\", \"HF Likes (June 2024)\": \"\", \"HF Yaml License\": \"\", \"PwC Date\": \"\", \"PwC Description\": \"Implemented in one code library.\", \"PwC License Name\": \"\", \"PwC License URL\": \"\", \"S2 Citation Count (June 2024)\": \"\", \"S2 Date\": \"\"}, \"Bibtex\": \"@Article{Somasundaram2023ProjectAA,\\n author = {K. Somasundaram and Jing Dong and Huixuan Tang and Julian Straub and Mingfei Yan and M. Goesele and Jakob J. Engel and R. D. Nardi and Richard A. Newcombe},\\n booktitle = {arXiv.org},\\n journal = {ArXiv},\\n title = {Project Aria: A New Tool for Egocentric Multi-Modal AI Research},\\n volume = {abs/2308.13561},\\n year = {2023}\\n}\\n\", \"License Use (DataProvenance)\": \"commercial\", \"License Attribution (DataProvenance)\": 1, \"License Share Alike (DataProvenance)\": 0, \"License Use (DataProvenance IgnoreOpenAI)\": \"commercial\", \"License Attribution (DataProvenance IgnoreOpenAI)\": 1, \"License Share Alike (DataProvenance IgnoreOpenAI)\": 0, \"License Type\": \"Commercial\"}, {\"Unique Dataset Identifier\": \"ego-exo4D\", \"Collection\": \"ego-exo4D\", \"Collection URL\": \"https://arxiv.org/abs/2311.18259\", \"Dataset Name\": \"Ego-Exo4D\", \"Paper Title\": \"Ego-Exo4D\", \"Paper URL\": \"https://arxiv.org/abs/2311.18259\", \"GitHub URL\": \"https://github.com/facebookresearch/Ego4d\", \"Hugging Face URL\": \"\", \"Papers with Code URL\": \"https://paperswithcode.com/paper/ego-exo4d-understanding-skilled-human/review/\", \"ArXiv URL\": \"https://arxiv.org/abs/2311.18259\", \"Semantic Scholar Corpus ID\": 265506384, \"Year Released\": \"2023\", \"Text Sources\": [\"human\"], \"Licenses\": [{\"License\": \"MIT License\", \"License URL\": \"https://github.com/facebookresearch/Ego4d/blob/main/LICENSE\"}], \"Creators\": [\"Meta\"], \"Countries\": [\"United States of America\"], \"License Verified By\": \"Vivek Sharma\", \"Video Hours\": 1422.0, \"Taken Down\": \"False\", \"Video Sources\": \"human\", \"Task Categories\": [\"Pose Estimation\", \"Video Classification\", \"Video Captioning\", \"Misc\"], \"Inferred Metadata\": {\"GitHub License\": \"MIT License\", \"GitHub Stars (June 2024)\": 297, \"GitHub Topics\": [\"computer-vision\", \"dataset\", \"feature-extraction\", \"video\", \"visuzalization\"], \"Github Date\": \"\", \"HF Config\": \"\", \"HF Config License\": \"\", \"HF Dataset\": \"\", \"HF Date\": \"\", \"HF Downloads (June 2024)\": \"\", \"HF Likes (June 2024)\": \"\", \"HF Yaml License\": \"\", \"PwC Date\": \"\", \"PwC Description\": \"Paper tables with annotated results for Ego-Exo4D: Understanding Skilled Human Activity from First- and Third-Person Perspectives\", \"PwC License Name\": \"\", \"PwC License URL\": \"\", \"S2 Citation Count (June 2024)\": \"\", \"S2 Date\": \"\"}, \"Bibtex\": \"@Article{Grauman2023EgoExo4DUS,\\n author = {K. Grauman and Andrew Westbury and L. Torresani and Kris Kitani and Jitendra Malik and Triantafyllos Afouras and Kumar Ashutosh and Vijay Baiyya and Siddhant Bansal and Bikram Boote and Eugene Byrne and Zachary Chavis and Joya Chen and Feng Cheng and Fu-Jen Chu and Sean Crane and Avijit Dasgupta and Jing Dong and Mar\\u00eda Escobar and Cristhian Forigua and A. Gebreselasie and S. Haresh and Jing Huang and Md Mohaiminul Islam and S. Jain and Rawal Khirodkar and Devansh Kukreja and Kevin J Liang and Jia-Wei Liu and Sagnik Majumder and Yongsen Mao and Miguel Martin and E. Mavroudi and Tushar Nagarajan and Francesco Ragusa and Santhosh K. Ramakrishnan and Luigi Seminara and Arjun Somayazulu and Yale Song and Shan Su and Zihui Xue and Edward Zhang and Jinxu Zhang and Angela Castillo and Changan Chen and Xinzhu Fu and Ryosuke Furuta and Cristina Gonzalez and Prince Gupta and Jiabo Hu and Yifei Huang and Yiming Huang and Weslie Khoo and Anush Kumar and Robert Kuo and Sach Lakhavani and Miao Liu and M. Luo and Zhengyi Luo and Brighid Meredith and Austin Miller and Oluwatumininu Oguntola and Xiaqing Pan and Penny Peng and Shraman Pramanick and Merey Ramazanova and Fiona Ryan and Wei Shan and Kiran Somasundaram and Chenan Song and Audrey Southerland and Masatoshi Tateno and Huiyu Wang and Yuchen Wang and Takuma Yagi and Mingfei Yan and Xitong Yang and Zecheng Yu and S. Zha and Chen Zhao and Ziwei Zhao and Zhifan Zhu and Jeff Zhuo and Pablo Arbel\\u00e1ez and Gedas Bertasius and David J. Crandall and D. Damen and J. Engel and G. Farinella and Antonino Furnari and Bernard Ghanem and Judy Hoffman and C. V. Jawahar and Richard A. Newcombe and Hyun Soo Park and James M. Rehg and Yoichi Sato and M. Savva and Jianbo Shi and Mike Zheng Shou and Michael Wray},\\n booktitle = {arXiv.org},\\n journal = {ArXiv},\\n title = {Ego-Exo4D: Understanding Skilled Human Activity from First- and Third-Person Perspectives},\\n volume = {abs/2311.18259},\\n year = {2023}\\n}\\n\", \"License Use (DataProvenance)\": \"commercial\", \"License Attribution (DataProvenance)\": 1, \"License Share Alike (DataProvenance)\": 0, \"License Use (DataProvenance IgnoreOpenAI)\": \"commercial\", \"License Attribution (DataProvenance IgnoreOpenAI)\": 1, \"License Share Alike (DataProvenance IgnoreOpenAI)\": 0, \"License Type\": \"Commercial\"}, {\"Unique Dataset Identifier\": \"cinepile\", \"Collection\": \"cinepile\", \"Collection URL\": \"https://arxiv.org/pdf/2405.08813\", \"Dataset Name\": \"CinePile: A Long Video Question Answering Dataset and Benchmark\", \"Paper Title\": \"CinePile: A Long Video Question Answering Dataset and Benchmark\", \"Paper URL\": \"https://arxiv.org/pdf/2405.08813\", \"GitHub URL\": \"\", \"Hugging Face URL\": \"https://huggingface.co/datasets/tomg-group-umd/cinepile\", \"Papers with Code URL\": \"https://paperswithcode.com/dataset/cinepile\", \"ArXiv URL\": \"https://arxiv.org/pdf/2405.08813\", \"Semantic Scholar Corpus ID\": 269761335, \"Year Released\": \"2024\", \"Text Sources\": [\"youtube\", \"movies\"], \"Licenses\": [{\"License\": \"CC BY 4.0\", \"License URL\": \"https://creativecommons.org/licenses/by/4.0/\"}], \"Creators\": [\"University of Maryland\", \"Weizmann Institute of Science\"], \"Countries\": [\"United States of America\", \"Israel\"], \"License Verified By\": \"Vivek Sharma\", \"Video Hours\": 417.6, \"Taken Down\": \"False\", \"Video Sources\": \"movies\", \"Task Categories\": [\"Video Q&A\"], \"Inferred Metadata\": null, \"Bibtex\": null, \"License Use (DataProvenance)\": \"commercial\", \"License Attribution (DataProvenance)\": 1, \"License Share Alike (DataProvenance)\": 0, \"License Use (DataProvenance IgnoreOpenAI)\": \"commercial\", \"License Attribution (DataProvenance IgnoreOpenAI)\": 1, \"License Share Alike (DataProvenance IgnoreOpenAI)\": 0, \"License Type\": \"Commercial\"}, {\"Unique Dataset Identifier\": \"cinepile\", \"Collection\": \"cinepile\", \"Collection URL\": \"https://arxiv.org/pdf/2405.08813\", \"Dataset Name\": \"CinePile: A Long Video Question Answering Dataset and Benchmark\", \"Paper Title\": \"CinePile: A Long Video Question Answering Dataset and Benchmark\", \"Paper URL\": \"https://arxiv.org/pdf/2405.08813\", \"GitHub URL\": \"\", \"Hugging Face URL\": \"https://huggingface.co/datasets/tomg-group-umd/cinepile\", \"Papers with Code URL\": \"https://paperswithcode.com/dataset/cinepile\", \"ArXiv URL\": \"https://arxiv.org/pdf/2405.08813\", \"Semantic Scholar Corpus ID\": 269761335, \"Year Released\": \"2024\", \"Text Sources\": [\"youtube\", \"movies\"], \"Licenses\": [{\"License\": \"CC BY 4.0\", \"License URL\": \"https://creativecommons.org/licenses/by/4.0/\"}], \"Creators\": [\"University of Maryland\", \"Weizmann Institute of Science\"], \"Countries\": [\"United States of America\", \"Israel\"], \"License Verified By\": \"Vivek Sharma\", \"Video Hours\": 417.6, \"Taken Down\": \"False\", \"Video Sources\": \"youtube\", \"Task Categories\": [\"Video Q&A\"], \"Inferred Metadata\": null, \"Bibtex\": null, \"License Use (DataProvenance)\": \"commercial\", \"License Attribution (DataProvenance)\": 1, \"License Share Alike (DataProvenance)\": 0, \"License Use (DataProvenance IgnoreOpenAI)\": \"commercial\", \"License Attribution (DataProvenance IgnoreOpenAI)\": 1, \"License Share Alike (DataProvenance IgnoreOpenAI)\": 0, \"License Type\": \"Commercial\"}, {\"Unique Dataset Identifier\": \"project-aria-dataset\", \"Collection\": \"project-aria-dataset\", \"Collection URL\": \"https://arxiv.org/pdf/2402.13349\", \"Dataset Name\": \"Aria Everyday Activities Dataset\", \"Paper Title\": \"Aria Everyday Activities Dataset\", \"Paper URL\": \"https://arxiv.org/pdf/2402.13349\", \"GitHub URL\": \"https://github.com/facebookresearch/projectaria_tools\", \"Hugging Face URL\": \"\", \"Papers with Code URL\": \"https://paperswithcode.com/paper/aria-everyday-activities-dataset\", \"ArXiv URL\": \"https://arxiv.org/pdf/2402.13349\", \"Semantic Scholar Corpus ID\": 267770215, \"Year Released\": \"2024\", \"Text Sources\": [\"human\"], \"Licenses\": [{\"License\": \"Apache License 2.0\", \"License URL\": \"https://github.com/facebookresearch/Aria_data_tools/blob/main/LICENSE\"}], \"Creators\": [\"Meta\"], \"Countries\": [\"United States of America\"], \"License Verified By\": \"Vivek Sharma\", \"Video Hours\": 1400.0, \"Taken Down\": \"False\", \"Video Sources\": \"human\", \"Task Categories\": [\"Misc (Scene Reconstruction)\"], \"Inferred Metadata\": {\"GitHub License\": \"Apache License 2.0\", \"GitHub Stars (June 2024)\": 361, \"GitHub Topics\": [], \"Github Date\": \"\", \"HF Config\": \"\", \"HF Config License\": \"\", \"HF Dataset\": \"\", \"HF Date\": \"\", \"HF Downloads (June 2024)\": \"\", \"HF Likes (June 2024)\": \"\", \"HF Yaml License\": \"\", \"PwC Date\": \"\", \"PwC Description\": \"Implemented in one code library.\", \"PwC License Name\": \"\", \"PwC License URL\": \"\", \"S2 Citation Count (June 2024)\": \"\", \"S2 Date\": \"\"}, \"Bibtex\": \"@Article{Lv2024AriaEA,\\n author = {Zhaoyang Lv and Nickolas Charron and Pierre Moulon and Alexander Gamino and Cheng Peng and Chris Sweeney and Edward Miller and Huixuan Tang and Jeff Meissner and Jing Dong and Kiran Somasundaram and Luis Pesqueira and Mark Schwesinger and Omkar M. Parkhi and Qiao Gu and R. D. Nardi and Shangyi Cheng and Steve Saarinen and Vijay Baiyya and Yuyang Zou and Richard A. Newcombe and J. Engel and Xiaqing Pan and Carl Ren},\\n booktitle = {arXiv.org},\\n journal = {ArXiv},\\n title = {Aria Everyday Activities Dataset},\\n volume = {abs/2402.13349},\\n year = {2024}\\n}\\n\", \"License Use (DataProvenance)\": \"commercial\", \"License Attribution (DataProvenance)\": 1, \"License Share Alike (DataProvenance)\": 0, \"License Use (DataProvenance IgnoreOpenAI)\": \"commercial\", \"License Attribution (DataProvenance IgnoreOpenAI)\": 1, \"License Share Alike (DataProvenance IgnoreOpenAI)\": 0, \"License Type\": \"Commercial\"}, {\"Unique Dataset Identifier\": \"egopet\", \"Collection\": \"egopet\", \"Collection URL\": \"https://arxiv.org/pdf/2404.09991\", \"Dataset Name\": \"EgoPet: Egomotion and Interaction Data from an Animal's Perspective\", \"Paper Title\": \"EgoPet: Egomotion and Interaction Data from an Animal's Perspective\", \"Paper URL\": \"https://arxiv.org/pdf/2404.09991\", \"GitHub URL\": \"\", \"Hugging Face URL\": \"\", \"Papers with Code URL\": \"https://paperswithcode.com/paper/egopet-egomotion-and-interaction-data-from-an\", \"ArXiv URL\": \"https://arxiv.org/pdf/2404.09991\", \"Semantic Scholar Corpus ID\": 269148727, \"Year Released\": \"2024\", \"Text Sources\": [\"tiktok\", \"youtube\"], \"Licenses\": [{\"License\": \"CC BY-NC 4.0\", \"License URL\": \"https://github.com/DannyTran123/egopet/blob/main/LICENSE\"}], \"Creators\": [\"Tel Aviv University\", \"UC Berkeley\", \"New York University\"], \"Countries\": [\"United States of America\", \"Israel\"], \"License Verified By\": \"Vivek Sharma\", \"Video Hours\": 84.0, \"Taken Down\": \"False\", \"Video Sources\": \"Other\", \"Task Categories\": [\"Misc (Locomotion Prediction\", \"Visual Interaction Prediction\", \"Vision to Proprioception Prediction)\"], \"Inferred Metadata\": null, \"Bibtex\": null, \"License Use (DataProvenance)\": \"non-commercial\", \"License Attribution (DataProvenance)\": 1, \"License Share Alike (DataProvenance)\": 0, \"License Use (DataProvenance IgnoreOpenAI)\": \"non-commercial\", \"License Attribution (DataProvenance IgnoreOpenAI)\": 1, \"License Share Alike (DataProvenance IgnoreOpenAI)\": 0, \"License Type\": \"Non-Commercial/Academic\"}, {\"Unique Dataset Identifier\": \"egoschema\", \"Collection\": \"egoschema\", \"Collection URL\": \"https://arxiv.org/pdf/2308.09126\", \"Dataset Name\": \"EgoSchema: A Diagnostic Benchmark for Very Long-form Video Language Understanding\", \"Paper Title\": \"EgoSchema: A Diagnostic Benchmark for Very Long-form Video Language Understanding\", \"Paper URL\": \"https://arxiv.org/pdf/2308.09126\", \"GitHub URL\": \"\", \"Hugging Face URL\": \"\", \"Papers with Code URL\": \"https://paperswithcode.com/paper/egoschema-a-diagnostic-benchmark-for-very-1\", \"ArXiv URL\": \"https://arxiv.org/pdf/2308.09126\", \"Semantic Scholar Corpus ID\": 261031047, \"Year Released\": \"2024\", \"Text Sources\": [\"human\"], \"Licenses\": [{\"License\": \"Unspecified\", \"License URL\": \"\"}], \"Creators\": [\"UC Berkeley\"], \"Countries\": [\"United States of America\"], \"License Verified By\": \"Vivek Sharma\", \"Video Hours\": 250.0, \"Taken Down\": \"False\", \"Video Sources\": \"human\", \"Task Categories\": [\"Video Q&A\"], \"Inferred Metadata\": null, \"Bibtex\": null, \"License Use (DataProvenance)\": \"unspecified\", \"License Attribution (DataProvenance)\": 0, \"License Share Alike (DataProvenance)\": 0, \"License Use (DataProvenance IgnoreOpenAI)\": \"unspecified\", \"License Attribution (DataProvenance IgnoreOpenAI)\": 0, \"License Share Alike (DataProvenance IgnoreOpenAI)\": 0, \"License Type\": \"Unspecified\"}, {\"Unique Dataset Identifier\": \"egopet\", \"Collection\": \"egopet\", \"Collection URL\": \"https://arxiv.org/pdf/2404.09991\", \"Dataset Name\": \"EgoPet: Egomotion and Interaction Data from an Animal's Perspective\", \"Paper Title\": \"EgoPet: Egomotion and Interaction Data from an Animal's Perspective\", \"Paper URL\": \"https://arxiv.org/pdf/2404.09991\", \"GitHub URL\": \"\", \"Hugging Face URL\": \"\", \"Papers with Code URL\": \"https://paperswithcode.com/paper/egopet-egomotion-and-interaction-data-from-an\", \"ArXiv URL\": \"https://arxiv.org/pdf/2404.09991\", \"Semantic Scholar Corpus ID\": 269148727, \"Year Released\": \"2024\", \"Text Sources\": [\"tiktok\", \"youtube\"], \"Licenses\": [{\"License\": \"CC BY-NC 4.0\", \"License URL\": \"https://github.com/DannyTran123/egopet/blob/main/LICENSE\"}], \"Creators\": [\"Tel Aviv University\", \"UC Berkeley\", \"New York University\"], \"Countries\": [\"United States of America\", \"Israel\"], \"License Verified By\": \"Vivek Sharma\", \"Video Hours\": 84.0, \"Taken Down\": \"False\", \"Video Sources\": \"youtube\", \"Task Categories\": [\"Misc (Locomotion Prediction\", \"Visual Interaction Prediction\", \"Vision to Proprioception Prediction)\"], \"Inferred Metadata\": null, \"Bibtex\": null, \"License Use (DataProvenance)\": \"non-commercial\", \"License Attribution (DataProvenance)\": 1, \"License Share Alike (DataProvenance)\": 0, \"License Use (DataProvenance IgnoreOpenAI)\": \"non-commercial\", \"License Attribution (DataProvenance IgnoreOpenAI)\": 1, \"License Share Alike (DataProvenance IgnoreOpenAI)\": 0, \"License Type\": \"Non-Commercial/Academic\"}]}}, {\"mode\": \"vega-lite\"});\n",
       "</script>"
      ],
      "text/plain": [
       "alt.LayerChart(...)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_sourceyear = alt.Chart(\n",
    "    df_videosourceyears\n",
    ").mark_bar().encode(\n",
    "    x=alt.X(\n",
    "        \"Year Released:N\",\n",
    "        title=\"Year Released\",\n",
    "        sort=YEARS_ORDER,\n",
    "        axis=alt.Axis(labelAngle=-30)\n",
    "    ),\n",
    "    y=alt.Y(\n",
    "        \"count():Q\",\n",
    "        stack=\"normalize\",\n",
    "        axis=alt.Axis(format=\"%\"),\n",
    "        title=\"Pct. Datasets\"\n",
    "    ),\n",
    "    color=alt.Color(\n",
    "        \"Video Sources:N\",\n",
    "        title=\"Video Sources\"\n",
    "    )\n",
    ").properties(\n",
    "    width=600,\n",
    "    height=160\n",
    ")\n",
    "\n",
    "text_sourceyear = alt.Chart(df_videosourceyears).mark_text(\n",
    "    dy=-90,\n",
    "    align=\"center\",\n",
    "    baseline=\"top\",\n",
    "    fontSize=12\n",
    ").encode(\n",
    "    x=alt.X(\n",
    "        \"Year Released:N\",\n",
    "        title=\"Year Released\",\n",
    "        sort=YEARS_ORDER\n",
    "    ),\n",
    "    text=\"count():Q\"\n",
    ")\n",
    "\n",
    "chart_sourceyear = (base_sourceyear + text_sourceyear).configure_axis(\n",
    "    labelFontSize=FONT_SIZE,\n",
    "    titleFontSize=FONT_SIZE\n",
    ").configure_legend(\n",
    "    labelFontSize=FONT_SIZE,\n",
    "    titleFontSize=FONT_SIZE,\n",
    "    orient=LEGEND_POSITION,\n",
    "    columns=4,\n",
    "    labelLimit=MAX_LABELLIMIT\n",
    ")\n",
    "\n",
    "\n",
    "if PLOT_TOFILE:\n",
    "    chart_sourceyear.save(\n",
    "        os.path.join(PLOT_DIR, \"video_sourcecategories-years.png\"),\n",
    "        ppi=PLOT_PPI\n",
    "    )\n",
    "\n",
    "chart_sourceyear"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creator Category by Year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creators\n",
      "Academic          129\n",
      "Corporation        20\n",
      "Industry Lab       17\n",
      "Research Group     11\n",
      "Other               5\n",
      "Government          1\n",
      "Startup             1\n",
      "Name: count, dtype: int64\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Creators</th>\n",
       "      <th>Year Released</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>Academic</td>\n",
       "      <td>2009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>Research Group</td>\n",
       "      <td>2009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>Academic</td>\n",
       "      <td>2011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>Academic</td>\n",
       "      <td>2011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>Academic</td>\n",
       "      <td>2011</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Creators Year Released\n",
       "58        Academic          2009\n",
       "88  Research Group          2009\n",
       "63        Academic          2011\n",
       "63        Academic          2011\n",
       "63        Academic          2011"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_video['Creators'] = df_video['Creators'].apply(\n",
    "    lambda x: [creator_categories.get(item, item) for item in x]\n",
    ")\n",
    "\n",
    "print(df_video['Creators'].explode().value_counts())\n",
    "\n",
    "INCLUDE_TOP_N_CATEGORIES = 6\n",
    "df_videocreatoryears = df_video.explode(\"Creators\")\n",
    "df_videocreatoryears = reduce_categories_to_topk(df_videocreatoryears, \"Creators\", INCLUDE_TOP_N_CATEGORIES)\n",
    "\n",
    "df_videocreatoryears = df_videocreatoryears.sort_values(by=\"Year Released\")\n",
    "df_videocreatoryears.head()[['Creators', 'Year Released']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "  #altair-viz-ef246dd33b3e44c0b0652d0efcf8e1a8.vega-embed {\n",
       "    width: 100%;\n",
       "    display: flex;\n",
       "  }\n",
       "\n",
       "  #altair-viz-ef246dd33b3e44c0b0652d0efcf8e1a8.vega-embed details,\n",
       "  #altair-viz-ef246dd33b3e44c0b0652d0efcf8e1a8.vega-embed details summary {\n",
       "    position: relative;\n",
       "  }\n",
       "</style>\n",
       "<div id=\"altair-viz-ef246dd33b3e44c0b0652d0efcf8e1a8\"></div>\n",
       "<script type=\"text/javascript\">\n",
       "  var VEGA_DEBUG = (typeof VEGA_DEBUG == \"undefined\") ? {} : VEGA_DEBUG;\n",
       "  (function(spec, embedOpt){\n",
       "    let outputDiv = document.currentScript.previousElementSibling;\n",
       "    if (outputDiv.id !== \"altair-viz-ef246dd33b3e44c0b0652d0efcf8e1a8\") {\n",
       "      outputDiv = document.getElementById(\"altair-viz-ef246dd33b3e44c0b0652d0efcf8e1a8\");\n",
       "    }\n",
       "    const paths = {\n",
       "      \"vega\": \"https://cdn.jsdelivr.net/npm/vega@5?noext\",\n",
       "      \"vega-lib\": \"https://cdn.jsdelivr.net/npm/vega-lib?noext\",\n",
       "      \"vega-lite\": \"https://cdn.jsdelivr.net/npm/vega-lite@5.20.1?noext\",\n",
       "      \"vega-embed\": \"https://cdn.jsdelivr.net/npm/vega-embed@6?noext\",\n",
       "    };\n",
       "\n",
       "    function maybeLoadScript(lib, version) {\n",
       "      var key = `${lib.replace(\"-\", \"\")}_version`;\n",
       "      return (VEGA_DEBUG[key] == version) ?\n",
       "        Promise.resolve(paths[lib]) :\n",
       "        new Promise(function(resolve, reject) {\n",
       "          var s = document.createElement('script');\n",
       "          document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
       "          s.async = true;\n",
       "          s.onload = () => {\n",
       "            VEGA_DEBUG[key] = version;\n",
       "            return resolve(paths[lib]);\n",
       "          };\n",
       "          s.onerror = () => reject(`Error loading script: ${paths[lib]}`);\n",
       "          s.src = paths[lib];\n",
       "        });\n",
       "    }\n",
       "\n",
       "    function showError(err) {\n",
       "      outputDiv.innerHTML = `<div class=\"error\" style=\"color:red;\">${err}</div>`;\n",
       "      throw err;\n",
       "    }\n",
       "\n",
       "    function displayChart(vegaEmbed) {\n",
       "      vegaEmbed(outputDiv, spec, embedOpt)\n",
       "        .catch(err => showError(`Javascript Error: ${err.message}<br>This usually means there's a typo in your chart specification. See the javascript console for the full traceback.`));\n",
       "    }\n",
       "\n",
       "    if(typeof define === \"function\" && define.amd) {\n",
       "      requirejs.config({paths});\n",
       "      require([\"vega-embed\"], displayChart, err => showError(`Error loading script: ${err.message}`));\n",
       "    } else {\n",
       "      maybeLoadScript(\"vega\", \"5\")\n",
       "        .then(() => maybeLoadScript(\"vega-lite\", \"5.20.1\"))\n",
       "        .then(() => maybeLoadScript(\"vega-embed\", \"6\"))\n",
       "        .catch(showError)\n",
       "        .then(() => displayChart(vegaEmbed));\n",
       "    }\n",
       "  })({\"config\": {\"title\": {\"font\": \"Times New Roman\"}, \"axis\": {\"labelFont\": \"Times New Roman\", \"titleFont\": \"Times New Roman\", \"labelFontSize\": 20, \"titleFontSize\": 20}, \"header\": {\"labelFont\": \"Times New Roman\", \"titleFont\": \"Times New Roman\"}, \"legend\": {\"labelFont\": \"Times New Roman\", \"titleFont\": \"Times New Roman\", \"columns\": 4, \"labelFontSize\": 20, \"labelLimit\": 1000, \"orient\": \"bottom\", \"titleFontSize\": 20}, \"text\": {\"font\": \"Times New Roman\"}}, \"layer\": [{\"mark\": {\"type\": \"bar\"}, \"encoding\": {\"color\": {\"field\": \"Creators\", \"title\": \"Video Creator Cateogies\", \"type\": \"nominal\"}, \"x\": {\"axis\": {\"labelAngle\": -30}, \"field\": \"Year Released\", \"sort\": [\"<2004\", \"2004\", \"2005\", \"2006\", \"2007\", \"2008\", \"2009\", \"2010\", \"2011\", \"2012\", \"2013\", \"2014\", \"2015\", \"2016\", \"2017\", \"2018\", \"2019\", \"2020\", \"2021\", \"2022\", \"2023\", \"2024\"], \"title\": \"Year Released\", \"type\": \"nominal\"}, \"y\": {\"aggregate\": \"count\", \"axis\": {\"format\": \"%\"}, \"stack\": \"normalize\", \"title\": \"Pct. Datasets\", \"type\": \"quantitative\"}}}, {\"mark\": {\"type\": \"text\", \"align\": \"center\", \"baseline\": \"top\", \"dy\": -90, \"fontSize\": 12}, \"encoding\": {\"text\": {\"aggregate\": \"count\", \"type\": \"quantitative\"}, \"x\": {\"field\": \"Year Released\", \"sort\": [\"<2004\", \"2004\", \"2005\", \"2006\", \"2007\", \"2008\", \"2009\", \"2010\", \"2011\", \"2012\", \"2013\", \"2014\", \"2015\", \"2016\", \"2017\", \"2018\", \"2019\", \"2020\", \"2021\", \"2022\", \"2023\", \"2024\"], \"title\": \"Year Released\", \"type\": \"nominal\"}}}], \"data\": {\"name\": \"data-f9f8bc2aeddfad6ee122ac77a87ba0df\"}, \"height\": 160, \"width\": 600, \"$schema\": \"https://vega.github.io/schema/vega-lite/v5.20.1.json\", \"datasets\": {\"data-f9f8bc2aeddfad6ee122ac77a87ba0df\": [{\"Unique Dataset Identifier\": \"collective\", \"Collection\": \"collective\", \"Collection URL\": \"https://cvgl.stanford.edu/papers/Wongun_CollectiveActivityRecognition09.pdf\", \"Dataset Name\": \"Collective: What are they doing? : Collective activity classification using spatio-temporal relationship among people\", \"Paper Title\": \"Collective: What are they doing? : Collective activity classification using spatio-temporal relationship among people\", \"Paper URL\": \"https://cvgl.stanford.edu/papers/Wongun_CollectiveActivityRecognition09.pdf\", \"GitHub URL\": \"\", \"Hugging Face URL\": \"\", \"Papers with Code URL\": \"\", \"ArXiv URL\": \"https://cvgl.stanford.edu/papers/Wongun_CollectiveActivityRecognition09.pdf\", \"Semantic Scholar Corpus ID\": 5925915, \"Year Released\": \"2009\", \"Text Sources\": [\"human\"], \"Licenses\": [{\"License\": \"Unspecified\", \"License URL\": \"\"}], \"Creators\": \"Academic\", \"Countries\": [\"United States of America\"], \"License Verified By\": \"Vivek Sharma\", \"Video Hours\": 0.1, \"Taken Down\": \"False\", \"Video Sources\": [\"human\"], \"Task Categories\": [\"Group Activity Recognition\"], \"Inferred Metadata\": {\"GitHub License\": \"\", \"GitHub Stars (June 2024)\": \"\", \"GitHub Topics\": \"\", \"Github Date\": \"\", \"HF Config\": \"\", \"HF Config License\": \"\", \"HF Dataset\": \"\", \"HF Date\": \"\", \"HF Downloads (June 2024)\": \"\", \"HF Likes (June 2024)\": \"\", \"HF Yaml License\": \"\", \"PwC Date\": \"\", \"PwC Description\": \"\", \"PwC License Name\": \"\", \"PwC License URL\": \"\", \"S2 Citation Count (June 2024)\": \"\", \"S2 Date\": \"\"}, \"Bibtex\": \"@Article{Choi2009WhatAT,\\n author = {Wongun Choi and Khuram Shahid and S. Savarese},\\n booktitle = {2009 IEEE 12th International Conference on Computer Vision Workshops, ICCV Workshops},\\n journal = {2009 IEEE 12th International Conference on Computer Vision Workshops, ICCV Workshops},\\n pages = {1282-1289},\\n title = {What are they doing? : Collective activity classification using spatio-temporal relationship among people},\\n year = {2009}\\n}\\n\", \"License Use (DataProvenance)\": \"unspecified\", \"License Attribution (DataProvenance)\": 0, \"License Share Alike (DataProvenance)\": 0, \"License Use (DataProvenance IgnoreOpenAI)\": \"unspecified\", \"License Attribution (DataProvenance IgnoreOpenAI)\": 0, \"License Share Alike (DataProvenance IgnoreOpenAI)\": 0, \"License Type\": \"Unspecified\"}, {\"Unique Dataset Identifier\": \"hollywood2-dataset\", \"Collection\": \"hollywood2-dataset\", \"Collection URL\": \"https://www.irisa.fr/vista/Papers/2009_cvpr_marszalek.pdf\", \"Dataset Name\": \"HOLLYWOOD2: Actions in Context (CVPR 2009)\", \"Paper Title\": \"HOLLYWOOD2: Actions in Context (CVPR 2009)\", \"Paper URL\": \"https://www.irisa.fr/vista/Papers/2009_cvpr_marszalek.pdf\", \"GitHub URL\": \"\", \"Hugging Face URL\": \"\", \"Papers with Code URL\": \"\", \"ArXiv URL\": \"https://www.irisa.fr/vista/Papers/2009_cvpr_marszalek.pdf\", \"Semantic Scholar Corpus ID\": 3155054, \"Year Released\": \"2009\", \"Text Sources\": [\"movies\"], \"Licenses\": [{\"License\": \"Unspecified\", \"License URL\": \"\"}], \"Creators\": \"Research Group\", \"Countries\": [\"France\"], \"License Verified By\": \"Vivek Sharma\", \"Video Hours\": 20.1, \"Taken Down\": \"False\", \"Video Sources\": [\"movies\"], \"Task Categories\": [\"Video Classification\"], \"Inferred Metadata\": {\"GitHub License\": \"\", \"GitHub Stars (June 2024)\": \"\", \"GitHub Topics\": \"\", \"Github Date\": \"\", \"HF Config\": \"\", \"HF Config License\": \"\", \"HF Dataset\": \"\", \"HF Date\": \"\", \"HF Downloads (June 2024)\": \"\", \"HF Likes (June 2024)\": \"\", \"HF Yaml License\": \"\", \"PwC Date\": \"\", \"PwC Description\": \"\", \"PwC License Name\": \"\", \"PwC License URL\": \"\", \"S2 Citation Count (June 2024)\": \"\", \"S2 Date\": \"\"}, \"Bibtex\": \"@Article{Marszalek2009ActionsIC,\\n author = {Marcin Marszalek and I. Laptev and C. Schmid},\\n booktitle = {2009 IEEE Conference on Computer Vision and Pattern Recognition},\\n journal = {2009 IEEE Conference on Computer Vision and Pattern Recognition},\\n pages = {2929-2936},\\n title = {Actions in context},\\n year = {2009}\\n}\\n\", \"License Use (DataProvenance)\": \"unspecified\", \"License Attribution (DataProvenance)\": 0, \"License Share Alike (DataProvenance)\": 0, \"License Use (DataProvenance IgnoreOpenAI)\": \"unspecified\", \"License Attribution (DataProvenance IgnoreOpenAI)\": 0, \"License Share Alike (DataProvenance IgnoreOpenAI)\": 0, \"License Type\": \"Unspecified\"}, {\"Unique Dataset Identifier\": \"hmdb-dataset\", \"Collection\": \"hmdb-dataset\", \"Collection URL\": \"https://ieeexplore.ieee.org/document/6126543\", \"Dataset Name\": \"HMDB: A Large Video Database for Human Motion Recognition (ICCV 2011)\", \"Paper Title\": \"HMDB: A Large Video Database for Human Motion Recognition (ICCV 2011)\", \"Paper URL\": \"https://ieeexplore.ieee.org/document/6126543\", \"GitHub URL\": \"\", \"Hugging Face URL\": \"\", \"Papers with Code URL\": \"https://paperswithcode.com/dataset/hmdb51\", \"ArXiv URL\": \"https://ieeexplore.ieee.org/document/6126543\", \"Semantic Scholar Corpus ID\": 206769852, \"Year Released\": \"2011\", \"Text Sources\": [\"movies\", \"prelinger archive\", \"undisclosed web\", \"youtube\", \"google videos\"], \"Licenses\": [{\"License\": \"CC BY 4.0\", \"License URL\": \"https://serre-lab.clps.brown.edu/resource/hmdb-a-large-human-motion-database/\"}], \"Creators\": \"Academic\", \"Countries\": [\"United States of America\", \"Germany\"], \"License Verified By\": \"Vivek Sharma\", \"Video Hours\": 7000.0, \"Taken Down\": \"False\", \"Video Sources\": [\"movies\", \"prelinger archive\", \"undisclosed web\", \"youtube\", \"google videos\"], \"Task Categories\": [\"Video Classification\"], \"Inferred Metadata\": {\"GitHub License\": \"\", \"GitHub Stars (June 2024)\": \"\", \"GitHub Topics\": \"\", \"Github Date\": \"\", \"HF Config\": \"\", \"HF Config License\": \"\", \"HF Dataset\": \"\", \"HF Date\": \"\", \"HF Downloads (June 2024)\": \"\", \"HF Likes (June 2024)\": \"\", \"HF Yaml License\": \"\", \"PwC Date\": \"2011-01-01\", \"PwC Description\": \"The HMDB51 dataset is a large collection of realistic videos from various sources, including movies and web videos. The dataset is composed of 6,766 video clips from 51 action categories (such as \\u201cjump\\u201d, \\u201ckiss\\u201d and \\u201claugh\\u201d), with each category containing at least 101 clips. The original evaluation scheme uses three different training/testing splits. In each split, each action class has 70 clips for training and 30 clips for testing. The average accuracy over these three splits is used to measure the final performance.\", \"PwC License Name\": \"CC BY 4.0\", \"PwC License URL\": \"https://creativecommons.org/licenses/by/4.0/\", \"S2 Citation Count (June 2024)\": \"\", \"S2 Date\": \"\"}, \"Bibtex\": \"@Article{Kuehne2011HMDBAL,\\n author = {Hilde Kuehne and Hueihan Jhuang and Est\\u00edbaliz Garrote and T. Poggio and Thomas Serre},\\n booktitle = {Vision},\\n journal = {2011 International Conference on Computer Vision},\\n pages = {2556-2563},\\n title = {HMDB: A large video database for human motion recognition},\\n year = {2011}\\n}\\n\", \"License Use (DataProvenance)\": \"commercial\", \"License Attribution (DataProvenance)\": 1, \"License Share Alike (DataProvenance)\": 0, \"License Use (DataProvenance IgnoreOpenAI)\": \"commercial\", \"License Attribution (DataProvenance IgnoreOpenAI)\": 1, \"License Share Alike (DataProvenance IgnoreOpenAI)\": 0, \"License Type\": \"Commercial\"}, {\"Unique Dataset Identifier\": \"hmdb-dataset\", \"Collection\": \"hmdb-dataset\", \"Collection URL\": \"https://ieeexplore.ieee.org/document/6126543\", \"Dataset Name\": \"HMDB: A Large Video Database for Human Motion Recognition (ICCV 2011)\", \"Paper Title\": \"HMDB: A Large Video Database for Human Motion Recognition (ICCV 2011)\", \"Paper URL\": \"https://ieeexplore.ieee.org/document/6126543\", \"GitHub URL\": \"\", \"Hugging Face URL\": \"\", \"Papers with Code URL\": \"https://paperswithcode.com/dataset/hmdb51\", \"ArXiv URL\": \"https://ieeexplore.ieee.org/document/6126543\", \"Semantic Scholar Corpus ID\": 206769852, \"Year Released\": \"2011\", \"Text Sources\": [\"movies\", \"prelinger archive\", \"undisclosed web\", \"youtube\", \"google videos\"], \"Licenses\": [{\"License\": \"CC BY 4.0\", \"License URL\": \"https://serre-lab.clps.brown.edu/resource/hmdb-a-large-human-motion-database/\"}], \"Creators\": \"Academic\", \"Countries\": [\"United States of America\", \"Germany\"], \"License Verified By\": \"Vivek Sharma\", \"Video Hours\": 7000.0, \"Taken Down\": \"False\", \"Video Sources\": [\"movies\", \"prelinger archive\", \"undisclosed web\", \"youtube\", \"google videos\"], \"Task Categories\": [\"Video Classification\"], \"Inferred Metadata\": {\"GitHub License\": \"\", \"GitHub Stars (June 2024)\": \"\", \"GitHub Topics\": \"\", \"Github Date\": \"\", \"HF Config\": \"\", \"HF Config License\": \"\", \"HF Dataset\": \"\", \"HF Date\": \"\", \"HF Downloads (June 2024)\": \"\", \"HF Likes (June 2024)\": \"\", \"HF Yaml License\": \"\", \"PwC Date\": \"2011-01-01\", \"PwC Description\": \"The HMDB51 dataset is a large collection of realistic videos from various sources, including movies and web videos. The dataset is composed of 6,766 video clips from 51 action categories (such as \\u201cjump\\u201d, \\u201ckiss\\u201d and \\u201claugh\\u201d), with each category containing at least 101 clips. The original evaluation scheme uses three different training/testing splits. In each split, each action class has 70 clips for training and 30 clips for testing. The average accuracy over these three splits is used to measure the final performance.\", \"PwC License Name\": \"CC BY 4.0\", \"PwC License URL\": \"https://creativecommons.org/licenses/by/4.0/\", \"S2 Citation Count (June 2024)\": \"\", \"S2 Date\": \"\"}, \"Bibtex\": \"@Article{Kuehne2011HMDBAL,\\n author = {Hilde Kuehne and Hueihan Jhuang and Est\\u00edbaliz Garrote and T. Poggio and Thomas Serre},\\n booktitle = {Vision},\\n journal = {2011 International Conference on Computer Vision},\\n pages = {2556-2563},\\n title = {HMDB: A large video database for human motion recognition},\\n year = {2011}\\n}\\n\", \"License Use (DataProvenance)\": \"commercial\", \"License Attribution (DataProvenance)\": 1, \"License Share Alike (DataProvenance)\": 0, \"License Use (DataProvenance IgnoreOpenAI)\": \"commercial\", \"License Attribution (DataProvenance IgnoreOpenAI)\": 1, \"License Share Alike (DataProvenance IgnoreOpenAI)\": 0, \"License Type\": \"Commercial\"}, {\"Unique Dataset Identifier\": \"hmdb-dataset\", \"Collection\": \"hmdb-dataset\", \"Collection URL\": \"https://ieeexplore.ieee.org/document/6126543\", \"Dataset Name\": \"HMDB: A Large Video Database for Human Motion Recognition (ICCV 2011)\", \"Paper Title\": \"HMDB: A Large Video Database for Human Motion Recognition (ICCV 2011)\", \"Paper URL\": \"https://ieeexplore.ieee.org/document/6126543\", \"GitHub URL\": \"\", \"Hugging Face URL\": \"\", \"Papers with Code URL\": \"https://paperswithcode.com/dataset/hmdb51\", \"ArXiv URL\": \"https://ieeexplore.ieee.org/document/6126543\", \"Semantic Scholar Corpus ID\": 206769852, \"Year Released\": \"2011\", \"Text Sources\": [\"movies\", \"prelinger archive\", \"undisclosed web\", \"youtube\", \"google videos\"], \"Licenses\": [{\"License\": \"CC BY 4.0\", \"License URL\": \"https://serre-lab.clps.brown.edu/resource/hmdb-a-large-human-motion-database/\"}], \"Creators\": \"Academic\", \"Countries\": [\"United States of America\", \"Germany\"], \"License Verified By\": \"Vivek Sharma\", \"Video Hours\": 7000.0, \"Taken Down\": \"False\", \"Video Sources\": [\"movies\", \"prelinger archive\", \"undisclosed web\", \"youtube\", \"google videos\"], \"Task Categories\": [\"Video Classification\"], \"Inferred Metadata\": {\"GitHub License\": \"\", \"GitHub Stars (June 2024)\": \"\", \"GitHub Topics\": \"\", \"Github Date\": \"\", \"HF Config\": \"\", \"HF Config License\": \"\", \"HF Dataset\": \"\", \"HF Date\": \"\", \"HF Downloads (June 2024)\": \"\", \"HF Likes (June 2024)\": \"\", \"HF Yaml License\": \"\", \"PwC Date\": \"2011-01-01\", \"PwC Description\": \"The HMDB51 dataset is a large collection of realistic videos from various sources, including movies and web videos. The dataset is composed of 6,766 video clips from 51 action categories (such as \\u201cjump\\u201d, \\u201ckiss\\u201d and \\u201claugh\\u201d), with each category containing at least 101 clips. The original evaluation scheme uses three different training/testing splits. In each split, each action class has 70 clips for training and 30 clips for testing. The average accuracy over these three splits is used to measure the final performance.\", \"PwC License Name\": \"CC BY 4.0\", \"PwC License URL\": \"https://creativecommons.org/licenses/by/4.0/\", \"S2 Citation Count (June 2024)\": \"\", \"S2 Date\": \"\"}, \"Bibtex\": \"@Article{Kuehne2011HMDBAL,\\n author = {Hilde Kuehne and Hueihan Jhuang and Est\\u00edbaliz Garrote and T. Poggio and Thomas Serre},\\n booktitle = {Vision},\\n journal = {2011 International Conference on Computer Vision},\\n pages = {2556-2563},\\n title = {HMDB: A large video database for human motion recognition},\\n year = {2011}\\n}\\n\", \"License Use (DataProvenance)\": \"commercial\", \"License Attribution (DataProvenance)\": 1, \"License Share Alike (DataProvenance)\": 0, \"License Use (DataProvenance IgnoreOpenAI)\": \"commercial\", \"License Attribution (DataProvenance IgnoreOpenAI)\": 1, \"License Share Alike (DataProvenance IgnoreOpenAI)\": 0, \"License Type\": \"Commercial\"}, {\"Unique Dataset Identifier\": \"ucf101-dataset\", \"Collection\": \"ucf101-dataset\", \"Collection URL\": \"https://arxiv.org/abs/1212.0402\", \"Dataset Name\": \"UCF101: A Dataset of 101 Human Actions Classes From Videos in The Wild\", \"Paper Title\": \"UCF101: A Dataset of 101 Human Actions Classes From Videos in The Wild\", \"Paper URL\": \"https://arxiv.org/abs/1212.0402\", \"GitHub URL\": \"\", \"Hugging Face URL\": \"\", \"Papers with Code URL\": \"https://paperswithcode.com/dataset/ucf101\", \"ArXiv URL\": \"https://arxiv.org/abs/1212.0402\", \"Semantic Scholar Corpus ID\": 7197134, \"Year Released\": \"2012\", \"Text Sources\": [\"youtube\"], \"Licenses\": [{\"License\": \"Unspecified\", \"License URL\": \"\"}], \"Creators\": \"Academic\", \"Countries\": [\"United States of America\"], \"License Verified By\": \"Vivek Sharma\", \"Video Hours\": 26.0, \"Taken Down\": \"False\", \"Video Sources\": [\"youtube\"], \"Task Categories\": [\"Video Classification\"], \"Inferred Metadata\": {\"GitHub License\": \"\", \"GitHub Stars (June 2024)\": \"\", \"GitHub Topics\": \"\", \"Github Date\": \"\", \"HF Config\": \"\", \"HF Config License\": \"\", \"HF Dataset\": \"\", \"HF Date\": \"\", \"HF Downloads (June 2024)\": \"\", \"HF Likes (June 2024)\": \"\", \"HF Yaml License\": \"\", \"PwC Date\": \"2012-12-03\", \"PwC Description\": \"UCF101 dataset is an extension of UCF50 and consists of 13,320 video clips, which are classified into 101 categories. These 101 categories can be classified into 5 types (Body motion, Human-human interactions, Human-object interactions, Playing musical instruments and Sports). The total length of these video clips is over 27 hours. All the videos are collected from YouTube and have a fixed frame rate of 25 FPS with the resolution of 320 \\u00d7 240.\", \"PwC License Name\": \"Unspecified\", \"PwC License URL\": null, \"S2 Citation Count (June 2024)\": \"\", \"S2 Date\": \"\"}, \"Bibtex\": \"@Article{Soomro2012UCF101AD,\\n author = {K. Soomro and Amir Zamir and M. Shah},\\n booktitle = {arXiv.org},\\n journal = {ArXiv},\\n title = {UCF101: A Dataset of 101 Human Actions Classes From Videos in The Wild},\\n volume = {abs/1212.0402},\\n year = {2012}\\n}\\n\", \"License Use (DataProvenance)\": \"unspecified\", \"License Attribution (DataProvenance)\": 0, \"License Share Alike (DataProvenance)\": 0, \"License Use (DataProvenance IgnoreOpenAI)\": \"unspecified\", \"License Attribution (DataProvenance IgnoreOpenAI)\": 0, \"License Share Alike (DataProvenance IgnoreOpenAI)\": 0, \"License Type\": \"Unspecified\"}, {\"Unique Dataset Identifier\": \"50salads\", \"Collection\": \"50salads\", \"Collection URL\": \"https://citeseerx.ist.psu.edu/document?repid=rep1&type=pdf&doi=b9410401cec076baef045e83953f3ff24f25d149\", \"Dataset Name\": \"50Salads\", \"Paper Title\": \"50Salads\", \"Paper URL\": \"https://citeseerx.ist.psu.edu/document?repid=rep1&type=pdf&doi=b9410401cec076baef045e83953f3ff24f25d149\", \"GitHub URL\": \"\", \"Hugging Face URL\": \"\", \"Papers with Code URL\": \"https://paperswithcode.com/dataset/50-salads\", \"ArXiv URL\": \"https://citeseerx.ist.psu.edu/document?repid=rep1&type=pdf&doi=b9410401cec076baef045e83953f3ff24f25d149\", \"Semantic Scholar Corpus ID\": 2333743, \"Year Released\": \"2013\", \"Text Sources\": [\"human\"], \"Licenses\": [{\"License\": \"CC BY-NC-SA 4.0\", \"License URL\": \"https://cvip.computing.dundee.ac.uk/datasets/foodpreparation/50salads/\"}], \"Creators\": \"Academic\", \"Countries\": [\"United Kingdom\"], \"License Verified By\": \"Vivek Sharma\", \"Video Hours\": 40.0, \"Taken Down\": \"False\", \"Video Sources\": [\"human\"], \"Task Categories\": [\"Action Segmentation\", \"Action Localization\"], \"Inferred Metadata\": {\"GitHub License\": \"\", \"GitHub Stars (June 2024)\": \"\", \"GitHub Topics\": \"\", \"Github Date\": \"\", \"HF Config\": \"\", \"HF Config License\": \"\", \"HF Dataset\": \"\", \"HF Date\": \"\", \"HF Downloads (June 2024)\": \"\", \"HF Likes (June 2024)\": \"\", \"HF Yaml License\": \"\", \"PwC Date\": \"2013-09-08\", \"PwC Description\": \"Activity recognition research has shifted focus from distinguishing full-body motion patterns to recognizing complex interactions of multiple entities. Manipulative gestures \\u2013 characterized by interactions between hands, tools, and manipulable objects \\u2013 frequently occur in food preparation, manufacturing, and assembly tasks, and have a variety of applications including situational support, automated supervision, and skill assessment. With the aim to stimulate research on recognizing manipulative gestures we introduce the 50 Salads dataset. It captures 25 people preparing 2 mixed salads each and contains over 4h of annotated accelerometer and RGB-D video data. Including detailed annotations, multiple sensor types, and two sequences per participant, the 50 Salads dataset may be used for research in areas such as activity recognition, activity spotting, sequence analysis, progress tracking, sensor fusion, transfer learning, and user-adaptation.\\n\\nThe dataset includes\\n\\nRGB video data 640\\u00d7480 pixels at 30 Hz\\nDepth maps 640\\u00d7480 pixels at 30 Hz\\n3-axis accelerometer data at 50 Hz of devices attached to a knife, a mixing spoon, a small spoon, a peeler, a glass, an oil bottle, and a pepper dispenser.\\nSynchronization parameters for temporal alignment of video and accelerometer data\\nAnnotations as temporal intervals of pre- core- and post-phases of activities corresponding to steps in a recipe\", \"PwC License Name\": \"CC BY-NC-SA 4.0\", \"PwC License URL\": null, \"S2 Citation Count (June 2024)\": \"\", \"S2 Date\": \"\"}, \"Bibtex\": \"@Book{Stein2013CombiningEA,\\n author = {Sebastian Stein and S. McKenna},\\n booktitle = {Ubiquitous Computing},\\n journal = {Proceedings of the 2013 ACM international joint conference on Pervasive and ubiquitous computing},\\n title = {Combining embedded accelerometers with computer vision for recognizing food preparation activities},\\n year = {2013}\\n}\\n\", \"License Use (DataProvenance)\": \"non-commercial\", \"License Attribution (DataProvenance)\": 1, \"License Share Alike (DataProvenance)\": 1, \"License Use (DataProvenance IgnoreOpenAI)\": \"non-commercial\", \"License Attribution (DataProvenance IgnoreOpenAI)\": 1, \"License Share Alike (DataProvenance IgnoreOpenAI)\": 1, \"License Type\": \"Non-Commercial/Academic\"}, {\"Unique Dataset Identifier\": \"youcook\", \"Collection\": \"youcook\", \"Collection URL\": \"https://openaccess.thecvf.com/content_cvpr_2013/papers/Das_A_Thousand_Frames_2013_CVPR_paper.pdf\", \"Dataset Name\": \"YouCook: A Thousand Frames in Just a Few Words: Lingual Description of Videos through Latent Topics and Sparse Object Stitching (CVPR 2013)\", \"Paper Title\": \"YouCook: A Thousand Frames in Just a Few Words: Lingual Description of Videos through Latent Topics and Sparse Object Stitching (CVPR 2013)\", \"Paper URL\": \"https://openaccess.thecvf.com/content_cvpr_2013/papers/Das_A_Thousand_Frames_2013_CVPR_paper.pdf\", \"GitHub URL\": \"\", \"Hugging Face URL\": \"\", \"Papers with Code URL\": \"https://paperswithcode.com/dataset/youcook\", \"ArXiv URL\": \"https://openaccess.thecvf.com/content_cvpr_2013/papers/Das_A_Thousand_Frames_2013_CVPR_paper.pdf\", \"Semantic Scholar Corpus ID\": 12284555, \"Year Released\": \"2013\", \"Text Sources\": [\"youtube\"], \"Licenses\": [{\"License\": \"Unspecified\", \"License URL\": \"\"}], \"Creators\": \"Academic\", \"Countries\": [\"United States of America\"], \"License Verified By\": \"Vivek Sharma\", \"Video Hours\": 1000.0, \"Taken Down\": \"False\", \"Video Sources\": [\"youtube\"], \"Task Categories\": [\"Video Captioning\"], \"Inferred Metadata\": {\"GitHub License\": \"\", \"GitHub Stars (June 2024)\": \"\", \"GitHub Topics\": \"\", \"Github Date\": \"\", \"HF Config\": \"\", \"HF Config License\": \"\", \"HF Dataset\": \"\", \"HF Date\": \"\", \"HF Downloads (June 2024)\": \"\", \"HF Likes (June 2024)\": \"\", \"HF Yaml License\": \"\", \"PwC Date\": \"\", \"PwC Description\": \"This data set was prepared from 88 open-source YouTube cooking videos. The YouCook dataset contains videos of people cooking various recipes. The videos were downloaded from YouTube and are all in the third-person viewpoint; they represent a significantly more challenging visual problem than existing cooking and kitchen datasets (the background kitchen/scene is different for many and most videos have dynamic camera changes). In addition, frame-by-frame object and action annotations are provided for training data (as well as a number of precomputed low-level features). Finally, each video has a number of human provided natural language descriptions (on average, there are eight different descriptions per video). This dataset has been created to serve as a benchmark in describing complex real-world videos with natural language descriptions.\", \"PwC License Name\": \"Unspecified\", \"PwC License URL\": null, \"S2 Citation Count (June 2024)\": \"\", \"S2 Date\": \"\"}, \"Bibtex\": \"@Article{Das2013ATF,\\n author = {Pradipto Das and Chenliang Xu and Richard F. Doell and Jason J. Corso},\\n booktitle = {2013 IEEE Conference on Computer Vision and Pattern Recognition},\\n journal = {2013 IEEE Conference on Computer Vision and Pattern Recognition},\\n pages = {2634-2641},\\n title = {A Thousand Frames in Just a Few Words: Lingual Description of Videos through Latent Topics and Sparse Object Stitching},\\n year = {2013}\\n}\\n\", \"License Use (DataProvenance)\": \"unspecified\", \"License Attribution (DataProvenance)\": 0, \"License Share Alike (DataProvenance)\": 0, \"License Use (DataProvenance IgnoreOpenAI)\": \"unspecified\", \"License Attribution (DataProvenance IgnoreOpenAI)\": 0, \"License Share Alike (DataProvenance IgnoreOpenAI)\": 0, \"License Type\": \"Unspecified\"}, {\"Unique Dataset Identifier\": \"thumos-challenge\", \"Collection\": \"thumos-challenge\", \"Collection URL\": \"https://arxiv.org/pdf/1604.06182.pdf\", \"Dataset Name\": \"The THUMOS Challenge on Action Recognition for Videos in the Wild\", \"Paper Title\": \"The THUMOS Challenge on Action Recognition for Videos in the Wild\", \"Paper URL\": \"https://arxiv.org/pdf/1604.06182.pdf\", \"GitHub URL\": \"\", \"Hugging Face URL\": \"\", \"Papers with Code URL\": \"https://paperswithcode.com/dataset/thumos14-1\", \"ArXiv URL\": \"https://arxiv.org/pdf/1604.06182.pdf\", \"Semantic Scholar Corpus ID\": 14049355, \"Year Released\": \"2014\", \"Text Sources\": [\"youtube\"], \"Licenses\": [{\"License\": \"Custom\", \"License URL\": \"https://docs.google.com/forms/d/e/1FAIpQLScs9davISAtYQS7SEF5qQNu0jUpLzNH3aHmPfuqk2q1VYDkmw/viewform\"}], \"Creators\": \"Research Group\", \"Countries\": [\"United States of America\", \"China\"], \"License Verified By\": \"Vivek Sharma\", \"Video Hours\": 254.0, \"Taken Down\": \"False\", \"Video Sources\": [\"youtube\"], \"Task Categories\": [\"Video Classification\"], \"Inferred Metadata\": {\"GitHub License\": \"\", \"GitHub Stars (June 2024)\": \"\", \"GitHub Topics\": \"\", \"Github Date\": \"\", \"HF Config\": \"\", \"HF Config License\": \"\", \"HF Dataset\": \"\", \"HF Date\": \"\", \"HF Downloads (June 2024)\": \"\", \"HF Likes (June 2024)\": \"\", \"HF Yaml License\": \"\", \"PwC Date\": \"\", \"PwC Description\": \"The THUMOS14 (THUMOS 2014) dataset is a large-scale video dataset that includes 1,010 videos for validation and 1,574 videos for testing from 20 classes. Among all the videos, there are 220 and 212 videos with temporal annotations in validation and testing set, respectively.\", \"PwC License Name\": \"Unspecified\", \"PwC License URL\": null, \"S2 Citation Count (June 2024)\": \"\", \"S2 Date\": \"\"}, \"Bibtex\": \"@Misc{None,\\n author = {Haroon Idrees and Amir Zamir and Yu-Gang Jiang and Alexander N. Gorban and I. Laptev and R. Sukthankar and M. Shah},\\n title = {Computer Vision and Image Understanding}\\n}\\n\", \"License Use (DataProvenance)\": \"academic-only\", \"License Attribution (DataProvenance)\": 1, \"License Share Alike (DataProvenance)\": 0, \"License Use (DataProvenance IgnoreOpenAI)\": \"academic-only\", \"License Attribution (DataProvenance IgnoreOpenAI)\": 1, \"License Share Alike (DataProvenance IgnoreOpenAI)\": 0, \"License Type\": \"Non-Commercial/Academic\"}, {\"Unique Dataset Identifier\": \"thumos-challenge\", \"Collection\": \"thumos-challenge\", \"Collection URL\": \"https://arxiv.org/pdf/1604.06182.pdf\", \"Dataset Name\": \"The THUMOS Challenge on Action Recognition for Videos in the Wild\", \"Paper Title\": \"The THUMOS Challenge on Action Recognition for Videos in the Wild\", \"Paper URL\": \"https://arxiv.org/pdf/1604.06182.pdf\", \"GitHub URL\": \"\", \"Hugging Face URL\": \"\", \"Papers with Code URL\": \"https://paperswithcode.com/dataset/thumos14-1\", \"ArXiv URL\": \"https://arxiv.org/pdf/1604.06182.pdf\", \"Semantic Scholar Corpus ID\": 14049355, \"Year Released\": \"2014\", \"Text Sources\": [\"youtube\"], \"Licenses\": [{\"License\": \"Custom\", \"License URL\": \"https://docs.google.com/forms/d/e/1FAIpQLScs9davISAtYQS7SEF5qQNu0jUpLzNH3aHmPfuqk2q1VYDkmw/viewform\"}], \"Creators\": \"Academic\", \"Countries\": [\"United States of America\", \"China\"], \"License Verified By\": \"Vivek Sharma\", \"Video Hours\": 254.0, \"Taken Down\": \"False\", \"Video Sources\": [\"youtube\"], \"Task Categories\": [\"Video Classification\"], \"Inferred Metadata\": {\"GitHub License\": \"\", \"GitHub Stars (June 2024)\": \"\", \"GitHub Topics\": \"\", \"Github Date\": \"\", \"HF Config\": \"\", \"HF Config License\": \"\", \"HF Dataset\": \"\", \"HF Date\": \"\", \"HF Downloads (June 2024)\": \"\", \"HF Likes (June 2024)\": \"\", \"HF Yaml License\": \"\", \"PwC Date\": \"\", \"PwC Description\": \"The THUMOS14 (THUMOS 2014) dataset is a large-scale video dataset that includes 1,010 videos for validation and 1,574 videos for testing from 20 classes. Among all the videos, there are 220 and 212 videos with temporal annotations in validation and testing set, respectively.\", \"PwC License Name\": \"Unspecified\", \"PwC License URL\": null, \"S2 Citation Count (June 2024)\": \"\", \"S2 Date\": \"\"}, \"Bibtex\": \"@Misc{None,\\n author = {Haroon Idrees and Amir Zamir and Yu-Gang Jiang and Alexander N. Gorban and I. Laptev and R. Sukthankar and M. Shah},\\n title = {Computer Vision and Image Understanding}\\n}\\n\", \"License Use (DataProvenance)\": \"academic-only\", \"License Attribution (DataProvenance)\": 1, \"License Share Alike (DataProvenance)\": 0, \"License Use (DataProvenance IgnoreOpenAI)\": \"academic-only\", \"License Attribution (DataProvenance IgnoreOpenAI)\": 1, \"License Share Alike (DataProvenance IgnoreOpenAI)\": 0, \"License Type\": \"Non-Commercial/Academic\"}, {\"Unique Dataset Identifier\": \"thumos-challenge\", \"Collection\": \"thumos-challenge\", \"Collection URL\": \"https://arxiv.org/pdf/1604.06182.pdf\", \"Dataset Name\": \"The THUMOS Challenge on Action Recognition for Videos in the Wild\", \"Paper Title\": \"The THUMOS Challenge on Action Recognition for Videos in the Wild\", \"Paper URL\": \"https://arxiv.org/pdf/1604.06182.pdf\", \"GitHub URL\": \"\", \"Hugging Face URL\": \"\", \"Papers with Code URL\": \"https://paperswithcode.com/dataset/thumos14-1\", \"ArXiv URL\": \"https://arxiv.org/pdf/1604.06182.pdf\", \"Semantic Scholar Corpus ID\": 14049355, \"Year Released\": \"2014\", \"Text Sources\": [\"youtube\"], \"Licenses\": [{\"License\": \"Custom\", \"License URL\": \"https://docs.google.com/forms/d/e/1FAIpQLScs9davISAtYQS7SEF5qQNu0jUpLzNH3aHmPfuqk2q1VYDkmw/viewform\"}], \"Creators\": \"Academic\", \"Countries\": [\"United States of America\", \"China\"], \"License Verified By\": \"Vivek Sharma\", \"Video Hours\": 254.0, \"Taken Down\": \"False\", \"Video Sources\": [\"youtube\"], \"Task Categories\": [\"Video Classification\"], \"Inferred Metadata\": {\"GitHub License\": \"\", \"GitHub Stars (June 2024)\": \"\", \"GitHub Topics\": \"\", \"Github Date\": \"\", \"HF Config\": \"\", \"HF Config License\": \"\", \"HF Dataset\": \"\", \"HF Date\": \"\", \"HF Downloads (June 2024)\": \"\", \"HF Likes (June 2024)\": \"\", \"HF Yaml License\": \"\", \"PwC Date\": \"\", \"PwC Description\": \"The THUMOS14 (THUMOS 2014) dataset is a large-scale video dataset that includes 1,010 videos for validation and 1,574 videos for testing from 20 classes. Among all the videos, there are 220 and 212 videos with temporal annotations in validation and testing set, respectively.\", \"PwC License Name\": \"Unspecified\", \"PwC License URL\": null, \"S2 Citation Count (June 2024)\": \"\", \"S2 Date\": \"\"}, \"Bibtex\": \"@Misc{None,\\n author = {Haroon Idrees and Amir Zamir and Yu-Gang Jiang and Alexander N. Gorban and I. Laptev and R. Sukthankar and M. Shah},\\n title = {Computer Vision and Image Understanding}\\n}\\n\", \"License Use (DataProvenance)\": \"academic-only\", \"License Attribution (DataProvenance)\": 1, \"License Share Alike (DataProvenance)\": 0, \"License Use (DataProvenance IgnoreOpenAI)\": \"academic-only\", \"License Attribution (DataProvenance IgnoreOpenAI)\": 1, \"License Share Alike (DataProvenance IgnoreOpenAI)\": 0, \"License Type\": \"Non-Commercial/Academic\"}, {\"Unique Dataset Identifier\": \"thumos-challenge\", \"Collection\": \"thumos-challenge\", \"Collection URL\": \"https://arxiv.org/pdf/1604.06182.pdf\", \"Dataset Name\": \"The THUMOS Challenge on Action Recognition for Videos in the Wild\", \"Paper Title\": \"The THUMOS Challenge on Action Recognition for Videos in the Wild\", \"Paper URL\": \"https://arxiv.org/pdf/1604.06182.pdf\", \"GitHub URL\": \"\", \"Hugging Face URL\": \"\", \"Papers with Code URL\": \"https://paperswithcode.com/dataset/thumos14-1\", \"ArXiv URL\": \"https://arxiv.org/pdf/1604.06182.pdf\", \"Semantic Scholar Corpus ID\": 14049355, \"Year Released\": \"2014\", \"Text Sources\": [\"youtube\"], \"Licenses\": [{\"License\": \"Custom\", \"License URL\": \"https://docs.google.com/forms/d/e/1FAIpQLScs9davISAtYQS7SEF5qQNu0jUpLzNH3aHmPfuqk2q1VYDkmw/viewform\"}], \"Creators\": \"Academic\", \"Countries\": [\"United States of America\", \"China\"], \"License Verified By\": \"Vivek Sharma\", \"Video Hours\": 254.0, \"Taken Down\": \"False\", \"Video Sources\": [\"youtube\"], \"Task Categories\": [\"Video Classification\"], \"Inferred Metadata\": {\"GitHub License\": \"\", \"GitHub Stars (June 2024)\": \"\", \"GitHub Topics\": \"\", \"Github Date\": \"\", \"HF Config\": \"\", \"HF Config License\": \"\", \"HF Dataset\": \"\", \"HF Date\": \"\", \"HF Downloads (June 2024)\": \"\", \"HF Likes (June 2024)\": \"\", \"HF Yaml License\": \"\", \"PwC Date\": \"\", \"PwC Description\": \"The THUMOS14 (THUMOS 2014) dataset is a large-scale video dataset that includes 1,010 videos for validation and 1,574 videos for testing from 20 classes. Among all the videos, there are 220 and 212 videos with temporal annotations in validation and testing set, respectively.\", \"PwC License Name\": \"Unspecified\", \"PwC License URL\": null, \"S2 Citation Count (June 2024)\": \"\", \"S2 Date\": \"\"}, \"Bibtex\": \"@Misc{None,\\n author = {Haroon Idrees and Amir Zamir and Yu-Gang Jiang and Alexander N. Gorban and I. Laptev and R. Sukthankar and M. Shah},\\n title = {Computer Vision and Image Understanding}\\n}\\n\", \"License Use (DataProvenance)\": \"academic-only\", \"License Attribution (DataProvenance)\": 1, \"License Share Alike (DataProvenance)\": 0, \"License Use (DataProvenance IgnoreOpenAI)\": \"academic-only\", \"License Attribution (DataProvenance IgnoreOpenAI)\": 1, \"License Share Alike (DataProvenance IgnoreOpenAI)\": 0, \"License Type\": \"Non-Commercial/Academic\"}, {\"Unique Dataset Identifier\": \"stroygraphs\", \"Collection\": \"stroygraphs\", \"Collection URL\": \"https://openaccess.thecvf.com/content_cvpr_2014/papers/Tapaswi_StoryGraphs_Visualizing_Character_2014_CVPR_paper.pdf\", \"Dataset Name\": \"StoryGraphs\", \"Paper Title\": \"StoryGraphs\", \"Paper URL\": \"https://openaccess.thecvf.com/content_cvpr_2014/papers/Tapaswi_StoryGraphs_Visualizing_Character_2014_CVPR_paper.pdf\", \"GitHub URL\": \"https://github.com/makarandtapaswi/StoryGraphs_CVPR2014\", \"Hugging Face URL\": \"\", \"Papers with Code URL\": \"https://paperswithcode.com/paper/storygraphs-visualizing-character\", \"ArXiv URL\": \"https://openaccess.thecvf.com/content_cvpr_2014/papers/Tapaswi_StoryGraphs_Visualizing_Character_2014_CVPR_paper.pdf\", \"Semantic Scholar Corpus ID\": 1055956, \"Year Released\": \"2014\", \"Text Sources\": [\"movies\"], \"Licenses\": [{\"License\": \"Unspecified\", \"License URL\": \"\"}], \"Creators\": \"Academic\", \"Countries\": [\"Germany\"], \"License Verified By\": \"Vivek Sharma\", \"Video Hours\": 7.3, \"Taken Down\": \"False\", \"Video Sources\": [\"movies\"], \"Task Categories\": [\"Misc\"], \"Inferred Metadata\": {\"GitHub License\": \"\", \"GitHub Stars (June 2024)\": 20, \"GitHub Topics\": [], \"Github Date\": \"\", \"HF Config\": \"\", \"HF Config License\": \"\", \"HF Dataset\": \"\", \"HF Date\": \"\", \"HF Downloads (June 2024)\": \"\", \"HF Likes (June 2024)\": \"\", \"HF Yaml License\": \"\", \"PwC Date\": \"\", \"PwC Description\": \"Implemented in one code library.\", \"PwC License Name\": \"\", \"PwC License URL\": \"\", \"S2 Citation Count (June 2024)\": \"\", \"S2 Date\": \"\"}, \"Bibtex\": \"@Article{Tapaswi2014StoryGraphsVC,\\n author = {Makarand Tapaswi and M. B\\u00e4uml and R. Stiefelhagen},\\n booktitle = {2014 IEEE Conference on Computer Vision and Pattern Recognition},\\n journal = {2014 IEEE Conference on Computer Vision and Pattern Recognition},\\n pages = {827-834},\\n title = {StoryGraphs: Visualizing Character Interactions as a Timeline},\\n year = {2014}\\n}\\n\", \"License Use (DataProvenance)\": \"unspecified\", \"License Attribution (DataProvenance)\": 0, \"License Share Alike (DataProvenance)\": 0, \"License Use (DataProvenance IgnoreOpenAI)\": \"unspecified\", \"License Attribution (DataProvenance IgnoreOpenAI)\": 0, \"License Share Alike (DataProvenance IgnoreOpenAI)\": 0, \"License Type\": \"Unspecified\"}, {\"Unique Dataset Identifier\": \"videostory\", \"Collection\": \"videostory\", \"Collection URL\": \"https://isis-data.science.uva.nl/cgmsnoek/pub/habibian-videostory-mm2014.pdf\", \"Dataset Name\": \"VideoStory: A New Multimedia Embedding for Few-Example Recognition and Translation of Events\", \"Paper Title\": \"VideoStory: A New Multimedia Embedding for Few-Example Recognition and Translation of Events\", \"Paper URL\": \"https://isis-data.science.uva.nl/cgmsnoek/pub/habibian-videostory-mm2014.pdf\", \"GitHub URL\": \"\", \"Hugging Face URL\": \"\", \"Papers with Code URL\": \"\", \"ArXiv URL\": \"https://isis-data.science.uva.nl/cgmsnoek/pub/habibian-videostory-mm2014.pdf\", \"Semantic Scholar Corpus ID\": 28203, \"Year Released\": \"2014\", \"Text Sources\": [\"youtube\"], \"Licenses\": [{\"License\": \"Unspecified\", \"License URL\": \"\"}], \"Creators\": \"Academic\", \"Countries\": [\"Netherlands\"], \"License Verified By\": \"Vivek Sharma\", \"Video Hours\": 743.0, \"Taken Down\": \"False\", \"Video Sources\": [\"youtube\"], \"Task Categories\": [\"Video Captioning\"], \"Inferred Metadata\": {\"GitHub License\": \"\", \"GitHub Stars (June 2024)\": \"\", \"GitHub Topics\": \"\", \"Github Date\": \"\", \"HF Config\": \"\", \"HF Config License\": \"\", \"HF Dataset\": \"\", \"HF Date\": \"\", \"HF Downloads (June 2024)\": \"\", \"HF Likes (June 2024)\": \"\", \"HF Yaml License\": \"\", \"PwC Date\": \"\", \"PwC Description\": \"\", \"PwC License Name\": \"\", \"PwC License URL\": \"\", \"S2 Citation Count (June 2024)\": \"\", \"S2 Date\": \"\"}, \"Bibtex\": \"@Book{Habibian2014VideoStoryAN,\\n author = {A. Habibian and Thomas Mensink and Cees G. M. Snoek},\\n booktitle = {ACM Multimedia},\\n journal = {Proceedings of the 22nd ACM international conference on Multimedia},\\n title = {VideoStory: A New Multimedia Embedding for Few-Example Recognition and Translation of Events},\\n year = {2014}\\n}\\n\", \"License Use (DataProvenance)\": \"unspecified\", \"License Attribution (DataProvenance)\": 0, \"License Share Alike (DataProvenance)\": 0, \"License Use (DataProvenance IgnoreOpenAI)\": \"unspecified\", \"License Attribution (DataProvenance IgnoreOpenAI)\": 0, \"License Share Alike (DataProvenance IgnoreOpenAI)\": 0, \"License Type\": \"Unspecified\"}, {\"Unique Dataset Identifier\": \"summe\", \"Collection\": \"summe\", \"Collection URL\": \"https://link.springer.com/chapter/10.1007/978-3-319-10584-0_33\", \"Dataset Name\": \"SumMe: Creating Summaries from User Videos (ECCV 2014)\", \"Paper Title\": \"SumMe: Creating Summaries from User Videos (ECCV 2014)\", \"Paper URL\": \"https://link.springer.com/chapter/10.1007/978-3-319-10584-0_33\", \"GitHub URL\": \"\", \"Hugging Face URL\": \"\", \"Papers with Code URL\": \"https://paperswithcode.com/dataset/summe\", \"ArXiv URL\": \"https://link.springer.com/chapter/10.1007/978-3-319-10584-0_33\", \"Semantic Scholar Corpus ID\": 2111093, \"Year Released\": \"2014\", \"Text Sources\": [\"crowdsourced\"], \"Licenses\": [{\"License\": \"Unspecified\", \"License URL\": \"\"}], \"Creators\": \"Academic\", \"Countries\": [\"Belgium\", \"Switzerland\"], \"License Verified By\": \"Vivek Sharma\", \"Video Hours\": 1.11, \"Taken Down\": \"False\", \"Video Sources\": [\"crowdsourced\"], \"Task Categories\": [\"Video Summarization\"], \"Inferred Metadata\": {\"GitHub License\": \"\", \"GitHub Stars (June 2024)\": \"\", \"GitHub Topics\": \"\", \"Github Date\": \"\", \"HF Config\": \"\", \"HF Config License\": \"\", \"HF Dataset\": \"\", \"HF Date\": \"\", \"HF Downloads (June 2024)\": \"\", \"HF Likes (June 2024)\": \"\", \"HF Yaml License\": \"\", \"PwC Date\": \"2014-01-01\", \"PwC Description\": \"The SumMe dataset is a video summarization dataset consisting of 25 videos, each annotated with at least 15 human summaries (390 in total).\", \"PwC License Name\": \"Academic Research Purposes Only\", \"PwC License URL\": \"https://gyglim.github.io/me/vsum/index.html#benchmark\", \"S2 Citation Count (June 2024)\": \"\", \"S2 Date\": \"\"}, \"Bibtex\": \"@Article{Gygli2014CreatingSF,\\n author = {Michael Gygli and H. Grabner and Hayko Riemenschneider and L. Gool},\\n booktitle = {European Conference on Computer Vision},\\n pages = {505-520},\\n title = {Creating Summaries from User Videos},\\n year = {2014}\\n}\\n\", \"License Use (DataProvenance)\": \"unspecified\", \"License Attribution (DataProvenance)\": 0, \"License Share Alike (DataProvenance)\": 0, \"License Use (DataProvenance IgnoreOpenAI)\": \"unspecified\", \"License Attribution (DataProvenance IgnoreOpenAI)\": 0, \"License Share Alike (DataProvenance IgnoreOpenAI)\": 0, \"License Type\": \"Unspecified\"}, {\"Unique Dataset Identifier\": \"summe\", \"Collection\": \"summe\", \"Collection URL\": \"https://link.springer.com/chapter/10.1007/978-3-319-10584-0_33\", \"Dataset Name\": \"SumMe: Creating Summaries from User Videos (ECCV 2014)\", \"Paper Title\": \"SumMe: Creating Summaries from User Videos (ECCV 2014)\", \"Paper URL\": \"https://link.springer.com/chapter/10.1007/978-3-319-10584-0_33\", \"GitHub URL\": \"\", \"Hugging Face URL\": \"\", \"Papers with Code URL\": \"https://paperswithcode.com/dataset/summe\", \"ArXiv URL\": \"https://link.springer.com/chapter/10.1007/978-3-319-10584-0_33\", \"Semantic Scholar Corpus ID\": 2111093, \"Year Released\": \"2014\", \"Text Sources\": [\"crowdsourced\"], \"Licenses\": [{\"License\": \"Unspecified\", \"License URL\": \"\"}], \"Creators\": \"Corporation\", \"Countries\": [\"Belgium\", \"Switzerland\"], \"License Verified By\": \"Vivek Sharma\", \"Video Hours\": 1.11, \"Taken Down\": \"False\", \"Video Sources\": [\"crowdsourced\"], \"Task Categories\": [\"Video Summarization\"], \"Inferred Metadata\": {\"GitHub License\": \"\", \"GitHub Stars (June 2024)\": \"\", \"GitHub Topics\": \"\", \"Github Date\": \"\", \"HF Config\": \"\", \"HF Config License\": \"\", \"HF Dataset\": \"\", \"HF Date\": \"\", \"HF Downloads (June 2024)\": \"\", \"HF Likes (June 2024)\": \"\", \"HF Yaml License\": \"\", \"PwC Date\": \"2014-01-01\", \"PwC Description\": \"The SumMe dataset is a video summarization dataset consisting of 25 videos, each annotated with at least 15 human summaries (390 in total).\", \"PwC License Name\": \"Academic Research Purposes Only\", \"PwC License URL\": \"https://gyglim.github.io/me/vsum/index.html#benchmark\", \"S2 Citation Count (June 2024)\": \"\", \"S2 Date\": \"\"}, \"Bibtex\": \"@Article{Gygli2014CreatingSF,\\n author = {Michael Gygli and H. Grabner and Hayko Riemenschneider and L. Gool},\\n booktitle = {European Conference on Computer Vision},\\n pages = {505-520},\\n title = {Creating Summaries from User Videos},\\n year = {2014}\\n}\\n\", \"License Use (DataProvenance)\": \"unspecified\", \"License Attribution (DataProvenance)\": 0, \"License Share Alike (DataProvenance)\": 0, \"License Use (DataProvenance IgnoreOpenAI)\": \"unspecified\", \"License Attribution (DataProvenance IgnoreOpenAI)\": 0, \"License Share Alike (DataProvenance IgnoreOpenAI)\": 0, \"License Type\": \"Unspecified\"}, {\"Unique Dataset Identifier\": \"summe\", \"Collection\": \"summe\", \"Collection URL\": \"https://link.springer.com/chapter/10.1007/978-3-319-10584-0_33\", \"Dataset Name\": \"SumMe: Creating Summaries from User Videos (ECCV 2014)\", \"Paper Title\": \"SumMe: Creating Summaries from User Videos (ECCV 2014)\", \"Paper URL\": \"https://link.springer.com/chapter/10.1007/978-3-319-10584-0_33\", \"GitHub URL\": \"\", \"Hugging Face URL\": \"\", \"Papers with Code URL\": \"https://paperswithcode.com/dataset/summe\", \"ArXiv URL\": \"https://link.springer.com/chapter/10.1007/978-3-319-10584-0_33\", \"Semantic Scholar Corpus ID\": 2111093, \"Year Released\": \"2014\", \"Text Sources\": [\"crowdsourced\"], \"Licenses\": [{\"License\": \"Unspecified\", \"License URL\": \"\"}], \"Creators\": \"Academic\", \"Countries\": [\"Belgium\", \"Switzerland\"], \"License Verified By\": \"Vivek Sharma\", \"Video Hours\": 1.11, \"Taken Down\": \"False\", \"Video Sources\": [\"crowdsourced\"], \"Task Categories\": [\"Video Summarization\"], \"Inferred Metadata\": {\"GitHub License\": \"\", \"GitHub Stars (June 2024)\": \"\", \"GitHub Topics\": \"\", \"Github Date\": \"\", \"HF Config\": \"\", \"HF Config License\": \"\", \"HF Dataset\": \"\", \"HF Date\": \"\", \"HF Downloads (June 2024)\": \"\", \"HF Likes (June 2024)\": \"\", \"HF Yaml License\": \"\", \"PwC Date\": \"2014-01-01\", \"PwC Description\": \"The SumMe dataset is a video summarization dataset consisting of 25 videos, each annotated with at least 15 human summaries (390 in total).\", \"PwC License Name\": \"Academic Research Purposes Only\", \"PwC License URL\": \"https://gyglim.github.io/me/vsum/index.html#benchmark\", \"S2 Citation Count (June 2024)\": \"\", \"S2 Date\": \"\"}, \"Bibtex\": \"@Article{Gygli2014CreatingSF,\\n author = {Michael Gygli and H. Grabner and Hayko Riemenschneider and L. Gool},\\n booktitle = {European Conference on Computer Vision},\\n pages = {505-520},\\n title = {Creating Summaries from User Videos},\\n year = {2014}\\n}\\n\", \"License Use (DataProvenance)\": \"unspecified\", \"License Attribution (DataProvenance)\": 0, \"License Share Alike (DataProvenance)\": 0, \"License Use (DataProvenance IgnoreOpenAI)\": \"unspecified\", \"License Attribution (DataProvenance IgnoreOpenAI)\": 0, \"License Share Alike (DataProvenance IgnoreOpenAI)\": 0, \"License Type\": \"Unspecified\"}, {\"Unique Dataset Identifier\": \"breakfast\", \"Collection\": \"breakfast\", \"Collection URL\": \"https://openaccess.thecvf.com/content_cvpr_2014/html/Kuehne_The_Language_of_2014_CVPR_paper.html\", \"Dataset Name\": \"Breakfast\", \"Paper Title\": \"Breakfast\", \"Paper URL\": \"https://openaccess.thecvf.com/content_cvpr_2014/html/Kuehne_The_Language_of_2014_CVPR_paper.html\", \"GitHub URL\": \"\", \"Hugging Face URL\": \"\", \"Papers with Code URL\": \"https://paperswithcode.com/dataset/breakfast\", \"ArXiv URL\": \"https://openaccess.thecvf.com/content_cvpr_2014/html/Kuehne_The_Language_of_2014_CVPR_paper.html\", \"Semantic Scholar Corpus ID\": 9621856, \"Year Released\": \"2014\", \"Text Sources\": [\"human\"], \"Licenses\": [{\"License\": \"CC BY 4.0\", \"License URL\": \"https://serre-lab.clps.brown.edu/resource/breakfast-actions-dataset/\"}], \"Creators\": \"Academic\", \"Countries\": [\"United States of America\", \"Germany\"], \"License Verified By\": \"Vivek Sharma\", \"Video Hours\": 77.0, \"Taken Down\": \"False\", \"Video Sources\": [\"human\"], \"Task Categories\": [\"Video Captioning\", \"Action Segmentation\"], \"Inferred Metadata\": {\"GitHub License\": \"\", \"GitHub Stars (June 2024)\": \"\", \"GitHub Topics\": \"\", \"Github Date\": \"\", \"HF Config\": \"\", \"HF Config License\": \"\", \"HF Dataset\": \"\", \"HF Date\": \"\", \"HF Downloads (June 2024)\": \"\", \"HF Likes (June 2024)\": \"\", \"HF Yaml License\": \"\", \"PwC Date\": \"2014-01-01\", \"PwC Description\": \"The Breakfast Actions Dataset comprises of 10 actions related to breakfast preparation, performed by 52 different individuals in 18 different kitchens. The dataset is one of the largest fully annotated datasets available. The actions are recorded \\u201cin the wild\\u201d as opposed to a single controlled lab environment. It consists of over 77 hours of video recordings.\", \"PwC License Name\": \"CC BY 4.0\", \"PwC License URL\": \"https://creativecommons.org/licenses/by/4.0/\", \"S2 Citation Count (June 2024)\": \"\", \"S2 Date\": \"\"}, \"Bibtex\": \"@Article{Kuehne2014TheLO,\\n author = {Hilde Kuehne and A. B. Arslan and Thomas Serre},\\n booktitle = {2014 IEEE Conference on Computer Vision and Pattern Recognition},\\n journal = {2014 IEEE Conference on Computer Vision and Pattern Recognition},\\n pages = {780-787},\\n title = {The Language of Actions: Recovering the Syntax and Semantics of Goal-Directed Human Activities},\\n year = {2014}\\n}\\n\", \"License Use (DataProvenance)\": \"commercial\", \"License Attribution (DataProvenance)\": 1, \"License Share Alike (DataProvenance)\": 0, \"License Use (DataProvenance IgnoreOpenAI)\": \"commercial\", \"License Attribution (DataProvenance IgnoreOpenAI)\": 1, \"License Share Alike (DataProvenance IgnoreOpenAI)\": 0, \"License Type\": \"Commercial\"}, {\"Unique Dataset Identifier\": \"breakfast\", \"Collection\": \"breakfast\", \"Collection URL\": \"https://openaccess.thecvf.com/content_cvpr_2014/html/Kuehne_The_Language_of_2014_CVPR_paper.html\", \"Dataset Name\": \"Breakfast\", \"Paper Title\": \"Breakfast\", \"Paper URL\": \"https://openaccess.thecvf.com/content_cvpr_2014/html/Kuehne_The_Language_of_2014_CVPR_paper.html\", \"GitHub URL\": \"\", \"Hugging Face URL\": \"\", \"Papers with Code URL\": \"https://paperswithcode.com/dataset/breakfast\", \"ArXiv URL\": \"https://openaccess.thecvf.com/content_cvpr_2014/html/Kuehne_The_Language_of_2014_CVPR_paper.html\", \"Semantic Scholar Corpus ID\": 9621856, \"Year Released\": \"2014\", \"Text Sources\": [\"human\"], \"Licenses\": [{\"License\": \"CC BY 4.0\", \"License URL\": \"https://serre-lab.clps.brown.edu/resource/breakfast-actions-dataset/\"}], \"Creators\": \"Research Group\", \"Countries\": [\"United States of America\", \"Germany\"], \"License Verified By\": \"Vivek Sharma\", \"Video Hours\": 77.0, \"Taken Down\": \"False\", \"Video Sources\": [\"human\"], \"Task Categories\": [\"Video Captioning\", \"Action Segmentation\"], \"Inferred Metadata\": {\"GitHub License\": \"\", \"GitHub Stars (June 2024)\": \"\", \"GitHub Topics\": \"\", \"Github Date\": \"\", \"HF Config\": \"\", \"HF Config License\": \"\", \"HF Dataset\": \"\", \"HF Date\": \"\", \"HF Downloads (June 2024)\": \"\", \"HF Likes (June 2024)\": \"\", \"HF Yaml License\": \"\", \"PwC Date\": \"2014-01-01\", \"PwC Description\": \"The Breakfast Actions Dataset comprises of 10 actions related to breakfast preparation, performed by 52 different individuals in 18 different kitchens. The dataset is one of the largest fully annotated datasets available. The actions are recorded \\u201cin the wild\\u201d as opposed to a single controlled lab environment. It consists of over 77 hours of video recordings.\", \"PwC License Name\": \"CC BY 4.0\", \"PwC License URL\": \"https://creativecommons.org/licenses/by/4.0/\", \"S2 Citation Count (June 2024)\": \"\", \"S2 Date\": \"\"}, \"Bibtex\": \"@Article{Kuehne2014TheLO,\\n author = {Hilde Kuehne and A. B. Arslan and Thomas Serre},\\n booktitle = {2014 IEEE Conference on Computer Vision and Pattern Recognition},\\n journal = {2014 IEEE Conference on Computer Vision and Pattern Recognition},\\n pages = {780-787},\\n title = {The Language of Actions: Recovering the Syntax and Semantics of Goal-Directed Human Activities},\\n year = {2014}\\n}\\n\", \"License Use (DataProvenance)\": \"commercial\", \"License Attribution (DataProvenance)\": 1, \"License Share Alike (DataProvenance)\": 0, \"License Use (DataProvenance IgnoreOpenAI)\": \"commercial\", \"License Attribution (DataProvenance IgnoreOpenAI)\": 1, \"License Share Alike (DataProvenance IgnoreOpenAI)\": 0, \"License Type\": \"Commercial\"}, {\"Unique Dataset Identifier\": \"sports1M-dataset\", \"Collection\": \"sports1M-dataset\", \"Collection URL\": \"https://static.googleusercontent.com/media/research.google.com/en//pubs/archive/42455.pdf\", \"Dataset Name\": \"Sports-1M: Large-scale Video Classification with Convolutional Neural Networks\", \"Paper Title\": \"Sports-1M: Large-scale Video Classification with Convolutional Neural Networks\", \"Paper URL\": \"https://static.googleusercontent.com/media/research.google.com/en//pubs/archive/42455.pdf\", \"GitHub URL\": \"https://github.com/gtoderici/sports-1m-dataset\", \"Hugging Face URL\": \"\", \"Papers with Code URL\": \"https://paperswithcode.com/paper/large-scale-video-classification-with-1\", \"ArXiv URL\": \"https://static.googleusercontent.com/media/research.google.com/en//pubs/archive/42455.pdf\", \"Semantic Scholar Corpus ID\": 206592218, \"Year Released\": \"2014\", \"Text Sources\": [\"youtube\"], \"Licenses\": [{\"License\": \"CC BY 3.0\", \"License URL\": \"https://github.com/gtoderici/sports-1m-dataset\"}], \"Creators\": \"Academic\", \"Countries\": [\"United States of America\"], \"License Verified By\": \"Vivek Sharma\", \"Video Hours\": 105761.41, \"Taken Down\": \"False\", \"Video Sources\": [\"youtube\"], \"Task Categories\": [\"Video Classification\"], \"Inferred Metadata\": {\"GitHub License\": \"\", \"GitHub Stars (June 2024)\": 273, \"GitHub Topics\": [], \"Github Date\": \"\", \"HF Config\": \"\", \"HF Config License\": \"\", \"HF Dataset\": \"\", \"HF Date\": \"\", \"HF Downloads (June 2024)\": \"\", \"HF Likes (June 2024)\": \"\", \"HF Yaml License\": \"\", \"PwC Date\": \"\", \"PwC Description\": \"#9 best model for Action Recognition on Sports-1M (Video hit@1  metric)\", \"PwC License Name\": \"\", \"PwC License URL\": \"\", \"S2 Citation Count (June 2024)\": \"\", \"S2 Date\": \"\"}, \"Bibtex\": \"@Article{Karpathy2014LargeScaleVC,\\n author = {A. Karpathy and G. Toderici and Sanketh Shetty and Thomas Leung and R. Sukthankar and Li Fei-Fei},\\n booktitle = {2014 IEEE Conference on Computer Vision and Pattern Recognition},\\n journal = {2014 IEEE Conference on Computer Vision and Pattern Recognition},\\n pages = {1725-1732},\\n title = {Large-Scale Video Classification with Convolutional Neural Networks},\\n year = {2014}\\n}\\n\", \"License Use (DataProvenance)\": \"commercial\", \"License Attribution (DataProvenance)\": 1, \"License Share Alike (DataProvenance)\": 0, \"License Use (DataProvenance IgnoreOpenAI)\": \"commercial\", \"License Attribution (DataProvenance IgnoreOpenAI)\": 1, \"License Share Alike (DataProvenance IgnoreOpenAI)\": 0, \"License Type\": \"Commercial\"}, {\"Unique Dataset Identifier\": \"hollywood-extended\", \"Collection\": \"hollywood-extended\", \"Collection URL\": \"https://arxiv.org/pdf/1407.1208\", \"Dataset Name\": \"Hollywood Extended\", \"Paper Title\": \"Hollywood Extended\", \"Paper URL\": \"https://arxiv.org/pdf/1407.1208\", \"GitHub URL\": \"https://github.com/piotr-bojanowski/action-ordering\", \"Hugging Face URL\": \"\", \"Papers with Code URL\": \"https://paperswithcode.com/paper/weakly-supervised-action-labeling-in-videos\", \"ArXiv URL\": \"https://arxiv.org/pdf/1407.1208\", \"Semantic Scholar Corpus ID\": 9342651, \"Year Released\": \"2014\", \"Text Sources\": [\"movies\"], \"Licenses\": [{\"License\": \"MIT License\", \"License URL\": \"https://github.com/piotr-bojanowski/action-ordering/blob/master/LICENSE\"}], \"Creators\": \"Research Group\", \"Countries\": [\"France\"], \"License Verified By\": \"Vivek Sharma\", \"Video Hours\": 8.75, \"Taken Down\": \"False\", \"Video Sources\": [\"movies\"], \"Task Categories\": [\"Temporal Action Detection\", \"Action Recognition\"], \"Inferred Metadata\": {\"GitHub License\": \"MIT License\", \"GitHub Stars (June 2024)\": 12, \"GitHub Topics\": [], \"Github Date\": \"\", \"HF Config\": \"\", \"HF Config License\": \"\", \"HF Dataset\": \"\", \"HF Date\": \"\", \"HF Downloads (June 2024)\": \"\", \"HF Likes (June 2024)\": \"\", \"HF Yaml License\": \"\", \"PwC Date\": \"\", \"PwC Description\": \"No code available yet.\", \"PwC License Name\": \"\", \"PwC License URL\": \"\", \"S2 Citation Count (June 2024)\": \"\", \"S2 Date\": \"\"}, \"Bibtex\": \"@Article{Bojanowski2014WeaklySA,\\n author = {Piotr Bojanowski and R\\u00e9mi Lajugie and F. Bach and I. Laptev and J. Ponce and C. Schmid and Josef Sivic},\\n booktitle = {European Conference on Computer Vision},\\n journal = {ArXiv},\\n title = {Weakly Supervised Action Labeling in Videos under Ordering Constraints},\\n volume = {abs/1407.1208},\\n year = {2014}\\n}\\n\", \"License Use (DataProvenance)\": \"commercial\", \"License Attribution (DataProvenance)\": 1, \"License Share Alike (DataProvenance)\": 0, \"License Use (DataProvenance IgnoreOpenAI)\": \"commercial\", \"License Attribution (DataProvenance IgnoreOpenAI)\": 1, \"License Share Alike (DataProvenance IgnoreOpenAI)\": 0, \"License Type\": \"Commercial\"}, {\"Unique Dataset Identifier\": \"movieqa\", \"Collection\": \"movieqa\", \"Collection URL\": \"https://arxiv.org/abs/1512.02902\", \"Dataset Name\": \"MovieQA\", \"Paper Title\": \"MovieQA\", \"Paper URL\": \"https://arxiv.org/abs/1512.02902\", \"GitHub URL\": \"https://github.com/makarandtapaswi/MovieQA_CVPR2016/\", \"Hugging Face URL\": \"\", \"Papers with Code URL\": \"https://paperswithcode.com/dataset/movieqa\", \"ArXiv URL\": \"https://arxiv.org/abs/1512.02902\", \"Semantic Scholar Corpus ID\": 1017389, \"Year Released\": \"2015\", \"Text Sources\": [\"movies\"], \"Licenses\": [{\"License\": \"Unspecified\", \"License URL\": \"\"}], \"Creators\": \"Academic\", \"Countries\": [\"Germany\", \"Canada\", \"United States of America\"], \"License Verified By\": \"Vivek Sharma\", \"Video Hours\": 381.0, \"Taken Down\": \"True\", \"Video Sources\": [\"movies\"], \"Task Categories\": [\"Video Question Answering\"], \"Inferred Metadata\": {\"GitHub License\": \"\", \"GitHub Stars (June 2024)\": 80, \"GitHub Topics\": [], \"Github Date\": \"\", \"HF Config\": \"\", \"HF Config License\": \"\", \"HF Dataset\": \"\", \"HF Date\": \"\", \"HF Downloads (June 2024)\": \"\", \"HF Likes (June 2024)\": \"\", \"HF Yaml License\": \"\", \"PwC Date\": \"2016-01-01\", \"PwC Description\": \"The MovieQA dataset is a dataset for movie question answering. to evaluate automatic story comprehension from both video and text. The data set consists of almost 15,000 multiple choice question answers obtained from over 400 movies and features high semantic diversity. Each question comes with a set of five highly plausible answers; only one of which is correct. The questions can be answered using multiple sources of information: movie clips, plots, subtitles, and for a subset scripts and DVS.\", \"PwC License Name\": \"Unspecified\", \"PwC License URL\": null, \"S2 Citation Count (June 2024)\": \"\", \"S2 Date\": \"\"}, \"Bibtex\": \"@Article{Tapaswi2015MovieQAUS,\\n author = {Makarand Tapaswi and Yukun Zhu and R. Stiefelhagen and A. Torralba and R. Urtasun and S. Fidler},\\n booktitle = {Computer Vision and Pattern Recognition},\\n journal = {2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},\\n pages = {4631-4640},\\n title = {MovieQA: Understanding Stories in Movies through Question-Answering},\\n year = {2015}\\n}\\n\", \"License Use (DataProvenance)\": \"unspecified\", \"License Attribution (DataProvenance)\": 0, \"License Share Alike (DataProvenance)\": 0, \"License Use (DataProvenance IgnoreOpenAI)\": \"unspecified\", \"License Attribution (DataProvenance IgnoreOpenAI)\": 0, \"License Share Alike (DataProvenance IgnoreOpenAI)\": 0, \"License Type\": \"Unspecified\"}, {\"Unique Dataset Identifier\": \"volleyball-vids\", \"Collection\": \"volleyball-vids\", \"Collection URL\": \"https://arxiv.org/abs/1511.06040\", \"Dataset Name\": \"Volleyball: A Hierarchical Deep Temporal Model for Group Activity Recognition\", \"Paper Title\": \"Volleyball: A Hierarchical Deep Temporal Model for Group Activity Recognition\", \"Paper URL\": \"https://arxiv.org/abs/1511.06040\", \"GitHub URL\": \"https://github.com/mostafa-saad/deep-activity-rec#dataset\", \"Hugging Face URL\": \"\", \"Papers with Code URL\": \"https://paperswithcode.com/dataset/volleyball\", \"ArXiv URL\": \"https://arxiv.org/abs/1511.06040\", \"Semantic Scholar Corpus ID\": 8483403, \"Year Released\": \"2015\", \"Text Sources\": [\"youtube\"], \"Licenses\": [{\"License\": \"Unspecified\", \"License URL\": \"\"}], \"Creators\": \"Academic\", \"Countries\": [\"Canada\"], \"License Verified By\": \"Vivek Sharma\", \"Video Hours\": 0.1, \"Taken Down\": \"False\", \"Video Sources\": [\"youtube\"], \"Task Categories\": [\"Group Activity Recognition\"], \"Inferred Metadata\": {\"GitHub License\": \"BSD 3-Clause License\", \"GitHub Stars (June 2024)\": 171, \"GitHub Topics\": [], \"Github Date\": \"\", \"HF Config\": \"\", \"HF Config License\": \"\", \"HF Dataset\": \"\", \"HF Date\": \"\", \"HF Downloads (June 2024)\": \"\", \"HF Likes (June 2024)\": \"\", \"HF Yaml License\": \"\", \"PwC Date\": \"2016-01-01\", \"PwC Description\": \"Volleyball is a video action recognition dataset. It has 4830 annotated frames that were handpicked from 55 videos with 9 player action labels and 8 team activity labels. It contains group activity annotations as well as individual activity annotations.\", \"PwC License Name\": \"Unspecified\", \"PwC License URL\": null, \"S2 Citation Count (June 2024)\": \"\", \"S2 Date\": \"\"}, \"Bibtex\": \"@Article{Ibrahim2016HierarchicalDT,\\n author = {Mostafa S. Ibrahim and S. Muralidharan and Zhiwei Deng and Arash Vahdat and Greg Mori},\\n booktitle = {arXiv.org},\\n journal = {ArXiv},\\n title = {Hierarchical Deep Temporal Models for Group Activity Recognition},\\n volume = {abs/1607.02643},\\n year = {2016}\\n}\\n\", \"License Use (DataProvenance)\": \"unspecified\", \"License Attribution (DataProvenance)\": 0, \"License Share Alike (DataProvenance)\": 0, \"License Use (DataProvenance IgnoreOpenAI)\": \"unspecified\", \"License Attribution (DataProvenance IgnoreOpenAI)\": 0, \"License Share Alike (DataProvenance IgnoreOpenAI)\": 0, \"License Type\": \"Unspecified\"}, {\"Unique Dataset Identifier\": \"mpii-cooking\", \"Collection\": \"mpii-cooking\", \"Collection URL\": \"https://arxiv.org/abs/1502.06648\", \"Dataset Name\": \"MPII-Cooking: Recognizing Fine-Grained and Composite Activities Using Hand-Centric Features and Script Data (IJCV 2015)\", \"Paper Title\": \"MPII-Cooking: Recognizing Fine-Grained and Composite Activities Using Hand-Centric Features and Script Data (IJCV 2015)\", \"Paper URL\": \"https://arxiv.org/abs/1502.06648\", \"GitHub URL\": \"\", \"Hugging Face URL\": \"\", \"Papers with Code URL\": \"https://paperswithcode.com/paper/recognizing-fine-grained-and-composite\", \"ArXiv URL\": \"https://arxiv.org/abs/1502.06648\", \"Semantic Scholar Corpus ID\": 14036544, \"Year Released\": \"2015\", \"Text Sources\": [\"human\"], \"Licenses\": [{\"License\": \"Custom\", \"License URL\": \"https://www.mpi-inf.mpg.de/departments/computer-vision-and-machine-learning/research/human-activity-recognition/mpii-cooking-activities-dataset\"}], \"Creators\": \"Academic\", \"Countries\": [\"Germany\"], \"License Verified By\": \"Vivek Sharma\", \"Video Hours\": 27.0, \"Taken Down\": \"False\", \"Video Sources\": [\"human\"], \"Task Categories\": [\"Video Classification\"], \"Inferred Metadata\": {\"GitHub License\": \"\", \"GitHub Stars (June 2024)\": \"\", \"GitHub Topics\": \"\", \"Github Date\": \"\", \"HF Config\": \"\", \"HF Config License\": \"\", \"HF Dataset\": \"\", \"HF Date\": \"\", \"HF Downloads (June 2024)\": \"\", \"HF Likes (June 2024)\": \"\", \"HF Yaml License\": \"\", \"PwC Date\": \"\", \"PwC Description\": \"No code available yet.\", \"PwC License Name\": \"\", \"PwC License URL\": \"\", \"S2 Citation Count (June 2024)\": \"\", \"S2 Date\": \"\"}, \"Bibtex\": \"@Article{Rohrbach2015RecognizingFA,\\n author = {Marcus Rohrbach and Anna Rohrbach and Michaela Regneri and S. Amin and Mykhaylo Andriluka and Manfred Pinkal and B. Schiele},\\n booktitle = {International Journal of Computer Vision},\\n journal = {International Journal of Computer Vision},\\n pages = {346 - 373},\\n title = {Recognizing Fine-Grained and Composite Activities Using Hand-Centric Features and Script Data},\\n volume = {119},\\n year = {2015}\\n}\\n\", \"License Use (DataProvenance)\": \"academic-only\", \"License Attribution (DataProvenance)\": 1, \"License Share Alike (DataProvenance)\": 0, \"License Use (DataProvenance IgnoreOpenAI)\": \"academic-only\", \"License Attribution (DataProvenance IgnoreOpenAI)\": 1, \"License Share Alike (DataProvenance IgnoreOpenAI)\": 0, \"License Type\": \"Non-Commercial/Academic\"}, {\"Unique Dataset Identifier\": \"movieqa\", \"Collection\": \"movieqa\", \"Collection URL\": \"https://arxiv.org/abs/1512.02902\", \"Dataset Name\": \"MovieQA\", \"Paper Title\": \"MovieQA\", \"Paper URL\": \"https://arxiv.org/abs/1512.02902\", \"GitHub URL\": \"https://github.com/makarandtapaswi/MovieQA_CVPR2016/\", \"Hugging Face URL\": \"\", \"Papers with Code URL\": \"https://paperswithcode.com/dataset/movieqa\", \"ArXiv URL\": \"https://arxiv.org/abs/1512.02902\", \"Semantic Scholar Corpus ID\": 1017389, \"Year Released\": \"2015\", \"Text Sources\": [\"movies\"], \"Licenses\": [{\"License\": \"Unspecified\", \"License URL\": \"\"}], \"Creators\": \"Academic\", \"Countries\": [\"Germany\", \"Canada\", \"United States of America\"], \"License Verified By\": \"Vivek Sharma\", \"Video Hours\": 381.0, \"Taken Down\": \"True\", \"Video Sources\": [\"movies\"], \"Task Categories\": [\"Video Question Answering\"], \"Inferred Metadata\": {\"GitHub License\": \"\", \"GitHub Stars (June 2024)\": 80, \"GitHub Topics\": [], \"Github Date\": \"\", \"HF Config\": \"\", \"HF Config License\": \"\", \"HF Dataset\": \"\", \"HF Date\": \"\", \"HF Downloads (June 2024)\": \"\", \"HF Likes (June 2024)\": \"\", \"HF Yaml License\": \"\", \"PwC Date\": \"2016-01-01\", \"PwC Description\": \"The MovieQA dataset is a dataset for movie question answering. to evaluate automatic story comprehension from both video and text. The data set consists of almost 15,000 multiple choice question answers obtained from over 400 movies and features high semantic diversity. Each question comes with a set of five highly plausible answers; only one of which is correct. The questions can be answered using multiple sources of information: movie clips, plots, subtitles, and for a subset scripts and DVS.\", \"PwC License Name\": \"Unspecified\", \"PwC License URL\": null, \"S2 Citation Count (June 2024)\": \"\", \"S2 Date\": \"\"}, \"Bibtex\": \"@Article{Tapaswi2015MovieQAUS,\\n author = {Makarand Tapaswi and Yukun Zhu and R. Stiefelhagen and A. Torralba and R. Urtasun and S. Fidler},\\n booktitle = {Computer Vision and Pattern Recognition},\\n journal = {2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},\\n pages = {4631-4640},\\n title = {MovieQA: Understanding Stories in Movies through Question-Answering},\\n year = {2015}\\n}\\n\", \"License Use (DataProvenance)\": \"unspecified\", \"License Attribution (DataProvenance)\": 0, \"License Share Alike (DataProvenance)\": 0, \"License Use (DataProvenance IgnoreOpenAI)\": \"unspecified\", \"License Attribution (DataProvenance IgnoreOpenAI)\": 0, \"License Share Alike (DataProvenance IgnoreOpenAI)\": 0, \"License Type\": \"Unspecified\"}, {\"Unique Dataset Identifier\": \"movieqa\", \"Collection\": \"movieqa\", \"Collection URL\": \"https://arxiv.org/abs/1512.02902\", \"Dataset Name\": \"MovieQA\", \"Paper Title\": \"MovieQA\", \"Paper URL\": \"https://arxiv.org/abs/1512.02902\", \"GitHub URL\": \"https://github.com/makarandtapaswi/MovieQA_CVPR2016/\", \"Hugging Face URL\": \"\", \"Papers with Code URL\": \"https://paperswithcode.com/dataset/movieqa\", \"ArXiv URL\": \"https://arxiv.org/abs/1512.02902\", \"Semantic Scholar Corpus ID\": 1017389, \"Year Released\": \"2015\", \"Text Sources\": [\"movies\"], \"Licenses\": [{\"License\": \"Unspecified\", \"License URL\": \"\"}], \"Creators\": \"Academic\", \"Countries\": [\"Germany\", \"Canada\", \"United States of America\"], \"License Verified By\": \"Vivek Sharma\", \"Video Hours\": 381.0, \"Taken Down\": \"True\", \"Video Sources\": [\"movies\"], \"Task Categories\": [\"Video Question Answering\"], \"Inferred Metadata\": {\"GitHub License\": \"\", \"GitHub Stars (June 2024)\": 80, \"GitHub Topics\": [], \"Github Date\": \"\", \"HF Config\": \"\", \"HF Config License\": \"\", \"HF Dataset\": \"\", \"HF Date\": \"\", \"HF Downloads (June 2024)\": \"\", \"HF Likes (June 2024)\": \"\", \"HF Yaml License\": \"\", \"PwC Date\": \"2016-01-01\", \"PwC Description\": \"The MovieQA dataset is a dataset for movie question answering. to evaluate automatic story comprehension from both video and text. The data set consists of almost 15,000 multiple choice question answers obtained from over 400 movies and features high semantic diversity. Each question comes with a set of five highly plausible answers; only one of which is correct. The questions can be answered using multiple sources of information: movie clips, plots, subtitles, and for a subset scripts and DVS.\", \"PwC License Name\": \"Unspecified\", \"PwC License URL\": null, \"S2 Citation Count (June 2024)\": \"\", \"S2 Date\": \"\"}, \"Bibtex\": \"@Article{Tapaswi2015MovieQAUS,\\n author = {Makarand Tapaswi and Yukun Zhu and R. Stiefelhagen and A. Torralba and R. Urtasun and S. Fidler},\\n booktitle = {Computer Vision and Pattern Recognition},\\n journal = {2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},\\n pages = {4631-4640},\\n title = {MovieQA: Understanding Stories in Movies through Question-Answering},\\n year = {2015}\\n}\\n\", \"License Use (DataProvenance)\": \"unspecified\", \"License Attribution (DataProvenance)\": 0, \"License Share Alike (DataProvenance)\": 0, \"License Use (DataProvenance IgnoreOpenAI)\": \"unspecified\", \"License Attribution (DataProvenance IgnoreOpenAI)\": 0, \"License Share Alike (DataProvenance IgnoreOpenAI)\": 0, \"License Type\": \"Unspecified\"}, {\"Unique Dataset Identifier\": \"activitynet\", \"Collection\": \"activitynet\", \"Collection URL\": \"https://ieeexplore.ieee.org/document/7298698\", \"Dataset Name\": \"ActivityNet\", \"Paper Title\": \"ActivityNet\", \"Paper URL\": \"https://ieeexplore.ieee.org/document/7298698\", \"GitHub URL\": \"\", \"Hugging Face URL\": \"https://huggingface.co/datasets/Leyo/ActivityNet_Captions\", \"Papers with Code URL\": \"https://paperswithcode.com/dataset/activitynet\", \"ArXiv URL\": \"https://ieeexplore.ieee.org/document/7298698\", \"Semantic Scholar Corpus ID\": 1710722, \"Year Released\": \"2015\", \"Text Sources\": [\"undisclosed web\"], \"Licenses\": [{\"License\": \"MIT License\", \"License URL\": \"https://github.com/facebookresearch/ActivityNet-Entities/blob/main/LICENSE\"}], \"Creators\": \"Academic\", \"Countries\": [\"Saudi Arabia\", \"Colombia\"], \"License Verified By\": \"Vivek Sharma\", \"Video Hours\": 849.0, \"Taken Down\": \"False\", \"Video Sources\": [\"undisclosed web\"], \"Task Categories\": [\"Video Captioning\"], \"Inferred Metadata\": {\"GitHub License\": \"\", \"GitHub Stars (June 2024)\": \"\", \"GitHub Topics\": \"\", \"Github Date\": \"\", \"HF Config\": \"all\", \"HF Config License\": \"\", \"HF Dataset\": \"Leyo/ActivityNet_Captions\", \"HF Date\": \"2022-05-13\", \"HF Downloads (June 2024)\": 21, \"HF Likes (June 2024)\": 0, \"HF Yaml License\": \"Unspecified\", \"PwC Date\": \"2015-01-01\", \"PwC Description\": \"The ActivityNet dataset contains 200 different types of activities and a total of 849 hours of videos collected from YouTube. ActivityNet is the largest benchmark for temporal activity detection to date in terms of both the number of activity categories and number of videos, making the task particularly challenging. Version 1.3 of the dataset contains 19994 untrimmed videos in total and is divided into three disjoint subsets, training, validation, and testing by a ratio of 2:1:1. On average, each activity category has 137 untrimmed videos. Each video on average has 1.41 activities which are annotated with temporal boundaries. The ground-truth annotations of test videos are not public.\", \"PwC License Name\": \"Unspecified\", \"PwC License URL\": null, \"S2 Citation Count (June 2024)\": \"\", \"S2 Date\": \"\"}, \"Bibtex\": \"@Article{Heilbron2015ActivityNetAL,\\n author = {Fabian Caba Heilbron and Victor Escorcia and Bernard Ghanem and Juan Carlos Niebles},\\n booktitle = {Computer Vision and Pattern Recognition},\\n journal = {2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},\\n pages = {961-970},\\n title = {ActivityNet: A large-scale video benchmark for human activity understanding},\\n year = {2015}\\n}\\n\", \"License Use (DataProvenance)\": \"commercial\", \"License Attribution (DataProvenance)\": 1, \"License Share Alike (DataProvenance)\": 0, \"License Use (DataProvenance IgnoreOpenAI)\": \"commercial\", \"License Attribution (DataProvenance IgnoreOpenAI)\": 1, \"License Share Alike (DataProvenance IgnoreOpenAI)\": 0, \"License Type\": \"Commercial\"}, {\"Unique Dataset Identifier\": \"activitynet\", \"Collection\": \"activitynet\", \"Collection URL\": \"https://ieeexplore.ieee.org/document/7298698\", \"Dataset Name\": \"ActivityNet\", \"Paper Title\": \"ActivityNet\", \"Paper URL\": \"https://ieeexplore.ieee.org/document/7298698\", \"GitHub URL\": \"\", \"Hugging Face URL\": \"https://huggingface.co/datasets/Leyo/ActivityNet_Captions\", \"Papers with Code URL\": \"https://paperswithcode.com/dataset/activitynet\", \"ArXiv URL\": \"https://ieeexplore.ieee.org/document/7298698\", \"Semantic Scholar Corpus ID\": 1710722, \"Year Released\": \"2015\", \"Text Sources\": [\"undisclosed web\"], \"Licenses\": [{\"License\": \"MIT License\", \"License URL\": \"https://github.com/facebookresearch/ActivityNet-Entities/blob/main/LICENSE\"}], \"Creators\": \"Academic\", \"Countries\": [\"Saudi Arabia\", \"Colombia\"], \"License Verified By\": \"Vivek Sharma\", \"Video Hours\": 849.0, \"Taken Down\": \"False\", \"Video Sources\": [\"undisclosed web\"], \"Task Categories\": [\"Video Captioning\"], \"Inferred Metadata\": {\"GitHub License\": \"\", \"GitHub Stars (June 2024)\": \"\", \"GitHub Topics\": \"\", \"Github Date\": \"\", \"HF Config\": \"all\", \"HF Config License\": \"\", \"HF Dataset\": \"Leyo/ActivityNet_Captions\", \"HF Date\": \"2022-05-13\", \"HF Downloads (June 2024)\": 21, \"HF Likes (June 2024)\": 0, \"HF Yaml License\": \"Unspecified\", \"PwC Date\": \"2015-01-01\", \"PwC Description\": \"The ActivityNet dataset contains 200 different types of activities and a total of 849 hours of videos collected from YouTube. ActivityNet is the largest benchmark for temporal activity detection to date in terms of both the number of activity categories and number of videos, making the task particularly challenging. Version 1.3 of the dataset contains 19994 untrimmed videos in total and is divided into three disjoint subsets, training, validation, and testing by a ratio of 2:1:1. On average, each activity category has 137 untrimmed videos. Each video on average has 1.41 activities which are annotated with temporal boundaries. The ground-truth annotations of test videos are not public.\", \"PwC License Name\": \"Unspecified\", \"PwC License URL\": null, \"S2 Citation Count (June 2024)\": \"\", \"S2 Date\": \"\"}, \"Bibtex\": \"@Article{Heilbron2015ActivityNetAL,\\n author = {Fabian Caba Heilbron and Victor Escorcia and Bernard Ghanem and Juan Carlos Niebles},\\n booktitle = {Computer Vision and Pattern Recognition},\\n journal = {2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},\\n pages = {961-970},\\n title = {ActivityNet: A large-scale video benchmark for human activity understanding},\\n year = {2015}\\n}\\n\", \"License Use (DataProvenance)\": \"commercial\", \"License Attribution (DataProvenance)\": 1, \"License Share Alike (DataProvenance)\": 0, \"License Use (DataProvenance IgnoreOpenAI)\": \"commercial\", \"License Attribution (DataProvenance IgnoreOpenAI)\": 1, \"License Share Alike (DataProvenance IgnoreOpenAI)\": 0, \"License Type\": \"Commercial\"}, {\"Unique Dataset Identifier\": \"tvsum\", \"Collection\": \"tvsum\", \"Collection URL\": \"https://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Song_TVSum_Summarizing_Web_2015_CVPR_paper.pdf\", \"Dataset Name\": \"TVSum: Summarizing web videos using titles (CVPR 2015)\", \"Paper Title\": \"TVSum: Summarizing web videos using titles (CVPR 2015)\", \"Paper URL\": \"https://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Song_TVSum_Summarizing_Web_2015_CVPR_paper.pdf\", \"GitHub URL\": \"https://github.com/yalesong/tvsum\", \"Hugging Face URL\": \"\", \"Papers with Code URL\": \"https://paperswithcode.com/dataset/tvsum-1\", \"ArXiv URL\": \"https://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Song_TVSum_Summarizing_Web_2015_CVPR_paper.pdf\", \"Semantic Scholar Corpus ID\": 7675635, \"Year Released\": \"2015\", \"Text Sources\": [\"youtube\"], \"Licenses\": [{\"License\": \"CC BY 3.0\", \"License URL\": \"https://github.com/yalesong/tvsum\"}], \"Creators\": \"Industry Lab\", \"Countries\": [\"United States of America\"], \"License Verified By\": \"Vivek Sharma\", \"Video Hours\": 3.5, \"Taken Down\": \"False\", \"Video Sources\": [\"youtube\"], \"Task Categories\": [\"Video Summarization\"], \"Inferred Metadata\": {\"GitHub License\": \"\", \"GitHub Stars (June 2024)\": 116, \"GitHub Topics\": [], \"Github Date\": \"\", \"HF Config\": \"\", \"HF Config License\": \"\", \"HF Dataset\": \"\", \"HF Date\": \"\", \"HF Downloads (June 2024)\": \"\", \"HF Likes (June 2024)\": \"\", \"HF Yaml License\": \"\", \"PwC Date\": \"2015-06-01\", \"PwC Description\": \"Title-based Video Summarization (TVSum) dataset serves as a benchmark to validate video summarization techniques. It contains 50 videos of various genres (e.g., news, how-to, documentary, vlog, egocentric) and 1,000 annotations of shot-level importance scores obtained via crowdsourcing (20 per video).\", \"PwC License Name\": \"Unspecified\", \"PwC License URL\": null, \"S2 Citation Count (June 2024)\": \"\", \"S2 Date\": \"\"}, \"Bibtex\": \"@Article{Song2015TVSumSW,\\n author = {Yale Song and Jordi Vallmitjana and Amanda Stent and A. Jaimes},\\n booktitle = {Computer Vision and Pattern Recognition},\\n journal = {2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},\\n pages = {5179-5187},\\n title = {TVSum: Summarizing web videos using titles},\\n year = {2015}\\n}\\n\", \"License Use (DataProvenance)\": \"commercial\", \"License Attribution (DataProvenance)\": 1, \"License Share Alike (DataProvenance)\": 0, \"License Use (DataProvenance IgnoreOpenAI)\": \"commercial\", \"License Attribution (DataProvenance IgnoreOpenAI)\": 1, \"License Share Alike (DataProvenance IgnoreOpenAI)\": 0, \"License Type\": \"Commercial\"}, {\"Unique Dataset Identifier\": \"mars\", \"Collection\": \"mars\", \"Collection URL\": \"https://link.springer.com/content/pdf/10.1007/978-3-319-46466-4_52.pdf\", \"Dataset Name\": \"Mars\", \"Paper Title\": \"Mars\", \"Paper URL\": \"https://link.springer.com/content/pdf/10.1007/978-3-319-46466-4_52.pdf\", \"GitHub URL\": \"\", \"Hugging Face URL\": \"\", \"Papers with Code URL\": \"https://paperswithcode.com/dataset/mars\", \"ArXiv URL\": \"https://link.springer.com/content/pdf/10.1007/978-3-319-46466-4_52.pdf\", \"Semantic Scholar Corpus ID\": 2214158, \"Year Released\": \"2016\", \"Text Sources\": [\"human\"], \"Licenses\": [{\"License\": \"Unspecified\", \"License URL\": \"\"}], \"Creators\": \"Academic\", \"Countries\": [\"China\"], \"License Verified By\": \"Vivek Sharma\", \"Video Hours\": 0.24, \"Taken Down\": \"False\", \"Video Sources\": [\"human\"], \"Task Categories\": [\"Video Segmentation\", \"Video Classification\"], \"Inferred Metadata\": {\"GitHub License\": \"\", \"GitHub Stars (June 2024)\": \"\", \"GitHub Topics\": \"\", \"Github Date\": \"\", \"HF Config\": \"\", \"HF Config License\": \"\", \"HF Dataset\": \"\", \"HF Date\": \"\", \"HF Downloads (June 2024)\": \"\", \"HF Likes (June 2024)\": \"\", \"HF Yaml License\": \"\", \"PwC Date\": \"2016-01-01\", \"PwC Description\": \"MARS (Motion Analysis and Re-identification Set) is a large scale video based person reidentification dataset, an extension of the Market-1501 dataset. It has been collected from six near-synchronized cameras. It consists of 1,261 different pedestrians, who are captured by at least 2 cameras. The variations in poses, colors and illuminations of pedestrians, as well as the poor image quality, make it very difficult to yield high matching accuracy. Moreover, the dataset contains 3,248 distractors in order to make it more realistic. Deformable Part Model and GMMCP tracker were used to automatically generate the tracklets (mostly 25-50 frames long).\", \"PwC License Name\": \"Unspecified\", \"PwC License URL\": null, \"S2 Citation Count (June 2024)\": \"\", \"S2 Date\": \"\"}, \"Bibtex\": \"@Article{Zheng2016MARSAV,\\n author = {Liang Zheng and Zhi Bie and Yifan Sun and Jingdong Wang and Chi Su and Shengjin Wang and Q. Tian},\\n booktitle = {European Conference on Computer Vision},\\n pages = {868-884},\\n title = {MARS: A Video Benchmark for Large-Scale Person Re-Identification},\\n year = {2016}\\n}\\n\", \"License Use (DataProvenance)\": \"unspecified\", \"License Attribution (DataProvenance)\": 0, \"License Share Alike (DataProvenance)\": 0, \"License Use (DataProvenance IgnoreOpenAI)\": \"unspecified\", \"License Attribution (DataProvenance IgnoreOpenAI)\": 0, \"License Share Alike (DataProvenance IgnoreOpenAI)\": 0, \"License Type\": \"Unspecified\"}, {\"Unique Dataset Identifier\": \"charades\", \"Collection\": \"charades\", \"Collection URL\": \"https://arxiv.org/abs/1604.01753\", \"Dataset Name\": \"Charades\", \"Paper Title\": \"Charades\", \"Paper URL\": \"https://arxiv.org/abs/1604.01753\", \"GitHub URL\": \"\", \"Hugging Face URL\": \"https://huggingface.co/datasets/HuggingFaceM4/charades\", \"Papers with Code URL\": \"https://paperswithcode.com/dataset/charades\", \"ArXiv URL\": \"https://arxiv.org/abs/1604.01753\", \"Semantic Scholar Corpus ID\": 18061547, \"Year Released\": \"2016\", \"Text Sources\": [\"human\"], \"Licenses\": [{\"License\": \"Custom\", \"License URL\": \"https://huggingface.co/datasets/HuggingFaceM4/charades#licensing-information\"}], \"Creators\": \"Academic\", \"Countries\": [\"United States of America\", \"France\"], \"License Verified By\": \"Vivek Sharma\", \"Video Hours\": 82.3, \"Taken Down\": \"False\", \"Video Sources\": [\"human\"], \"Task Categories\": [\"Video Captioning\"], \"Inferred Metadata\": {\"GitHub License\": \"\", \"GitHub Stars (June 2024)\": \"\", \"GitHub Topics\": \"\", \"Github Date\": \"\", \"HF Config\": \"default\", \"HF Config License\": \"\", \"HF Dataset\": \"HuggingFaceM4/charades\", \"HF Date\": \"2022-05-11\", \"HF Downloads (June 2024)\": 7, \"HF Likes (June 2024)\": 2, \"HF Yaml License\": \"Unspecified\", \"PwC Date\": \"2016-01-01\", \"PwC Description\": \"The Charades dataset is composed of 9,848 videos of daily indoors activities with an average length of 30 seconds, involving interactions with 46 objects classes in 15 types of indoor scenes and containing a vocabulary of 30 verbs leading to 157 action classes. Each video in this dataset is annotated by multiple free-text descriptions, action labels, action intervals and classes of interacting objects. 267 different users were presented with a sentence, which includes objects and actions from a fixed vocabulary, and they recorded a video acting out the sentence. In total, the dataset contains 66,500 temporal annotations for 157 action classes, 41,104 labels for 46 object classes, and 27,847 textual descriptions of the videos. In the standard split there are7,986 training video and 1,863 validation video.\", \"PwC License Name\": \"Non Commercial\", \"PwC License URL\": \"http://vuchallenge.org/license-charades.txt\", \"S2 Citation Count (June 2024)\": \"\", \"S2 Date\": \"\"}, \"Bibtex\": \"@Article{Sigurdsson2016HollywoodIH,\\n author = {Gunnar A. Sigurdsson and G\\u00fcl Varol and X. Wang and Ali Farhadi and I. Laptev and A. Gupta},\\n booktitle = {European Conference on Computer Vision},\\n pages = {510-526},\\n title = {Hollywood in Homes: Crowdsourcing Data Collection for Activity Understanding},\\n year = {2016}\\n}\\n\", \"License Use (DataProvenance)\": \"commercial\", \"License Attribution (DataProvenance)\": 1, \"License Share Alike (DataProvenance)\": 1, \"License Use (DataProvenance IgnoreOpenAI)\": \"commercial\", \"License Attribution (DataProvenance IgnoreOpenAI)\": 1, \"License Share Alike (DataProvenance IgnoreOpenAI)\": 1, \"License Type\": \"Commercial\"}, {\"Unique Dataset Identifier\": \"vtw\", \"Collection\": \"vtw\", \"Collection URL\": \"https://arxiv.org/abs/1608.07068\", \"Dataset Name\": \"VTW\", \"Paper Title\": \"VTW\", \"Paper URL\": \"https://arxiv.org/abs/1608.07068\", \"GitHub URL\": \"\", \"Hugging Face URL\": \"\", \"Papers with Code URL\": \"https://paperswithcode.com/paper/title-generation-for-user-generated-videos\", \"ArXiv URL\": \"https://arxiv.org/abs/1608.07068\", \"Semantic Scholar Corpus ID\": 6155397, \"Year Released\": \"2016\", \"Text Sources\": [\"undisclosed web\"], \"Licenses\": [{\"License\": \"Unspecified\", \"License URL\": \"\"}], \"Creators\": \"Academic\", \"Countries\": [\"China\", \"United States of America\"], \"License Verified By\": \"Vivek Sharma\", \"Video Hours\": 213.2, \"Taken Down\": \"False\", \"Video Sources\": [\"undisclosed web\"], \"Task Categories\": [\"Video Captioning\"], \"Inferred Metadata\": {\"GitHub License\": \"\", \"GitHub Stars (June 2024)\": \"\", \"GitHub Topics\": \"\", \"Github Date\": \"\", \"HF Config\": \"\", \"HF Config License\": \"\", \"HF Dataset\": \"\", \"HF Date\": \"\", \"HF Downloads (June 2024)\": \"\", \"HF Likes (June 2024)\": \"\", \"HF Yaml License\": \"\", \"PwC Date\": \"\", \"PwC Description\": \"No code available yet.\", \"PwC License Name\": \"\", \"PwC License URL\": \"\", \"S2 Citation Count (June 2024)\": \"\", \"S2 Date\": \"\"}, \"Bibtex\": \"@Article{Zeng2016TitleGF,\\n author = {Kuo-Hao Zeng and Tseng-Hung Chen and Juan Carlos Niebles and Min Sun},\\n booktitle = {European Conference on Computer Vision},\\n pages = {609-625},\\n title = {Title Generation for User Generated Videos},\\n year = {2016}\\n}\\n\", \"License Use (DataProvenance)\": \"unspecified\", \"License Attribution (DataProvenance)\": 0, \"License Share Alike (DataProvenance)\": 0, \"License Use (DataProvenance IgnoreOpenAI)\": \"unspecified\", \"License Attribution (DataProvenance IgnoreOpenAI)\": 0, \"License Share Alike (DataProvenance IgnoreOpenAI)\": 0, \"License Type\": \"Unspecified\"}, {\"Unique Dataset Identifier\": \"vtw\", \"Collection\": \"vtw\", \"Collection URL\": \"https://arxiv.org/abs/1608.07068\", \"Dataset Name\": \"VTW\", \"Paper Title\": \"VTW\", \"Paper URL\": \"https://arxiv.org/abs/1608.07068\", \"GitHub URL\": \"\", \"Hugging Face URL\": \"\", \"Papers with Code URL\": \"https://paperswithcode.com/paper/title-generation-for-user-generated-videos\", \"ArXiv URL\": \"https://arxiv.org/abs/1608.07068\", \"Semantic Scholar Corpus ID\": 6155397, \"Year Released\": \"2016\", \"Text Sources\": [\"undisclosed web\"], \"Licenses\": [{\"License\": \"Unspecified\", \"License URL\": \"\"}], \"Creators\": \"Academic\", \"Countries\": [\"China\", \"United States of America\"], \"License Verified By\": \"Vivek Sharma\", \"Video Hours\": 213.2, \"Taken Down\": \"False\", \"Video Sources\": [\"undisclosed web\"], \"Task Categories\": [\"Video Captioning\"], \"Inferred Metadata\": {\"GitHub License\": \"\", \"GitHub Stars (June 2024)\": \"\", \"GitHub Topics\": \"\", \"Github Date\": \"\", \"HF Config\": \"\", \"HF Config License\": \"\", \"HF Dataset\": \"\", \"HF Date\": \"\", \"HF Downloads (June 2024)\": \"\", \"HF Likes (June 2024)\": \"\", \"HF Yaml License\": \"\", \"PwC Date\": \"\", \"PwC Description\": \"No code available yet.\", \"PwC License Name\": \"\", \"PwC License URL\": \"\", \"S2 Citation Count (June 2024)\": \"\", \"S2 Date\": \"\"}, \"Bibtex\": \"@Article{Zeng2016TitleGF,\\n author = {Kuo-Hao Zeng and Tseng-Hung Chen and Juan Carlos Niebles and Min Sun},\\n booktitle = {European Conference on Computer Vision},\\n pages = {609-625},\\n title = {Title Generation for User Generated Videos},\\n year = {2016}\\n}\\n\", \"License Use (DataProvenance)\": \"unspecified\", \"License Attribution (DataProvenance)\": 0, \"License Share Alike (DataProvenance)\": 0, \"License Use (DataProvenance IgnoreOpenAI)\": \"unspecified\", \"License Attribution (DataProvenance IgnoreOpenAI)\": 0, \"License Share Alike (DataProvenance IgnoreOpenAI)\": 0, \"License Type\": \"Unspecified\"}, {\"Unique Dataset Identifier\": \"mars\", \"Collection\": \"mars\", \"Collection URL\": \"https://link.springer.com/content/pdf/10.1007/978-3-319-46466-4_52.pdf\", \"Dataset Name\": \"Mars\", \"Paper Title\": \"Mars\", \"Paper URL\": \"https://link.springer.com/content/pdf/10.1007/978-3-319-46466-4_52.pdf\", \"GitHub URL\": \"\", \"Hugging Face URL\": \"\", \"Papers with Code URL\": \"https://paperswithcode.com/dataset/mars\", \"ArXiv URL\": \"https://link.springer.com/content/pdf/10.1007/978-3-319-46466-4_52.pdf\", \"Semantic Scholar Corpus ID\": 2214158, \"Year Released\": \"2016\", \"Text Sources\": [\"human\"], \"Licenses\": [{\"License\": \"Unspecified\", \"License URL\": \"\"}], \"Creators\": \"Academic\", \"Countries\": [\"China\"], \"License Verified By\": \"Vivek Sharma\", \"Video Hours\": 0.24, \"Taken Down\": \"False\", \"Video Sources\": [\"human\"], \"Task Categories\": [\"Video Segmentation\", \"Video Classification\"], \"Inferred Metadata\": {\"GitHub License\": \"\", \"GitHub Stars (June 2024)\": \"\", \"GitHub Topics\": \"\", \"Github Date\": \"\", \"HF Config\": \"\", \"HF Config License\": \"\", \"HF Dataset\": \"\", \"HF Date\": \"\", \"HF Downloads (June 2024)\": \"\", \"HF Likes (June 2024)\": \"\", \"HF Yaml License\": \"\", \"PwC Date\": \"2016-01-01\", \"PwC Description\": \"MARS (Motion Analysis and Re-identification Set) is a large scale video based person reidentification dataset, an extension of the Market-1501 dataset. It has been collected from six near-synchronized cameras. It consists of 1,261 different pedestrians, who are captured by at least 2 cameras. The variations in poses, colors and illuminations of pedestrians, as well as the poor image quality, make it very difficult to yield high matching accuracy. Moreover, the dataset contains 3,248 distractors in order to make it more realistic. Deformable Part Model and GMMCP tracker were used to automatically generate the tracklets (mostly 25-50 frames long).\", \"PwC License Name\": \"Unspecified\", \"PwC License URL\": null, \"S2 Citation Count (June 2024)\": \"\", \"S2 Date\": \"\"}, \"Bibtex\": \"@Article{Zheng2016MARSAV,\\n author = {Liang Zheng and Zhi Bie and Yifan Sun and Jingdong Wang and Chi Su and Shengjin Wang and Q. Tian},\\n booktitle = {European Conference on Computer Vision},\\n pages = {868-884},\\n title = {MARS: A Video Benchmark for Large-Scale Person Re-Identification},\\n year = {2016}\\n}\\n\", \"License Use (DataProvenance)\": \"unspecified\", \"License Attribution (DataProvenance)\": 0, \"License Share Alike (DataProvenance)\": 0, \"License Use (DataProvenance IgnoreOpenAI)\": \"unspecified\", \"License Attribution (DataProvenance IgnoreOpenAI)\": 0, \"License Share Alike (DataProvenance IgnoreOpenAI)\": 0, \"License Type\": \"Unspecified\"}, {\"Unique Dataset Identifier\": \"narrated-instruction-vids\", \"Collection\": \"narrated-instruction-vids\", \"Collection URL\": \"https://arxiv.org/abs/1506.09215\", \"Dataset Name\": \"Narrated Instruction Videos\", \"Paper Title\": \"Narrated Instruction Videos\", \"Paper URL\": \"https://arxiv.org/abs/1506.09215\", \"GitHub URL\": \"https://github.com/jalayrac/instructionVideos\", \"Hugging Face URL\": \"\", \"Papers with Code URL\": \"https://paperswithcode.com/dataset/youtube-inria-instructional\", \"ArXiv URL\": \"https://arxiv.org/abs/1506.09215\", \"Semantic Scholar Corpus ID\": 2617244, \"Year Released\": \"2016\", \"Text Sources\": [\"youtube\"], \"Licenses\": [{\"License\": \"MIT License\", \"License URL\": \"https://github.com/jalayrac/instructionVideos?tab=readme-ov-file#license\"}], \"Creators\": \"Research Group\", \"Countries\": [\"United States of America\", \"India\"], \"License Verified By\": \"Vivek Sharma\", \"Video Hours\": 7.0, \"Taken Down\": \"False\", \"Video Sources\": [\"youtube\"], \"Task Categories\": [\"Video Captioning\"], \"Inferred Metadata\": {\"GitHub License\": \"MIT License\", \"GitHub Stars (June 2024)\": 19, \"GitHub Topics\": [], \"Github Date\": \"\", \"HF Config\": \"\", \"HF Config License\": \"\", \"HF Dataset\": \"\", \"HF Date\": \"\", \"HF Downloads (June 2024)\": \"\", \"HF Likes (June 2024)\": \"\", \"HF Yaml License\": \"\", \"PwC Date\": \"\", \"PwC Description\": \"We address the problem of automatically learning the main steps to complete a certain task, such as changing a car tire, from a set of narrated instruction videos. The contributions of this paper are three-fold. First, we develop a new unsupervised learning approach that takes advantage of the complementary nature of the input video and the associated narration. The method solves two clustering problems, one in text and one in video, applied one after each other and linked by joint constraints to obtain a single coherent sequence of steps in both modalities. Second, we collect and annotate a new challenging dataset of real-world instruction videos from the Internet. The dataset contains about 800,000 frames for five different tasks (How to : change a car tire, perform CardioPulmonary resuscitation (CPR), jump cars, repot a plant and make coffee) that include complex interactions between people and objects, and are captured in a variety of indoor and outdoor settings. Third, we experimentally demonstrate that the proposed method can automatically discover, in an unsupervised manner , the main steps to achieve the task and locate the steps in the input videos.\\n\\nThis video presents our results of automatically discovering the scenario for the two following task : changing a tire and performing CardioPulmonary Resuscitation (CPR). At the bottom of the videos, there are three bars. The first one corresponds to our ground truth annotation. The second one corresponds to our time interval prediction in video. Finally the third one corresponds to the constraints that we obtain from the text domain. On the right, there is a list of label. They corresponds to the label recovered by our NLP method in an unsupervised manner.\", \"PwC License Name\": \"Unspecified\", \"PwC License URL\": null, \"S2 Citation Count (June 2024)\": \"\", \"S2 Date\": \"\"}, \"Bibtex\": \"@Article{Alayrac2015UnsupervisedLF,\\n author = {Jean-Baptiste Alayrac and Piotr Bojanowski and Nishant Agrawal and Josef Sivic and I. Laptev and Simon Lacoste-Julien},\\n booktitle = {Computer Vision and Pattern Recognition},\\n journal = {2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},\\n pages = {4575-4583},\\n title = {Unsupervised Learning from Narrated Instruction Videos},\\n year = {2015}\\n}\\n\", \"License Use (DataProvenance)\": \"commercial\", \"License Attribution (DataProvenance)\": 1, \"License Share Alike (DataProvenance)\": 0, \"License Use (DataProvenance IgnoreOpenAI)\": \"commercial\", \"License Attribution (DataProvenance IgnoreOpenAI)\": 1, \"License Share Alike (DataProvenance IgnoreOpenAI)\": 0, \"License Type\": \"Commercial\"}, {\"Unique Dataset Identifier\": \"charades\", \"Collection\": \"charades\", \"Collection URL\": \"https://arxiv.org/abs/1604.01753\", \"Dataset Name\": \"Charades\", \"Paper Title\": \"Charades\", \"Paper URL\": \"https://arxiv.org/abs/1604.01753\", \"GitHub URL\": \"\", \"Hugging Face URL\": \"https://huggingface.co/datasets/HuggingFaceM4/charades\", \"Papers with Code URL\": \"https://paperswithcode.com/dataset/charades\", \"ArXiv URL\": \"https://arxiv.org/abs/1604.01753\", \"Semantic Scholar Corpus ID\": 18061547, \"Year Released\": \"2016\", \"Text Sources\": [\"human\"], \"Licenses\": [{\"License\": \"Custom\", \"License URL\": \"https://huggingface.co/datasets/HuggingFaceM4/charades#licensing-information\"}], \"Creators\": \"Academic\", \"Countries\": [\"United States of America\", \"France\"], \"License Verified By\": \"Vivek Sharma\", \"Video Hours\": 82.3, \"Taken Down\": \"False\", \"Video Sources\": [\"human\"], \"Task Categories\": [\"Video Captioning\"], \"Inferred Metadata\": {\"GitHub License\": \"\", \"GitHub Stars (June 2024)\": \"\", \"GitHub Topics\": \"\", \"Github Date\": \"\", \"HF Config\": \"default\", \"HF Config License\": \"\", \"HF Dataset\": \"HuggingFaceM4/charades\", \"HF Date\": \"2022-05-11\", \"HF Downloads (June 2024)\": 7, \"HF Likes (June 2024)\": 2, \"HF Yaml License\": \"Unspecified\", \"PwC Date\": \"2016-01-01\", \"PwC Description\": \"The Charades dataset is composed of 9,848 videos of daily indoors activities with an average length of 30 seconds, involving interactions with 46 objects classes in 15 types of indoor scenes and containing a vocabulary of 30 verbs leading to 157 action classes. Each video in this dataset is annotated by multiple free-text descriptions, action labels, action intervals and classes of interacting objects. 267 different users were presented with a sentence, which includes objects and actions from a fixed vocabulary, and they recorded a video acting out the sentence. In total, the dataset contains 66,500 temporal annotations for 157 action classes, 41,104 labels for 46 object classes, and 27,847 textual descriptions of the videos. In the standard split there are7,986 training video and 1,863 validation video.\", \"PwC License Name\": \"Non Commercial\", \"PwC License URL\": \"http://vuchallenge.org/license-charades.txt\", \"S2 Citation Count (June 2024)\": \"\", \"S2 Date\": \"\"}, \"Bibtex\": \"@Article{Sigurdsson2016HollywoodIH,\\n author = {Gunnar A. Sigurdsson and G\\u00fcl Varol and X. Wang and Ali Farhadi and I. Laptev and A. Gupta},\\n booktitle = {European Conference on Computer Vision},\\n pages = {510-526},\\n title = {Hollywood in Homes: Crowdsourcing Data Collection for Activity Understanding},\\n year = {2016}\\n}\\n\", \"License Use (DataProvenance)\": \"commercial\", \"License Attribution (DataProvenance)\": 1, \"License Share Alike (DataProvenance)\": 1, \"License Use (DataProvenance IgnoreOpenAI)\": \"commercial\", \"License Attribution (DataProvenance IgnoreOpenAI)\": 1, \"License Share Alike (DataProvenance IgnoreOpenAI)\": 1, \"License Type\": \"Commercial\"}, {\"Unique Dataset Identifier\": \"charades\", \"Collection\": \"charades\", \"Collection URL\": \"https://arxiv.org/abs/1604.01753\", \"Dataset Name\": \"Charades\", \"Paper Title\": \"Charades\", \"Paper URL\": \"https://arxiv.org/abs/1604.01753\", \"GitHub URL\": \"\", \"Hugging Face URL\": \"https://huggingface.co/datasets/HuggingFaceM4/charades\", \"Papers with Code URL\": \"https://paperswithcode.com/dataset/charades\", \"ArXiv URL\": \"https://arxiv.org/abs/1604.01753\", \"Semantic Scholar Corpus ID\": 18061547, \"Year Released\": \"2016\", \"Text Sources\": [\"human\"], \"Licenses\": [{\"License\": \"Custom\", \"License URL\": \"https://huggingface.co/datasets/HuggingFaceM4/charades#licensing-information\"}], \"Creators\": \"Research Group\", \"Countries\": [\"United States of America\", \"France\"], \"License Verified By\": \"Vivek Sharma\", \"Video Hours\": 82.3, \"Taken Down\": \"False\", \"Video Sources\": [\"human\"], \"Task Categories\": [\"Video Captioning\"], \"Inferred Metadata\": {\"GitHub License\": \"\", \"GitHub Stars (June 2024)\": \"\", \"GitHub Topics\": \"\", \"Github Date\": \"\", \"HF Config\": \"default\", \"HF Config License\": \"\", \"HF Dataset\": \"HuggingFaceM4/charades\", \"HF Date\": \"2022-05-11\", \"HF Downloads (June 2024)\": 7, \"HF Likes (June 2024)\": 2, \"HF Yaml License\": \"Unspecified\", \"PwC Date\": \"2016-01-01\", \"PwC Description\": \"The Charades dataset is composed of 9,848 videos of daily indoors activities with an average length of 30 seconds, involving interactions with 46 objects classes in 15 types of indoor scenes and containing a vocabulary of 30 verbs leading to 157 action classes. Each video in this dataset is annotated by multiple free-text descriptions, action labels, action intervals and classes of interacting objects. 267 different users were presented with a sentence, which includes objects and actions from a fixed vocabulary, and they recorded a video acting out the sentence. In total, the dataset contains 66,500 temporal annotations for 157 action classes, 41,104 labels for 46 object classes, and 27,847 textual descriptions of the videos. In the standard split there are7,986 training video and 1,863 validation video.\", \"PwC License Name\": \"Non Commercial\", \"PwC License URL\": \"http://vuchallenge.org/license-charades.txt\", \"S2 Citation Count (June 2024)\": \"\", \"S2 Date\": \"\"}, \"Bibtex\": \"@Article{Sigurdsson2016HollywoodIH,\\n author = {Gunnar A. Sigurdsson and G\\u00fcl Varol and X. Wang and Ali Farhadi and I. Laptev and A. Gupta},\\n booktitle = {European Conference on Computer Vision},\\n pages = {510-526},\\n title = {Hollywood in Homes: Crowdsourcing Data Collection for Activity Understanding},\\n year = {2016}\\n}\\n\", \"License Use (DataProvenance)\": \"commercial\", \"License Attribution (DataProvenance)\": 1, \"License Share Alike (DataProvenance)\": 1, \"License Use (DataProvenance IgnoreOpenAI)\": \"commercial\", \"License Attribution (DataProvenance IgnoreOpenAI)\": 1, \"License Share Alike (DataProvenance IgnoreOpenAI)\": 1, \"License Type\": \"Commercial\"}, {\"Unique Dataset Identifier\": \"msr-vtt\", \"Collection\": \"msr-vtt\", \"Collection URL\": \"https://ieeexplore.ieee.org/document/7780940\", \"Dataset Name\": \"MSR-VTT\", \"Paper Title\": \"MSR-VTT\", \"Paper URL\": \"https://ieeexplore.ieee.org/document/7780940\", \"GitHub URL\": \"\", \"Hugging Face URL\": \"\", \"Papers with Code URL\": \"https://paperswithcode.com/dataset/msr-vtt\", \"ArXiv URL\": \"https://ieeexplore.ieee.org/document/7780940\", \"Semantic Scholar Corpus ID\": 206594535, \"Year Released\": \"2016\", \"Text Sources\": [\"undisclosed web\"], \"Licenses\": [{\"License\": \"Unspecified\", \"License URL\": \"\"}], \"Creators\": \"Industry Lab\", \"Countries\": [\"United States of America\"], \"License Verified By\": \"Vivek Sharma\", \"Video Hours\": 41.2, \"Taken Down\": \"False\", \"Video Sources\": [\"undisclosed web\"], \"Task Categories\": [\"Video Captioning\"], \"Inferred Metadata\": {\"GitHub License\": \"\", \"GitHub Stars (June 2024)\": \"\", \"GitHub Topics\": \"\", \"Github Date\": \"\", \"HF Config\": \"\", \"HF Config License\": \"\", \"HF Dataset\": \"\", \"HF Date\": \"\", \"HF Downloads (June 2024)\": \"\", \"HF Likes (June 2024)\": \"\", \"HF Yaml License\": \"\", \"PwC Date\": \"2016-01-01\", \"PwC Description\": \"MSR-VTT (Microsoft Research Video to Text) is a large-scale dataset for the open domain video captioning, which consists of 10,000 video clips from 20 categories, and each video clip is annotated with 20 English sentences by Amazon Mechanical Turks. There are about 29,000 unique words in all captions. The standard splits uses 6,513 clips for training, 497 clips for validation, and 2,990 clips for testing.\", \"PwC License Name\": \"Unspecified\", \"PwC License URL\": null, \"S2 Citation Count (June 2024)\": \"\", \"S2 Date\": \"\"}, \"Bibtex\": \"@Article{Xu2016MSRVTTAL,\\n author = {Jun Xu and Tao Mei and Ting Yao and Y. Rui},\\n booktitle = {Computer Vision and Pattern Recognition},\\n journal = {2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},\\n pages = {5288-5296},\\n title = {MSR-VTT: A Large Video Description Dataset for Bridging Video and Language},\\n year = {2016}\\n}\\n\", \"License Use (DataProvenance)\": \"unspecified\", \"License Attribution (DataProvenance)\": 0, \"License Share Alike (DataProvenance)\": 0, \"License Use (DataProvenance IgnoreOpenAI)\": \"unspecified\", \"License Attribution (DataProvenance IgnoreOpenAI)\": 0, \"License Share Alike (DataProvenance IgnoreOpenAI)\": 0, \"License Type\": \"Unspecified\"}, {\"Unique Dataset Identifier\": \"charades\", \"Collection\": \"charades\", \"Collection URL\": \"https://arxiv.org/abs/1604.01753\", \"Dataset Name\": \"Charades\", \"Paper Title\": \"Charades\", \"Paper URL\": \"https://arxiv.org/abs/1604.01753\", \"GitHub URL\": \"\", \"Hugging Face URL\": \"https://huggingface.co/datasets/HuggingFaceM4/charades\", \"Papers with Code URL\": \"https://paperswithcode.com/dataset/charades\", \"ArXiv URL\": \"https://arxiv.org/abs/1604.01753\", \"Semantic Scholar Corpus ID\": 18061547, \"Year Released\": \"2016\", \"Text Sources\": [\"human\"], \"Licenses\": [{\"License\": \"Custom\", \"License URL\": \"https://huggingface.co/datasets/HuggingFaceM4/charades#licensing-information\"}], \"Creators\": \"Academic\", \"Countries\": [\"United States of America\", \"France\"], \"License Verified By\": \"Vivek Sharma\", \"Video Hours\": 82.3, \"Taken Down\": \"False\", \"Video Sources\": [\"human\"], \"Task Categories\": [\"Video Captioning\"], \"Inferred Metadata\": {\"GitHub License\": \"\", \"GitHub Stars (June 2024)\": \"\", \"GitHub Topics\": \"\", \"Github Date\": \"\", \"HF Config\": \"default\", \"HF Config License\": \"\", \"HF Dataset\": \"HuggingFaceM4/charades\", \"HF Date\": \"2022-05-11\", \"HF Downloads (June 2024)\": 7, \"HF Likes (June 2024)\": 2, \"HF Yaml License\": \"Unspecified\", \"PwC Date\": \"2016-01-01\", \"PwC Description\": \"The Charades dataset is composed of 9,848 videos of daily indoors activities with an average length of 30 seconds, involving interactions with 46 objects classes in 15 types of indoor scenes and containing a vocabulary of 30 verbs leading to 157 action classes. Each video in this dataset is annotated by multiple free-text descriptions, action labels, action intervals and classes of interacting objects. 267 different users were presented with a sentence, which includes objects and actions from a fixed vocabulary, and they recorded a video acting out the sentence. In total, the dataset contains 66,500 temporal annotations for 157 action classes, 41,104 labels for 46 object classes, and 27,847 textual descriptions of the videos. In the standard split there are7,986 training video and 1,863 validation video.\", \"PwC License Name\": \"Non Commercial\", \"PwC License URL\": \"http://vuchallenge.org/license-charades.txt\", \"S2 Citation Count (June 2024)\": \"\", \"S2 Date\": \"\"}, \"Bibtex\": \"@Article{Sigurdsson2016HollywoodIH,\\n author = {Gunnar A. Sigurdsson and G\\u00fcl Varol and X. Wang and Ali Farhadi and I. Laptev and A. Gupta},\\n booktitle = {European Conference on Computer Vision},\\n pages = {510-526},\\n title = {Hollywood in Homes: Crowdsourcing Data Collection for Activity Understanding},\\n year = {2016}\\n}\\n\", \"License Use (DataProvenance)\": \"commercial\", \"License Attribution (DataProvenance)\": 1, \"License Share Alike (DataProvenance)\": 1, \"License Use (DataProvenance IgnoreOpenAI)\": \"commercial\", \"License Attribution (DataProvenance IgnoreOpenAI)\": 1, \"License Share Alike (DataProvenance IgnoreOpenAI)\": 1, \"License Type\": \"Commercial\"}, {\"Unique Dataset Identifier\": \"tgif\", \"Collection\": \"tgif\", \"Collection URL\": \"https://arxiv.org/abs/1604.02748\", \"Dataset Name\": \"TGIF\", \"Paper Title\": \"TGIF\", \"Paper URL\": \"https://arxiv.org/abs/1604.02748\", \"GitHub URL\": \"https://github.com/raingo/TGIF-Release\", \"Hugging Face URL\": \"https://huggingface.co/datasets/HuggingFaceM4/TGIF\", \"Papers with Code URL\": \"https://paperswithcode.com/dataset/tgif\", \"ArXiv URL\": \"https://arxiv.org/abs/1604.02748\", \"Semantic Scholar Corpus ID\": 6262415, \"Year Released\": \"2016\", \"Text Sources\": [\"tumblr\"], \"Licenses\": [{\"License\": \"Custom\", \"License URL\": \"https://github.com/raingo/TGIF-Release\"}], \"Creators\": \"Corporation\", \"Countries\": [\"United States of America\"], \"License Verified By\": \"Vivek Sharma\", \"Video Hours\": 86.1, \"Taken Down\": \"False\", \"Video Sources\": [\"tumblr\"], \"Task Categories\": [\"Video Captioning\"], \"Inferred Metadata\": {\"GitHub License\": \"BSD 3-Clause License\", \"GitHub Stars (June 2024)\": 111, \"GitHub Topics\": [], \"Github Date\": \"\", \"HF Config\": \"all\", \"HF Config License\": \"\", \"HF Dataset\": \"HuggingFaceM4/TGIF\", \"HF Date\": \"2022-05-17\", \"HF Downloads (June 2024)\": 9, \"HF Likes (June 2024)\": 11, \"HF Yaml License\": \"Unspecified\", \"PwC Date\": \"2016-04-10\", \"PwC Description\": \"The Tumblr GIF (TGIF) dataset contains 100K animated GIFs and 120K sentences describing visual content of the animated GIFs. The animated GIFs have been collected from Tumblr, from randomly selected posts published between May and June of 2015. The dataset provides the URLs of animated GIFs. The sentences are collected via crowdsourcing, with a carefully designed annotation interface that ensures high quality dataset. There is one sentence per animated GIF for the training and validation splits, and three sentences per GIF for the test split. The dataset can be used to evaluate animated GIF/video description techniques.\", \"PwC License Name\": \"Academic Research Purposes Only\", \"PwC License URL\": \"https://github.com/raingo/TGIF-Release#license\", \"S2 Citation Count (June 2024)\": \"\", \"S2 Date\": \"\"}, \"Bibtex\": \"@Article{Li2016TGIFAN,\\n author = {Yuncheng Li and Yale Song and Liangliang Cao and Joel R. Tetreault and Larry Goldberg and A. Jaimes and Jiebo Luo},\\n booktitle = {Computer Vision and Pattern Recognition},\\n journal = {2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},\\n pages = {4641-4650},\\n title = {TGIF: A New Dataset and Benchmark on Animated GIF Description},\\n year = {2016}\\n}\\n\", \"License Use (DataProvenance)\": \"commercial\", \"License Attribution (DataProvenance)\": 1, \"License Share Alike (DataProvenance)\": 0, \"License Use (DataProvenance IgnoreOpenAI)\": \"commercial\", \"License Attribution (DataProvenance IgnoreOpenAI)\": 1, \"License Share Alike (DataProvenance IgnoreOpenAI)\": 0, \"License Type\": \"Commercial\"}, {\"Unique Dataset Identifier\": \"tgif\", \"Collection\": \"tgif\", \"Collection URL\": \"https://arxiv.org/abs/1604.02748\", \"Dataset Name\": \"TGIF\", \"Paper Title\": \"TGIF\", \"Paper URL\": \"https://arxiv.org/abs/1604.02748\", \"GitHub URL\": \"https://github.com/raingo/TGIF-Release\", \"Hugging Face URL\": \"https://huggingface.co/datasets/HuggingFaceM4/TGIF\", \"Papers with Code URL\": \"https://paperswithcode.com/dataset/tgif\", \"ArXiv URL\": \"https://arxiv.org/abs/1604.02748\", \"Semantic Scholar Corpus ID\": 6262415, \"Year Released\": \"2016\", \"Text Sources\": [\"tumblr\"], \"Licenses\": [{\"License\": \"Custom\", \"License URL\": \"https://github.com/raingo/TGIF-Release\"}], \"Creators\": \"Corporation\", \"Countries\": [\"United States of America\"], \"License Verified By\": \"Vivek Sharma\", \"Video Hours\": 86.1, \"Taken Down\": \"False\", \"Video Sources\": [\"tumblr\"], \"Task Categories\": [\"Video Captioning\"], \"Inferred Metadata\": {\"GitHub License\": \"BSD 3-Clause License\", \"GitHub Stars (June 2024)\": 111, \"GitHub Topics\": [], \"Github Date\": \"\", \"HF Config\": \"all\", \"HF Config License\": \"\", \"HF Dataset\": \"HuggingFaceM4/TGIF\", \"HF Date\": \"2022-05-17\", \"HF Downloads (June 2024)\": 9, \"HF Likes (June 2024)\": 11, \"HF Yaml License\": \"Unspecified\", \"PwC Date\": \"2016-04-10\", \"PwC Description\": \"The Tumblr GIF (TGIF) dataset contains 100K animated GIFs and 120K sentences describing visual content of the animated GIFs. The animated GIFs have been collected from Tumblr, from randomly selected posts published between May and June of 2015. The dataset provides the URLs of animated GIFs. The sentences are collected via crowdsourcing, with a carefully designed annotation interface that ensures high quality dataset. There is one sentence per animated GIF for the training and validation splits, and three sentences per GIF for the test split. The dataset can be used to evaluate animated GIF/video description techniques.\", \"PwC License Name\": \"Academic Research Purposes Only\", \"PwC License URL\": \"https://github.com/raingo/TGIF-Release#license\", \"S2 Citation Count (June 2024)\": \"\", \"S2 Date\": \"\"}, \"Bibtex\": \"@Article{Li2016TGIFAN,\\n author = {Yuncheng Li and Yale Song and Liangliang Cao and Joel R. Tetreault and Larry Goldberg and A. Jaimes and Jiebo Luo},\\n booktitle = {Computer Vision and Pattern Recognition},\\n journal = {2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},\\n pages = {4641-4650},\\n title = {TGIF: A New Dataset and Benchmark on Animated GIF Description},\\n year = {2016}\\n}\\n\", \"License Use (DataProvenance)\": \"commercial\", \"License Attribution (DataProvenance)\": 1, \"License Share Alike (DataProvenance)\": 0, \"License Use (DataProvenance IgnoreOpenAI)\": \"commercial\", \"License Attribution (DataProvenance IgnoreOpenAI)\": 1, \"License Share Alike (DataProvenance IgnoreOpenAI)\": 0, \"License Type\": \"Commercial\"}, {\"Unique Dataset Identifier\": \"mars\", \"Collection\": \"mars\", \"Collection URL\": \"https://link.springer.com/content/pdf/10.1007/978-3-319-46466-4_52.pdf\", \"Dataset Name\": \"Mars\", \"Paper Title\": \"Mars\", \"Paper URL\": \"https://link.springer.com/content/pdf/10.1007/978-3-319-46466-4_52.pdf\", \"GitHub URL\": \"\", \"Hugging Face URL\": \"\", \"Papers with Code URL\": \"https://paperswithcode.com/dataset/mars\", \"ArXiv URL\": \"https://link.springer.com/content/pdf/10.1007/978-3-319-46466-4_52.pdf\", \"Semantic Scholar Corpus ID\": 2214158, \"Year Released\": \"2016\", \"Text Sources\": [\"human\"], \"Licenses\": [{\"License\": \"Unspecified\", \"License URL\": \"\"}], \"Creators\": \"Academic\", \"Countries\": [\"China\"], \"License Verified By\": \"Vivek Sharma\", \"Video Hours\": 0.24, \"Taken Down\": \"False\", \"Video Sources\": [\"human\"], \"Task Categories\": [\"Video Segmentation\", \"Video Classification\"], \"Inferred Metadata\": {\"GitHub License\": \"\", \"GitHub Stars (June 2024)\": \"\", \"GitHub Topics\": \"\", \"Github Date\": \"\", \"HF Config\": \"\", \"HF Config License\": \"\", \"HF Dataset\": \"\", \"HF Date\": \"\", \"HF Downloads (June 2024)\": \"\", \"HF Likes (June 2024)\": \"\", \"HF Yaml License\": \"\", \"PwC Date\": \"2016-01-01\", \"PwC Description\": \"MARS (Motion Analysis and Re-identification Set) is a large scale video based person reidentification dataset, an extension of the Market-1501 dataset. It has been collected from six near-synchronized cameras. It consists of 1,261 different pedestrians, who are captured by at least 2 cameras. The variations in poses, colors and illuminations of pedestrians, as well as the poor image quality, make it very difficult to yield high matching accuracy. Moreover, the dataset contains 3,248 distractors in order to make it more realistic. Deformable Part Model and GMMCP tracker were used to automatically generate the tracklets (mostly 25-50 frames long).\", \"PwC License Name\": \"Unspecified\", \"PwC License URL\": null, \"S2 Citation Count (June 2024)\": \"\", \"S2 Date\": \"\"}, \"Bibtex\": \"@Article{Zheng2016MARSAV,\\n author = {Liang Zheng and Zhi Bie and Yifan Sun and Jingdong Wang and Chi Su and Shengjin Wang and Q. Tian},\\n booktitle = {European Conference on Computer Vision},\\n pages = {868-884},\\n title = {MARS: A Video Benchmark for Large-Scale Person Re-Identification},\\n year = {2016}\\n}\\n\", \"License Use (DataProvenance)\": \"unspecified\", \"License Attribution (DataProvenance)\": 0, \"License Share Alike (DataProvenance)\": 0, \"License Use (DataProvenance IgnoreOpenAI)\": \"unspecified\", \"License Attribution (DataProvenance IgnoreOpenAI)\": 0, \"License Share Alike (DataProvenance IgnoreOpenAI)\": 0, \"License Type\": \"Unspecified\"}, {\"Unique Dataset Identifier\": \"tgif\", \"Collection\": \"tgif\", \"Collection URL\": \"https://arxiv.org/abs/1604.02748\", \"Dataset Name\": \"TGIF\", \"Paper Title\": \"TGIF\", \"Paper URL\": \"https://arxiv.org/abs/1604.02748\", \"GitHub URL\": \"https://github.com/raingo/TGIF-Release\", \"Hugging Face URL\": \"https://huggingface.co/datasets/HuggingFaceM4/TGIF\", \"Papers with Code URL\": \"https://paperswithcode.com/dataset/tgif\", \"ArXiv URL\": \"https://arxiv.org/abs/1604.02748\", \"Semantic Scholar Corpus ID\": 6262415, \"Year Released\": \"2016\", \"Text Sources\": [\"tumblr\"], \"Licenses\": [{\"License\": \"Custom\", \"License URL\": \"https://github.com/raingo/TGIF-Release\"}], \"Creators\": \"Academic\", \"Countries\": [\"United States of America\"], \"License Verified By\": \"Vivek Sharma\", \"Video Hours\": 86.1, \"Taken Down\": \"False\", \"Video Sources\": [\"tumblr\"], \"Task Categories\": [\"Video Captioning\"], \"Inferred Metadata\": {\"GitHub License\": \"BSD 3-Clause License\", \"GitHub Stars (June 2024)\": 111, \"GitHub Topics\": [], \"Github Date\": \"\", \"HF Config\": \"all\", \"HF Config License\": \"\", \"HF Dataset\": \"HuggingFaceM4/TGIF\", \"HF Date\": \"2022-05-17\", \"HF Downloads (June 2024)\": 9, \"HF Likes (June 2024)\": 11, \"HF Yaml License\": \"Unspecified\", \"PwC Date\": \"2016-04-10\", \"PwC Description\": \"The Tumblr GIF (TGIF) dataset contains 100K animated GIFs and 120K sentences describing visual content of the animated GIFs. The animated GIFs have been collected from Tumblr, from randomly selected posts published between May and June of 2015. The dataset provides the URLs of animated GIFs. The sentences are collected via crowdsourcing, with a carefully designed annotation interface that ensures high quality dataset. There is one sentence per animated GIF for the training and validation splits, and three sentences per GIF for the test split. The dataset can be used to evaluate animated GIF/video description techniques.\", \"PwC License Name\": \"Academic Research Purposes Only\", \"PwC License URL\": \"https://github.com/raingo/TGIF-Release#license\", \"S2 Citation Count (June 2024)\": \"\", \"S2 Date\": \"\"}, \"Bibtex\": \"@Article{Li2016TGIFAN,\\n author = {Yuncheng Li and Yale Song and Liangliang Cao and Joel R. Tetreault and Larry Goldberg and A. Jaimes and Jiebo Luo},\\n booktitle = {Computer Vision and Pattern Recognition},\\n journal = {2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},\\n pages = {4641-4650},\\n title = {TGIF: A New Dataset and Benchmark on Animated GIF Description},\\n year = {2016}\\n}\\n\", \"License Use (DataProvenance)\": \"commercial\", \"License Attribution (DataProvenance)\": 1, \"License Share Alike (DataProvenance)\": 0, \"License Use (DataProvenance IgnoreOpenAI)\": \"commercial\", \"License Attribution (DataProvenance IgnoreOpenAI)\": 1, \"License Share Alike (DataProvenance IgnoreOpenAI)\": 0, \"License Type\": \"Commercial\"}, {\"Unique Dataset Identifier\": \"narrated-instruction-vids\", \"Collection\": \"narrated-instruction-vids\", \"Collection URL\": \"https://arxiv.org/abs/1506.09215\", \"Dataset Name\": \"Narrated Instruction Videos\", \"Paper Title\": \"Narrated Instruction Videos\", \"Paper URL\": \"https://arxiv.org/abs/1506.09215\", \"GitHub URL\": \"https://github.com/jalayrac/instructionVideos\", \"Hugging Face URL\": \"\", \"Papers with Code URL\": \"https://paperswithcode.com/dataset/youtube-inria-instructional\", \"ArXiv URL\": \"https://arxiv.org/abs/1506.09215\", \"Semantic Scholar Corpus ID\": 2617244, \"Year Released\": \"2016\", \"Text Sources\": [\"youtube\"], \"Licenses\": [{\"License\": \"MIT License\", \"License URL\": \"https://github.com/jalayrac/instructionVideos?tab=readme-ov-file#license\"}], \"Creators\": \"Academic\", \"Countries\": [\"United States of America\", \"India\"], \"License Verified By\": \"Vivek Sharma\", \"Video Hours\": 7.0, \"Taken Down\": \"False\", \"Video Sources\": [\"youtube\"], \"Task Categories\": [\"Video Captioning\"], \"Inferred Metadata\": {\"GitHub License\": \"MIT License\", \"GitHub Stars (June 2024)\": 19, \"GitHub Topics\": [], \"Github Date\": \"\", \"HF Config\": \"\", \"HF Config License\": \"\", \"HF Dataset\": \"\", \"HF Date\": \"\", \"HF Downloads (June 2024)\": \"\", \"HF Likes (June 2024)\": \"\", \"HF Yaml License\": \"\", \"PwC Date\": \"\", \"PwC Description\": \"We address the problem of automatically learning the main steps to complete a certain task, such as changing a car tire, from a set of narrated instruction videos. The contributions of this paper are three-fold. First, we develop a new unsupervised learning approach that takes advantage of the complementary nature of the input video and the associated narration. The method solves two clustering problems, one in text and one in video, applied one after each other and linked by joint constraints to obtain a single coherent sequence of steps in both modalities. Second, we collect and annotate a new challenging dataset of real-world instruction videos from the Internet. The dataset contains about 800,000 frames for five different tasks (How to : change a car tire, perform CardioPulmonary resuscitation (CPR), jump cars, repot a plant and make coffee) that include complex interactions between people and objects, and are captured in a variety of indoor and outdoor settings. Third, we experimentally demonstrate that the proposed method can automatically discover, in an unsupervised manner , the main steps to achieve the task and locate the steps in the input videos.\\n\\nThis video presents our results of automatically discovering the scenario for the two following task : changing a tire and performing CardioPulmonary Resuscitation (CPR). At the bottom of the videos, there are three bars. The first one corresponds to our ground truth annotation. The second one corresponds to our time interval prediction in video. Finally the third one corresponds to the constraints that we obtain from the text domain. On the right, there is a list of label. They corresponds to the label recovered by our NLP method in an unsupervised manner.\", \"PwC License Name\": \"Unspecified\", \"PwC License URL\": null, \"S2 Citation Count (June 2024)\": \"\", \"S2 Date\": \"\"}, \"Bibtex\": \"@Article{Alayrac2015UnsupervisedLF,\\n author = {Jean-Baptiste Alayrac and Piotr Bojanowski and Nishant Agrawal and Josef Sivic and I. Laptev and Simon Lacoste-Julien},\\n booktitle = {Computer Vision and Pattern Recognition},\\n journal = {2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},\\n pages = {4575-4583},\\n title = {Unsupervised Learning from Narrated Instruction Videos},\\n year = {2015}\\n}\\n\", \"License Use (DataProvenance)\": \"commercial\", \"License Attribution (DataProvenance)\": 1, \"License Share Alike (DataProvenance)\": 0, \"License Use (DataProvenance IgnoreOpenAI)\": \"commercial\", \"License Attribution (DataProvenance IgnoreOpenAI)\": 1, \"License Share Alike (DataProvenance IgnoreOpenAI)\": 0, \"License Type\": \"Commercial\"}, {\"Unique Dataset Identifier\": \"narrated-instruction-vids\", \"Collection\": \"narrated-instruction-vids\", \"Collection URL\": \"https://arxiv.org/abs/1506.09215\", \"Dataset Name\": \"Narrated Instruction Videos\", \"Paper Title\": \"Narrated Instruction Videos\", \"Paper URL\": \"https://arxiv.org/abs/1506.09215\", \"GitHub URL\": \"https://github.com/jalayrac/instructionVideos\", \"Hugging Face URL\": \"\", \"Papers with Code URL\": \"https://paperswithcode.com/dataset/youtube-inria-instructional\", \"ArXiv URL\": \"https://arxiv.org/abs/1506.09215\", \"Semantic Scholar Corpus ID\": 2617244, \"Year Released\": \"2016\", \"Text Sources\": [\"youtube\"], \"Licenses\": [{\"License\": \"MIT License\", \"License URL\": \"https://github.com/jalayrac/instructionVideos?tab=readme-ov-file#license\"}], \"Creators\": \"Academic\", \"Countries\": [\"United States of America\", \"India\"], \"License Verified By\": \"Vivek Sharma\", \"Video Hours\": 7.0, \"Taken Down\": \"False\", \"Video Sources\": [\"youtube\"], \"Task Categories\": [\"Video Captioning\"], \"Inferred Metadata\": {\"GitHub License\": \"MIT License\", \"GitHub Stars (June 2024)\": 19, \"GitHub Topics\": [], \"Github Date\": \"\", \"HF Config\": \"\", \"HF Config License\": \"\", \"HF Dataset\": \"\", \"HF Date\": \"\", \"HF Downloads (June 2024)\": \"\", \"HF Likes (June 2024)\": \"\", \"HF Yaml License\": \"\", \"PwC Date\": \"\", \"PwC Description\": \"We address the problem of automatically learning the main steps to complete a certain task, such as changing a car tire, from a set of narrated instruction videos. The contributions of this paper are three-fold. First, we develop a new unsupervised learning approach that takes advantage of the complementary nature of the input video and the associated narration. The method solves two clustering problems, one in text and one in video, applied one after each other and linked by joint constraints to obtain a single coherent sequence of steps in both modalities. Second, we collect and annotate a new challenging dataset of real-world instruction videos from the Internet. The dataset contains about 800,000 frames for five different tasks (How to : change a car tire, perform CardioPulmonary resuscitation (CPR), jump cars, repot a plant and make coffee) that include complex interactions between people and objects, and are captured in a variety of indoor and outdoor settings. Third, we experimentally demonstrate that the proposed method can automatically discover, in an unsupervised manner , the main steps to achieve the task and locate the steps in the input videos.\\n\\nThis video presents our results of automatically discovering the scenario for the two following task : changing a tire and performing CardioPulmonary Resuscitation (CPR). At the bottom of the videos, there are three bars. The first one corresponds to our ground truth annotation. The second one corresponds to our time interval prediction in video. Finally the third one corresponds to the constraints that we obtain from the text domain. On the right, there is a list of label. They corresponds to the label recovered by our NLP method in an unsupervised manner.\", \"PwC License Name\": \"Unspecified\", \"PwC License URL\": null, \"S2 Citation Count (June 2024)\": \"\", \"S2 Date\": \"\"}, \"Bibtex\": \"@Article{Alayrac2015UnsupervisedLF,\\n author = {Jean-Baptiste Alayrac and Piotr Bojanowski and Nishant Agrawal and Josef Sivic and I. Laptev and Simon Lacoste-Julien},\\n booktitle = {Computer Vision and Pattern Recognition},\\n journal = {2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},\\n pages = {4575-4583},\\n title = {Unsupervised Learning from Narrated Instruction Videos},\\n year = {2015}\\n}\\n\", \"License Use (DataProvenance)\": \"commercial\", \"License Attribution (DataProvenance)\": 1, \"License Share Alike (DataProvenance)\": 0, \"License Use (DataProvenance IgnoreOpenAI)\": \"commercial\", \"License Attribution (DataProvenance IgnoreOpenAI)\": 1, \"License Share Alike (DataProvenance IgnoreOpenAI)\": 0, \"License Type\": \"Commercial\"}, {\"Unique Dataset Identifier\": \"narrated-instruction-vids\", \"Collection\": \"narrated-instruction-vids\", \"Collection URL\": \"https://arxiv.org/abs/1506.09215\", \"Dataset Name\": \"Narrated Instruction Videos\", \"Paper Title\": \"Narrated Instruction Videos\", \"Paper URL\": \"https://arxiv.org/abs/1506.09215\", \"GitHub URL\": \"https://github.com/jalayrac/instructionVideos\", \"Hugging Face URL\": \"\", \"Papers with Code URL\": \"https://paperswithcode.com/dataset/youtube-inria-instructional\", \"ArXiv URL\": \"https://arxiv.org/abs/1506.09215\", \"Semantic Scholar Corpus ID\": 2617244, \"Year Released\": \"2016\", \"Text Sources\": [\"youtube\"], \"Licenses\": [{\"License\": \"MIT License\", \"License URL\": \"https://github.com/jalayrac/instructionVideos?tab=readme-ov-file#license\"}], \"Creators\": \"Academic\", \"Countries\": [\"United States of America\", \"India\"], \"License Verified By\": \"Vivek Sharma\", \"Video Hours\": 7.0, \"Taken Down\": \"False\", \"Video Sources\": [\"youtube\"], \"Task Categories\": [\"Video Captioning\"], \"Inferred Metadata\": {\"GitHub License\": \"MIT License\", \"GitHub Stars (June 2024)\": 19, \"GitHub Topics\": [], \"Github Date\": \"\", \"HF Config\": \"\", \"HF Config License\": \"\", \"HF Dataset\": \"\", \"HF Date\": \"\", \"HF Downloads (June 2024)\": \"\", \"HF Likes (June 2024)\": \"\", \"HF Yaml License\": \"\", \"PwC Date\": \"\", \"PwC Description\": \"We address the problem of automatically learning the main steps to complete a certain task, such as changing a car tire, from a set of narrated instruction videos. The contributions of this paper are three-fold. First, we develop a new unsupervised learning approach that takes advantage of the complementary nature of the input video and the associated narration. The method solves two clustering problems, one in text and one in video, applied one after each other and linked by joint constraints to obtain a single coherent sequence of steps in both modalities. Second, we collect and annotate a new challenging dataset of real-world instruction videos from the Internet. The dataset contains about 800,000 frames for five different tasks (How to : change a car tire, perform CardioPulmonary resuscitation (CPR), jump cars, repot a plant and make coffee) that include complex interactions between people and objects, and are captured in a variety of indoor and outdoor settings. Third, we experimentally demonstrate that the proposed method can automatically discover, in an unsupervised manner , the main steps to achieve the task and locate the steps in the input videos.\\n\\nThis video presents our results of automatically discovering the scenario for the two following task : changing a tire and performing CardioPulmonary Resuscitation (CPR). At the bottom of the videos, there are three bars. The first one corresponds to our ground truth annotation. The second one corresponds to our time interval prediction in video. Finally the third one corresponds to the constraints that we obtain from the text domain. On the right, there is a list of label. They corresponds to the label recovered by our NLP method in an unsupervised manner.\", \"PwC License Name\": \"Unspecified\", \"PwC License URL\": null, \"S2 Citation Count (June 2024)\": \"\", \"S2 Date\": \"\"}, \"Bibtex\": \"@Article{Alayrac2015UnsupervisedLF,\\n author = {Jean-Baptiste Alayrac and Piotr Bojanowski and Nishant Agrawal and Josef Sivic and I. Laptev and Simon Lacoste-Julien},\\n booktitle = {Computer Vision and Pattern Recognition},\\n journal = {2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},\\n pages = {4575-4583},\\n title = {Unsupervised Learning from Narrated Instruction Videos},\\n year = {2015}\\n}\\n\", \"License Use (DataProvenance)\": \"commercial\", \"License Attribution (DataProvenance)\": 1, \"License Share Alike (DataProvenance)\": 0, \"License Use (DataProvenance IgnoreOpenAI)\": \"commercial\", \"License Attribution (DataProvenance IgnoreOpenAI)\": 1, \"License Share Alike (DataProvenance IgnoreOpenAI)\": 0, \"License Type\": \"Commercial\"}, {\"Unique Dataset Identifier\": \"mars\", \"Collection\": \"mars\", \"Collection URL\": \"https://link.springer.com/content/pdf/10.1007/978-3-319-46466-4_52.pdf\", \"Dataset Name\": \"Mars\", \"Paper Title\": \"Mars\", \"Paper URL\": \"https://link.springer.com/content/pdf/10.1007/978-3-319-46466-4_52.pdf\", \"GitHub URL\": \"\", \"Hugging Face URL\": \"\", \"Papers with Code URL\": \"https://paperswithcode.com/dataset/mars\", \"ArXiv URL\": \"https://link.springer.com/content/pdf/10.1007/978-3-319-46466-4_52.pdf\", \"Semantic Scholar Corpus ID\": 2214158, \"Year Released\": \"2016\", \"Text Sources\": [\"human\"], \"Licenses\": [{\"License\": \"Unspecified\", \"License URL\": \"\"}], \"Creators\": \"Industry Lab\", \"Countries\": [\"China\"], \"License Verified By\": \"Vivek Sharma\", \"Video Hours\": 0.24, \"Taken Down\": \"False\", \"Video Sources\": [\"human\"], \"Task Categories\": [\"Video Segmentation\", \"Video Classification\"], \"Inferred Metadata\": {\"GitHub License\": \"\", \"GitHub Stars (June 2024)\": \"\", \"GitHub Topics\": \"\", \"Github Date\": \"\", \"HF Config\": \"\", \"HF Config License\": \"\", \"HF Dataset\": \"\", \"HF Date\": \"\", \"HF Downloads (June 2024)\": \"\", \"HF Likes (June 2024)\": \"\", \"HF Yaml License\": \"\", \"PwC Date\": \"2016-01-01\", \"PwC Description\": \"MARS (Motion Analysis and Re-identification Set) is a large scale video based person reidentification dataset, an extension of the Market-1501 dataset. It has been collected from six near-synchronized cameras. It consists of 1,261 different pedestrians, who are captured by at least 2 cameras. The variations in poses, colors and illuminations of pedestrians, as well as the poor image quality, make it very difficult to yield high matching accuracy. Moreover, the dataset contains 3,248 distractors in order to make it more realistic. Deformable Part Model and GMMCP tracker were used to automatically generate the tracklets (mostly 25-50 frames long).\", \"PwC License Name\": \"Unspecified\", \"PwC License URL\": null, \"S2 Citation Count (June 2024)\": \"\", \"S2 Date\": \"\"}, \"Bibtex\": \"@Article{Zheng2016MARSAV,\\n author = {Liang Zheng and Zhi Bie and Yifan Sun and Jingdong Wang and Chi Su and Shengjin Wang and Q. Tian},\\n booktitle = {European Conference on Computer Vision},\\n pages = {868-884},\\n title = {MARS: A Video Benchmark for Large-Scale Person Re-Identification},\\n year = {2016}\\n}\\n\", \"License Use (DataProvenance)\": \"unspecified\", \"License Attribution (DataProvenance)\": 0, \"License Share Alike (DataProvenance)\": 0, \"License Use (DataProvenance IgnoreOpenAI)\": \"unspecified\", \"License Attribution (DataProvenance IgnoreOpenAI)\": 0, \"License Share Alike (DataProvenance IgnoreOpenAI)\": 0, \"License Type\": \"Unspecified\"}, {\"Unique Dataset Identifier\": \"youtube-8m\", \"Collection\": \"youtube-8m\", \"Collection URL\": \"https://arxiv.org/abs/1609.08675\", \"Dataset Name\": \"Youtube-8M: A Large-Scale Video Classification Benchmark\", \"Paper Title\": \"Youtube-8M: A Large-Scale Video Classification Benchmark\", \"Paper URL\": \"https://arxiv.org/abs/1609.08675\", \"GitHub URL\": \"\", \"Hugging Face URL\": \"\", \"Papers with Code URL\": \"https://paperswithcode.com/dataset/youtube-8m\", \"ArXiv URL\": \"https://arxiv.org/abs/1609.08675\", \"Semantic Scholar Corpus ID\": 11241677, \"Year Released\": \"2016\", \"Text Sources\": [\"youtube\"], \"Licenses\": [{\"License\": \"Unspecified\", \"License URL\": \"\"}], \"Creators\": \"Industry Lab\", \"Countries\": [\"United States of America\"], \"License Verified By\": \"Vivek Sharma\", \"Video Hours\": 350000.0, \"Taken Down\": \"False\", \"Video Sources\": [\"youtube\"], \"Task Categories\": [\"Video Classification\"], \"Inferred Metadata\": {\"GitHub License\": \"\", \"GitHub Stars (June 2024)\": \"\", \"GitHub Topics\": \"\", \"Github Date\": \"\", \"HF Config\": \"\", \"HF Config License\": \"\", \"HF Dataset\": \"\", \"HF Date\": \"\", \"HF Downloads (June 2024)\": \"\", \"HF Likes (June 2024)\": \"\", \"HF Yaml License\": \"\", \"PwC Date\": \"2016-01-01\", \"PwC Description\": \"The YouTube-8M dataset is a large scale video dataset, which includes more than 7 million videos with 4716 classes labeled by the annotation system. The dataset consists of three parts: training set, validate set, and test set. In the training set, each class contains at least 100 training videos. Features of these videos are extracted by the state-of-the-art popular pre-trained models and released for public use. Each video contains audio and visual modality. Based on the visual information, videos are divided into 24 topics, such as sports, game, arts & entertainment, etc\", \"PwC License Name\": \"CC BY 4.0\", \"PwC License URL\": \"https://creativecommons.org/licenses/by/4.0/\", \"S2 Citation Count (June 2024)\": \"\", \"S2 Date\": \"\"}, \"Bibtex\": \"@Article{Abu-El-Haija2016YouTube8MAL,\\n author = {Sami Abu-El-Haija and Nisarg Kothari and Joonseok Lee and A. Natsev and G. Toderici and Balakrishnan Varadarajan and Sudheendra Vijayanarasimhan},\\n booktitle = {arXiv.org},\\n journal = {ArXiv},\\n title = {YouTube-8M: A Large-Scale Video Classification Benchmark},\\n volume = {abs/1609.08675},\\n year = {2016}\\n}\\n\", \"License Use (DataProvenance)\": \"unspecified\", \"License Attribution (DataProvenance)\": 0, \"License Share Alike (DataProvenance)\": 0, \"License Use (DataProvenance IgnoreOpenAI)\": \"unspecified\", \"License Attribution (DataProvenance IgnoreOpenAI)\": 0, \"License Share Alike (DataProvenance IgnoreOpenAI)\": 0, \"License Type\": \"Unspecified\"}, {\"Unique Dataset Identifier\": \"ntu-rgbd\", \"Collection\": \"ntu-rgbd\", \"Collection URL\": \"https://arxiv.org/pdf/1604.02808.pdf\", \"Dataset Name\": \"NTU RGB+D: A Large Scale Dataset for 3D Human Activity Analysis (CVPR 2016, TPAMI 2019)\", \"Paper Title\": \"NTU RGB+D: A Large Scale Dataset for 3D Human Activity Analysis (CVPR 2016, TPAMI 2019)\", \"Paper URL\": \"https://arxiv.org/pdf/1604.02808.pdf\", \"GitHub URL\": \"https://github.com/shahroudy/NTURGB-D\", \"Hugging Face URL\": \"\", \"Papers with Code URL\": \"https://paperswithcode.com/paper/ntu-rgbd-a-large-scale-dataset-for-3d-human\", \"ArXiv URL\": \"https://arxiv.org/pdf/1604.02808.pdf\", \"Semantic Scholar Corpus ID\": 15928602, \"Year Released\": \"2016\", \"Text Sources\": [\"crowdsourced\"], \"Licenses\": [{\"License\": \"Custom\", \"License URL\": \"https://rose1.ntu.edu.sg/dataset/actionRecognition/\"}], \"Creators\": \"Academic\", \"Countries\": [\"Singapore\"], \"License Verified By\": \"Vivek Sharma\", \"Video Hours\": 74.1, \"Taken Down\": \"False\", \"Video Sources\": [\"crowdsourced\"], \"Task Categories\": [\"Video Classification\"], \"Inferred Metadata\": {\"GitHub License\": \"\", \"GitHub Stars (June 2024)\": 715, \"GitHub Topics\": [], \"Github Date\": \"\", \"HF Config\": \"\", \"HF Config License\": \"\", \"HF Dataset\": \"\", \"HF Date\": \"\", \"HF Downloads (June 2024)\": \"\", \"HF Likes (June 2024)\": \"\", \"HF Yaml License\": \"\", \"PwC Date\": \"\", \"PwC Description\": \"#4 best model for Skeleton Based Action Recognition on Varying-view RGB-D Action-Skeleton (Accuracy (CS) metric)\", \"PwC License Name\": \"\", \"PwC License URL\": \"\", \"S2 Citation Count (June 2024)\": \"\", \"S2 Date\": \"\"}, \"Bibtex\": \"@Article{Shahroudy2016NTURA,\\n author = {Amir Shahroudy and Jun Liu and T. Ng and G. Wang},\\n booktitle = {Computer Vision and Pattern Recognition},\\n journal = {2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},\\n pages = {1010-1019},\\n title = {NTU RGB+D: A Large Scale Dataset for 3D Human Activity Analysis},\\n year = {2016}\\n}\\n\", \"License Use (DataProvenance)\": \"academic-only\", \"License Attribution (DataProvenance)\": 1, \"License Share Alike (DataProvenance)\": 0, \"License Use (DataProvenance IgnoreOpenAI)\": \"academic-only\", \"License Attribution (DataProvenance IgnoreOpenAI)\": 1, \"License Share Alike (DataProvenance IgnoreOpenAI)\": 0, \"License Type\": \"Non-Commercial/Academic\"}, {\"Unique Dataset Identifier\": \"mpii-cooking2\", \"Collection\": \"mpii-cooking2\", \"Collection URL\": \"https://arxiv.org/pdf/1502.06648.pdf\", \"Dataset Name\": \"MPII Cooking 2\", \"Paper Title\": \"MPII Cooking 2\", \"Paper URL\": \"https://arxiv.org/pdf/1502.06648.pdf\", \"GitHub URL\": \"\", \"Hugging Face URL\": \"\", \"Papers with Code URL\": \"https://paperswithcode.com/dataset/mpii-cooking-2-dataset\", \"ArXiv URL\": \"https://arxiv.org/pdf/1502.06648.pdf\", \"Semantic Scholar Corpus ID\": 14036544, \"Year Released\": \"2016\", \"Text Sources\": [\"human\"], \"Licenses\": [{\"License\": \"Custom\", \"License URL\": \"https://www.mpi-inf.mpg.de/departments/computer-vision-and-machine-learning/research/human-activity-recognition/mpii-cooking-2-dataset/\"}], \"Creators\": \"Academic\", \"Countries\": [\"Germany\"], \"License Verified By\": \"Vivek Sharma\", \"Video Hours\": 27.0, \"Taken Down\": \"False\", \"Video Sources\": [\"human\"], \"Task Categories\": [\"Temporal Action Detection\"], \"Inferred Metadata\": {\"GitHub License\": \"\", \"GitHub Stars (June 2024)\": \"\", \"GitHub Topics\": \"\", \"Github Date\": \"\", \"HF Config\": \"\", \"HF Config License\": \"\", \"HF Dataset\": \"\", \"HF Date\": \"\", \"HF Downloads (June 2024)\": \"\", \"HF Likes (June 2024)\": \"\", \"HF Yaml License\": \"\", \"PwC Date\": \"\", \"PwC Description\": \"A dataset which provides detailed annotations for activity recognition.\", \"PwC License Name\": \"Unspecified\", \"PwC License URL\": null, \"S2 Citation Count (June 2024)\": \"\", \"S2 Date\": \"\"}, \"Bibtex\": \"@Article{Rohrbach2015RecognizingFA,\\n author = {Marcus Rohrbach and Anna Rohrbach and Michaela Regneri and S. Amin and Mykhaylo Andriluka and Manfred Pinkal and B. Schiele},\\n booktitle = {International Journal of Computer Vision},\\n journal = {International Journal of Computer Vision},\\n pages = {346 - 373},\\n title = {Recognizing Fine-Grained and Composite Activities Using Hand-Centric Features and Script Data},\\n volume = {119},\\n year = {2015}\\n}\\n\", \"License Use (DataProvenance)\": \"academic-only\", \"License Attribution (DataProvenance)\": 1, \"License Share Alike (DataProvenance)\": 0, \"License Use (DataProvenance IgnoreOpenAI)\": \"academic-only\", \"License Attribution (DataProvenance IgnoreOpenAI)\": 1, \"License Share Alike (DataProvenance IgnoreOpenAI)\": 0, \"License Type\": \"Non-Commercial/Academic\"}, {\"Unique Dataset Identifier\": \"mpii-cooking2\", \"Collection\": \"mpii-cooking2\", \"Collection URL\": \"https://arxiv.org/pdf/1502.06648.pdf\", \"Dataset Name\": \"MPII Cooking 2\", \"Paper Title\": \"MPII Cooking 2\", \"Paper URL\": \"https://arxiv.org/pdf/1502.06648.pdf\", \"GitHub URL\": \"\", \"Hugging Face URL\": \"\", \"Papers with Code URL\": \"https://paperswithcode.com/dataset/mpii-cooking-2-dataset\", \"ArXiv URL\": \"https://arxiv.org/pdf/1502.06648.pdf\", \"Semantic Scholar Corpus ID\": 14036544, \"Year Released\": \"2016\", \"Text Sources\": [\"human\"], \"Licenses\": [{\"License\": \"Custom\", \"License URL\": \"https://www.mpi-inf.mpg.de/departments/computer-vision-and-machine-learning/research/human-activity-recognition/mpii-cooking-2-dataset/\"}], \"Creators\": \"Academic\", \"Countries\": [\"Germany\"], \"License Verified By\": \"Vivek Sharma\", \"Video Hours\": 27.0, \"Taken Down\": \"False\", \"Video Sources\": [\"human\"], \"Task Categories\": [\"Temporal Action Detection\"], \"Inferred Metadata\": {\"GitHub License\": \"\", \"GitHub Stars (June 2024)\": \"\", \"GitHub Topics\": \"\", \"Github Date\": \"\", \"HF Config\": \"\", \"HF Config License\": \"\", \"HF Dataset\": \"\", \"HF Date\": \"\", \"HF Downloads (June 2024)\": \"\", \"HF Likes (June 2024)\": \"\", \"HF Yaml License\": \"\", \"PwC Date\": \"\", \"PwC Description\": \"A dataset which provides detailed annotations for activity recognition.\", \"PwC License Name\": \"Unspecified\", \"PwC License URL\": null, \"S2 Citation Count (June 2024)\": \"\", \"S2 Date\": \"\"}, \"Bibtex\": \"@Article{Rohrbach2015RecognizingFA,\\n author = {Marcus Rohrbach and Anna Rohrbach and Michaela Regneri and S. Amin and Mykhaylo Andriluka and Manfred Pinkal and B. Schiele},\\n booktitle = {International Journal of Computer Vision},\\n journal = {International Journal of Computer Vision},\\n pages = {346 - 373},\\n title = {Recognizing Fine-Grained and Composite Activities Using Hand-Centric Features and Script Data},\\n volume = {119},\\n year = {2015}\\n}\\n\", \"License Use (DataProvenance)\": \"academic-only\", \"License Attribution (DataProvenance)\": 1, \"License Share Alike (DataProvenance)\": 0, \"License Use (DataProvenance IgnoreOpenAI)\": \"academic-only\", \"License Attribution (DataProvenance IgnoreOpenAI)\": 1, \"License Share Alike (DataProvenance IgnoreOpenAI)\": 0, \"License Type\": \"Non-Commercial/Academic\"}, {\"Unique Dataset Identifier\": \"qfvs\", \"Collection\": \"qfvs\", \"Collection URL\": \"https://arxiv.org/abs/1707.04960\", \"Dataset Name\": \"QFVS: Query-Focused Video Summarization: Dataset, Evaluation, and A Memory Network Based Approach (CVPR 2017)\", \"Paper Title\": \"QFVS: Query-Focused Video Summarization: Dataset, Evaluation, and A Memory Network Based Approach (CVPR 2017)\", \"Paper URL\": \"https://arxiv.org/abs/1707.04960\", \"GitHub URL\": \"\", \"Hugging Face URL\": \"\", \"Papers with Code URL\": \"https://paperswithcode.com/dataset/query-focused-video-summarization-dataset\", \"ArXiv URL\": \"https://arxiv.org/abs/1707.04960\", \"Semantic Scholar Corpus ID\": 2774608, \"Year Released\": \"2017\", \"Text Sources\": [\"crowdsourced\"], \"Licenses\": [{\"License\": \"Unspecified\", \"License URL\": \"\"}], \"Creators\": \"Academic\", \"Countries\": [\"United States of America\"], \"License Verified By\": \"Vivek Sharma\", \"Video Hours\": 20.0, \"Taken Down\": \"False\", \"Video Sources\": [\"crowdsourced\"], \"Task Categories\": [\"Video Summarization\"], \"Inferred Metadata\": {\"GitHub License\": \"\", \"GitHub Stars (June 2024)\": \"\", \"GitHub Topics\": \"\", \"Github Date\": \"\", \"HF Config\": \"\", \"HF Config License\": \"\", \"HF Dataset\": \"\", \"HF Date\": \"\", \"HF Downloads (June 2024)\": \"\", \"HF Likes (June 2024)\": \"\", \"HF Yaml License\": \"\", \"PwC Date\": \"\", \"PwC Description\": \"Collects dense per-video-shot concept annotations.\", \"PwC License Name\": \"Unspecified\", \"PwC License URL\": null, \"S2 Citation Count (June 2024)\": \"\", \"S2 Date\": \"\"}, \"Bibtex\": \"@Article{Sharghi2017QueryFocusedVS,\\n author = {Aidean Sharghi and Jacob S. Laurel and Boqing Gong},\\n booktitle = {Computer Vision and Pattern Recognition},\\n journal = {2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},\\n pages = {2127-2136},\\n title = {Query-Focused Video Summarization: Dataset, Evaluation, and a Memory Network Based Approach},\\n year = {2017}\\n}\\n\", \"License Use (DataProvenance)\": \"unspecified\", \"License Attribution (DataProvenance)\": 0, \"License Share Alike (DataProvenance)\": 0, \"License Use (DataProvenance IgnoreOpenAI)\": \"unspecified\", \"License Attribution (DataProvenance IgnoreOpenAI)\": 0, \"License Share Alike (DataProvenance IgnoreOpenAI)\": 0, \"License Type\": \"Unspecified\"}, {\"Unique Dataset Identifier\": \"youcook-2\", \"Collection\": \"youcook-2\", \"Collection URL\": \"https://arxiv.org/abs/1703.09788\", \"Dataset Name\": \"YouCook2\", \"Paper Title\": \"YouCook2\", \"Paper URL\": \"https://arxiv.org/abs/1703.09788\", \"GitHub URL\": \"\", \"Hugging Face URL\": \"\", \"Papers with Code URL\": \"https://paperswithcode.com/dataset/youcook2\", \"ArXiv URL\": \"https://arxiv.org/abs/1703.09788\", \"Semantic Scholar Corpus ID\": 19713015, \"Year Released\": \"2017\", \"Text Sources\": [\"youtube\"], \"Licenses\": [{\"License\": \"MIT License\", \"License URL\": \"http://youcook2.eecs.umich.edu/static/YouCookII/LICENSE_YOUCOOK2.txt\"}], \"Creators\": \"Academic\", \"Countries\": [\"United States of America\"], \"License Verified By\": \"Vivek Sharma\", \"Video Hours\": 175.6, \"Taken Down\": \"False\", \"Video Sources\": [\"youtube\"], \"Task Categories\": [\"Video Captioning\"], \"Inferred Metadata\": {\"GitHub License\": \"\", \"GitHub Stars (June 2024)\": \"\", \"GitHub Topics\": \"\", \"Github Date\": \"\", \"HF Config\": \"\", \"HF Config License\": \"\", \"HF Dataset\": \"\", \"HF Date\": \"\", \"HF Downloads (June 2024)\": \"\", \"HF Likes (June 2024)\": \"\", \"HF Yaml License\": \"\", \"PwC Date\": \"2018-01-01\", \"PwC Description\": \"YouCook2 is the largest task-oriented, instructional video dataset in the vision community. It contains 2000 long untrimmed videos from 89 cooking recipes; on average, each distinct recipe has 22 videos. The procedure steps for each video are annotated with temporal boundaries and described by imperative English sentences (see the example below). The videos were downloaded from YouTube and are all in the third-person viewpoint. All the videos are unconstrained and can be performed by individual persons at their houses with unfixed cameras. YouCook2 contains rich recipe types and various cooking styles from all over the world.\", \"PwC License Name\": \"Various\", \"PwC License URL\": \"http://youcook2.eecs.umich.edu/download\", \"S2 Citation Count (June 2024)\": \"\", \"S2 Date\": \"\"}, \"Bibtex\": \"@Article{Zhou2017TowardsAL,\\n author = {Luowei Zhou and Chenliang Xu and Jason J. Corso},\\n booktitle = {AAAI Conference on Artificial Intelligence},\\n pages = {7590-7598},\\n title = {Towards Automatic Learning of Procedures From Web Instructional Videos},\\n year = {2017}\\n}\\n\", \"License Use (DataProvenance)\": \"commercial\", \"License Attribution (DataProvenance)\": 1, \"License Share Alike (DataProvenance)\": 0, \"License Use (DataProvenance IgnoreOpenAI)\": \"commercial\", \"License Attribution (DataProvenance IgnoreOpenAI)\": 1, \"License Share Alike (DataProvenance IgnoreOpenAI)\": 0, \"License Type\": \"Commercial\"}, {\"Unique Dataset Identifier\": \"qfvs\", \"Collection\": \"qfvs\", \"Collection URL\": \"https://arxiv.org/abs/1707.04960\", \"Dataset Name\": \"QFVS: Query-Focused Video Summarization: Dataset, Evaluation, and A Memory Network Based Approach (CVPR 2017)\", \"Paper Title\": \"QFVS: Query-Focused Video Summarization: Dataset, Evaluation, and A Memory Network Based Approach (CVPR 2017)\", \"Paper URL\": \"https://arxiv.org/abs/1707.04960\", \"GitHub URL\": \"\", \"Hugging Face URL\": \"\", \"Papers with Code URL\": \"https://paperswithcode.com/dataset/query-focused-video-summarization-dataset\", \"ArXiv URL\": \"https://arxiv.org/abs/1707.04960\", \"Semantic Scholar Corpus ID\": 2774608, \"Year Released\": \"2017\", \"Text Sources\": [\"crowdsourced\"], \"Licenses\": [{\"License\": \"Unspecified\", \"License URL\": \"\"}], \"Creators\": \"Academic\", \"Countries\": [\"United States of America\"], \"License Verified By\": \"Vivek Sharma\", \"Video Hours\": 20.0, \"Taken Down\": \"False\", \"Video Sources\": [\"crowdsourced\"], \"Task Categories\": [\"Video Summarization\"], \"Inferred Metadata\": {\"GitHub License\": \"\", \"GitHub Stars (June 2024)\": \"\", \"GitHub Topics\": \"\", \"Github Date\": \"\", \"HF Config\": \"\", \"HF Config License\": \"\", \"HF Dataset\": \"\", \"HF Date\": \"\", \"HF Downloads (June 2024)\": \"\", \"HF Likes (June 2024)\": \"\", \"HF Yaml License\": \"\", \"PwC Date\": \"\", \"PwC Description\": \"Collects dense per-video-shot concept annotations.\", \"PwC License Name\": \"Unspecified\", \"PwC License URL\": null, \"S2 Citation Count (June 2024)\": \"\", \"S2 Date\": \"\"}, \"Bibtex\": \"@Article{Sharghi2017QueryFocusedVS,\\n author = {Aidean Sharghi and Jacob S. Laurel and Boqing Gong},\\n booktitle = {Computer Vision and Pattern Recognition},\\n journal = {2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},\\n pages = {2127-2136},\\n title = {Query-Focused Video Summarization: Dataset, Evaluation, and a Memory Network Based Approach},\\n year = {2017}\\n}\\n\", \"License Use (DataProvenance)\": \"unspecified\", \"License Attribution (DataProvenance)\": 0, \"License Share Alike (DataProvenance)\": 0, \"License Use (DataProvenance IgnoreOpenAI)\": \"unspecified\", \"License Attribution (DataProvenance IgnoreOpenAI)\": 0, \"License Share Alike (DataProvenance IgnoreOpenAI)\": 0, \"License Type\": \"Unspecified\"}, {\"Unique Dataset Identifier\": \"kinetics-400\", \"Collection\": \"kinetics-400\", \"Collection URL\": \"https://arxiv.org/abs/1705.06950\", \"Dataset Name\": \"Kinetics 400\", \"Paper Title\": \"Kinetics 400\", \"Paper URL\": \"https://arxiv.org/abs/1705.06950\", \"GitHub URL\": \"https://github.com/cvdfoundation/kinetics-dataset\", \"Hugging Face URL\": \"\", \"Papers with Code URL\": \"https://paperswithcode.com/dataset/kinetics\", \"ArXiv URL\": \"https://arxiv.org/abs/1705.06950\", \"Semantic Scholar Corpus ID\": 27300853, \"Year Released\": \"2017\", \"Text Sources\": [\"human\"], \"Licenses\": [{\"License\": \"Unspecified\", \"License URL\": \"\"}], \"Creators\": \"Industry Lab\", \"Countries\": [\"United Kingdom\"], \"License Verified By\": \"Vivek Sharma\", \"Video Hours\": 850.68, \"Taken Down\": \"False\", \"Video Sources\": [\"human\"], \"Task Categories\": [\"Video Classification\"], \"Inferred Metadata\": {\"GitHub License\": \"\", \"GitHub Stars (June 2024)\": 704, \"GitHub Topics\": [], \"Github Date\": \"\", \"HF Config\": \"\", \"HF Config License\": \"\", \"HF Dataset\": \"\", \"HF Date\": \"\", \"HF Downloads (June 2024)\": \"\", \"HF Likes (June 2024)\": \"\", \"HF Yaml License\": \"\", \"PwC Date\": \"2017-05-19\", \"PwC Description\": \"The Kinetics dataset is a large-scale, high-quality dataset for human action recognition in videos. The dataset consists of around 500,000 video clips covering 600 human action classes with at least 600 video clips for each action class. Each video clip lasts around 10 seconds and is labeled with a single action class. The videos are collected from YouTube.\", \"PwC License Name\": \"CC BY 4.0\", \"PwC License URL\": \"https://creativecommons.org/licenses/by/4.0/\", \"S2 Citation Count (June 2024)\": \"\", \"S2 Date\": \"\"}, \"Bibtex\": \"@Article{Kay2017TheKH,\\n author = {W. Kay and Jo\\u00e3o Carreira and K. Simonyan and Brian Zhang and Chloe Hillier and Sudheendra Vijayanarasimhan and Fabio Viola and Tim Green and T. Back and A. Natsev and Mustafa Suleyman and Andrew Zisserman},\\n booktitle = {arXiv.org},\\n journal = {ArXiv},\\n title = {The Kinetics Human Action Video Dataset},\\n volume = {abs/1705.06950},\\n year = {2017}\\n}\\n\", \"License Use (DataProvenance)\": \"unspecified\", \"License Attribution (DataProvenance)\": 0, \"License Share Alike (DataProvenance)\": 0, \"License Use (DataProvenance IgnoreOpenAI)\": \"unspecified\", \"License Attribution (DataProvenance IgnoreOpenAI)\": 0, \"License Share Alike (DataProvenance IgnoreOpenAI)\": 0, \"License Type\": \"Unspecified\"}, {\"Unique Dataset Identifier\": \"mpii-md\", \"Collection\": \"mpii-md\", \"Collection URL\": \"https://arxiv.org/pdf/1501.02530.pdf\", \"Dataset Name\": \"MPII-MD: A Dataset for Movie Description\", \"Paper Title\": \"MPII-MD: A Dataset for Movie Description\", \"Paper URL\": \"https://arxiv.org/pdf/1501.02530.pdf\", \"GitHub URL\": \"\", \"Hugging Face URL\": \"\", \"Papers with Code URL\": \"\", \"ArXiv URL\": \"https://arxiv.org/pdf/1501.02530.pdf\", \"Semantic Scholar Corpus ID\": 15184723, \"Year Released\": \"2017\", \"Text Sources\": [\"movies\"], \"Licenses\": [{\"License\": \"Unspecified\", \"License URL\": \"\"}], \"Creators\": \"Academic\", \"Countries\": [\"Germany\"], \"License Verified By\": \"Vivek Sharma\", \"Video Hours\": 56.5, \"Taken Down\": \"False\", \"Video Sources\": [\"movies\"], \"Task Categories\": [\"Video Captioning\"], \"Inferred Metadata\": {\"GitHub License\": \"\", \"GitHub Stars (June 2024)\": \"\", \"GitHub Topics\": \"\", \"Github Date\": \"\", \"HF Config\": \"\", \"HF Config License\": \"\", \"HF Dataset\": \"\", \"HF Date\": \"\", \"HF Downloads (June 2024)\": \"\", \"HF Likes (June 2024)\": \"\", \"HF Yaml License\": \"\", \"PwC Date\": \"\", \"PwC Description\": \"\", \"PwC License Name\": \"\", \"PwC License URL\": \"\", \"S2 Citation Count (June 2024)\": \"\", \"S2 Date\": \"\"}, \"Bibtex\": \"@Article{Rohrbach2015ADF,\\n author = {Anna Rohrbach and Marcus Rohrbach and Niket Tandon and B. Schiele},\\n booktitle = {Computer Vision and Pattern Recognition},\\n journal = {2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},\\n pages = {3202-3212},\\n title = {A dataset for Movie Description},\\n year = {2015}\\n}\\n\", \"License Use (DataProvenance)\": \"unspecified\", \"License Attribution (DataProvenance)\": 0, \"License Share Alike (DataProvenance)\": 0, \"License Use (DataProvenance IgnoreOpenAI)\": \"unspecified\", \"License Attribution (DataProvenance IgnoreOpenAI)\": 0, \"License Share Alike (DataProvenance IgnoreOpenAI)\": 0, \"License Type\": \"Unspecified\"}, {\"Unique Dataset Identifier\": \"multi-thumos-challenge\", \"Collection\": \"multi-thumos-challenge\", \"Collection URL\": \"https://arxiv.org/pdf/1507.05738v3.pdf\", \"Dataset Name\": \"MultiTHUMOS: Every Moment Counts: Dense Detailed Labeling of Actions in Complex Videos (IJCV 2017)\", \"Paper Title\": \"MultiTHUMOS: Every Moment Counts: Dense Detailed Labeling of Actions in Complex Videos (IJCV 2017)\", \"Paper URL\": \"https://arxiv.org/pdf/1507.05738v3.pdf\", \"GitHub URL\": \"\", \"Hugging Face URL\": \"\", \"Papers with Code URL\": \"https://paperswithcode.com/dataset/multithumos\", \"ArXiv URL\": \"https://arxiv.org/pdf/1507.05738v3.pdf\", \"Semantic Scholar Corpus ID\": 3337929, \"Year Released\": \"2017\", \"Text Sources\": [\"youtube\"], \"Licenses\": [{\"License\": \"CC BY 4.0\", \"License URL\": \"https://ai.stanford.edu/~syyeung/resources/multithumos.zip\"}], \"Creators\": \"Academic\", \"Countries\": [\"United States of America\", \"Canada\"], \"License Verified By\": \"Vivek Sharma\", \"Video Hours\": 30.0, \"Taken Down\": \"False\", \"Video Sources\": [\"youtube\"], \"Task Categories\": [\"Video Classification\"], \"Inferred Metadata\": {\"GitHub License\": \"\", \"GitHub Stars (June 2024)\": \"\", \"GitHub Topics\": \"\", \"Github Date\": \"\", \"HF Config\": \"\", \"HF Config License\": \"\", \"HF Dataset\": \"\", \"HF Date\": \"\", \"HF Downloads (June 2024)\": \"\", \"HF Likes (June 2024)\": \"\", \"HF Yaml License\": \"\", \"PwC Date\": \"2018-01-01\", \"PwC Description\": \"The MultiTHUMOS dataset contains dense, multilabel, frame-level action annotations for 30 hours across 400 videos in the THUMOS'14 action detection dataset. It consists of 38,690 annotations of 65 action classes, with an average of 1.5 labels per frame and 10.5 action classes per video.\", \"PwC License Name\": \"CC BY 4.0\", \"PwC License URL\": null, \"S2 Citation Count (June 2024)\": \"\", \"S2 Date\": \"\"}, \"Bibtex\": \"@Article{Yeung2015EveryMC,\\n author = {Serena Yeung and Olga Russakovsky and Ning Jin and Mykhaylo Andriluka and Greg Mori and Li Fei-Fei},\\n booktitle = {International Journal of Computer Vision},\\n journal = {International Journal of Computer Vision},\\n pages = {375 - 389},\\n title = {Every Moment Counts: Dense Detailed Labeling of Actions in Complex Videos},\\n volume = {126},\\n year = {2015}\\n}\\n\", \"License Use (DataProvenance)\": \"commercial\", \"License Attribution (DataProvenance)\": 1, \"License Share Alike (DataProvenance)\": 0, \"License Use (DataProvenance IgnoreOpenAI)\": \"commercial\", \"License Attribution (DataProvenance IgnoreOpenAI)\": 1, \"License Share Alike (DataProvenance IgnoreOpenAI)\": 0, \"License Type\": \"Commercial\"}, {\"Unique Dataset Identifier\": \"multi-thumos-challenge\", \"Collection\": \"multi-thumos-challenge\", \"Collection URL\": \"https://arxiv.org/pdf/1507.05738v3.pdf\", \"Dataset Name\": \"MultiTHUMOS: Every Moment Counts: Dense Detailed Labeling of Actions in Complex Videos (IJCV 2017)\", \"Paper Title\": \"MultiTHUMOS: Every Moment Counts: Dense Detailed Labeling of Actions in Complex Videos (IJCV 2017)\", \"Paper URL\": \"https://arxiv.org/pdf/1507.05738v3.pdf\", \"GitHub URL\": \"\", \"Hugging Face URL\": \"\", \"Papers with Code URL\": \"https://paperswithcode.com/dataset/multithumos\", \"ArXiv URL\": \"https://arxiv.org/pdf/1507.05738v3.pdf\", \"Semantic Scholar Corpus ID\": 3337929, \"Year Released\": \"2017\", \"Text Sources\": [\"youtube\"], \"Licenses\": [{\"License\": \"CC BY 4.0\", \"License URL\": \"https://ai.stanford.edu/~syyeung/resources/multithumos.zip\"}], \"Creators\": \"Academic\", \"Countries\": [\"United States of America\", \"Canada\"], \"License Verified By\": \"Vivek Sharma\", \"Video Hours\": 30.0, \"Taken Down\": \"False\", \"Video Sources\": [\"youtube\"], \"Task Categories\": [\"Video Classification\"], \"Inferred Metadata\": {\"GitHub License\": \"\", \"GitHub Stars (June 2024)\": \"\", \"GitHub Topics\": \"\", \"Github Date\": \"\", \"HF Config\": \"\", \"HF Config License\": \"\", \"HF Dataset\": \"\", \"HF Date\": \"\", \"HF Downloads (June 2024)\": \"\", \"HF Likes (June 2024)\": \"\", \"HF Yaml License\": \"\", \"PwC Date\": \"2018-01-01\", \"PwC Description\": \"The MultiTHUMOS dataset contains dense, multilabel, frame-level action annotations for 30 hours across 400 videos in the THUMOS'14 action detection dataset. It consists of 38,690 annotations of 65 action classes, with an average of 1.5 labels per frame and 10.5 action classes per video.\", \"PwC License Name\": \"CC BY 4.0\", \"PwC License URL\": null, \"S2 Citation Count (June 2024)\": \"\", \"S2 Date\": \"\"}, \"Bibtex\": \"@Article{Yeung2015EveryMC,\\n author = {Serena Yeung and Olga Russakovsky and Ning Jin and Mykhaylo Andriluka and Greg Mori and Li Fei-Fei},\\n booktitle = {International Journal of Computer Vision},\\n journal = {International Journal of Computer Vision},\\n pages = {375 - 389},\\n title = {Every Moment Counts: Dense Detailed Labeling of Actions in Complex Videos},\\n volume = {126},\\n year = {2015}\\n}\\n\", \"License Use (DataProvenance)\": \"commercial\", \"License Attribution (DataProvenance)\": 1, \"License Share Alike (DataProvenance)\": 0, \"License Use (DataProvenance IgnoreOpenAI)\": \"commercial\", \"License Attribution (DataProvenance IgnoreOpenAI)\": 1, \"License Share Alike (DataProvenance IgnoreOpenAI)\": 0, \"License Type\": \"Commercial\"}, {\"Unique Dataset Identifier\": \"multi-thumos-challenge\", \"Collection\": \"multi-thumos-challenge\", \"Collection URL\": \"https://arxiv.org/pdf/1507.05738v3.pdf\", \"Dataset Name\": \"MultiTHUMOS: Every Moment Counts: Dense Detailed Labeling of Actions in Complex Videos (IJCV 2017)\", \"Paper Title\": \"MultiTHUMOS: Every Moment Counts: Dense Detailed Labeling of Actions in Complex Videos (IJCV 2017)\", \"Paper URL\": \"https://arxiv.org/pdf/1507.05738v3.pdf\", \"GitHub URL\": \"\", \"Hugging Face URL\": \"\", \"Papers with Code URL\": \"https://paperswithcode.com/dataset/multithumos\", \"ArXiv URL\": \"https://arxiv.org/pdf/1507.05738v3.pdf\", \"Semantic Scholar Corpus ID\": 3337929, \"Year Released\": \"2017\", \"Text Sources\": [\"youtube\"], \"Licenses\": [{\"License\": \"CC BY 4.0\", \"License URL\": \"https://ai.stanford.edu/~syyeung/resources/multithumos.zip\"}], \"Creators\": \"Academic\", \"Countries\": [\"United States of America\", \"Canada\"], \"License Verified By\": \"Vivek Sharma\", \"Video Hours\": 30.0, \"Taken Down\": \"False\", \"Video Sources\": [\"youtube\"], \"Task Categories\": [\"Video Classification\"], \"Inferred Metadata\": {\"GitHub License\": \"\", \"GitHub Stars (June 2024)\": \"\", \"GitHub Topics\": \"\", \"Github Date\": \"\", \"HF Config\": \"\", \"HF Config License\": \"\", \"HF Dataset\": \"\", \"HF Date\": \"\", \"HF Downloads (June 2024)\": \"\", \"HF Likes (June 2024)\": \"\", \"HF Yaml License\": \"\", \"PwC Date\": \"2018-01-01\", \"PwC Description\": \"The MultiTHUMOS dataset contains dense, multilabel, frame-level action annotations for 30 hours across 400 videos in the THUMOS'14 action detection dataset. It consists of 38,690 annotations of 65 action classes, with an average of 1.5 labels per frame and 10.5 action classes per video.\", \"PwC License Name\": \"CC BY 4.0\", \"PwC License URL\": null, \"S2 Citation Count (June 2024)\": \"\", \"S2 Date\": \"\"}, \"Bibtex\": \"@Article{Yeung2015EveryMC,\\n author = {Serena Yeung and Olga Russakovsky and Ning Jin and Mykhaylo Andriluka and Greg Mori and Li Fei-Fei},\\n booktitle = {International Journal of Computer Vision},\\n journal = {International Journal of Computer Vision},\\n pages = {375 - 389},\\n title = {Every Moment Counts: Dense Detailed Labeling of Actions in Complex Videos},\\n volume = {126},\\n year = {2015}\\n}\\n\", \"License Use (DataProvenance)\": \"commercial\", \"License Attribution (DataProvenance)\": 1, \"License Share Alike (DataProvenance)\": 0, \"License Use (DataProvenance IgnoreOpenAI)\": \"commercial\", \"License Attribution (DataProvenance IgnoreOpenAI)\": 1, \"License Share Alike (DataProvenance IgnoreOpenAI)\": 0, \"License Type\": \"Commercial\"}, {\"Unique Dataset Identifier\": \"lsmdc\", \"Collection\": \"lsmdc\", \"Collection URL\": \"https://arxiv.org/pdf/1605.03705\", \"Dataset Name\": \"LSMDC\", \"Paper Title\": \"LSMDC\", \"Paper URL\": \"https://arxiv.org/pdf/1605.03705\", \"GitHub URL\": \"\", \"Hugging Face URL\": \"\", \"Papers with Code URL\": \"https://paperswithcode.com/paper/movie-description\", \"ArXiv URL\": \"https://arxiv.org/pdf/1605.03705\", \"Semantic Scholar Corpus ID\": 18217052, \"Year Released\": \"2017\", \"Text Sources\": [\"movies\"], \"Licenses\": [{\"License\": \"Custom\", \"License URL\": \"https://datasets.d2.mpi-inf.mpg.de/movieDescription/protected/lsmdc2016/README.txt\"}], \"Creators\": \"Corporation\", \"Countries\": [\"United States of America\", \"Canada\"], \"License Verified By\": \"Vivek Sharma\", \"Video Hours\": 158.0, \"Taken Down\": \"False\", \"Video Sources\": [\"movies\"], \"Task Categories\": [\"Video Summarization\", \"Misc\", \"Video Classification\"], \"Inferred Metadata\": {\"GitHub License\": \"\", \"GitHub Stars (June 2024)\": \"\", \"GitHub Topics\": \"\", \"Github Date\": \"\", \"HF Config\": \"\", \"HF Config License\": \"\", \"HF Dataset\": \"\", \"HF Date\": \"\", \"HF Downloads (June 2024)\": \"\", \"HF Likes (June 2024)\": \"\", \"HF Yaml License\": \"\", \"PwC Date\": \"\", \"PwC Description\": \"No code available yet.\", \"PwC License Name\": \"\", \"PwC License URL\": \"\", \"S2 Citation Count (June 2024)\": \"\", \"S2 Date\": \"\"}, \"Bibtex\": \"@Article{Rohrbach2016MovieD,\\n author = {Anna Rohrbach and Atousa Torabi and Marcus Rohrbach and Niket Tandon and C. Pal and H. Larochelle and Aaron C. Courville and B. Schiele},\\n booktitle = {International Journal of Computer Vision},\\n journal = {International Journal of Computer Vision},\\n pages = {94 - 120},\\n title = {Movie Description},\\n volume = {123},\\n year = {2016}\\n}\\n\", \"License Use (DataProvenance)\": \"academic-only\", \"License Attribution (DataProvenance)\": 1, \"License Share Alike (DataProvenance)\": 0, \"License Use (DataProvenance IgnoreOpenAI)\": \"academic-only\", \"License Attribution (DataProvenance IgnoreOpenAI)\": 1, \"License Share Alike (DataProvenance IgnoreOpenAI)\": 0, \"License Type\": \"Non-Commercial/Academic\"}, {\"Unique Dataset Identifier\": \"lsmdc\", \"Collection\": \"lsmdc\", \"Collection URL\": \"https://arxiv.org/pdf/1605.03705\", \"Dataset Name\": \"LSMDC\", \"Paper Title\": \"LSMDC\", \"Paper URL\": \"https://arxiv.org/pdf/1605.03705\", \"GitHub URL\": \"\", \"Hugging Face URL\": \"\", \"Papers with Code URL\": \"https://paperswithcode.com/paper/movie-description\", \"ArXiv URL\": \"https://arxiv.org/pdf/1605.03705\", \"Semantic Scholar Corpus ID\": 18217052, \"Year Released\": \"2017\", \"Text Sources\": [\"movies\"], \"Licenses\": [{\"License\": \"Custom\", \"License URL\": \"https://datasets.d2.mpi-inf.mpg.de/movieDescription/protected/lsmdc2016/README.txt\"}], \"Creators\": \"Academic\", \"Countries\": [\"United States of America\", \"Canada\"], \"License Verified By\": \"Vivek Sharma\", \"Video Hours\": 158.0, \"Taken Down\": \"False\", \"Video Sources\": [\"movies\"], \"Task Categories\": [\"Video Summarization\", \"Misc\", \"Video Classification\"], \"Inferred Metadata\": {\"GitHub License\": \"\", \"GitHub Stars (June 2024)\": \"\", \"GitHub Topics\": \"\", \"Github Date\": \"\", \"HF Config\": \"\", \"HF Config License\": \"\", \"HF Dataset\": \"\", \"HF Date\": \"\", \"HF Downloads (June 2024)\": \"\", \"HF Likes (June 2024)\": \"\", \"HF Yaml License\": \"\", \"PwC Date\": \"\", \"PwC Description\": \"No code available yet.\", \"PwC License Name\": \"\", \"PwC License URL\": \"\", \"S2 Citation Count (June 2024)\": \"\", \"S2 Date\": \"\"}, \"Bibtex\": \"@Article{Rohrbach2016MovieD,\\n author = {Anna Rohrbach and Atousa Torabi and Marcus Rohrbach and Niket Tandon and C. Pal and H. Larochelle and Aaron C. Courville and B. Schiele},\\n booktitle = {International Journal of Computer Vision},\\n journal = {International Journal of Computer Vision},\\n pages = {94 - 120},\\n title = {Movie Description},\\n volume = {123},\\n year = {2016}\\n}\\n\", \"License Use (DataProvenance)\": \"academic-only\", \"License Attribution (DataProvenance)\": 1, \"License Share Alike (DataProvenance)\": 0, \"License Use (DataProvenance IgnoreOpenAI)\": \"academic-only\", \"License Attribution (DataProvenance IgnoreOpenAI)\": 1, \"License Share Alike (DataProvenance IgnoreOpenAI)\": 0, \"License Type\": \"Non-Commercial/Academic\"}, {\"Unique Dataset Identifier\": \"lsmdc\", \"Collection\": \"lsmdc\", \"Collection URL\": \"https://arxiv.org/pdf/1605.03705\", \"Dataset Name\": \"LSMDC\", \"Paper Title\": \"LSMDC\", \"Paper URL\": \"https://arxiv.org/pdf/1605.03705\", \"GitHub URL\": \"\", \"Hugging Face URL\": \"\", \"Papers with Code URL\": \"https://paperswithcode.com/paper/movie-description\", \"ArXiv URL\": \"https://arxiv.org/pdf/1605.03705\", \"Semantic Scholar Corpus ID\": 18217052, \"Year Released\": \"2017\", \"Text Sources\": [\"movies\"], \"Licenses\": [{\"License\": \"Custom\", \"License URL\": \"https://datasets.d2.mpi-inf.mpg.de/movieDescription/protected/lsmdc2016/README.txt\"}], \"Creators\": \"Academic\", \"Countries\": [\"United States of America\", \"Canada\"], \"License Verified By\": \"Vivek Sharma\", \"Video Hours\": 158.0, \"Taken Down\": \"False\", \"Video Sources\": [\"movies\"], \"Task Categories\": [\"Video Summarization\", \"Misc\", \"Video Classification\"], \"Inferred Metadata\": {\"GitHub License\": \"\", \"GitHub Stars (June 2024)\": \"\", \"GitHub Topics\": \"\", \"Github Date\": \"\", \"HF Config\": \"\", \"HF Config License\": \"\", \"HF Dataset\": \"\", \"HF Date\": \"\", \"HF Downloads (June 2024)\": \"\", \"HF Likes (June 2024)\": \"\", \"HF Yaml License\": \"\", \"PwC Date\": \"\", \"PwC Description\": \"No code available yet.\", \"PwC License Name\": \"\", \"PwC License URL\": \"\", \"S2 Citation Count (June 2024)\": \"\", \"S2 Date\": \"\"}, \"Bibtex\": \"@Article{Rohrbach2016MovieD,\\n author = {Anna Rohrbach and Atousa Torabi and Marcus Rohrbach and Niket Tandon and C. Pal and H. Larochelle and Aaron C. Courville and B. Schiele},\\n booktitle = {International Journal of Computer Vision},\\n journal = {International Journal of Computer Vision},\\n pages = {94 - 120},\\n title = {Movie Description},\\n volume = {123},\\n year = {2016}\\n}\\n\", \"License Use (DataProvenance)\": \"academic-only\", \"License Attribution (DataProvenance)\": 1, \"License Share Alike (DataProvenance)\": 0, \"License Use (DataProvenance IgnoreOpenAI)\": \"academic-only\", \"License Attribution (DataProvenance IgnoreOpenAI)\": 1, \"License Share Alike (DataProvenance IgnoreOpenAI)\": 0, \"License Type\": \"Non-Commercial/Academic\"}, {\"Unique Dataset Identifier\": \"lsmdc\", \"Collection\": \"lsmdc\", \"Collection URL\": \"https://arxiv.org/pdf/1605.03705\", \"Dataset Name\": \"LSMDC\", \"Paper Title\": \"LSMDC\", \"Paper URL\": \"https://arxiv.org/pdf/1605.03705\", \"GitHub URL\": \"\", \"Hugging Face URL\": \"\", \"Papers with Code URL\": \"https://paperswithcode.com/paper/movie-description\", \"ArXiv URL\": \"https://arxiv.org/pdf/1605.03705\", \"Semantic Scholar Corpus ID\": 18217052, \"Year Released\": \"2017\", \"Text Sources\": [\"movies\"], \"Licenses\": [{\"License\": \"Custom\", \"License URL\": \"https://datasets.d2.mpi-inf.mpg.de/movieDescription/protected/lsmdc2016/README.txt\"}], \"Creators\": \"Academic\", \"Countries\": [\"United States of America\", \"Canada\"], \"License Verified By\": \"Vivek Sharma\", \"Video Hours\": 158.0, \"Taken Down\": \"False\", \"Video Sources\": [\"movies\"], \"Task Categories\": [\"Video Summarization\", \"Misc\", \"Video Classification\"], \"Inferred Metadata\": {\"GitHub License\": \"\", \"GitHub Stars (June 2024)\": \"\", \"GitHub Topics\": \"\", \"Github Date\": \"\", \"HF Config\": \"\", \"HF Config License\": \"\", \"HF Dataset\": \"\", \"HF Date\": \"\", \"HF Downloads (June 2024)\": \"\", \"HF Likes (June 2024)\": \"\", \"HF Yaml License\": \"\", \"PwC Date\": \"\", \"PwC Description\": \"No code available yet.\", \"PwC License Name\": \"\", \"PwC License URL\": \"\", \"S2 Citation Count (June 2024)\": \"\", \"S2 Date\": \"\"}, \"Bibtex\": \"@Article{Rohrbach2016MovieD,\\n author = {Anna Rohrbach and Atousa Torabi and Marcus Rohrbach and Niket Tandon and C. Pal and H. Larochelle and Aaron C. Courville and B. Schiele},\\n booktitle = {International Journal of Computer Vision},\\n journal = {International Journal of Computer Vision},\\n pages = {94 - 120},\\n title = {Movie Description},\\n volume = {123},\\n year = {2016}\\n}\\n\", \"License Use (DataProvenance)\": \"academic-only\", \"License Attribution (DataProvenance)\": 1, \"License Share Alike (DataProvenance)\": 0, \"License Use (DataProvenance IgnoreOpenAI)\": \"academic-only\", \"License Attribution (DataProvenance IgnoreOpenAI)\": 1, \"License Share Alike (DataProvenance IgnoreOpenAI)\": 0, \"License Type\": \"Non-Commercial/Academic\"}, {\"Unique Dataset Identifier\": \"lsmdc\", \"Collection\": \"lsmdc\", \"Collection URL\": \"https://arxiv.org/pdf/1605.03705\", \"Dataset Name\": \"LSMDC\", \"Paper Title\": \"LSMDC\", \"Paper URL\": \"https://arxiv.org/pdf/1605.03705\", \"GitHub URL\": \"\", \"Hugging Face URL\": \"\", \"Papers with Code URL\": \"https://paperswithcode.com/paper/movie-description\", \"ArXiv URL\": \"https://arxiv.org/pdf/1605.03705\", \"Semantic Scholar Corpus ID\": 18217052, \"Year Released\": \"2017\", \"Text Sources\": [\"movies\"], \"Licenses\": [{\"License\": \"Custom\", \"License URL\": \"https://datasets.d2.mpi-inf.mpg.de/movieDescription/protected/lsmdc2016/README.txt\"}], \"Creators\": \"Industry Lab\", \"Countries\": [\"United States of America\", \"Canada\"], \"License Verified By\": \"Vivek Sharma\", \"Video Hours\": 158.0, \"Taken Down\": \"False\", \"Video Sources\": [\"movies\"], \"Task Categories\": [\"Video Summarization\", \"Misc\", \"Video Classification\"], \"Inferred Metadata\": {\"GitHub License\": \"\", \"GitHub Stars (June 2024)\": \"\", \"GitHub Topics\": \"\", \"Github Date\": \"\", \"HF Config\": \"\", \"HF Config License\": \"\", \"HF Dataset\": \"\", \"HF Date\": \"\", \"HF Downloads (June 2024)\": \"\", \"HF Likes (June 2024)\": \"\", \"HF Yaml License\": \"\", \"PwC Date\": \"\", \"PwC Description\": \"No code available yet.\", \"PwC License Name\": \"\", \"PwC License URL\": \"\", \"S2 Citation Count (June 2024)\": \"\", \"S2 Date\": \"\"}, \"Bibtex\": \"@Article{Rohrbach2016MovieD,\\n author = {Anna Rohrbach and Atousa Torabi and Marcus Rohrbach and Niket Tandon and C. Pal and H. Larochelle and Aaron C. Courville and B. Schiele},\\n booktitle = {International Journal of Computer Vision},\\n journal = {International Journal of Computer Vision},\\n pages = {94 - 120},\\n title = {Movie Description},\\n volume = {123},\\n year = {2016}\\n}\\n\", \"License Use (DataProvenance)\": \"academic-only\", \"License Attribution (DataProvenance)\": 1, \"License Share Alike (DataProvenance)\": 0, \"License Use (DataProvenance IgnoreOpenAI)\": \"academic-only\", \"License Attribution (DataProvenance IgnoreOpenAI)\": 1, \"License Share Alike (DataProvenance IgnoreOpenAI)\": 0, \"License Type\": \"Non-Commercial/Academic\"}, {\"Unique Dataset Identifier\": \"lsmdc\", \"Collection\": \"lsmdc\", \"Collection URL\": \"https://arxiv.org/pdf/1605.03705\", \"Dataset Name\": \"LSMDC\", \"Paper Title\": \"LSMDC\", \"Paper URL\": \"https://arxiv.org/pdf/1605.03705\", \"GitHub URL\": \"\", \"Hugging Face URL\": \"\", \"Papers with Code URL\": \"https://paperswithcode.com/paper/movie-description\", \"ArXiv URL\": \"https://arxiv.org/pdf/1605.03705\", \"Semantic Scholar Corpus ID\": 18217052, \"Year Released\": \"2017\", \"Text Sources\": [\"movies\"], \"Licenses\": [{\"License\": \"Custom\", \"License URL\": \"https://datasets.d2.mpi-inf.mpg.de/movieDescription/protected/lsmdc2016/README.txt\"}], \"Creators\": \"Academic\", \"Countries\": [\"United States of America\", \"Canada\"], \"License Verified By\": \"Vivek Sharma\", \"Video Hours\": 158.0, \"Taken Down\": \"False\", \"Video Sources\": [\"movies\"], \"Task Categories\": [\"Video Summarization\", \"Misc\", \"Video Classification\"], \"Inferred Metadata\": {\"GitHub License\": \"\", \"GitHub Stars (June 2024)\": \"\", \"GitHub Topics\": \"\", \"Github Date\": \"\", \"HF Config\": \"\", \"HF Config License\": \"\", \"HF Dataset\": \"\", \"HF Date\": \"\", \"HF Downloads (June 2024)\": \"\", \"HF Likes (June 2024)\": \"\", \"HF Yaml License\": \"\", \"PwC Date\": \"\", \"PwC Description\": \"No code available yet.\", \"PwC License Name\": \"\", \"PwC License URL\": \"\", \"S2 Citation Count (June 2024)\": \"\", \"S2 Date\": \"\"}, \"Bibtex\": \"@Article{Rohrbach2016MovieD,\\n author = {Anna Rohrbach and Atousa Torabi and Marcus Rohrbach and Niket Tandon and C. Pal and H. Larochelle and Aaron C. Courville and B. Schiele},\\n booktitle = {International Journal of Computer Vision},\\n journal = {International Journal of Computer Vision},\\n pages = {94 - 120},\\n title = {Movie Description},\\n volume = {123},\\n year = {2016}\\n}\\n\", \"License Use (DataProvenance)\": \"academic-only\", \"License Attribution (DataProvenance)\": 1, \"License Share Alike (DataProvenance)\": 0, \"License Use (DataProvenance IgnoreOpenAI)\": \"academic-only\", \"License Attribution (DataProvenance IgnoreOpenAI)\": 1, \"License Share Alike (DataProvenance IgnoreOpenAI)\": 0, \"License Type\": \"Non-Commercial/Academic\"}, {\"Unique Dataset Identifier\": \"ava\", \"Collection\": \"ava\", \"Collection URL\": \"https://arxiv.org/pdf/1705.08421\", \"Dataset Name\": \"AVA\", \"Paper Title\": \"AVA\", \"Paper URL\": \"https://arxiv.org/pdf/1705.08421\", \"GitHub URL\": \"\", \"Hugging Face URL\": \"\", \"Papers with Code URL\": \"https://paperswithcode.com/paper/ava-a-video-dataset-of-spatio-temporally\", \"ArXiv URL\": \"https://arxiv.org/pdf/1705.08421\", \"Semantic Scholar Corpus ID\": 688013, \"Year Released\": \"2017\", \"Text Sources\": [\"youtube\"], \"Licenses\": [{\"License\": \"CC BY 4.0\", \"License URL\": \"https://research.google.com/ava/\"}], \"Creators\": \"Industry Lab\", \"Countries\": [\"United States of America\"], \"License Verified By\": \"Vivek Sharma\", \"Video Hours\": 107.5, \"Taken Down\": \"False\", \"Video Sources\": [\"youtube\"], \"Task Categories\": [\"Temporal Localization\", \"Video Classification\"], \"Inferred Metadata\": {\"GitHub License\": \"\", \"GitHub Stars (June 2024)\": \"\", \"GitHub Topics\": \"\", \"Github Date\": \"\", \"HF Config\": \"\", \"HF Config License\": \"\", \"HF Dataset\": \"\", \"HF Date\": \"\", \"HF Downloads (June 2024)\": \"\", \"HF Likes (June 2024)\": \"\", \"HF Yaml License\": \"\", \"PwC Date\": \"\", \"PwC Description\": \"#6 best model for Action Detection on UCF101-24 (Frame-mAP 0.5 metric)\", \"PwC License Name\": \"\", \"PwC License URL\": \"\", \"S2 Citation Count (June 2024)\": \"\", \"S2 Date\": \"\"}, \"Bibtex\": \"@Article{Gu2017AVAAV,\\n author = {Chunhui Gu and Chen Sun and Sudheendra Vijayanarasimhan and C. Pantofaru and David A. Ross and G. Toderici and Yeqing Li and Susanna Ricco and R. Sukthankar and C. Schmid and J. Malik},\\n booktitle = {2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition},\\n journal = {2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition},\\n pages = {6047-6056},\\n title = {AVA: A Video Dataset of Spatio-Temporally Localized Atomic Visual Actions},\\n year = {2017}\\n}\\n\", \"License Use (DataProvenance)\": \"commercial\", \"License Attribution (DataProvenance)\": 1, \"License Share Alike (DataProvenance)\": 0, \"License Use (DataProvenance IgnoreOpenAI)\": \"commercial\", \"License Attribution (DataProvenance IgnoreOpenAI)\": 1, \"License Share Alike (DataProvenance IgnoreOpenAI)\": 0, \"License Type\": \"Commercial\"}, {\"Unique Dataset Identifier\": \"20bn-something\", \"Collection\": \"20bn-something\", \"Collection URL\": \"https://arxiv.org/abs/1706.04261\", \"Dataset Name\": \"20BN-SOMETHING-SOMETHING: The \\\"something something\\\" video database for learning and evaluating visual common sense\", \"Paper Title\": \"20BN-SOMETHING-SOMETHING: The \\\"something something\\\" video database for learning and evaluating visual common sense\", \"Paper URL\": \"https://arxiv.org/abs/1706.04261\", \"GitHub URL\": \"\", \"Hugging Face URL\": \"\", \"Papers with Code URL\": \"https://paperswithcode.com/dataset/something-something-v2\", \"ArXiv URL\": \"https://arxiv.org/abs/1706.04261\", \"Semantic Scholar Corpus ID\": 834612, \"Year Released\": \"2017\", \"Text Sources\": [\"crowdsourced\"], \"Licenses\": [{\"License\": \"Custom\", \"License URL\": \"https://developer.qualcomm.com/software/ai-datasets/something-something\"}], \"Creators\": \"Corporation\", \"Countries\": [\"Germany\"], \"License Verified By\": \"Vivek Sharma\", \"Video Hours\": 121.46, \"Taken Down\": \"False\", \"Video Sources\": [\"crowdsourced\"], \"Task Categories\": [\"Video Classification\"], \"Inferred Metadata\": {\"GitHub License\": \"\", \"GitHub Stars (June 2024)\": \"\", \"GitHub Topics\": \"\", \"Github Date\": \"\", \"HF Config\": \"\", \"HF Config License\": \"\", \"HF Dataset\": \"\", \"HF Date\": \"\", \"HF Downloads (June 2024)\": \"\", \"HF Likes (June 2024)\": \"\", \"HF Yaml License\": \"\", \"PwC Date\": \"2017-01-01\", \"PwC Description\": \"The 20BN-SOMETHING-SOMETHING V2 dataset is a large collection of labeled video clips that show humans performing pre-defined basic actions with everyday objects. The dataset was created by a large number of crowd workers. It allows machine learning models to develop fine-grained understanding of basic actions that occur in the physical world. It contains 220,847 videos, with 168,913 in the training set, 24,777 in the validation set and 27,157 in the test set. There are 174 labels.\\n\\nSource\\n\\nImage Source\", \"PwC License Name\": \"Various\", \"PwC License URL\": \"https://20bn.com/licensing/datasets/academic\", \"S2 Citation Count (June 2024)\": \"\", \"S2 Date\": \"\"}, \"Bibtex\": \"@Article{Goyal2017TheS,\\n author = {Raghav Goyal and Samira Ebrahimi Kahou and Vincent Michalski and Joanna Materzynska and S. Westphal and Heuna Kim and V. Haenel and Ingo Fr\\u00fcnd and P. Yianilos and Moritz Mueller-Freitag and F. Hoppe and Christian Thurau and Ingo Bax and R. Memisevic},\\n booktitle = {IEEE International Conference on Computer Vision},\\n journal = {2017 IEEE International Conference on Computer Vision (ICCV)},\\n pages = {5843-5851},\\n title = {The \\u201cSomething Something\\u201d Video Database for Learning and Evaluating Visual Common Sense},\\n year = {2017}\\n}\\n\", \"License Use (DataProvenance)\": \"commercial\", \"License Attribution (DataProvenance)\": 1, \"License Share Alike (DataProvenance)\": 1, \"License Use (DataProvenance IgnoreOpenAI)\": \"commercial\", \"License Attribution (DataProvenance IgnoreOpenAI)\": 1, \"License Share Alike (DataProvenance IgnoreOpenAI)\": 1, \"License Type\": \"Commercial\"}, {\"Unique Dataset Identifier\": \"davis\", \"Collection\": \"davis\", \"Collection URL\": \"https://ieeexplore.ieee.org/document/7780454\", \"Dataset Name\": \"Davis\", \"Paper Title\": \"Davis\", \"Paper URL\": \"https://ieeexplore.ieee.org/document/7780454\", \"GitHub URL\": \"https://github.com/fperazzi/davis\", \"Hugging Face URL\": \"\", \"Papers with Code URL\": \"https://paperswithcode.com/paper/a-benchmark-dataset-and-evaluation\", \"ArXiv URL\": \"https://ieeexplore.ieee.org/document/7780454\", \"Semantic Scholar Corpus ID\": 3619941, \"Year Released\": \"2017\", \"Text Sources\": [\"undisclosed web\"], \"Licenses\": [{\"License\": \"Custom\", \"License URL\": \"https://github.com/fperazzi/davis/blob/main/LICENSE\"}], \"Creators\": \"Academic\", \"Countries\": [\"Switzerland\"], \"License Verified By\": \"Vivek Sharma\", \"Video Hours\": 0.04, \"Taken Down\": \"False\", \"Video Sources\": [\"undisclosed web\"], \"Task Categories\": [\"Video Segmentation\", \"Video Classification\"], \"Inferred Metadata\": {\"GitHub License\": \"\", \"GitHub Stars (June 2024)\": 6, \"GitHub Topics\": [], \"Github Date\": \"\", \"HF Config\": \"\", \"HF Config License\": \"\", \"HF Dataset\": \"\", \"HF Date\": \"\", \"HF Downloads (June 2024)\": \"\", \"HF Likes (June 2024)\": \"\", \"HF Yaml License\": \"\", \"PwC Date\": \"\", \"PwC Description\": \"Implemented in one code library.\", \"PwC License Name\": \"\", \"PwC License URL\": \"\", \"S2 Citation Count (June 2024)\": \"\", \"S2 Date\": \"\"}, \"Bibtex\": \"@Article{Pont-Tuset2017The2D,\\n author = {J. Pont-Tuset and Federico Perazzi and Sergi Caelles and Pablo Arbel\\u00e1ez and A. Sorkine-Hornung and L. Gool},\\n booktitle = {arXiv.org},\\n journal = {ArXiv},\\n title = {The 2017 DAVIS Challenge on Video Object Segmentation},\\n volume = {abs/1704.00675},\\n year = {2017}\\n}\\n\", \"License Use (DataProvenance)\": \"commercial\", \"License Attribution (DataProvenance)\": 1, \"License Share Alike (DataProvenance)\": 0, \"License Use (DataProvenance IgnoreOpenAI)\": \"commercial\", \"License Attribution (DataProvenance IgnoreOpenAI)\": 1, \"License Share Alike (DataProvenance IgnoreOpenAI)\": 0, \"License Type\": \"Commercial\"}, {\"Unique Dataset Identifier\": \"davis\", \"Collection\": \"davis\", \"Collection URL\": \"https://ieeexplore.ieee.org/document/7780454\", \"Dataset Name\": \"Davis\", \"Paper Title\": \"Davis\", \"Paper URL\": \"https://ieeexplore.ieee.org/document/7780454\", \"GitHub URL\": \"https://github.com/fperazzi/davis\", \"Hugging Face URL\": \"\", \"Papers with Code URL\": \"https://paperswithcode.com/paper/a-benchmark-dataset-and-evaluation\", \"ArXiv URL\": \"https://ieeexplore.ieee.org/document/7780454\", \"Semantic Scholar Corpus ID\": 3619941, \"Year Released\": \"2017\", \"Text Sources\": [\"undisclosed web\"], \"Licenses\": [{\"License\": \"Custom\", \"License URL\": \"https://github.com/fperazzi/davis/blob/main/LICENSE\"}], \"Creators\": \"Industry Lab\", \"Countries\": [\"Switzerland\"], \"License Verified By\": \"Vivek Sharma\", \"Video Hours\": 0.04, \"Taken Down\": \"False\", \"Video Sources\": [\"undisclosed web\"], \"Task Categories\": [\"Video Segmentation\", \"Video Classification\"], \"Inferred Metadata\": {\"GitHub License\": \"\", \"GitHub Stars (June 2024)\": 6, \"GitHub Topics\": [], \"Github Date\": \"\", \"HF Config\": \"\", \"HF Config License\": \"\", \"HF Dataset\": \"\", \"HF Date\": \"\", \"HF Downloads (June 2024)\": \"\", \"HF Likes (June 2024)\": \"\", \"HF Yaml License\": \"\", \"PwC Date\": \"\", \"PwC Description\": \"Implemented in one code library.\", \"PwC License Name\": \"\", \"PwC License URL\": \"\", \"S2 Citation Count (June 2024)\": \"\", \"S2 Date\": \"\"}, \"Bibtex\": \"@Article{Pont-Tuset2017The2D,\\n author = {J. Pont-Tuset and Federico Perazzi and Sergi Caelles and Pablo Arbel\\u00e1ez and A. Sorkine-Hornung and L. Gool},\\n booktitle = {arXiv.org},\\n journal = {ArXiv},\\n title = {The 2017 DAVIS Challenge on Video Object Segmentation},\\n volume = {abs/1704.00675},\\n year = {2017}\\n}\\n\", \"License Use (DataProvenance)\": \"commercial\", \"License Attribution (DataProvenance)\": 1, \"License Share Alike (DataProvenance)\": 0, \"License Use (DataProvenance IgnoreOpenAI)\": \"commercial\", \"License Attribution (DataProvenance IgnoreOpenAI)\": 1, \"License Share Alike (DataProvenance IgnoreOpenAI)\": 0, \"License Type\": \"Commercial\"}, {\"Unique Dataset Identifier\": \"pku-mmd-dataset\", \"Collection\": \"pku-mmd-dataset\", \"Collection URL\": \"https://arxiv.org/abs/1703.07475\", \"Dataset Name\": \"PKU-MMD: A Large Scale Benchmark for Continuous Multi-Modal Human Action Understanding (ACM Multimedia Workshop)\", \"Paper Title\": \"PKU-MMD: A Large Scale Benchmark for Continuous Multi-Modal Human Action Understanding (ACM Multimedia Workshop)\", \"Paper URL\": \"https://arxiv.org/abs/1703.07475\", \"GitHub URL\": \"https://struct002.github.io/PKUMMD/\", \"Hugging Face URL\": \"\", \"Papers with Code URL\": \"https://paperswithcode.com/paper/pku-mmd-a-large-scale-benchmark-for\", \"ArXiv URL\": \"https://arxiv.org/abs/1703.07475\", \"Semantic Scholar Corpus ID\": 1904265, \"Year Released\": \"2017\", \"Text Sources\": [\"crowdsourced\"], \"Licenses\": [{\"License\": \"Unspecified\", \"License URL\": \"\"}], \"Creators\": \"Academic\", \"Countries\": [\"China\"], \"License Verified By\": \"Vivek Sharma\", \"Video Hours\": 50.0, \"Taken Down\": \"False\", \"Video Sources\": [\"crowdsourced\"], \"Task Categories\": [\"Video Classification\"], \"Inferred Metadata\": {\"GitHub License\": \"\", \"GitHub Stars (June 2024)\": \"\", \"GitHub Topics\": \"\", \"Github Date\": \"\", \"HF Config\": \"\", \"HF Config License\": \"\", \"HF Dataset\": \"\", \"HF Date\": \"\", \"HF Downloads (June 2024)\": \"\", \"HF Likes (June 2024)\": \"\", \"HF Yaml License\": \"\", \"PwC Date\": \"\", \"PwC Description\": \"No code available yet.\", \"PwC License Name\": \"\", \"PwC License URL\": \"\", \"S2 Citation Count (June 2024)\": \"\", \"S2 Date\": \"\"}, \"Bibtex\": \"@Article{Liu2017PKUMMDAL,\\n author = {Chunhui Liu and Yueyu Hu and Yanghao Li and Sijie Song and Jiaying Liu},\\n booktitle = {arXiv.org},\\n journal = {ArXiv},\\n title = {PKU-MMD: A Large Scale Benchmark for Continuous Multi-Modal Human Action Understanding},\\n volume = {abs/1703.07475},\\n year = {2017}\\n}\\n\", \"License Use (DataProvenance)\": \"unspecified\", \"License Attribution (DataProvenance)\": 0, \"License Share Alike (DataProvenance)\": 0, \"License Use (DataProvenance IgnoreOpenAI)\": \"unspecified\", \"License Attribution (DataProvenance IgnoreOpenAI)\": 0, \"License Share Alike (DataProvenance IgnoreOpenAI)\": 0, \"License Type\": \"Unspecified\"}, {\"Unique Dataset Identifier\": \"pku-mmd-dataset\", \"Collection\": \"pku-mmd-dataset\", \"Collection URL\": \"https://arxiv.org/abs/1703.07475\", \"Dataset Name\": \"PKU-MMD: A Large Scale Benchmark for Continuous Multi-Modal Human Action Understanding (ACM Multimedia Workshop)\", \"Paper Title\": \"PKU-MMD: A Large Scale Benchmark for Continuous Multi-Modal Human Action Understanding (ACM Multimedia Workshop)\", \"Paper URL\": \"https://arxiv.org/abs/1703.07475\", \"GitHub URL\": \"https://struct002.github.io/PKUMMD/\", \"Hugging Face URL\": \"\", \"Papers with Code URL\": \"https://paperswithcode.com/paper/pku-mmd-a-large-scale-benchmark-for\", \"ArXiv URL\": \"https://arxiv.org/abs/1703.07475\", \"Semantic Scholar Corpus ID\": 1904265, \"Year Released\": \"2017\", \"Text Sources\": [\"crowdsourced\"], \"Licenses\": [{\"License\": \"Unspecified\", \"License URL\": \"\"}], \"Creators\": \"Industry Lab\", \"Countries\": [\"China\"], \"License Verified By\": \"Vivek Sharma\", \"Video Hours\": 50.0, \"Taken Down\": \"False\", \"Video Sources\": [\"crowdsourced\"], \"Task Categories\": [\"Video Classification\"], \"Inferred Metadata\": {\"GitHub License\": \"\", \"GitHub Stars (June 2024)\": \"\", \"GitHub Topics\": \"\", \"Github Date\": \"\", \"HF Config\": \"\", \"HF Config License\": \"\", \"HF Dataset\": \"\", \"HF Date\": \"\", \"HF Downloads (June 2024)\": \"\", \"HF Likes (June 2024)\": \"\", \"HF Yaml License\": \"\", \"PwC Date\": \"\", \"PwC Description\": \"No code available yet.\", \"PwC License Name\": \"\", \"PwC License URL\": \"\", \"S2 Citation Count (June 2024)\": \"\", \"S2 Date\": \"\"}, \"Bibtex\": \"@Article{Liu2017PKUMMDAL,\\n author = {Chunhui Liu and Yueyu Hu and Yanghao Li and Sijie Song and Jiaying Liu},\\n booktitle = {arXiv.org},\\n journal = {ArXiv},\\n title = {PKU-MMD: A Large Scale Benchmark for Continuous Multi-Modal Human Action Understanding},\\n volume = {abs/1703.07475},\\n year = {2017}\\n}\\n\", \"License Use (DataProvenance)\": \"unspecified\", \"License Attribution (DataProvenance)\": 0, \"License Share Alike (DataProvenance)\": 0, \"License Use (DataProvenance IgnoreOpenAI)\": \"unspecified\", \"License Attribution (DataProvenance IgnoreOpenAI)\": 0, \"License Share Alike (DataProvenance IgnoreOpenAI)\": 0, \"License Type\": \"Unspecified\"}, {\"Unique Dataset Identifier\": \"youcook-2\", \"Collection\": \"youcook-2\", \"Collection URL\": \"https://arxiv.org/abs/1703.09788\", \"Dataset Name\": \"YouCook2\", \"Paper Title\": \"YouCook2\", \"Paper URL\": \"https://arxiv.org/abs/1703.09788\", \"GitHub URL\": \"\", \"Hugging Face URL\": \"\", \"Papers with Code URL\": \"https://paperswithcode.com/dataset/youcook2\", \"ArXiv URL\": \"https://arxiv.org/abs/1703.09788\", \"Semantic Scholar Corpus ID\": 19713015, \"Year Released\": \"2017\", \"Text Sources\": [\"youtube\"], \"Licenses\": [{\"License\": \"MIT License\", \"License URL\": \"http://youcook2.eecs.umich.edu/static/YouCookII/LICENSE_YOUCOOK2.txt\"}], \"Creators\": \"Academic\", \"Countries\": [\"United States of America\"], \"License Verified By\": \"Vivek Sharma\", \"Video Hours\": 175.6, \"Taken Down\": \"False\", \"Video Sources\": [\"youtube\"], \"Task Categories\": [\"Video Captioning\"], \"Inferred Metadata\": {\"GitHub License\": \"\", \"GitHub Stars (June 2024)\": \"\", \"GitHub Topics\": \"\", \"Github Date\": \"\", \"HF Config\": \"\", \"HF Config License\": \"\", \"HF Dataset\": \"\", \"HF Date\": \"\", \"HF Downloads (June 2024)\": \"\", \"HF Likes (June 2024)\": \"\", \"HF Yaml License\": \"\", \"PwC Date\": \"2018-01-01\", \"PwC Description\": \"YouCook2 is the largest task-oriented, instructional video dataset in the vision community. It contains 2000 long untrimmed videos from 89 cooking recipes; on average, each distinct recipe has 22 videos. The procedure steps for each video are annotated with temporal boundaries and described by imperative English sentences (see the example below). The videos were downloaded from YouTube and are all in the third-person viewpoint. All the videos are unconstrained and can be performed by individual persons at their houses with unfixed cameras. YouCook2 contains rich recipe types and various cooking styles from all over the world.\", \"PwC License Name\": \"Various\", \"PwC License URL\": \"http://youcook2.eecs.umich.edu/download\", \"S2 Citation Count (June 2024)\": \"\", \"S2 Date\": \"\"}, \"Bibtex\": \"@Article{Zhou2017TowardsAL,\\n author = {Luowei Zhou and Chenliang Xu and Jason J. Corso},\\n booktitle = {AAAI Conference on Artificial Intelligence},\\n pages = {7590-7598},\\n title = {Towards Automatic Learning of Procedures From Web Instructional Videos},\\n year = {2017}\\n}\\n\", \"License Use (DataProvenance)\": \"commercial\", \"License Attribution (DataProvenance)\": 1, \"License Share Alike (DataProvenance)\": 0, \"License Use (DataProvenance IgnoreOpenAI)\": \"commercial\", \"License Attribution (DataProvenance IgnoreOpenAI)\": 1, \"License Share Alike (DataProvenance IgnoreOpenAI)\": 0, \"License Type\": \"Commercial\"}, {\"Unique Dataset Identifier\": \"voxceleb\", \"Collection\": \"voxceleb\", \"Collection URL\": \"https://arxiv.org/abs/1706.08612\", \"Dataset Name\": \"VoxCeleb\", \"Paper Title\": \"VoxCeleb\", \"Paper URL\": \"https://arxiv.org/abs/1706.08612\", \"GitHub URL\": \"https://github.com/a-nagrani/VGGVox\", \"Hugging Face URL\": \"\", \"Papers with Code URL\": \"https://cs.paperswithcode.com/paper/voxceleb-a-large-scale-speaker-identification\", \"ArXiv URL\": \"https://arxiv.org/abs/1706.08612\", \"Semantic Scholar Corpus ID\": 10475843, \"Year Released\": \"2017\", \"Text Sources\": [\"youtube\"], \"Licenses\": [{\"License\": \"Custom\", \"License URL\": \"https://www.robots.ox.ac.uk/~vgg/data/voxceleb/\"}], \"Creators\": \"Academic\", \"Countries\": [\"United Kingdom\", \"South Korea\"], \"License Verified By\": \"Vivek Sharma\", \"Video Hours\": 2000.0, \"Taken Down\": \"False\", \"Video Sources\": [\"youtube\"], \"Task Categories\": [\"Video Classification\"], \"Inferred Metadata\": {\"GitHub License\": \"\", \"GitHub Stars (June 2024)\": 366, \"GitHub Topics\": [], \"Github Date\": \"\", \"HF Config\": \"\", \"HF Config License\": \"\", \"HF Dataset\": \"\", \"HF Date\": \"\", \"HF Downloads (June 2024)\": \"\", \"HF Likes (June 2024)\": \"\", \"HF Yaml License\": \"\", \"PwC Date\": \"\", \"PwC Description\": \"Implemented in 8 code libraries.\", \"PwC License Name\": \"\", \"PwC License URL\": \"\", \"S2 Citation Count (June 2024)\": \"\", \"S2 Date\": \"\"}, \"Bibtex\": \"@Article{Nagrani2017VoxCelebAL,\\n author = {Arsha Nagrani and Joon Son Chung and Andrew Zisserman},\\n booktitle = {Interspeech},\\n pages = {2616-2620},\\n title = {VoxCeleb: A Large-Scale Speaker Identification Dataset},\\n year = {2017}\\n}\\n\", \"License Use (DataProvenance)\": \"academic-only\", \"License Attribution (DataProvenance)\": 1, \"License Share Alike (DataProvenance)\": 0, \"License Use (DataProvenance IgnoreOpenAI)\": \"academic-only\", \"License Attribution (DataProvenance IgnoreOpenAI)\": 1, \"License Share Alike (DataProvenance IgnoreOpenAI)\": 0, \"License Type\": \"Non-Commercial/Academic\"}, {\"Unique Dataset Identifier\": \"imagenet-vid\", \"Collection\": \"imagenet-vid\", \"Collection URL\": \"https://link.springer.com/article/10.1007/s11263-015-0816-y?sa_campaign=email/event/articleAuthor/onlineFirst#\", \"Dataset Name\": \"ImageNet VID\", \"Paper Title\": \"ImageNet VID\", \"Paper URL\": \"https://link.springer.com/article/10.1007/s11263-015-0816-y?sa_campaign=email/event/articleAuthor/onlineFirst#\", \"GitHub URL\": \"\", \"Hugging Face URL\": \"\", \"Papers with Code URL\": \"https://paperswithcode.com/sota/video-object-detection-on-imagenet-vid\", \"ArXiv URL\": \"https://link.springer.com/article/10.1007/s11263-015-0816-y?sa_campaign=email/event/articleAuthor/onlineFirst#\", \"Semantic Scholar Corpus ID\": 2930547, \"Year Released\": \"2017\", \"Text Sources\": [\"undisclosed web\"], \"Licenses\": [{\"License\": \"CC BY-NC 4.0\", \"License URL\": \"https://www.image-net.org/challenges/LSVRC/2017/index.php\"}], \"Creators\": \"Academic\", \"Countries\": [\"United States of America\"], \"License Verified By\": \"Vivek Sharma\", \"Video Hours\": 9.26, \"Taken Down\": \"False\", \"Video Sources\": [\"undisclosed web\"], \"Task Categories\": [\"Video Classification\"], \"Inferred Metadata\": {\"GitHub License\": \"\", \"GitHub Stars (June 2024)\": \"\", \"GitHub Topics\": \"\", \"Github Date\": \"\", \"HF Config\": \"\", \"HF Config License\": \"\", \"HF Dataset\": \"\", \"HF Date\": \"\", \"HF Downloads (June 2024)\": \"\", \"HF Likes (June 2024)\": \"\", \"HF Yaml License\": \"\", \"PwC Date\": \"\", \"PwC Description\": \"The current state-of-the-art on ImageNet VID is DiffusionVID (Swin-B). See a full comparison of 31 papers with code.\", \"PwC License Name\": \"\", \"PwC License URL\": \"\", \"S2 Citation Count (June 2024)\": \"\", \"S2 Date\": \"\"}, \"Bibtex\": \"@Article{Russakovsky2014ImageNetLS,\\n author = {Olga Russakovsky and Jia Deng and Hao Su and J. Krause and S. Satheesh and Sean Ma and Zhiheng Huang and A. Karpathy and A. Khosla and Michael S. Bernstein and A. Berg and Li Fei-Fei},\\n booktitle = {International Journal of Computer Vision},\\n journal = {International Journal of Computer Vision},\\n pages = {211 - 252},\\n title = {ImageNet Large Scale Visual Recognition Challenge},\\n volume = {115},\\n year = {2014}\\n}\\n\", \"License Use (DataProvenance)\": \"non-commercial\", \"License Attribution (DataProvenance)\": 1, \"License Share Alike (DataProvenance)\": 0, \"License Use (DataProvenance IgnoreOpenAI)\": \"non-commercial\", \"License Attribution (DataProvenance IgnoreOpenAI)\": 1, \"License Share Alike (DataProvenance IgnoreOpenAI)\": 0, \"License Type\": \"Non-Commercial/Academic\"}, {\"Unique Dataset Identifier\": \"didemo\", \"Collection\": \"didemo\", \"Collection URL\": \"https://paperswithcode.com/dataset/didemo\", \"Dataset Name\": \"DiDeMo: Localizing Moments in Video with Temporal Language (EMNLP 2018)\", \"Paper Title\": \"DiDeMo: Localizing Moments in Video with Temporal Language (EMNLP 2018)\", \"Paper URL\": \"https://paperswithcode.com/dataset/didemo\", \"GitHub URL\": \"https://github.com/LisaAnne/TemporalLanguageRelease\", \"Hugging Face URL\": \"\", \"Papers with Code URL\": \"\", \"ArXiv URL\": \"https://paperswithcode.com/dataset/didemo\", \"Semantic Scholar Corpus ID\": 52164739, \"Year Released\": \"2018\", \"Text Sources\": [\"flickr\"], \"Licenses\": [{\"License\": \"BSD 2-Clause License\", \"License URL\": \"https://github.com/LisaAnne/TemporalLanguageRelease\"}], \"Creators\": \"Academic\", \"Countries\": [\"United States of America\"], \"License Verified By\": \"Vivek Sharma\", \"Video Hours\": 275.0, \"Taken Down\": \"False\", \"Video Sources\": [\"flickr\"], \"Task Categories\": [\"Temporal Action Localization\"], \"Inferred Metadata\": {\"GitHub License\": \"\", \"GitHub Stars (June 2024)\": 40, \"GitHub Topics\": [], \"Github Date\": \"\", \"HF Config\": \"\", \"HF Config License\": \"\", \"HF Dataset\": \"\", \"HF Date\": \"\", \"HF Downloads (June 2024)\": \"\", \"HF Likes (June 2024)\": \"\", \"HF Yaml License\": \"\", \"PwC Date\": \"\", \"PwC Description\": \"\", \"PwC License Name\": \"\", \"PwC License URL\": \"\", \"S2 Citation Count (June 2024)\": \"\", \"S2 Date\": \"\"}, \"Bibtex\": \"@Article{Hendricks2018LocalizingMI,\\n author = {Lisa Anne Hendricks and Oliver Wang and Eli Shechtman and Josef Sivic and Trevor Darrell and Bryan C. Russell},\\n booktitle = {Conference on Empirical Methods in Natural Language Processing},\\n journal = {ArXiv},\\n title = {Localizing Moments in Video with Temporal Language},\\n volume = {abs/1809.01337},\\n year = {2018}\\n}\\n\", \"License Use (DataProvenance)\": \"commercial\", \"License Attribution (DataProvenance)\": 1, \"License Share Alike (DataProvenance)\": 0, \"License Use (DataProvenance IgnoreOpenAI)\": \"commercial\", \"License Attribution (DataProvenance IgnoreOpenAI)\": 1, \"License Share Alike (DataProvenance IgnoreOpenAI)\": 0, \"License Type\": \"Commercial\"}, {\"Unique Dataset Identifier\": \"epic-kitchenes\", \"Collection\": \"epic-kitchenes\", \"Collection URL\": \"https://arxiv.org/abs/1804.02748\", \"Dataset Name\": \"EPIC-KITCHENS\", \"Paper Title\": \"EPIC-KITCHENS\", \"Paper URL\": \"https://arxiv.org/abs/1804.02748\", \"GitHub URL\": \"https://github.com/epic-kitchens\", \"Hugging Face URL\": \"\", \"Papers with Code URL\": \"https://paperswithcode.com/dataset/epic-kitchens-100\", \"ArXiv URL\": \"https://arxiv.org/abs/1804.02748\", \"Semantic Scholar Corpus ID\": 4710439, \"Year Released\": \"2018\", \"Text Sources\": [\"human\"], \"Licenses\": [{\"License\": \"CC BY-NC 4.0\", \"License URL\": \"https://epic-kitchens.github.io/2024\"}], \"Creators\": \"Academic\", \"Countries\": [\"United Kingdom\", \"Canada\", \"Spain\"], \"License Verified By\": \"Vivek Sharma\", \"Video Hours\": 100.0, \"Taken Down\": \"False\", \"Video Sources\": [\"human\"], \"Task Categories\": [\"Video Captioning\"], \"Inferred Metadata\": {\"GitHub License\": \"\", \"GitHub Stars (June 2024)\": 0, \"GitHub Topics\": [\"\"], \"Github Date\": \"\", \"HF Config\": \"\", \"HF Config License\": \"\", \"HF Dataset\": \"\", \"HF Date\": \"\", \"HF Downloads (June 2024)\": \"\", \"HF Likes (June 2024)\": \"\", \"HF Yaml License\": \"\", \"PwC Date\": \"2020-06-23\", \"PwC Description\": \"This paper introduces the pipeline to scale the largest dataset in egocentric vision EPIC-KITCHENS. The effort culminates in EPIC-KITCHENS-100, a collection of 100 hours, 20M frames, 90K actions in 700 variable-length videos, capturing long-term unscripted activities in 45 environments, using head-mounted cameras. Compared to its previous version (EPIC-KITCHENS-55), EPIC-KITCHENS-100 has been annotated using a novel pipeline that allows denser (54% more actions per minute) and more complete annotations of fine-grained actions (+128% more action segments). This collection also enables evaluating the \\\"test of time\\\" - i.e. whether models trained on data collected in 2018 can generalise to new footage collected under the same hypotheses albeit \\\"two years on\\\".\\nThe dataset is aligned with 6 challenges: action recognition (full and weak supervision), action detection, action anticipation, cross-modal retrieval (from captions), as well as unsupervised domain adaptation for action recognition. For each challenge, we define the task, provide baselines and evaluation metrics.\", \"PwC License Name\": \"CC BY-NC 4.0\", \"PwC License URL\": \"https://epic-kitchens.github.io/2021\", \"S2 Citation Count (June 2024)\": \"\", \"S2 Date\": \"\"}, \"Bibtex\": \"@Article{Damen2018ScalingEV,\\n author = {D. Damen and Hazel Doughty and G. Farinella and S. Fidler and Antonino Furnari and E. Kazakos and D. Moltisanti and Jonathan Munro and Toby Perrett and Will Price and Michael Wray},\\n booktitle = {arXiv.org},\\n journal = {ArXiv},\\n title = {Scaling Egocentric Vision: The EPIC-KITCHENS Dataset},\\n volume = {abs/1804.02748},\\n year = {2018}\\n}\\n\", \"License Use (DataProvenance)\": \"non-commercial\", \"License Attribution (DataProvenance)\": 1, \"License Share Alike (DataProvenance)\": 0, \"License Use (DataProvenance IgnoreOpenAI)\": \"non-commercial\", \"License Attribution (DataProvenance IgnoreOpenAI)\": 1, \"License Share Alike (DataProvenance IgnoreOpenAI)\": 0, \"License Type\": \"Non-Commercial/Academic\"}, {\"Unique Dataset Identifier\": \"epic-kitchenes\", \"Collection\": \"epic-kitchenes\", \"Collection URL\": \"https://arxiv.org/abs/1804.02748\", \"Dataset Name\": \"EPIC-KITCHENS\", \"Paper Title\": \"EPIC-KITCHENS\", \"Paper URL\": \"https://arxiv.org/abs/1804.02748\", \"GitHub URL\": \"https://github.com/epic-kitchens\", \"Hugging Face URL\": \"\", \"Papers with Code URL\": \"https://paperswithcode.com/dataset/epic-kitchens-100\", \"ArXiv URL\": \"https://arxiv.org/abs/1804.02748\", \"Semantic Scholar Corpus ID\": 4710439, \"Year Released\": \"2018\", \"Text Sources\": [\"human\"], \"Licenses\": [{\"License\": \"CC BY-NC 4.0\", \"License URL\": \"https://epic-kitchens.github.io/2024\"}], \"Creators\": \"Academic\", \"Countries\": [\"United Kingdom\", \"Canada\", \"Spain\"], \"License Verified By\": \"Vivek Sharma\", \"Video Hours\": 100.0, \"Taken Down\": \"False\", \"Video Sources\": [\"human\"], \"Task Categories\": [\"Video Captioning\"], \"Inferred Metadata\": {\"GitHub License\": \"\", \"GitHub Stars (June 2024)\": 0, \"GitHub Topics\": [\"\"], \"Github Date\": \"\", \"HF Config\": \"\", \"HF Config License\": \"\", \"HF Dataset\": \"\", \"HF Date\": \"\", \"HF Downloads (June 2024)\": \"\", \"HF Likes (June 2024)\": \"\", \"HF Yaml License\": \"\", \"PwC Date\": \"2020-06-23\", \"PwC Description\": \"This paper introduces the pipeline to scale the largest dataset in egocentric vision EPIC-KITCHENS. The effort culminates in EPIC-KITCHENS-100, a collection of 100 hours, 20M frames, 90K actions in 700 variable-length videos, capturing long-term unscripted activities in 45 environments, using head-mounted cameras. Compared to its previous version (EPIC-KITCHENS-55), EPIC-KITCHENS-100 has been annotated using a novel pipeline that allows denser (54% more actions per minute) and more complete annotations of fine-grained actions (+128% more action segments). This collection also enables evaluating the \\\"test of time\\\" - i.e. whether models trained on data collected in 2018 can generalise to new footage collected under the same hypotheses albeit \\\"two years on\\\".\\nThe dataset is aligned with 6 challenges: action recognition (full and weak supervision), action detection, action anticipation, cross-modal retrieval (from captions), as well as unsupervised domain adaptation for action recognition. For each challenge, we define the task, provide baselines and evaluation metrics.\", \"PwC License Name\": \"CC BY-NC 4.0\", \"PwC License URL\": \"https://epic-kitchens.github.io/2021\", \"S2 Citation Count (June 2024)\": \"\", \"S2 Date\": \"\"}, \"Bibtex\": \"@Article{Damen2018ScalingEV,\\n author = {D. Damen and Hazel Doughty and G. Farinella and S. Fidler and Antonino Furnari and E. Kazakos and D. Moltisanti and Jonathan Munro and Toby Perrett and Will Price and Michael Wray},\\n booktitle = {arXiv.org},\\n journal = {ArXiv},\\n title = {Scaling Egocentric Vision: The EPIC-KITCHENS Dataset},\\n volume = {abs/1804.02748},\\n year = {2018}\\n}\\n\", \"License Use (DataProvenance)\": \"non-commercial\", \"License Attribution (DataProvenance)\": 1, \"License Share Alike (DataProvenance)\": 0, \"License Use (DataProvenance IgnoreOpenAI)\": \"non-commercial\", \"License Attribution (DataProvenance IgnoreOpenAI)\": 1, \"License Share Alike (DataProvenance IgnoreOpenAI)\": 0, \"License Type\": \"Non-Commercial/Academic\"}, {\"Unique Dataset Identifier\": \"soa-dataset\", \"Collection\": \"soa-dataset\", \"Collection URL\": \"https://arxiv.org/pdf/1904.11451\", \"Dataset Name\": \"Scenes-objects-actions: A multi-task, multi-label video dataset\", \"Paper Title\": \"Scenes-objects-actions: A multi-task, multi-label video dataset\", \"Paper URL\": \"https://arxiv.org/pdf/1904.11451\", \"GitHub URL\": \"\", \"Hugging Face URL\": \"\", \"Papers with Code URL\": \"https://paperswithcode.com/paper/scenes-objects-actions-a-multi-task-multi\", \"ArXiv URL\": \"https://arxiv.org/pdf/1904.11451\", \"Semantic Scholar Corpus ID\": 52968009, \"Year Released\": \"2018\", \"Text Sources\": [\"facebook\"], \"Licenses\": [{\"License\": \"Unspecified\", \"License URL\": \"\"}], \"Creators\": \"Corporation\", \"Countries\": [\"United States of America\"], \"License Verified By\": \"Vivek Sharma\", \"Video Hours\": 1561.1, \"Taken Down\": \"False\", \"Video Sources\": [\"facebook\"], \"Task Categories\": [\"Video Classification\"], \"Inferred Metadata\": {\"GitHub License\": \"\", \"GitHub Stars (June 2024)\": \"\", \"GitHub Topics\": \"\", \"Github Date\": \"\", \"HF Config\": \"\", \"HF Config License\": \"\", \"HF Dataset\": \"\", \"HF Date\": \"\", \"HF Downloads (June 2024)\": \"\", \"HF Likes (June 2024)\": \"\", \"HF Yaml License\": \"\", \"PwC Date\": \"\", \"PwC Description\": \"No code available yet.\", \"PwC License Name\": \"\", \"PwC License URL\": \"\", \"S2 Citation Count (June 2024)\": \"\", \"S2 Date\": \"\"}, \"Bibtex\": \"@Article{Ray2018ScenesObjectsActionsAM,\\n author = {Jamie Ray and Heng Wang and Du Tran and Yufei Wang and Matt Feiszli and L. Torresani and Manohar Paluri},\\n booktitle = {European Conference on Computer Vision},\\n pages = {660-676},\\n title = {Scenes-Objects-Actions: A Multi-task, Multi-label Video Dataset},\\n year = {2018}\\n}\\n\", \"License Use (DataProvenance)\": \"unspecified\", \"License Attribution (DataProvenance)\": 0, \"License Share Alike (DataProvenance)\": 0, \"License Use (DataProvenance IgnoreOpenAI)\": \"unspecified\", \"License Attribution (DataProvenance IgnoreOpenAI)\": 0, \"License Share Alike (DataProvenance IgnoreOpenAI)\": 0, \"License Type\": \"Unspecified\"}, {\"Unique Dataset Identifier\": \"epic-kitchenes\", \"Collection\": \"epic-kitchenes\", \"Collection URL\": \"https://arxiv.org/abs/1804.02748\", \"Dataset Name\": \"EPIC-KITCHENS\", \"Paper Title\": \"EPIC-KITCHENS\", \"Paper URL\": \"https://arxiv.org/abs/1804.02748\", \"GitHub URL\": \"https://github.com/epic-kitchens\", \"Hugging Face URL\": \"\", \"Papers with Code URL\": \"https://paperswithcode.com/dataset/epic-kitchens-100\", \"ArXiv URL\": \"https://arxiv.org/abs/1804.02748\", \"Semantic Scholar Corpus ID\": 4710439, \"Year Released\": \"2018\", \"Text Sources\": [\"human\"], \"Licenses\": [{\"License\": \"CC BY-NC 4.0\", \"License URL\": \"https://epic-kitchens.github.io/2024\"}], \"Creators\": \"Academic\", \"Countries\": [\"United Kingdom\", \"Canada\", \"Spain\"], \"License Verified By\": \"Vivek Sharma\", \"Video Hours\": 100.0, \"Taken Down\": \"False\", \"Video Sources\": [\"human\"], \"Task Categories\": [\"Video Captioning\"], \"Inferred Metadata\": {\"GitHub License\": \"\", \"GitHub Stars (June 2024)\": 0, \"GitHub Topics\": [\"\"], \"Github Date\": \"\", \"HF Config\": \"\", \"HF Config License\": \"\", \"HF Dataset\": \"\", \"HF Date\": \"\", \"HF Downloads (June 2024)\": \"\", \"HF Likes (June 2024)\": \"\", \"HF Yaml License\": \"\", \"PwC Date\": \"2020-06-23\", \"PwC Description\": \"This paper introduces the pipeline to scale the largest dataset in egocentric vision EPIC-KITCHENS. The effort culminates in EPIC-KITCHENS-100, a collection of 100 hours, 20M frames, 90K actions in 700 variable-length videos, capturing long-term unscripted activities in 45 environments, using head-mounted cameras. Compared to its previous version (EPIC-KITCHENS-55), EPIC-KITCHENS-100 has been annotated using a novel pipeline that allows denser (54% more actions per minute) and more complete annotations of fine-grained actions (+128% more action segments). This collection also enables evaluating the \\\"test of time\\\" - i.e. whether models trained on data collected in 2018 can generalise to new footage collected under the same hypotheses albeit \\\"two years on\\\".\\nThe dataset is aligned with 6 challenges: action recognition (full and weak supervision), action detection, action anticipation, cross-modal retrieval (from captions), as well as unsupervised domain adaptation for action recognition. For each challenge, we define the task, provide baselines and evaluation metrics.\", \"PwC License Name\": \"CC BY-NC 4.0\", \"PwC License URL\": \"https://epic-kitchens.github.io/2021\", \"S2 Citation Count (June 2024)\": \"\", \"S2 Date\": \"\"}, \"Bibtex\": \"@Article{Damen2018ScalingEV,\\n author = {D. Damen and Hazel Doughty and G. Farinella and S. Fidler and Antonino Furnari and E. Kazakos and D. Moltisanti and Jonathan Munro and Toby Perrett and Will Price and Michael Wray},\\n booktitle = {arXiv.org},\\n journal = {ArXiv},\\n title = {Scaling Egocentric Vision: The EPIC-KITCHENS Dataset},\\n volume = {abs/1804.02748},\\n year = {2018}\\n}\\n\", \"License Use (DataProvenance)\": \"non-commercial\", \"License Attribution (DataProvenance)\": 1, \"License Share Alike (DataProvenance)\": 0, \"License Use (DataProvenance IgnoreOpenAI)\": \"non-commercial\", \"License Attribution (DataProvenance IgnoreOpenAI)\": 1, \"License Share Alike (DataProvenance IgnoreOpenAI)\": 0, \"License Type\": \"Non-Commercial/Academic\"}, {\"Unique Dataset Identifier\": \"kinetics-600\", \"Collection\": \"kinetics-600\", \"Collection URL\": \"https://arxiv.org/abs/1808.01340\", \"Dataset Name\": \"Kinetics 600\", \"Paper Title\": \"Kinetics 600\", \"Paper URL\": \"https://arxiv.org/abs/1808.01340\", \"GitHub URL\": \"https://github.com/cvdfoundation/kinetics-dataset\", \"Hugging Face URL\": \"\", \"Papers with Code URL\": \"https://paperswithcode.com/dataset/kinetics-600\", \"ArXiv URL\": \"https://arxiv.org/abs/1808.01340\", \"Semantic Scholar Corpus ID\": 51927456, \"Year Released\": \"2018\", \"Text Sources\": [\"youtube\"], \"Licenses\": [{\"License\": \"Unspecified\", \"License URL\": \"\"}], \"Creators\": \"Industry Lab\", \"Countries\": [\"United Kingdom\"], \"License Verified By\": \"Vivek Sharma\", \"Video Hours\": 1376.52, \"Taken Down\": \"False\", \"Video Sources\": [\"youtube\"], \"Task Categories\": [\"Video Classification\"], \"Inferred Metadata\": {\"GitHub License\": \"\", \"GitHub Stars (June 2024)\": 704, \"GitHub Topics\": [], \"Github Date\": \"\", \"HF Config\": \"\", \"HF Config License\": \"\", \"HF Dataset\": \"\", \"HF Date\": \"\", \"HF Downloads (June 2024)\": \"\", \"HF Likes (June 2024)\": \"\", \"HF Yaml License\": \"\", \"PwC Date\": \"2018-01-01\", \"PwC Description\": \"The Kinetics-600 is a large-scale action recognition dataset which consists of around 480K videos from 600 action categories. The 480K videos are divided into 390K, 30K, 60K for training, validation and test sets, respectively. Each video in the dataset is a 10-second clip of action moment annotated from raw YouTube video. It is an extensions of the Kinetics-400 dataset.\", \"PwC License Name\": \"CC BY 4.0\", \"PwC License URL\": \"https://creativecommons.org/licenses/by/4.0/\", \"S2 Citation Count (June 2024)\": \"\", \"S2 Date\": \"\"}, \"Bibtex\": \"@Article{Carreira2018ASN,\\n author = {Jo\\u00e3o Carreira and Eric Noland and Andras Banki-Horvath and Chloe Hillier and Andrew Zisserman},\\n booktitle = {arXiv.org},\\n journal = {ArXiv},\\n title = {A Short Note about Kinetics-600},\\n volume = {abs/1808.01340},\\n year = {2018}\\n}\\n\", \"License Use (DataProvenance)\": \"unspecified\", \"License Attribution (DataProvenance)\": 0, \"License Share Alike (DataProvenance)\": 0, \"License Use (DataProvenance IgnoreOpenAI)\": \"unspecified\", \"License Attribution (DataProvenance IgnoreOpenAI)\": 0, \"License Share Alike (DataProvenance IgnoreOpenAI)\": 0, \"License Type\": \"Unspecified\"}, {\"Unique Dataset Identifier\": \"charades-ego\", \"Collection\": \"charades-ego\", \"Collection URL\": \"https://arxiv.org/abs/1804.09627\", \"Dataset Name\": \"Charades-Ego: Actor and Observer: Joint Modeling of First and Third-Person Videos (CVPR 2018)\", \"Paper Title\": \"Charades-Ego: Actor and Observer: Joint Modeling of First and Third-Person Videos (CVPR 2018)\", \"Paper URL\": \"https://arxiv.org/abs/1804.09627\", \"GitHub URL\": \"https://github.com/gsig/actor-observer\", \"Hugging Face URL\": \"\", \"Papers with Code URL\": \"https://paperswithcode.com/search?q_meta=&q_type=&q=Actor+and+Observer%3A+Joint+Modeling+of+First+and+Third-Person+Videos\", \"ArXiv URL\": \"https://arxiv.org/abs/1804.09627\", \"Semantic Scholar Corpus ID\": 4562167, \"Year Released\": \"2018\", \"Text Sources\": [\"human\"], \"Licenses\": [{\"License\": \"Custom\", \"License URL\": \"https://prior.allenai.org/projects/data/charades-ego/license.txt\"}], \"Creators\": \"Academic\", \"Countries\": [\"United States of America\"], \"License Verified By\": \"Vivek Sharma\", \"Video Hours\": 69.33, \"Taken Down\": \"False\", \"Video Sources\": [\"human\"], \"Task Categories\": [\"Video Classification\"], \"Inferred Metadata\": {\"GitHub License\": \"GNU General Public License v3.0\", \"GitHub Stars (June 2024)\": 75, \"GitHub Topics\": [], \"Github Date\": \"\", \"HF Config\": \"\", \"HF Config License\": \"\", \"HF Dataset\": \"\", \"HF Date\": \"\", \"HF Downloads (June 2024)\": \"\", \"HF Likes (June 2024)\": \"\", \"HF Yaml License\": \"\", \"PwC Date\": \"\", \"PwC Description\": \"\", \"PwC License Name\": \"\", \"PwC License URL\": \"\", \"S2 Citation Count (June 2024)\": \"\", \"S2 Date\": \"\"}, \"Bibtex\": \"@Book{Sigurdsson2018ActorAO,\\n author = {Gunnar A. Sigurdsson and A. Gupta and C. Schmid and Ali Farhadi and Alahari Karteek},\\n booktitle = {2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition},\\n journal = {2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition},\\n pages = {7396-7404},\\n title = {Actor and Observer: Joint Modeling of First and Third-Person Videos},\\n year = {2018}\\n}\\n\", \"License Use (DataProvenance)\": \"academic-only\", \"License Attribution (DataProvenance)\": 1, \"License Share Alike (DataProvenance)\": 0, \"License Use (DataProvenance IgnoreOpenAI)\": \"academic-only\", \"License Attribution (DataProvenance IgnoreOpenAI)\": 1, \"License Share Alike (DataProvenance IgnoreOpenAI)\": 0, \"License Type\": \"Non-Commercial/Academic\"}, {\"Unique Dataset Identifier\": \"vlog-vids\", \"Collection\": \"vlog-vids\", \"Collection URL\": \"https://arxiv.org/abs/1712.02310\", \"Dataset Name\": \"VLOG: From Lifestyle Vlogs to Everyday Interactions (CVPR 2018)\", \"Paper Title\": \"VLOG: From Lifestyle Vlogs to Everyday Interactions (CVPR 2018)\", \"Paper URL\": \"https://arxiv.org/abs/1712.02310\", \"GitHub URL\": \"\", \"Hugging Face URL\": \"\", \"Papers with Code URL\": \"https://paperswithcode.com/dataset/vlog-dataset\", \"ArXiv URL\": \"https://arxiv.org/abs/1712.02310\", \"Semantic Scholar Corpus ID\": 22264672, \"Year Released\": \"2018\", \"Text Sources\": [\"youtube\"], \"Licenses\": [{\"License\": \"Custom\", \"License URL\": \"https://web.eecs.umich.edu/~fouhey/2017/VLOG/index.html\"}], \"Creators\": \"Academic\", \"Countries\": [\"United States of America\"], \"License Verified By\": \"Vivek Sharma\", \"Video Hours\": 336.0, \"Taken Down\": \"False\", \"Video Sources\": [\"youtube\"], \"Task Categories\": [\"Video Classification\"], \"Inferred Metadata\": {\"GitHub License\": \"\", \"GitHub Stars (June 2024)\": \"\", \"GitHub Topics\": \"\", \"Github Date\": \"\", \"HF Config\": \"\", \"HF Config License\": \"\", \"HF Dataset\": \"\", \"HF Date\": \"\", \"HF Downloads (June 2024)\": \"\", \"HF Likes (June 2024)\": \"\", \"HF Yaml License\": \"\", \"PwC Date\": \"\", \"PwC Description\": \"A large collection of interaction-rich video data which are annotated and analyzed.\", \"PwC License Name\": \"Unspecified\", \"PwC License URL\": null, \"S2 Citation Count (June 2024)\": \"\", \"S2 Date\": \"\"}, \"Bibtex\": \"@Article{Fouhey2017FromLV,\\n author = {D. Fouhey and Weicheng Kuo and Alexei A. Efros and Jitendra Malik},\\n booktitle = {2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition},\\n journal = {2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition},\\n pages = {4991-5000},\\n title = {From Lifestyle Vlogs to Everyday Interactions},\\n year = {2017}\\n}\\n\", \"License Use (DataProvenance)\": \"academic-only\", \"License Attribution (DataProvenance)\": 1, \"License Share Alike (DataProvenance)\": 0, \"License Use (DataProvenance IgnoreOpenAI)\": \"academic-only\", \"License Attribution (DataProvenance IgnoreOpenAI)\": 1, \"License Share Alike (DataProvenance IgnoreOpenAI)\": 0, \"License Type\": \"Non-Commercial/Academic\"}, {\"Unique Dataset Identifier\": \"moviegraphs\", \"Collection\": \"moviegraphs\", \"Collection URL\": \"https://arxiv.org/pdf/1712.06761\", \"Dataset Name\": \"MovieGraphs\", \"Paper Title\": \"MovieGraphs\", \"Paper URL\": \"https://arxiv.org/pdf/1712.06761\", \"GitHub URL\": \"\", \"Hugging Face URL\": \"\", \"Papers with Code URL\": \"https://paperswithcode.com/dataset/moviegraphs\", \"ArXiv URL\": \"https://arxiv.org/pdf/1712.06761\", \"Semantic Scholar Corpus ID\": 4856028, \"Year Released\": \"2018\", \"Text Sources\": [\"movies\"], \"Licenses\": [{\"License\": \"Custom\", \"License URL\": \"http://moviegraphs.cs.toronto.edu/download.html\"}], \"Creators\": \"Other\", \"Countries\": [\"Canada\"], \"License Verified By\": \"Vivek Sharma\", \"Video Hours\": 93.9, \"Taken Down\": \"True\", \"Video Sources\": [\"movies\"], \"Task Categories\": [\"Misc (video retrieval\", \"interaction understanding via ordering\", \"reason prediction)\"], \"Inferred Metadata\": {\"GitHub License\": \"\", \"GitHub Stars (June 2024)\": \"\", \"GitHub Topics\": \"\", \"Github Date\": \"\", \"HF Config\": \"\", \"HF Config License\": \"\", \"HF Dataset\": \"\", \"HF Date\": \"\", \"HF Downloads (June 2024)\": \"\", \"HF Likes (June 2024)\": \"\", \"HF Yaml License\": \"\", \"PwC Date\": \"\", \"PwC Description\": \"Provides detailed, graph-based annotations of social situations depicted in movie clips. Each graph consists of several types of nodes, to capture who is present in the clip, their emotional and physical attributes, their relationships (i.e., parent/child), and the interactions between them. Most interactions are associated with topics that provide additional details, and reasons that give motivations for actions.\", \"PwC License Name\": \"Academic Research Purposes Only\", \"PwC License URL\": \"https://docs.google.com/forms/d/e/1FAIpQLScytuCn4kRBKFPPei0t01Sfadpu8Qh5i9fFvfODWAAJGyEs7g/viewform\", \"S2 Citation Count (June 2024)\": \"\", \"S2 Date\": \"\"}, \"Bibtex\": \"@Article{Vicol2017MovieGraphsTU,\\n author = {Paul Vicol and Makarand Tapaswi and Llu\\u00eds Castrej\\u00f3n and S. Fidler},\\n booktitle = {2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition},\\n journal = {2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition},\\n pages = {8581-8590},\\n title = {MovieGraphs: Towards Understanding Human-Centric Situations from Videos},\\n year = {2017}\\n}\\n\", \"License Use (DataProvenance)\": \"academic-only\", \"License Attribution (DataProvenance)\": 1, \"License Share Alike (DataProvenance)\": 0, \"License Use (DataProvenance IgnoreOpenAI)\": \"academic-only\", \"License Attribution (DataProvenance IgnoreOpenAI)\": 1, \"License Share Alike (DataProvenance IgnoreOpenAI)\": 0, \"License Type\": \"Non-Commercial/Academic\"}, {\"Unique Dataset Identifier\": \"moviegraphs\", \"Collection\": \"moviegraphs\", \"Collection URL\": \"https://arxiv.org/pdf/1712.06761\", \"Dataset Name\": \"MovieGraphs\", \"Paper Title\": \"MovieGraphs\", \"Paper URL\": \"https://arxiv.org/pdf/1712.06761\", \"GitHub URL\": \"\", \"Hugging Face URL\": \"\", \"Papers with Code URL\": \"https://paperswithcode.com/dataset/moviegraphs\", \"ArXiv URL\": \"https://arxiv.org/pdf/1712.06761\", \"Semantic Scholar Corpus ID\": 4856028, \"Year Released\": \"2018\", \"Text Sources\": [\"movies\"], \"Licenses\": [{\"License\": \"Custom\", \"License URL\": \"http://moviegraphs.cs.toronto.edu/download.html\"}], \"Creators\": \"Academic\", \"Countries\": [\"Canada\"], \"License Verified By\": \"Vivek Sharma\", \"Video Hours\": 93.9, \"Taken Down\": \"True\", \"Video Sources\": [\"movies\"], \"Task Categories\": [\"Misc (video retrieval\", \"interaction understanding via ordering\", \"reason prediction)\"], \"Inferred Metadata\": {\"GitHub License\": \"\", \"GitHub Stars (June 2024)\": \"\", \"GitHub Topics\": \"\", \"Github Date\": \"\", \"HF Config\": \"\", \"HF Config License\": \"\", \"HF Dataset\": \"\", \"HF Date\": \"\", \"HF Downloads (June 2024)\": \"\", \"HF Likes (June 2024)\": \"\", \"HF Yaml License\": \"\", \"PwC Date\": \"\", \"PwC Description\": \"Provides detailed, graph-based annotations of social situations depicted in movie clips. Each graph consists of several types of nodes, to capture who is present in the clip, their emotional and physical attributes, their relationships (i.e., parent/child), and the interactions between them. Most interactions are associated with topics that provide additional details, and reasons that give motivations for actions.\", \"PwC License Name\": \"Academic Research Purposes Only\", \"PwC License URL\": \"https://docs.google.com/forms/d/e/1FAIpQLScytuCn4kRBKFPPei0t01Sfadpu8Qh5i9fFvfODWAAJGyEs7g/viewform\", \"S2 Citation Count (June 2024)\": \"\", \"S2 Date\": \"\"}, \"Bibtex\": \"@Article{Vicol2017MovieGraphsTU,\\n author = {Paul Vicol and Makarand Tapaswi and Llu\\u00eds Castrej\\u00f3n and S. Fidler},\\n booktitle = {2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition},\\n journal = {2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition},\\n pages = {8581-8590},\\n title = {MovieGraphs: Towards Understanding Human-Centric Situations from Videos},\\n year = {2017}\\n}\\n\", \"License Use (DataProvenance)\": \"academic-only\", \"License Attribution (DataProvenance)\": 1, \"License Share Alike (DataProvenance)\": 0, \"License Use (DataProvenance IgnoreOpenAI)\": \"academic-only\", \"License Attribution (DataProvenance IgnoreOpenAI)\": 1, \"License Share Alike (DataProvenance IgnoreOpenAI)\": 0, \"License Type\": \"Non-Commercial/Academic\"}, {\"Unique Dataset Identifier\": \"moviegraphs\", \"Collection\": \"moviegraphs\", \"Collection URL\": \"https://arxiv.org/pdf/1712.06761\", \"Dataset Name\": \"MovieGraphs\", \"Paper Title\": \"MovieGraphs\", \"Paper URL\": \"https://arxiv.org/pdf/1712.06761\", \"GitHub URL\": \"\", \"Hugging Face URL\": \"\", \"Papers with Code URL\": \"https://paperswithcode.com/dataset/moviegraphs\", \"ArXiv URL\": \"https://arxiv.org/pdf/1712.06761\", \"Semantic Scholar Corpus ID\": 4856028, \"Year Released\": \"2018\", \"Text Sources\": [\"movies\"], \"Licenses\": [{\"License\": \"Custom\", \"License URL\": \"http://moviegraphs.cs.toronto.edu/download.html\"}], \"Creators\": \"Academic\", \"Countries\": [\"Canada\"], \"License Verified By\": \"Vivek Sharma\", \"Video Hours\": 93.9, \"Taken Down\": \"True\", \"Video Sources\": [\"movies\"], \"Task Categories\": [\"Misc (video retrieval\", \"interaction understanding via ordering\", \"reason prediction)\"], \"Inferred Metadata\": {\"GitHub License\": \"\", \"GitHub Stars (June 2024)\": \"\", \"GitHub Topics\": \"\", \"Github Date\": \"\", \"HF Config\": \"\", \"HF Config License\": \"\", \"HF Dataset\": \"\", \"HF Date\": \"\", \"HF Downloads (June 2024)\": \"\", \"HF Likes (June 2024)\": \"\", \"HF Yaml License\": \"\", \"PwC Date\": \"\", \"PwC Description\": \"Provides detailed, graph-based annotations of social situations depicted in movie clips. Each graph consists of several types of nodes, to capture who is present in the clip, their emotional and physical attributes, their relationships (i.e., parent/child), and the interactions between them. Most interactions are associated with topics that provide additional details, and reasons that give motivations for actions.\", \"PwC License Name\": \"Academic Research Purposes Only\", \"PwC License URL\": \"https://docs.google.com/forms/d/e/1FAIpQLScytuCn4kRBKFPPei0t01Sfadpu8Qh5i9fFvfODWAAJGyEs7g/viewform\", \"S2 Citation Count (June 2024)\": \"\", \"S2 Date\": \"\"}, \"Bibtex\": \"@Article{Vicol2017MovieGraphsTU,\\n author = {Paul Vicol and Makarand Tapaswi and Llu\\u00eds Castrej\\u00f3n and S. Fidler},\\n booktitle = {2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition},\\n journal = {2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition},\\n pages = {8581-8590},\\n title = {MovieGraphs: Towards Understanding Human-Centric Situations from Videos},\\n year = {2017}\\n}\\n\", \"License Use (DataProvenance)\": \"academic-only\", \"License Attribution (DataProvenance)\": 1, \"License Share Alike (DataProvenance)\": 0, \"License Use (DataProvenance IgnoreOpenAI)\": \"academic-only\", \"License Attribution (DataProvenance IgnoreOpenAI)\": 1, \"License Share Alike (DataProvenance IgnoreOpenAI)\": 0, \"License Type\": \"Non-Commercial/Academic\"}, {\"Unique Dataset Identifier\": \"how2\", \"Collection\": \"how2\", \"Collection URL\": \"https://arxiv.org/abs/1811.00347\", \"Dataset Name\": \"How2: A Large-scale Dataset for Multimodal Language Understanding (NeurIPS 2018)\", \"Paper Title\": \"How2: A Large-scale Dataset for Multimodal Language Understanding (NeurIPS 2018)\", \"Paper URL\": \"https://arxiv.org/abs/1811.00347\", \"GitHub URL\": \"https://github.com/srvk/how2-dataset\", \"Hugging Face URL\": \"\", \"Papers with Code URL\": \"\", \"ArXiv URL\": \"https://arxiv.org/abs/1811.00347\", \"Semantic Scholar Corpus ID\": 53186236, \"Year Released\": \"2018\", \"Text Sources\": [\"youtube\"], \"Licenses\": [{\"License\": \"Various\", \"License URL\": \"https://github.com/srvk/how2-dataset?tab=readme-ov-file#how2-license\"}], \"Creators\": \"Academic\", \"Countries\": [\"United States of America\"], \"License Verified By\": \"Vivek Sharma\", \"Video Hours\": 2300.0, \"Taken Down\": \"False\", \"Video Sources\": [\"youtube\"], \"Task Categories\": [\"Video Captioning\"], \"Inferred Metadata\": {\"GitHub License\": \"\", \"GitHub Stars (June 2024)\": 150, \"GitHub Topics\": [\"corpus\", \"dataset\", \"how2-dataset\", \"language\", \"machine-translation\", \"multimodality\", \"speech-recognition\", \"video\"], \"Github Date\": \"\", \"HF Config\": \"\", \"HF Config License\": \"\", \"HF Dataset\": \"\", \"HF Date\": \"\", \"HF Downloads (June 2024)\": \"\", \"HF Likes (June 2024)\": \"\", \"HF Yaml License\": \"\", \"PwC Date\": \"\", \"PwC Description\": \"\", \"PwC License Name\": \"\", \"PwC License URL\": \"\", \"S2 Citation Count (June 2024)\": \"\", \"S2 Date\": \"\"}, \"Bibtex\": \"@Article{Sanabria2018How2AL,\\n author = {Ramon Sanabria and Ozan Caglayan and Shruti Palaskar and Desmond Elliott and Lo\\u00efc Barrault and Lucia Specia and Florian Metze},\\n booktitle = {Neural Information Processing Systems},\\n journal = {ArXiv},\\n title = {How2: A Large-scale Dataset for Multimodal Language Understanding},\\n volume = {abs/1811.00347},\\n year = {2018}\\n}\\n\", \"License Use (DataProvenance)\": \"academic-only\", \"License Attribution (DataProvenance)\": 1, \"License Share Alike (DataProvenance)\": 1, \"License Use (DataProvenance IgnoreOpenAI)\": \"academic-only\", \"License Attribution (DataProvenance IgnoreOpenAI)\": 1, \"License Share Alike (DataProvenance IgnoreOpenAI)\": 1, \"License Type\": \"Non-Commercial/Academic\"}, {\"Unique Dataset Identifier\": \"crosstask\", \"Collection\": \"crosstask\", \"Collection URL\": \"https://arxiv.org/abs/1903.08225\", \"Dataset Name\": \"CrossTask: weakly supervised learning from instructional videos (CVPR 2019)\", \"Paper Title\": \"CrossTask: weakly supervised learning from instructional videos (CVPR 2019)\", \"Paper URL\": \"https://arxiv.org/abs/1903.08225\", \"GitHub URL\": \"https://github.com/DmZhukov/CrossTask\", \"Hugging Face URL\": \"\", \"Papers with Code URL\": \"https://paperswithcode.com/dataset/crosstask\", \"ArXiv URL\": \"https://arxiv.org/abs/1903.08225\", \"Semantic Scholar Corpus ID\": 84187266, \"Year Released\": \"2019\", \"Text Sources\": [\"youtube\"], \"Licenses\": [{\"License\": \"Unspecified\", \"License URL\": \"\"}], \"Creators\": \"Other\", \"Countries\": [\"France\", \"Turkey\", \"United States of America\", \"Czech Republic\"], \"License Verified By\": \"Vivek Sharma\", \"Video Hours\": 376.0, \"Taken Down\": \"False\", \"Video Sources\": [\"youtube\"], \"Task Categories\": [\"Temporal Action Localization\"], \"Inferred Metadata\": {\"GitHub License\": \"BSD 3-Clause License\", \"GitHub Stars (June 2024)\": 83, \"GitHub Topics\": [], \"Github Date\": \"\", \"HF Config\": \"\", \"HF Config License\": \"\", \"HF Dataset\": \"\", \"HF Date\": \"\", \"HF Downloads (June 2024)\": \"\", \"HF Likes (June 2024)\": \"\", \"HF Yaml License\": \"\", \"PwC Date\": \"2019-01-01\", \"PwC Description\": \"CrossTask dataset contains instructional videos, collected for 83 different tasks. For each task an ordered list of steps with manual descriptions is provided. The dataset is divided in two parts: 18 primary and 65 related tasks. Videos for the primary tasks are collected manually and provided with annotations for temporal step boundaries. Videos for the related tasks are collected automatically and don't have annotations.\", \"PwC License Name\": \"Unspecified\", \"PwC License URL\": null, \"S2 Citation Count (June 2024)\": \"\", \"S2 Date\": \"\"}, \"Bibtex\": \"@Article{Zhukov2019CrossTaskWS,\\n author = {Dimitri Zhukov and Jean-Baptiste Alayrac and R. G. Cinbis and D. Fouhey and I. Laptev and Josef Sivic},\\n booktitle = {Computer Vision and Pattern Recognition},\\n journal = {2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},\\n pages = {3532-3540},\\n title = {Cross-Task Weakly Supervised Learning From Instructional Videos},\\n year = {2019}\\n}\\n\", \"License Use (DataProvenance)\": \"unspecified\", \"License Attribution (DataProvenance)\": 0, \"License Share Alike (DataProvenance)\": 0, \"License Use (DataProvenance IgnoreOpenAI)\": \"unspecified\", \"License Attribution (DataProvenance IgnoreOpenAI)\": 0, \"License Share Alike (DataProvenance IgnoreOpenAI)\": 0, \"License Type\": \"Unspecified\"}, {\"Unique Dataset Identifier\": \"mmact\", \"Collection\": \"mmact\", \"Collection URL\": \"https://ieeexplore.ieee.org/document/9009579\", \"Dataset Name\": \"MMAct\", \"Paper Title\": \"MMAct\", \"Paper URL\": \"https://ieeexplore.ieee.org/document/9009579\", \"GitHub URL\": \"\", \"Hugging Face URL\": \"\", \"Papers with Code URL\": \"https://paperswithcode.com/dataset/mmact\", \"ArXiv URL\": \"https://ieeexplore.ieee.org/document/9009579\", \"Semantic Scholar Corpus ID\": 207980205, \"Year Released\": \"2019\", \"Text Sources\": [\"human\"], \"Licenses\": [{\"License\": \"Custom\", \"License URL\": \"https://mmact19.github.io/2019/\"}], \"Creators\": \"Corporation\", \"Countries\": [\"Hong Kong\", \"China\"], \"License Verified By\": \"Vivek Sharma\", \"Video Hours\": 100.0, \"Taken Down\": \"False\", \"Video Sources\": [\"human\"], \"Task Categories\": [\"Temporal Localization\", \"Action Recognition\", \"Spatial-Temporal Action Localization\"], \"Inferred Metadata\": {\"GitHub License\": \"\", \"GitHub Stars (June 2024)\": \"\", \"GitHub Topics\": \"\", \"Github Date\": \"\", \"HF Config\": \"\", \"HF Config License\": \"\", \"HF Dataset\": \"\", \"HF Date\": \"\", \"HF Downloads (June 2024)\": \"\", \"HF Likes (June 2024)\": \"\", \"HF Yaml License\": \"\", \"PwC Date\": \"\", \"PwC Description\": \"MMAct is a large-scale dataset for multi/cross modal action understanding. This dataset has been recorded from 20 distinct subjects with seven different types of modalities: RGB videos, keypoints, acceleration, gyroscope, orientation, Wi-Fi and pressure signal. The dataset consists of more than 36k video clips for 37 action classes covering a wide range of daily life activities such as desktop-related and check-in-based ones in four different distinct scenarios.\", \"PwC License Name\": \"Unspecified\", \"PwC License URL\": null, \"S2 Citation Count (June 2024)\": \"\", \"S2 Date\": \"\"}, \"Bibtex\": \"@Article{Kong2019MMActAL,\\n author = {Quan Kong and Ziming Wu and Ziwei Deng and Martin Klinkigt and Bin Tong and Tomokazu Murakami},\\n booktitle = {IEEE International Conference on Computer Vision},\\n journal = {2019 IEEE/CVF International Conference on Computer Vision (ICCV)},\\n pages = {8657-8666},\\n title = {MMAct: A Large-Scale Dataset for Cross Modal Human Action Understanding},\\n year = {2019}\\n}\\n\", \"License Use (DataProvenance)\": \"academic-only\", \"License Attribution (DataProvenance)\": 1, \"License Share Alike (DataProvenance)\": 0, \"License Use (DataProvenance IgnoreOpenAI)\": \"academic-only\", \"License Attribution (DataProvenance IgnoreOpenAI)\": 1, \"License Share Alike (DataProvenance IgnoreOpenAI)\": 0, \"License Type\": \"Non-Commercial/Academic\"}, {\"Unique Dataset Identifier\": \"20BN-jester\", \"Collection\": \"20BN-jester\", \"Collection URL\": \"https://openaccess.thecvf.com/content_ICCVW_2019/papers/HANDS/Materzynska_The_Jester_Dataset_A_Large-Scale_Video_Dataset_of_Human_Gestures_ICCVW_2019_paper.pdf\", \"Dataset Name\": \"20BN-jester: The Jester Dataset: A Large-Scale Video Dataset of Human Gestures (ICCVW 2019)\", \"Paper Title\": \"20BN-jester: The Jester Dataset: A Large-Scale Video Dataset of Human Gestures (ICCVW 2019)\", \"Paper URL\": \"https://openaccess.thecvf.com/content_ICCVW_2019/papers/HANDS/Materzynska_The_Jester_Dataset_A_Large-Scale_Video_Dataset_of_Human_Gestures_ICCVW_2019_paper.pdf\", \"GitHub URL\": \"\", \"Hugging Face URL\": \"\", \"Papers with Code URL\": \"\", \"ArXiv URL\": \"https://openaccess.thecvf.com/content_ICCVW_2019/papers/HANDS/Materzynska_The_Jester_Dataset_A_Large-Scale_Video_Dataset_of_Human_Gestures_ICCVW_2019_paper.pdf\", \"Semantic Scholar Corpus ID\": 208010438, \"Year Released\": \"2019\", \"Text Sources\": [\"crowdsourced\"], \"Licenses\": [{\"License\": \"Custom\", \"License URL\": \"https://developer.qualcomm.com/software/ai-datasets/jester\"}], \"Creators\": \"Corporation\", \"Countries\": [\"Germany\"], \"License Verified By\": \"Vivek Sharma\", \"Video Hours\": 13.0, \"Taken Down\": \"False\", \"Video Sources\": [\"crowdsourced\"], \"Task Categories\": [\"Video Classification\"], \"Inferred Metadata\": {\"GitHub License\": \"\", \"GitHub Stars (June 2024)\": \"\", \"GitHub Topics\": \"\", \"Github Date\": \"\", \"HF Config\": \"\", \"HF Config License\": \"\", \"HF Dataset\": \"\", \"HF Date\": \"\", \"HF Downloads (June 2024)\": \"\", \"HF Likes (June 2024)\": \"\", \"HF Yaml License\": \"\", \"PwC Date\": \"\", \"PwC Description\": \"\", \"PwC License Name\": \"\", \"PwC License URL\": \"\", \"S2 Citation Count (June 2024)\": \"\", \"S2 Date\": \"\"}, \"Bibtex\": \"@Article{Materzynska2019TheJD,\\n author = {Joanna Materzynska and Guillaume Berger and Ingo Bax and R. Memisevic},\\n booktitle = {2019 IEEE/CVF International Conference on Computer Vision Workshop (ICCVW)},\\n journal = {2019 IEEE/CVF International Conference on Computer Vision Workshop (ICCVW)},\\n pages = {2874-2882},\\n title = {The Jester Dataset: A Large-Scale Video Dataset of Human Gestures},\\n year = {2019}\\n}\\n\", \"License Use (DataProvenance)\": \"commercial\", \"License Attribution (DataProvenance)\": 1, \"License Share Alike (DataProvenance)\": 1, \"License Use (DataProvenance IgnoreOpenAI)\": \"commercial\", \"License Attribution (DataProvenance IgnoreOpenAI)\": 1, \"License Share Alike (DataProvenance IgnoreOpenAI)\": 1, \"License Type\": \"Commercial\"}, {\"Unique Dataset Identifier\": \"hacs-dataset\", \"Collection\": \"hacs-dataset\", \"Collection URL\": \"https://arxiv.org/abs/1712.09374\", \"Dataset Name\": \"HACS: Human Action Clips and Segments Dataset for Recognition and Temporal Localization\", \"Paper Title\": \"HACS: Human Action Clips and Segments Dataset for Recognition and Temporal Localization\", \"Paper URL\": \"https://arxiv.org/abs/1712.09374\", \"GitHub URL\": \"https://github.com/hangzhaomit/HACS-dataset\", \"Hugging Face URL\": \"\", \"Papers with Code URL\": \"https://paperswithcode.com/dataset/hacs\", \"ArXiv URL\": \"https://arxiv.org/abs/1712.09374\", \"Semantic Scholar Corpus ID\": 68049510, \"Year Released\": \"2019\", \"Text Sources\": [\"youtube\"], \"Licenses\": [{\"License\": \"Custom\", \"License URL\": \"https://github.com/hangzhaomit/HACS-dataset?tab=readme-ov-file#request-testing-videos-and-missing-videos-new\"}], \"Creators\": \"Academic\", \"Countries\": [\"United States of America\"], \"License Verified By\": \"Vivek Sharma\", \"Video Hours\": 833.0, \"Taken Down\": \"False\", \"Video Sources\": [\"youtube\"], \"Task Categories\": [\"Video Classification\"], \"Inferred Metadata\": {\"GitHub License\": \"BSD 3-Clause License\", \"GitHub Stars (June 2024)\": 184, \"GitHub Topics\": [], \"Github Date\": \"\", \"HF Config\": \"\", \"HF Config License\": \"\", \"HF Dataset\": \"\", \"HF Date\": \"\", \"HF Downloads (June 2024)\": \"\", \"HF Likes (June 2024)\": \"\", \"HF Yaml License\": \"\", \"PwC Date\": \"\", \"PwC Description\": \"HACS is a dataset for human action recognition. It uses a taxonomy of 200 action classes, which is identical to that of the ActivityNet-v1.3 dataset. It has 504K videos retrieved from YouTube. Each one is strictly shorter than 4 minutes, and the average length is 2.6 minutes. A total of 1.5M clips of 2-second duration are sparsely sampled by methods based on both uniform randomness and consensus/disagreement of image classifiers. 0.6M and 0.9M clips are annotated as positive and negative samples, respectively.\\n\\nAuthors split the collection into training, validation and testing sets of size 1.4M, 50K and 50K clips, which are sampled\\nfrom 492K, 6K and 6K videos, respectively.\", \"PwC License Name\": \"BSD 3-Clause License\", \"PwC License URL\": \"https://github.com/hangzhaomit/HACS-dataset/blob/master/LICENSE\", \"S2 Citation Count (June 2024)\": \"\", \"S2 Date\": \"\"}, \"Bibtex\": \"@Article{Zhao2017HACSHA,\\n author = {Hang Zhao and A. Torralba and L. Torresani and Zhicheng Yan},\\n booktitle = {IEEE International Conference on Computer Vision},\\n journal = {2019 IEEE/CVF International Conference on Computer Vision (ICCV)},\\n pages = {8667-8677},\\n title = {HACS: Human Action Clips and Segments Dataset for Recognition and Temporal Localization},\\n year = {2017}\\n}\\n\", \"License Use (DataProvenance)\": \"academic-only\", \"License Attribution (DataProvenance)\": 1, \"License Share Alike (DataProvenance)\": 0, \"License Use (DataProvenance IgnoreOpenAI)\": \"academic-only\", \"License Attribution (DataProvenance IgnoreOpenAI)\": 1, \"License Share Alike (DataProvenance IgnoreOpenAI)\": 0, \"License Type\": \"Non-Commercial/Academic\"}, {\"Unique Dataset Identifier\": \"moments-in-time-dataset\", \"Collection\": \"moments-in-time-dataset\", \"Collection URL\": \"https://arxiv.org/pdf/1801.03150.pdf\", \"Dataset Name\": \"Moments in Time Dataset: one million videos for event understanding (TPAMI 2019)\", \"Paper Title\": \"Moments in Time Dataset: one million videos for event understanding (TPAMI 2019)\", \"Paper URL\": \"https://arxiv.org/pdf/1801.03150.pdf\", \"GitHub URL\": \"\", \"Hugging Face URL\": \"\", \"Papers with Code URL\": \"\", \"ArXiv URL\": \"https://arxiv.org/pdf/1801.03150.pdf\", \"Semantic Scholar Corpus ID\": 11868155, \"Year Released\": \"2019\", \"Text Sources\": [\"youtube\", \"flickr\", \"vine\", \"metacafe\", \"peeks\", \"vimeo\", \"videoblocks\", \"bing videos\", \"giphy\", \"weather channel\", \"getty-images\"], \"Licenses\": [{\"License\": \"Custom\", \"License URL\": \"http://moments.csail.mit.edu/\"}], \"Creators\": \"Academic\", \"Countries\": [\"United States of America\"], \"License Verified By\": \"Vivek Sharma\", \"Video Hours\": 833.0, \"Taken Down\": \"False\", \"Video Sources\": [\"youtube\", \"flickr\", \"vine\", \"metacafe\", \"peeks\", \"vimeo\", \"videoblocks\", \"bing videos\", \"giphy\", \"weather channel\", \"getty-images\"], \"Task Categories\": [\"Video Classification\"], \"Inferred Metadata\": {\"GitHub License\": \"\", \"GitHub Stars (June 2024)\": \"\", \"GitHub Topics\": \"\", \"Github Date\": \"\", \"HF Config\": \"\", \"HF Config License\": \"\", \"HF Dataset\": \"\", \"HF Date\": \"\", \"HF Downloads (June 2024)\": \"\", \"HF Likes (June 2024)\": \"\", \"HF Yaml License\": \"\", \"PwC Date\": \"\", \"PwC Description\": \"\", \"PwC License Name\": \"\", \"PwC License URL\": \"\", \"S2 Citation Count (June 2024)\": \"\", \"S2 Date\": \"\"}, \"Bibtex\": \"@Article{Monfort2018MomentsIT,\\n author = {Mathew Monfort and Bolei Zhou and Sarah Adel Bargal and A. Andonian and Tom Yan and K. Ramakrishnan and L. Brown and Quanfu Fan and Dan Gutfreund and Carl Vondrick and A. Oliva},\\n booktitle = {IEEE Transactions on Pattern Analysis and Machine Intelligence},\\n journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},\\n pages = {502-508},\\n title = {Moments in Time Dataset: One Million Videos for Event Understanding},\\n volume = {42},\\n year = {2018}\\n}\\n\", \"License Use (DataProvenance)\": \"academic-only\", \"License Attribution (DataProvenance)\": 1, \"License Share Alike (DataProvenance)\": 0, \"License Use (DataProvenance IgnoreOpenAI)\": \"academic-only\", \"License Attribution (DataProvenance IgnoreOpenAI)\": 1, \"License Share Alike (DataProvenance IgnoreOpenAI)\": 0, \"License Type\": \"Non-Commercial/Academic\"}, {\"Unique Dataset Identifier\": \"vatex\", \"Collection\": \"vatex\", \"Collection URL\": \"https://arxiv.org/abs/1904.03493\", \"Dataset Name\": \"VaTeX\", \"Paper Title\": \"VaTeX\", \"Paper URL\": \"https://arxiv.org/abs/1904.03493\", \"GitHub URL\": \"\", \"Hugging Face URL\": \"https://huggingface.co/datasets/HuggingFaceM4/vatex\", \"Papers with Code URL\": \"https://paperswithcode.com/dataset/vatex\", \"ArXiv URL\": \"https://arxiv.org/abs/1904.03493\", \"Semantic Scholar Corpus ID\": 102352148, \"Year Released\": \"2019\", \"Text Sources\": [\"youtube\"], \"Licenses\": [{\"License\": \"CC BY 4.0\", \"License URL\": \"https://eric-xw.github.io/vatex-website/index.html\"}], \"Creators\": \"Corporation\", \"Countries\": [\"China\", \"United States of America\"], \"License Verified By\": \"Vivek Sharma\", \"Video Hours\": 114.58, \"Taken Down\": \"False\", \"Video Sources\": [\"youtube\"], \"Task Categories\": [\"Video Captioning\"], \"Inferred Metadata\": {\"GitHub License\": \"\", \"GitHub Stars (June 2024)\": \"\", \"GitHub Topics\": \"\", \"Github Date\": \"\", \"HF Config\": \"v1.1\", \"HF Config License\": \"\", \"HF Dataset\": \"HuggingFaceM4/vatex\", \"HF Date\": \"2022-05-13\", \"HF Downloads (June 2024)\": 112, \"HF Likes (June 2024)\": 4, \"HF Yaml License\": \"\", \"PwC Date\": \"2019-01-01\", \"PwC Description\": \"VATEX is multilingual, large, linguistically complex, and diverse dataset in terms of both video and natural language descriptions. It has two tasks for video-and-language research: (1) Multilingual Video Captioning, aimed at describing a video in various languages with a compact unified captioning model, and (2) Video-guided Machine Translation, to translate a source language description into the target language using the video information as additional spatiotemporal context.\", \"PwC License Name\": \"CC BY 4.0\", \"PwC License URL\": \"https://creativecommons.org/licenses/by/4.0/\", \"S2 Citation Count (June 2024)\": \"\", \"S2 Date\": \"\"}, \"Bibtex\": \"@Article{Wang2019VaTeXAL,\\n author = {Xin Eric Wang and Jiawei Wu and Junkun Chen and Lei Li and Yuan-fang Wang and William Yang Wang},\\n booktitle = {IEEE International Conference on Computer Vision},\\n journal = {2019 IEEE/CVF International Conference on Computer Vision (ICCV)},\\n pages = {4580-4590},\\n title = {VaTeX: A Large-Scale, High-Quality Multilingual Dataset for Video-and-Language Research},\\n year = {2019}\\n}\\n\", \"License Use (DataProvenance)\": \"commercial\", \"License Attribution (DataProvenance)\": 1, \"License Share Alike (DataProvenance)\": 0, \"License Use (DataProvenance IgnoreOpenAI)\": \"commercial\", \"License Attribution (DataProvenance IgnoreOpenAI)\": 1, \"License Share Alike (DataProvenance IgnoreOpenAI)\": 0, \"License Type\": \"Commercial\"}, {\"Unique Dataset Identifier\": \"crosstask\", \"Collection\": \"crosstask\", \"Collection URL\": \"https://arxiv.org/abs/1903.08225\", \"Dataset Name\": \"CrossTask: weakly supervised learning from instructional videos (CVPR 2019)\", \"Paper Title\": \"CrossTask: weakly supervised learning from instructional videos (CVPR 2019)\", \"Paper URL\": \"https://arxiv.org/abs/1903.08225\", \"GitHub URL\": \"https://github.com/DmZhukov/CrossTask\", \"Hugging Face URL\": \"\", \"Papers with Code URL\": \"https://paperswithcode.com/dataset/crosstask\", \"ArXiv URL\": \"https://arxiv.org/abs/1903.08225\", \"Semantic Scholar Corpus ID\": 84187266, \"Year Released\": \"2019\", \"Text Sources\": [\"youtube\"], \"Licenses\": [{\"License\": \"Unspecified\", \"License URL\": \"\"}], \"Creators\": \"Academic\", \"Countries\": [\"France\", \"Turkey\", \"United States of America\", \"Czech Republic\"], \"License Verified By\": \"Vivek Sharma\", \"Video Hours\": 376.0, \"Taken Down\": \"False\", \"Video Sources\": [\"youtube\"], \"Task Categories\": [\"Temporal Action Localization\"], \"Inferred Metadata\": {\"GitHub License\": \"BSD 3-Clause License\", \"GitHub Stars (June 2024)\": 83, \"GitHub Topics\": [], \"Github Date\": \"\", \"HF Config\": \"\", \"HF Config License\": \"\", \"HF Dataset\": \"\", \"HF Date\": \"\", \"HF Downloads (June 2024)\": \"\", \"HF Likes (June 2024)\": \"\", \"HF Yaml License\": \"\", \"PwC Date\": \"2019-01-01\", \"PwC Description\": \"CrossTask dataset contains instructional videos, collected for 83 different tasks. For each task an ordered list of steps with manual descriptions is provided. The dataset is divided in two parts: 18 primary and 65 related tasks. Videos for the primary tasks are collected manually and provided with annotations for temporal step boundaries. Videos for the related tasks are collected automatically and don't have annotations.\", \"PwC License Name\": \"Unspecified\", \"PwC License URL\": null, \"S2 Citation Count (June 2024)\": \"\", \"S2 Date\": \"\"}, \"Bibtex\": \"@Article{Zhukov2019CrossTaskWS,\\n author = {Dimitri Zhukov and Jean-Baptiste Alayrac and R. G. Cinbis and D. Fouhey and I. Laptev and Josef Sivic},\\n booktitle = {Computer Vision and Pattern Recognition},\\n journal = {2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},\\n pages = {3532-3540},\\n title = {Cross-Task Weakly Supervised Learning From Instructional Videos},\\n year = {2019}\\n}\\n\", \"License Use (DataProvenance)\": \"unspecified\", \"License Attribution (DataProvenance)\": 0, \"License Share Alike (DataProvenance)\": 0, \"License Use (DataProvenance IgnoreOpenAI)\": \"unspecified\", \"License Attribution (DataProvenance IgnoreOpenAI)\": 0, \"License Share Alike (DataProvenance IgnoreOpenAI)\": 0, \"License Type\": \"Unspecified\"}, {\"Unique Dataset Identifier\": \"vatex\", \"Collection\": \"vatex\", \"Collection URL\": \"https://arxiv.org/abs/1904.03493\", \"Dataset Name\": \"VaTeX\", \"Paper Title\": \"VaTeX\", \"Paper URL\": \"https://arxiv.org/abs/1904.03493\", \"GitHub URL\": \"\", \"Hugging Face URL\": \"https://huggingface.co/datasets/HuggingFaceM4/vatex\", \"Papers with Code URL\": \"https://paperswithcode.com/dataset/vatex\", \"ArXiv URL\": \"https://arxiv.org/abs/1904.03493\", \"Semantic Scholar Corpus ID\": 102352148, \"Year Released\": \"2019\", \"Text Sources\": [\"youtube\"], \"Licenses\": [{\"License\": \"CC BY 4.0\", \"License URL\": \"https://eric-xw.github.io/vatex-website/index.html\"}], \"Creators\": \"Academic\", \"Countries\": [\"China\", \"United States of America\"], \"License Verified By\": \"Vivek Sharma\", \"Video Hours\": 114.58, \"Taken Down\": \"False\", \"Video Sources\": [\"youtube\"], \"Task Categories\": [\"Video Captioning\"], \"Inferred Metadata\": {\"GitHub License\": \"\", \"GitHub Stars (June 2024)\": \"\", \"GitHub Topics\": \"\", \"Github Date\": \"\", \"HF Config\": \"v1.1\", \"HF Config License\": \"\", \"HF Dataset\": \"HuggingFaceM4/vatex\", \"HF Date\": \"2022-05-13\", \"HF Downloads (June 2024)\": 112, \"HF Likes (June 2024)\": 4, \"HF Yaml License\": \"\", \"PwC Date\": \"2019-01-01\", \"PwC Description\": \"VATEX is multilingual, large, linguistically complex, and diverse dataset in terms of both video and natural language descriptions. It has two tasks for video-and-language research: (1) Multilingual Video Captioning, aimed at describing a video in various languages with a compact unified captioning model, and (2) Video-guided Machine Translation, to translate a source language description into the target language using the video information as additional spatiotemporal context.\", \"PwC License Name\": \"CC BY 4.0\", \"PwC License URL\": \"https://creativecommons.org/licenses/by/4.0/\", \"S2 Citation Count (June 2024)\": \"\", \"S2 Date\": \"\"}, \"Bibtex\": \"@Article{Wang2019VaTeXAL,\\n author = {Xin Eric Wang and Jiawei Wu and Junkun Chen and Lei Li and Yuan-fang Wang and William Yang Wang},\\n booktitle = {IEEE International Conference on Computer Vision},\\n journal = {2019 IEEE/CVF International Conference on Computer Vision (ICCV)},\\n pages = {4580-4590},\\n title = {VaTeX: A Large-Scale, High-Quality Multilingual Dataset for Video-and-Language Research},\\n year = {2019}\\n}\\n\", \"License Use (DataProvenance)\": \"commercial\", \"License Attribution (DataProvenance)\": 1, \"License Share Alike (DataProvenance)\": 0, \"License Use (DataProvenance IgnoreOpenAI)\": \"commercial\", \"License Attribution (DataProvenance IgnoreOpenAI)\": 1, \"License Share Alike (DataProvenance IgnoreOpenAI)\": 0, \"License Type\": \"Commercial\"}, {\"Unique Dataset Identifier\": \"trecvid\", \"Collection\": \"trecvid\", \"Collection URL\": \"https://arxiv.org/abs/2009.09984\", \"Dataset Name\": \"TRECVID\", \"Paper Title\": \"TRECVID\", \"Paper URL\": \"https://arxiv.org/abs/2009.09984\", \"GitHub URL\": \"\", \"Hugging Face URL\": \"\", \"Papers with Code URL\": \"https://paperswithcode.com/paper/trecvid-2019-an-evaluation-campaign-to/review/\", \"ArXiv URL\": \"https://arxiv.org/abs/2009.09984\", \"Semantic Scholar Corpus ID\": 212694843, \"Year Released\": \"2019\", \"Text Sources\": [\"undisclosed web\", \"bbc\"], \"Licenses\": [{\"License\": \"CC BY-NC-SA 4.0\", \"License URL\": \"https://trecvid.nist.gov/\"}], \"Creators\": \"Government\", \"Countries\": [\"United States of America\"], \"License Verified By\": \"Vivek Sharma\", \"Video Hours\": 1000.0, \"Taken Down\": \"False\", \"Video Sources\": [\"undisclosed web\", \"bbc\"], \"Task Categories\": [\"Video Captioning\", \"Video Classification\"], \"Inferred Metadata\": {\"GitHub License\": \"\", \"GitHub Stars (June 2024)\": \"\", \"GitHub Topics\": \"\", \"Github Date\": \"\", \"HF Config\": \"\", \"HF Config License\": \"\", \"HF Dataset\": \"\", \"HF Date\": \"\", \"HF Downloads (June 2024)\": \"\", \"HF Likes (June 2024)\": \"\", \"HF Yaml License\": \"\", \"PwC Date\": \"\", \"PwC Description\": \"Paper tables with annotated results for TRECVID 2019: An Evaluation Campaign to Benchmark Video Activity Detection, Video Captioning and Matching, and Video Search & Retrieval\", \"PwC License Name\": \"\", \"PwC License URL\": \"\", \"S2 Citation Count (June 2024)\": \"\", \"S2 Date\": \"\"}, \"Bibtex\": \"@Article{Awad2019TRECVID2A,\\n author = {G. Awad and A. Butt and Keith Curtis and Yooyoung Lee and Jonathan G. Fiscus and A. Godil and Andrew Delgado and Jesse Zhang and Eliot Godard and Lukas L. Diduch and A. Smeaton and Yyette Graham and Wessel Kraaij and G. Qu\\u00e9not},\\n booktitle = {TREC Video Retrieval Evaluation},\\n journal = {ArXiv},\\n title = {TRECVID 2019: An evaluation campaign to benchmark Video Activity Detection, Video Captioning and Matching, and Video Search & retrieval},\\n volume = {abs/2009.09984},\\n year = {2019}\\n}\\n\", \"License Use (DataProvenance)\": \"non-commercial\", \"License Attribution (DataProvenance)\": 1, \"License Share Alike (DataProvenance)\": 1, \"License Use (DataProvenance IgnoreOpenAI)\": \"non-commercial\", \"License Attribution (DataProvenance IgnoreOpenAI)\": 1, \"License Share Alike (DataProvenance IgnoreOpenAI)\": 1, \"License Type\": \"Non-Commercial/Academic\"}, {\"Unique Dataset Identifier\": \"hacs-dataset\", \"Collection\": \"hacs-dataset\", \"Collection URL\": \"https://arxiv.org/abs/1712.09374\", \"Dataset Name\": \"HACS: Human Action Clips and Segments Dataset for Recognition and Temporal Localization\", \"Paper Title\": \"HACS: Human Action Clips and Segments Dataset for Recognition and Temporal Localization\", \"Paper URL\": \"https://arxiv.org/abs/1712.09374\", \"GitHub URL\": \"https://github.com/hangzhaomit/HACS-dataset\", \"Hugging Face URL\": \"\", \"Papers with Code URL\": \"https://paperswithcode.com/dataset/hacs\", \"ArXiv URL\": \"https://arxiv.org/abs/1712.09374\", \"Semantic Scholar Corpus ID\": 68049510, \"Year Released\": \"2019\", \"Text Sources\": [\"youtube\"], \"Licenses\": [{\"License\": \"Custom\", \"License URL\": \"https://github.com/hangzhaomit/HACS-dataset?tab=readme-ov-file#request-testing-videos-and-missing-videos-new\"}], \"Creators\": \"Academic\", \"Countries\": [\"United States of America\"], \"License Verified By\": \"Vivek Sharma\", \"Video Hours\": 833.0, \"Taken Down\": \"False\", \"Video Sources\": [\"youtube\"], \"Task Categories\": [\"Video Classification\"], \"Inferred Metadata\": {\"GitHub License\": \"BSD 3-Clause License\", \"GitHub Stars (June 2024)\": 184, \"GitHub Topics\": [], \"Github Date\": \"\", \"HF Config\": \"\", \"HF Config License\": \"\", \"HF Dataset\": \"\", \"HF Date\": \"\", \"HF Downloads (June 2024)\": \"\", \"HF Likes (June 2024)\": \"\", \"HF Yaml License\": \"\", \"PwC Date\": \"\", \"PwC Description\": \"HACS is a dataset for human action recognition. It uses a taxonomy of 200 action classes, which is identical to that of the ActivityNet-v1.3 dataset. It has 504K videos retrieved from YouTube. Each one is strictly shorter than 4 minutes, and the average length is 2.6 minutes. A total of 1.5M clips of 2-second duration are sparsely sampled by methods based on both uniform randomness and consensus/disagreement of image classifiers. 0.6M and 0.9M clips are annotated as positive and negative samples, respectively.\\n\\nAuthors split the collection into training, validation and testing sets of size 1.4M, 50K and 50K clips, which are sampled\\nfrom 492K, 6K and 6K videos, respectively.\", \"PwC License Name\": \"BSD 3-Clause License\", \"PwC License URL\": \"https://github.com/hangzhaomit/HACS-dataset/blob/master/LICENSE\", \"S2 Citation Count (June 2024)\": \"\", \"S2 Date\": \"\"}, \"Bibtex\": \"@Article{Zhao2017HACSHA,\\n author = {Hang Zhao and A. Torralba and L. Torresani and Zhicheng Yan},\\n booktitle = {IEEE International Conference on Computer Vision},\\n journal = {2019 IEEE/CVF International Conference on Computer Vision (ICCV)},\\n pages = {8667-8677},\\n title = {HACS: Human Action Clips and Segments Dataset for Recognition and Temporal Localization},\\n year = {2017}\\n}\\n\", \"License Use (DataProvenance)\": \"academic-only\", \"License Attribution (DataProvenance)\": 1, \"License Share Alike (DataProvenance)\": 0, \"License Use (DataProvenance IgnoreOpenAI)\": \"academic-only\", \"License Attribution (DataProvenance IgnoreOpenAI)\": 1, \"License Share Alike (DataProvenance IgnoreOpenAI)\": 0, \"License Type\": \"Non-Commercial/Academic\"}, {\"Unique Dataset Identifier\": \"hacs-dataset\", \"Collection\": \"hacs-dataset\", \"Collection URL\": \"https://arxiv.org/abs/1712.09374\", \"Dataset Name\": \"HACS: Human Action Clips and Segments Dataset for Recognition and Temporal Localization\", \"Paper Title\": \"HACS: Human Action Clips and Segments Dataset for Recognition and Temporal Localization\", \"Paper URL\": \"https://arxiv.org/abs/1712.09374\", \"GitHub URL\": \"https://github.com/hangzhaomit/HACS-dataset\", \"Hugging Face URL\": \"\", \"Papers with Code URL\": \"https://paperswithcode.com/dataset/hacs\", \"ArXiv URL\": \"https://arxiv.org/abs/1712.09374\", \"Semantic Scholar Corpus ID\": 68049510, \"Year Released\": \"2019\", \"Text Sources\": [\"youtube\"], \"Licenses\": [{\"License\": \"Custom\", \"License URL\": \"https://github.com/hangzhaomit/HACS-dataset?tab=readme-ov-file#request-testing-videos-and-missing-videos-new\"}], \"Creators\": \"Academic\", \"Countries\": [\"United States of America\"], \"License Verified By\": \"Vivek Sharma\", \"Video Hours\": 833.0, \"Taken Down\": \"False\", \"Video Sources\": [\"youtube\"], \"Task Categories\": [\"Video Classification\"], \"Inferred Metadata\": {\"GitHub License\": \"BSD 3-Clause License\", \"GitHub Stars (June 2024)\": 184, \"GitHub Topics\": [], \"Github Date\": \"\", \"HF Config\": \"\", \"HF Config License\": \"\", \"HF Dataset\": \"\", \"HF Date\": \"\", \"HF Downloads (June 2024)\": \"\", \"HF Likes (June 2024)\": \"\", \"HF Yaml License\": \"\", \"PwC Date\": \"\", \"PwC Description\": \"HACS is a dataset for human action recognition. It uses a taxonomy of 200 action classes, which is identical to that of the ActivityNet-v1.3 dataset. It has 504K videos retrieved from YouTube. Each one is strictly shorter than 4 minutes, and the average length is 2.6 minutes. A total of 1.5M clips of 2-second duration are sparsely sampled by methods based on both uniform randomness and consensus/disagreement of image classifiers. 0.6M and 0.9M clips are annotated as positive and negative samples, respectively.\\n\\nAuthors split the collection into training, validation and testing sets of size 1.4M, 50K and 50K clips, which are sampled\\nfrom 492K, 6K and 6K videos, respectively.\", \"PwC License Name\": \"BSD 3-Clause License\", \"PwC License URL\": \"https://github.com/hangzhaomit/HACS-dataset/blob/master/LICENSE\", \"S2 Citation Count (June 2024)\": \"\", \"S2 Date\": \"\"}, \"Bibtex\": \"@Article{Zhao2017HACSHA,\\n author = {Hang Zhao and A. Torralba and L. Torresani and Zhicheng Yan},\\n booktitle = {IEEE International Conference on Computer Vision},\\n journal = {2019 IEEE/CVF International Conference on Computer Vision (ICCV)},\\n pages = {8667-8677},\\n title = {HACS: Human Action Clips and Segments Dataset for Recognition and Temporal Localization},\\n year = {2017}\\n}\\n\", \"License Use (DataProvenance)\": \"academic-only\", \"License Attribution (DataProvenance)\": 1, \"License Share Alike (DataProvenance)\": 0, \"License Use (DataProvenance IgnoreOpenAI)\": \"academic-only\", \"License Attribution (DataProvenance IgnoreOpenAI)\": 1, \"License Share Alike (DataProvenance IgnoreOpenAI)\": 0, \"License Type\": \"Non-Commercial/Academic\"}, {\"Unique Dataset Identifier\": \"crosstask\", \"Collection\": \"crosstask\", \"Collection URL\": \"https://arxiv.org/abs/1903.08225\", \"Dataset Name\": \"CrossTask: weakly supervised learning from instructional videos (CVPR 2019)\", \"Paper Title\": \"CrossTask: weakly supervised learning from instructional videos (CVPR 2019)\", \"Paper URL\": \"https://arxiv.org/abs/1903.08225\", \"GitHub URL\": \"https://github.com/DmZhukov/CrossTask\", \"Hugging Face URL\": \"\", \"Papers with Code URL\": \"https://paperswithcode.com/dataset/crosstask\", \"ArXiv URL\": \"https://arxiv.org/abs/1903.08225\", \"Semantic Scholar Corpus ID\": 84187266, \"Year Released\": \"2019\", \"Text Sources\": [\"youtube\"], \"Licenses\": [{\"License\": \"Unspecified\", \"License URL\": \"\"}], \"Creators\": \"Academic\", \"Countries\": [\"France\", \"Turkey\", \"United States of America\", \"Czech Republic\"], \"License Verified By\": \"Vivek Sharma\", \"Video Hours\": 376.0, \"Taken Down\": \"False\", \"Video Sources\": [\"youtube\"], \"Task Categories\": [\"Temporal Action Localization\"], \"Inferred Metadata\": {\"GitHub License\": \"BSD 3-Clause License\", \"GitHub Stars (June 2024)\": 83, \"GitHub Topics\": [], \"Github Date\": \"\", \"HF Config\": \"\", \"HF Config License\": \"\", \"HF Dataset\": \"\", \"HF Date\": \"\", \"HF Downloads (June 2024)\": \"\", \"HF Likes (June 2024)\": \"\", \"HF Yaml License\": \"\", \"PwC Date\": \"2019-01-01\", \"PwC Description\": \"CrossTask dataset contains instructional videos, collected for 83 different tasks. For each task an ordered list of steps with manual descriptions is provided. The dataset is divided in two parts: 18 primary and 65 related tasks. Videos for the primary tasks are collected manually and provided with annotations for temporal step boundaries. Videos for the related tasks are collected automatically and don't have annotations.\", \"PwC License Name\": \"Unspecified\", \"PwC License URL\": null, \"S2 Citation Count (June 2024)\": \"\", \"S2 Date\": \"\"}, \"Bibtex\": \"@Article{Zhukov2019CrossTaskWS,\\n author = {Dimitri Zhukov and Jean-Baptiste Alayrac and R. G. Cinbis and D. Fouhey and I. Laptev and Josef Sivic},\\n booktitle = {Computer Vision and Pattern Recognition},\\n journal = {2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},\\n pages = {3532-3540},\\n title = {Cross-Task Weakly Supervised Learning From Instructional Videos},\\n year = {2019}\\n}\\n\", \"License Use (DataProvenance)\": \"unspecified\", \"License Attribution (DataProvenance)\": 0, \"License Share Alike (DataProvenance)\": 0, \"License Use (DataProvenance IgnoreOpenAI)\": \"unspecified\", \"License Attribution (DataProvenance IgnoreOpenAI)\": 0, \"License Share Alike (DataProvenance IgnoreOpenAI)\": 0, \"License Type\": \"Unspecified\"}, {\"Unique Dataset Identifier\": \"crosstask\", \"Collection\": \"crosstask\", \"Collection URL\": \"https://arxiv.org/abs/1903.08225\", \"Dataset Name\": \"CrossTask: weakly supervised learning from instructional videos (CVPR 2019)\", \"Paper Title\": \"CrossTask: weakly supervised learning from instructional videos (CVPR 2019)\", \"Paper URL\": \"https://arxiv.org/abs/1903.08225\", \"GitHub URL\": \"https://github.com/DmZhukov/CrossTask\", \"Hugging Face URL\": \"\", \"Papers with Code URL\": \"https://paperswithcode.com/dataset/crosstask\", \"ArXiv URL\": \"https://arxiv.org/abs/1903.08225\", \"Semantic Scholar Corpus ID\": 84187266, \"Year Released\": \"2019\", \"Text Sources\": [\"youtube\"], \"Licenses\": [{\"License\": \"Unspecified\", \"License URL\": \"\"}], \"Creators\": \"Academic\", \"Countries\": [\"France\", \"Turkey\", \"United States of America\", \"Czech Republic\"], \"License Verified By\": \"Vivek Sharma\", \"Video Hours\": 376.0, \"Taken Down\": \"False\", \"Video Sources\": [\"youtube\"], \"Task Categories\": [\"Temporal Action Localization\"], \"Inferred Metadata\": {\"GitHub License\": \"BSD 3-Clause License\", \"GitHub Stars (June 2024)\": 83, \"GitHub Topics\": [], \"Github Date\": \"\", \"HF Config\": \"\", \"HF Config License\": \"\", \"HF Dataset\": \"\", \"HF Date\": \"\", \"HF Downloads (June 2024)\": \"\", \"HF Likes (June 2024)\": \"\", \"HF Yaml License\": \"\", \"PwC Date\": \"2019-01-01\", \"PwC Description\": \"CrossTask dataset contains instructional videos, collected for 83 different tasks. For each task an ordered list of steps with manual descriptions is provided. The dataset is divided in two parts: 18 primary and 65 related tasks. Videos for the primary tasks are collected manually and provided with annotations for temporal step boundaries. Videos for the related tasks are collected automatically and don't have annotations.\", \"PwC License Name\": \"Unspecified\", \"PwC License URL\": null, \"S2 Citation Count (June 2024)\": \"\", \"S2 Date\": \"\"}, \"Bibtex\": \"@Article{Zhukov2019CrossTaskWS,\\n author = {Dimitri Zhukov and Jean-Baptiste Alayrac and R. G. Cinbis and D. Fouhey and I. Laptev and Josef Sivic},\\n booktitle = {Computer Vision and Pattern Recognition},\\n journal = {2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},\\n pages = {3532-3540},\\n title = {Cross-Task Weakly Supervised Learning From Instructional Videos},\\n year = {2019}\\n}\\n\", \"License Use (DataProvenance)\": \"unspecified\", \"License Attribution (DataProvenance)\": 0, \"License Share Alike (DataProvenance)\": 0, \"License Use (DataProvenance IgnoreOpenAI)\": \"unspecified\", \"License Attribution (DataProvenance IgnoreOpenAI)\": 0, \"License Share Alike (DataProvenance IgnoreOpenAI)\": 0, \"License Type\": \"Unspecified\"}, {\"Unique Dataset Identifier\": \"toyota-smarthome\", \"Collection\": \"toyota-smarthome\", \"Collection URL\": \"https://openaccess.thecvf.com/content_ICCV_2019/papers/Das_Toyota_Smarthome_Real-World_Activities_of_Daily_Living_ICCV_2019_paper.pdf\", \"Dataset Name\": \"Toyota Smarthome: Real-World Activities of Daily Living (ICCV 2019)\", \"Paper Title\": \"Toyota Smarthome: Real-World Activities of Daily Living (ICCV 2019)\", \"Paper URL\": \"https://openaccess.thecvf.com/content_ICCV_2019/papers/Das_Toyota_Smarthome_Real-World_Activities_of_Daily_Living_ICCV_2019_paper.pdf\", \"GitHub URL\": \"\", \"Hugging Face URL\": \"\", \"Papers with Code URL\": \"\", \"ArXiv URL\": \"https://openaccess.thecvf.com/content_ICCV_2019/papers/Das_Toyota_Smarthome_Real-World_Activities_of_Daily_Living_ICCV_2019_paper.pdf\", \"Semantic Scholar Corpus ID\": 207971208, \"Year Released\": \"2019\", \"Text Sources\": [\"human\"], \"Licenses\": [{\"License\": \"Custom\", \"License URL\": \"https://project.inria.fr/toyotasmarthome/files/2020/12/License_v2.pdf\"}], \"Creators\": \"Corporation\", \"Countries\": [\"France\"], \"License Verified By\": \"Vivek Sharma\", \"Video Hours\": 268.58, \"Taken Down\": \"False\", \"Video Sources\": [\"human\"], \"Task Categories\": [\"Video Classification\"], \"Inferred Metadata\": {\"GitHub License\": \"\", \"GitHub Stars (June 2024)\": \"\", \"GitHub Topics\": \"\", \"Github Date\": \"\", \"HF Config\": \"\", \"HF Config License\": \"\", \"HF Dataset\": \"\", \"HF Date\": \"\", \"HF Downloads (June 2024)\": \"\", \"HF Likes (June 2024)\": \"\", \"HF Yaml License\": \"\", \"PwC Date\": \"\", \"PwC Description\": \"\", \"PwC License Name\": \"\", \"PwC License URL\": \"\", \"S2 Citation Count (June 2024)\": \"\", \"S2 Date\": \"\"}, \"Bibtex\": \"@Article{Das2019ToyotaSR,\\n author = {Srijan Das and Rui Dai and Michal Koperski and Luca Minciullo and L. Garattoni and F. Br\\u00e9mond and G. Francesca},\\n booktitle = {IEEE International Conference on Computer Vision},\\n journal = {2019 IEEE/CVF International Conference on Computer Vision (ICCV)},\\n pages = {833-842},\\n title = {Toyota Smarthome: Real-World Activities of Daily Living},\\n year = {2019}\\n}\\n\", \"License Use (DataProvenance)\": \"academic-only\", \"License Attribution (DataProvenance)\": 1, \"License Share Alike (DataProvenance)\": 0, \"License Use (DataProvenance IgnoreOpenAI)\": \"academic-only\", \"License Attribution (DataProvenance IgnoreOpenAI)\": 1, \"License Share Alike (DataProvenance IgnoreOpenAI)\": 0, \"License Type\": \"Non-Commercial/Academic\"}, {\"Unique Dataset Identifier\": \"ava-dataset\", \"Collection\": \"ava-dataset\", \"Collection URL\": \"https://arxiv.org/abs/1901.01342\", \"Dataset Name\": \"AVA Active Speaker\", \"Paper Title\": \"AVA Active Speaker\", \"Paper URL\": \"https://arxiv.org/abs/1901.01342\", \"GitHub URL\": \"https://github.com/cvdfoundation/ava-dataset\", \"Hugging Face URL\": \"\", \"Papers with Code URL\": \"https://paperswithcode.com/dataset/ava-activespeaker\", \"ArXiv URL\": \"https://arxiv.org/abs/1901.01342\", \"Semantic Scholar Corpus ID\": 216211909, \"Year Released\": \"2019\", \"Text Sources\": [\"Not Prohibited\"], \"Licenses\": [{\"License\": \"CC BY 4.0\", \"License URL\": \"https://research.google.com/ava/download.html#ava_active_speaker_download\"}], \"Creators\": \"Industry Lab\", \"Countries\": [\"United States of America\"], \"License Verified By\": \"Vivek Sharma\", \"Video Hours\": 38.5, \"Taken Down\": \"False\", \"Video Sources\": [\"Not Prohibited\"], \"Task Categories\": [\"Video Classification\"], \"Inferred Metadata\": {\"GitHub License\": \"\", \"GitHub Stars (June 2024)\": 305, \"GitHub Topics\": [], \"Github Date\": \"\", \"HF Config\": \"\", \"HF Config License\": \"\", \"HF Dataset\": \"\", \"HF Date\": \"\", \"HF Downloads (June 2024)\": \"\", \"HF Likes (June 2024)\": \"\", \"HF Yaml License\": \"\", \"PwC Date\": \"\", \"PwC Description\": \"Contains temporally labeled face tracks in video, where each face instance is labeled as speaking or not, and whether the speech is audible. This dataset contains about 3.65 million human labeled frames or about 38.5 hours of face tracks, and the corresponding audio.\", \"PwC License Name\": \"Unspecified\", \"PwC License URL\": null, \"S2 Citation Count (June 2024)\": \"\", \"S2 Date\": \"\"}, \"Bibtex\": \"@Article{Roth2019AvaAS,\\n author = {Joseph Roth and Sourish Chaudhuri and Ondrej Klejch and Radhika Marvin and Andrew C. Gallagher and Liat Kaver and S. Ramaswamy and Arkadiusz Stopczynski and C. Schmid and Zhonghua Xi and C. Pantofaru},\\n booktitle = {IEEE International Conference on Acoustics, Speech, and Signal Processing},\\n journal = {ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},\\n pages = {4492-4496},\\n title = {Ava Active Speaker: An Audio-Visual Dataset for Active Speaker Detection},\\n year = {2019}\\n}\\n\", \"License Use (DataProvenance)\": \"commercial\", \"License Attribution (DataProvenance)\": 1, \"License Share Alike (DataProvenance)\": 0, \"License Use (DataProvenance IgnoreOpenAI)\": \"commercial\", \"License Attribution (DataProvenance IgnoreOpenAI)\": 1, \"License Share Alike (DataProvenance IgnoreOpenAI)\": 0, \"License Type\": \"Commercial\"}, {\"Unique Dataset Identifier\": \"mmact\", \"Collection\": \"mmact\", \"Collection URL\": \"https://ieeexplore.ieee.org/document/9009579\", \"Dataset Name\": \"MMAct\", \"Paper Title\": \"MMAct\", \"Paper URL\": \"https://ieeexplore.ieee.org/document/9009579\", \"GitHub URL\": \"\", \"Hugging Face URL\": \"\", \"Papers with Code URL\": \"https://paperswithcode.com/dataset/mmact\", \"ArXiv URL\": \"https://ieeexplore.ieee.org/document/9009579\", \"Semantic Scholar Corpus ID\": 207980205, \"Year Released\": \"2019\", \"Text Sources\": [\"human\"], \"Licenses\": [{\"License\": \"Custom\", \"License URL\": \"https://mmact19.github.io/2019/\"}], \"Creators\": \"Academic\", \"Countries\": [\"Hong Kong\", \"China\"], \"License Verified By\": \"Vivek Sharma\", \"Video Hours\": 100.0, \"Taken Down\": \"False\", \"Video Sources\": [\"human\"], \"Task Categories\": [\"Temporal Localization\", \"Action Recognition\", \"Spatial-Temporal Action Localization\"], \"Inferred Metadata\": {\"GitHub License\": \"\", \"GitHub Stars (June 2024)\": \"\", \"GitHub Topics\": \"\", \"Github Date\": \"\", \"HF Config\": \"\", \"HF Config License\": \"\", \"HF Dataset\": \"\", \"HF Date\": \"\", \"HF Downloads (June 2024)\": \"\", \"HF Likes (June 2024)\": \"\", \"HF Yaml License\": \"\", \"PwC Date\": \"\", \"PwC Description\": \"MMAct is a large-scale dataset for multi/cross modal action understanding. This dataset has been recorded from 20 distinct subjects with seven different types of modalities: RGB videos, keypoints, acceleration, gyroscope, orientation, Wi-Fi and pressure signal. The dataset consists of more than 36k video clips for 37 action classes covering a wide range of daily life activities such as desktop-related and check-in-based ones in four different distinct scenarios.\", \"PwC License Name\": \"Unspecified\", \"PwC License URL\": null, \"S2 Citation Count (June 2024)\": \"\", \"S2 Date\": \"\"}, \"Bibtex\": \"@Article{Kong2019MMActAL,\\n author = {Quan Kong and Ziming Wu and Ziwei Deng and Martin Klinkigt and Bin Tong and Tomokazu Murakami},\\n booktitle = {IEEE International Conference on Computer Vision},\\n journal = {2019 IEEE/CVF International Conference on Computer Vision (ICCV)},\\n pages = {8657-8666},\\n title = {MMAct: A Large-Scale Dataset for Cross Modal Human Action Understanding},\\n year = {2019}\\n}\\n\", \"License Use (DataProvenance)\": \"academic-only\", \"License Attribution (DataProvenance)\": 1, \"License Share Alike (DataProvenance)\": 0, \"License Use (DataProvenance IgnoreOpenAI)\": \"academic-only\", \"License Attribution (DataProvenance IgnoreOpenAI)\": 1, \"License Share Alike (DataProvenance IgnoreOpenAI)\": 0, \"License Type\": \"Non-Commercial/Academic\"}, {\"Unique Dataset Identifier\": \"howto100m\", \"Collection\": \"howto100m\", \"Collection URL\": \"https://arxiv.org/abs/1906.03327\", \"Dataset Name\": \"HowTo100M\", \"Paper Title\": \"HowTo100M\", \"Paper URL\": \"https://arxiv.org/abs/1906.03327\", \"GitHub URL\": \"https://github.com/antoine77340/howto100m\", \"Hugging Face URL\": \"\", \"Papers with Code URL\": \"https://paperswithcode.com/dataset/howto100m\", \"ArXiv URL\": \"https://arxiv.org/abs/1906.03327\", \"Semantic Scholar Corpus ID\": 182952863, \"Year Released\": \"2019\", \"Text Sources\": [\"youtube\"], \"Licenses\": [{\"License\": \"Unspecified\", \"License URL\": \"\"}], \"Creators\": \"Academic\", \"Countries\": [\"France\", \"Czech Republic\"], \"License Verified By\": \"Vivek Sharma\", \"Video Hours\": 134472.0, \"Taken Down\": \"False\", \"Video Sources\": [\"youtube\"], \"Task Categories\": [\"Video Captioning\"], \"Inferred Metadata\": {\"GitHub License\": \"Apache License 2.0\", \"GitHub Stars (June 2024)\": 242, \"GitHub Topics\": [], \"Github Date\": \"\", \"HF Config\": \"\", \"HF Config License\": \"\", \"HF Dataset\": \"\", \"HF Date\": \"\", \"HF Downloads (June 2024)\": \"\", \"HF Likes (June 2024)\": \"\", \"HF Yaml License\": \"\", \"PwC Date\": \"2019-06-07\", \"PwC Description\": \"HowTo100M is a large-scale dataset of narrated videos with an emphasis on instructional videos where content creators teach complex tasks with an explicit intention of explaining the visual content on screen. HowTo100M features a total of:\\n\\n\\n136M video clips with captions sourced from 1.2M Youtube videos (15 years of video)\\n23k activities from domains such as cooking, hand crafting, personal care, gardening or fitness\\n\\nEach video is associated with a narration available as subtitles automatically downloaded from Youtube.\", \"PwC License Name\": \"Various\", \"PwC License URL\": \"https://www.di.ens.fr/willow/research/howto100m/\", \"S2 Citation Count (June 2024)\": \"\", \"S2 Date\": \"\"}, \"Bibtex\": \"@Article{Miech2019HowTo100MLA,\\n author = {Antoine Miech and Dimitri Zhukov and Jean-Baptiste Alayrac and Makarand Tapaswi and I. Laptev and Josef Sivic},\\n booktitle = {IEEE International Conference on Computer Vision},\\n journal = {2019 IEEE/CVF International Conference on Computer Vision (ICCV)},\\n pages = {2630-2640},\\n title = {HowTo100M: Learning a Text-Video Embedding by Watching Hundred Million Narrated Video Clips},\\n year = {2019}\\n}\\n\", \"License Use (DataProvenance)\": \"unspecified\", \"License Attribution (DataProvenance)\": 0, \"License Share Alike (DataProvenance)\": 0, \"License Use (DataProvenance IgnoreOpenAI)\": \"unspecified\", \"License Attribution (DataProvenance IgnoreOpenAI)\": 0, \"License Share Alike (DataProvenance IgnoreOpenAI)\": 0, \"License Type\": \"Unspecified\"}, {\"Unique Dataset Identifier\": \"howto100m\", \"Collection\": \"howto100m\", \"Collection URL\": \"https://arxiv.org/abs/1906.03327\", \"Dataset Name\": \"HowTo100M\", \"Paper Title\": \"HowTo100M\", \"Paper URL\": \"https://arxiv.org/abs/1906.03327\", \"GitHub URL\": \"https://github.com/antoine77340/howto100m\", \"Hugging Face URL\": \"\", \"Papers with Code URL\": \"https://paperswithcode.com/dataset/howto100m\", \"ArXiv URL\": \"https://arxiv.org/abs/1906.03327\", \"Semantic Scholar Corpus ID\": 182952863, \"Year Released\": \"2019\", \"Text Sources\": [\"youtube\"], \"Licenses\": [{\"License\": \"Unspecified\", \"License URL\": \"\"}], \"Creators\": \"Other\", \"Countries\": [\"France\", \"Czech Republic\"], \"License Verified By\": \"Vivek Sharma\", \"Video Hours\": 134472.0, \"Taken Down\": \"False\", \"Video Sources\": [\"youtube\"], \"Task Categories\": [\"Video Captioning\"], \"Inferred Metadata\": {\"GitHub License\": \"Apache License 2.0\", \"GitHub Stars (June 2024)\": 242, \"GitHub Topics\": [], \"Github Date\": \"\", \"HF Config\": \"\", \"HF Config License\": \"\", \"HF Dataset\": \"\", \"HF Date\": \"\", \"HF Downloads (June 2024)\": \"\", \"HF Likes (June 2024)\": \"\", \"HF Yaml License\": \"\", \"PwC Date\": \"2019-06-07\", \"PwC Description\": \"HowTo100M is a large-scale dataset of narrated videos with an emphasis on instructional videos where content creators teach complex tasks with an explicit intention of explaining the visual content on screen. HowTo100M features a total of:\\n\\n\\n136M video clips with captions sourced from 1.2M Youtube videos (15 years of video)\\n23k activities from domains such as cooking, hand crafting, personal care, gardening or fitness\\n\\nEach video is associated with a narration available as subtitles automatically downloaded from Youtube.\", \"PwC License Name\": \"Various\", \"PwC License URL\": \"https://www.di.ens.fr/willow/research/howto100m/\", \"S2 Citation Count (June 2024)\": \"\", \"S2 Date\": \"\"}, \"Bibtex\": \"@Article{Miech2019HowTo100MLA,\\n author = {Antoine Miech and Dimitri Zhukov and Jean-Baptiste Alayrac and Makarand Tapaswi and I. Laptev and Josef Sivic},\\n booktitle = {IEEE International Conference on Computer Vision},\\n journal = {2019 IEEE/CVF International Conference on Computer Vision (ICCV)},\\n pages = {2630-2640},\\n title = {HowTo100M: Learning a Text-Video Embedding by Watching Hundred Million Narrated Video Clips},\\n year = {2019}\\n}\\n\", \"License Use (DataProvenance)\": \"unspecified\", \"License Attribution (DataProvenance)\": 0, \"License Share Alike (DataProvenance)\": 0, \"License Use (DataProvenance IgnoreOpenAI)\": \"unspecified\", \"License Attribution (DataProvenance IgnoreOpenAI)\": 0, \"License Share Alike (DataProvenance IgnoreOpenAI)\": 0, \"License Type\": \"Unspecified\"}, {\"Unique Dataset Identifier\": \"howto100m\", \"Collection\": \"howto100m\", \"Collection URL\": \"https://arxiv.org/abs/1906.03327\", \"Dataset Name\": \"HowTo100M\", \"Paper Title\": \"HowTo100M\", \"Paper URL\": \"https://arxiv.org/abs/1906.03327\", \"GitHub URL\": \"https://github.com/antoine77340/howto100m\", \"Hugging Face URL\": \"\", \"Papers with Code URL\": \"https://paperswithcode.com/dataset/howto100m\", \"ArXiv URL\": \"https://arxiv.org/abs/1906.03327\", \"Semantic Scholar Corpus ID\": 182952863, \"Year Released\": \"2019\", \"Text Sources\": [\"youtube\"], \"Licenses\": [{\"License\": \"Unspecified\", \"License URL\": \"\"}], \"Creators\": \"Research Group\", \"Countries\": [\"France\", \"Czech Republic\"], \"License Verified By\": \"Vivek Sharma\", \"Video Hours\": 134472.0, \"Taken Down\": \"False\", \"Video Sources\": [\"youtube\"], \"Task Categories\": [\"Video Captioning\"], \"Inferred Metadata\": {\"GitHub License\": \"Apache License 2.0\", \"GitHub Stars (June 2024)\": 242, \"GitHub Topics\": [], \"Github Date\": \"\", \"HF Config\": \"\", \"HF Config License\": \"\", \"HF Dataset\": \"\", \"HF Date\": \"\", \"HF Downloads (June 2024)\": \"\", \"HF Likes (June 2024)\": \"\", \"HF Yaml License\": \"\", \"PwC Date\": \"2019-06-07\", \"PwC Description\": \"HowTo100M is a large-scale dataset of narrated videos with an emphasis on instructional videos where content creators teach complex tasks with an explicit intention of explaining the visual content on screen. HowTo100M features a total of:\\n\\n\\n136M video clips with captions sourced from 1.2M Youtube videos (15 years of video)\\n23k activities from domains such as cooking, hand crafting, personal care, gardening or fitness\\n\\nEach video is associated with a narration available as subtitles automatically downloaded from Youtube.\", \"PwC License Name\": \"Various\", \"PwC License URL\": \"https://www.di.ens.fr/willow/research/howto100m/\", \"S2 Citation Count (June 2024)\": \"\", \"S2 Date\": \"\"}, \"Bibtex\": \"@Article{Miech2019HowTo100MLA,\\n author = {Antoine Miech and Dimitri Zhukov and Jean-Baptiste Alayrac and Makarand Tapaswi and I. Laptev and Josef Sivic},\\n booktitle = {IEEE International Conference on Computer Vision},\\n journal = {2019 IEEE/CVF International Conference on Computer Vision (ICCV)},\\n pages = {2630-2640},\\n title = {HowTo100M: Learning a Text-Video Embedding by Watching Hundred Million Narrated Video Clips},\\n year = {2019}\\n}\\n\", \"License Use (DataProvenance)\": \"unspecified\", \"License Attribution (DataProvenance)\": 0, \"License Share Alike (DataProvenance)\": 0, \"License Use (DataProvenance IgnoreOpenAI)\": \"unspecified\", \"License Attribution (DataProvenance IgnoreOpenAI)\": 0, \"License Share Alike (DataProvenance IgnoreOpenAI)\": 0, \"License Type\": \"Unspecified\"}, {\"Unique Dataset Identifier\": \"msa\", \"Collection\": \"msa\", \"Collection URL\": \"https://arxiv.org/abs/1910.11009\", \"Dataset Name\": \"MSA\", \"Paper Title\": \"MSA\", \"Paper URL\": \"https://arxiv.org/abs/1910.11009\", \"GitHub URL\": \"https://github.com/ycxioooong/MovieSynopsisAssociation\", \"Hugging Face URL\": \"\", \"Papers with Code URL\": \"https://paperswithcode.com/paper/a-graph-based-framework-to-bridge-movies-and-1\", \"ArXiv URL\": \"https://arxiv.org/abs/1910.11009\", \"Semantic Scholar Corpus ID\": 204852218, \"Year Released\": \"2019\", \"Text Sources\": [\"movies\"], \"Licenses\": [{\"License\": \"Unspecified\", \"License URL\": \"\"}], \"Creators\": \"Academic\", \"Countries\": [\"China\", \"United States of America\"], \"License Verified By\": \"Vivek Sharma\", \"Video Hours\": 516.0, \"Taken Down\": \"False\", \"Video Sources\": [\"movies\"], \"Task Categories\": [\"Video Summarization\"], \"Inferred Metadata\": {\"GitHub License\": \"\", \"GitHub Stars (June 2024)\": 52, \"GitHub Topics\": [], \"Github Date\": \"\", \"HF Config\": \"\", \"HF Config License\": \"\", \"HF Dataset\": \"\", \"HF Date\": \"\", \"HF Downloads (June 2024)\": \"\", \"HF Likes (June 2024)\": \"\", \"HF Yaml License\": \"\", \"PwC Date\": \"\", \"PwC Description\": \"No code available yet.\", \"PwC License Name\": \"\", \"PwC License URL\": \"\", \"S2 Citation Count (June 2024)\": \"\", \"S2 Date\": \"\"}, \"Bibtex\": \"@Article{Xiong2019AGF,\\n author = {Yu Xiong and Qingqiu Huang and Lingfeng Guo and Hang Zhou and Bolei Zhou and Dahua Lin},\\n booktitle = {IEEE International Conference on Computer Vision},\\n journal = {2019 IEEE/CVF International Conference on Computer Vision (ICCV)},\\n pages = {4591-4600},\\n title = {A Graph-Based Framework to Bridge Movies and Synopses},\\n year = {2019}\\n}\\n\", \"License Use (DataProvenance)\": \"unspecified\", \"License Attribution (DataProvenance)\": 0, \"License Share Alike (DataProvenance)\": 0, \"License Use (DataProvenance IgnoreOpenAI)\": \"unspecified\", \"License Attribution (DataProvenance IgnoreOpenAI)\": 0, \"License Share Alike (DataProvenance IgnoreOpenAI)\": 0, \"License Type\": \"Unspecified\"}, {\"Unique Dataset Identifier\": \"msa\", \"Collection\": \"msa\", \"Collection URL\": \"https://arxiv.org/abs/1910.11009\", \"Dataset Name\": \"MSA\", \"Paper Title\": \"MSA\", \"Paper URL\": \"https://arxiv.org/abs/1910.11009\", \"GitHub URL\": \"https://github.com/ycxioooong/MovieSynopsisAssociation\", \"Hugging Face URL\": \"\", \"Papers with Code URL\": \"https://paperswithcode.com/paper/a-graph-based-framework-to-bridge-movies-and-1\", \"ArXiv URL\": \"https://arxiv.org/abs/1910.11009\", \"Semantic Scholar Corpus ID\": 204852218, \"Year Released\": \"2019\", \"Text Sources\": [\"movies\"], \"Licenses\": [{\"License\": \"Unspecified\", \"License URL\": \"\"}], \"Creators\": \"Academic\", \"Countries\": [\"China\", \"United States of America\"], \"License Verified By\": \"Vivek Sharma\", \"Video Hours\": 516.0, \"Taken Down\": \"False\", \"Video Sources\": [\"movies\"], \"Task Categories\": [\"Video Summarization\"], \"Inferred Metadata\": {\"GitHub License\": \"\", \"GitHub Stars (June 2024)\": 52, \"GitHub Topics\": [], \"Github Date\": \"\", \"HF Config\": \"\", \"HF Config License\": \"\", \"HF Dataset\": \"\", \"HF Date\": \"\", \"HF Downloads (June 2024)\": \"\", \"HF Likes (June 2024)\": \"\", \"HF Yaml License\": \"\", \"PwC Date\": \"\", \"PwC Description\": \"No code available yet.\", \"PwC License Name\": \"\", \"PwC License URL\": \"\", \"S2 Citation Count (June 2024)\": \"\", \"S2 Date\": \"\"}, \"Bibtex\": \"@Article{Xiong2019AGF,\\n author = {Yu Xiong and Qingqiu Huang and Lingfeng Guo and Hang Zhou and Bolei Zhou and Dahua Lin},\\n booktitle = {IEEE International Conference on Computer Vision},\\n journal = {2019 IEEE/CVF International Conference on Computer Vision (ICCV)},\\n pages = {4591-4600},\\n title = {A Graph-Based Framework to Bridge Movies and Synopses},\\n year = {2019}\\n}\\n\", \"License Use (DataProvenance)\": \"unspecified\", \"License Attribution (DataProvenance)\": 0, \"License Share Alike (DataProvenance)\": 0, \"License Use (DataProvenance IgnoreOpenAI)\": \"unspecified\", \"License Attribution (DataProvenance IgnoreOpenAI)\": 0, \"License Share Alike (DataProvenance IgnoreOpenAI)\": 0, \"License Type\": \"Unspecified\"}, {\"Unique Dataset Identifier\": \"howto100m\", \"Collection\": \"howto100m\", \"Collection URL\": \"https://arxiv.org/abs/1906.03327\", \"Dataset Name\": \"HowTo100M\", \"Paper Title\": \"HowTo100M\", \"Paper URL\": \"https://arxiv.org/abs/1906.03327\", \"GitHub URL\": \"https://github.com/antoine77340/howto100m\", \"Hugging Face URL\": \"\", \"Papers with Code URL\": \"https://paperswithcode.com/dataset/howto100m\", \"ArXiv URL\": \"https://arxiv.org/abs/1906.03327\", \"Semantic Scholar Corpus ID\": 182952863, \"Year Released\": \"2019\", \"Text Sources\": [\"youtube\"], \"Licenses\": [{\"License\": \"Unspecified\", \"License URL\": \"\"}], \"Creators\": \"Academic\", \"Countries\": [\"France\", \"Czech Republic\"], \"License Verified By\": \"Vivek Sharma\", \"Video Hours\": 134472.0, \"Taken Down\": \"False\", \"Video Sources\": [\"youtube\"], \"Task Categories\": [\"Video Captioning\"], \"Inferred Metadata\": {\"GitHub License\": \"Apache License 2.0\", \"GitHub Stars (June 2024)\": 242, \"GitHub Topics\": [], \"Github Date\": \"\", \"HF Config\": \"\", \"HF Config License\": \"\", \"HF Dataset\": \"\", \"HF Date\": \"\", \"HF Downloads (June 2024)\": \"\", \"HF Likes (June 2024)\": \"\", \"HF Yaml License\": \"\", \"PwC Date\": \"2019-06-07\", \"PwC Description\": \"HowTo100M is a large-scale dataset of narrated videos with an emphasis on instructional videos where content creators teach complex tasks with an explicit intention of explaining the visual content on screen. HowTo100M features a total of:\\n\\n\\n136M video clips with captions sourced from 1.2M Youtube videos (15 years of video)\\n23k activities from domains such as cooking, hand crafting, personal care, gardening or fitness\\n\\nEach video is associated with a narration available as subtitles automatically downloaded from Youtube.\", \"PwC License Name\": \"Various\", \"PwC License URL\": \"https://www.di.ens.fr/willow/research/howto100m/\", \"S2 Citation Count (June 2024)\": \"\", \"S2 Date\": \"\"}, \"Bibtex\": \"@Article{Miech2019HowTo100MLA,\\n author = {Antoine Miech and Dimitri Zhukov and Jean-Baptiste Alayrac and Makarand Tapaswi and I. Laptev and Josef Sivic},\\n booktitle = {IEEE International Conference on Computer Vision},\\n journal = {2019 IEEE/CVF International Conference on Computer Vision (ICCV)},\\n pages = {2630-2640},\\n title = {HowTo100M: Learning a Text-Video Embedding by Watching Hundred Million Narrated Video Clips},\\n year = {2019}\\n}\\n\", \"License Use (DataProvenance)\": \"unspecified\", \"License Attribution (DataProvenance)\": 0, \"License Share Alike (DataProvenance)\": 0, \"License Use (DataProvenance IgnoreOpenAI)\": \"unspecified\", \"License Attribution (DataProvenance IgnoreOpenAI)\": 0, \"License Share Alike (DataProvenance IgnoreOpenAI)\": 0, \"License Type\": \"Unspecified\"}, {\"Unique Dataset Identifier\": \"coin-dataset\", \"Collection\": \"coin-dataset\", \"Collection URL\": \"https://arxiv.org/abs/1903.02874\", \"Dataset Name\": \"COIN: A Large-scale Dataset for Comprehensive Instructional Video Analysis (CVPR 2019)\", \"Paper Title\": \"COIN: A Large-scale Dataset for Comprehensive Instructional Video Analysis (CVPR 2019)\", \"Paper URL\": \"https://arxiv.org/abs/1903.02874\", \"GitHub URL\": \"https://github.com/coin-dataset/annotations\", \"Hugging Face URL\": \"\", \"Papers with Code URL\": \"https://paperswithcode.com/dataset/coin#:~:text=The%20COIN%20dataset%20(a%20large,are%20all%20collected%20from%20YouTube.\", \"ArXiv URL\": \"https://arxiv.org/abs/1903.02874\", \"Semantic Scholar Corpus ID\": 71147568, \"Year Released\": \"2019\", \"Text Sources\": [\"youtube\"], \"Licenses\": [{\"License\": \"Custom\", \"License URL\": \"https://github.com/coin-dataset/annotations?tab=readme-ov-file#license\"}], \"Creators\": \"Corporation\", \"Countries\": [\"China\"], \"License Verified By\": \"Vivek Sharma\", \"Video Hours\": 476.0, \"Taken Down\": \"False\", \"Video Sources\": [\"youtube\"], \"Task Categories\": [\"Video Classification\"], \"Inferred Metadata\": {\"GitHub License\": \"\", \"GitHub Stars (June 2024)\": 100, \"GitHub Topics\": [], \"Github Date\": \"\", \"HF Config\": \"\", \"HF Config License\": \"\", \"HF Dataset\": \"\", \"HF Date\": \"\", \"HF Downloads (June 2024)\": \"\", \"HF Likes (June 2024)\": \"\", \"HF Yaml License\": \"\", \"PwC Date\": \"\", \"PwC Description\": \"The COIN dataset (a large-scale dataset for COmprehensive INstructional video analysis) consists of 11,827 videos related to 180 different tasks in 12 domains (e.g., vehicles, gadgets, etc.) related to our daily life. The videos are all collected from YouTube. The average length of a video is 2.36 minutes. Each video is labelled with 3.91 step segments, where each segment lasts 14.91 seconds on average. In total, the dataset contains videos of 476 hours, with 46,354 annotated segments.\", \"PwC License Name\": \"Various\", \"PwC License URL\": \"https://coin-dataset.github.io/\", \"S2 Citation Count (June 2024)\": \"\", \"S2 Date\": \"\"}, \"Bibtex\": \"@Article{Tang2019COINAL,\\n author = {Yansong Tang and Dajun Ding and Yongming Rao and Yu Zheng and Danyang Zhang and Lili Zhao and Jiwen Lu and Jie Zhou},\\n booktitle = {Computer Vision and Pattern Recognition},\\n journal = {2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},\\n pages = {1207-1216},\\n title = {COIN: A Large-Scale Dataset for Comprehensive Instructional Video Analysis},\\n year = {2019}\\n}\\n\", \"License Use (DataProvenance)\": \"academic-only\", \"License Attribution (DataProvenance)\": 1, \"License Share Alike (DataProvenance)\": 0, \"License Use (DataProvenance IgnoreOpenAI)\": \"academic-only\", \"License Attribution (DataProvenance IgnoreOpenAI)\": 1, \"License Share Alike (DataProvenance IgnoreOpenAI)\": 0, \"License Type\": \"Non-Commercial/Academic\"}, {\"Unique Dataset Identifier\": \"coin-dataset\", \"Collection\": \"coin-dataset\", \"Collection URL\": \"https://arxiv.org/abs/1903.02874\", \"Dataset Name\": \"COIN: A Large-scale Dataset for Comprehensive Instructional Video Analysis (CVPR 2019)\", \"Paper Title\": \"COIN: A Large-scale Dataset for Comprehensive Instructional Video Analysis (CVPR 2019)\", \"Paper URL\": \"https://arxiv.org/abs/1903.02874\", \"GitHub URL\": \"https://github.com/coin-dataset/annotations\", \"Hugging Face URL\": \"\", \"Papers with Code URL\": \"https://paperswithcode.com/dataset/coin#:~:text=The%20COIN%20dataset%20(a%20large,are%20all%20collected%20from%20YouTube.\", \"ArXiv URL\": \"https://arxiv.org/abs/1903.02874\", \"Semantic Scholar Corpus ID\": 71147568, \"Year Released\": \"2019\", \"Text Sources\": [\"youtube\"], \"Licenses\": [{\"License\": \"Custom\", \"License URL\": \"https://github.com/coin-dataset/annotations?tab=readme-ov-file#license\"}], \"Creators\": \"Academic\", \"Countries\": [\"China\"], \"License Verified By\": \"Vivek Sharma\", \"Video Hours\": 476.0, \"Taken Down\": \"False\", \"Video Sources\": [\"youtube\"], \"Task Categories\": [\"Video Classification\"], \"Inferred Metadata\": {\"GitHub License\": \"\", \"GitHub Stars (June 2024)\": 100, \"GitHub Topics\": [], \"Github Date\": \"\", \"HF Config\": \"\", \"HF Config License\": \"\", \"HF Dataset\": \"\", \"HF Date\": \"\", \"HF Downloads (June 2024)\": \"\", \"HF Likes (June 2024)\": \"\", \"HF Yaml License\": \"\", \"PwC Date\": \"\", \"PwC Description\": \"The COIN dataset (a large-scale dataset for COmprehensive INstructional video analysis) consists of 11,827 videos related to 180 different tasks in 12 domains (e.g., vehicles, gadgets, etc.) related to our daily life. The videos are all collected from YouTube. The average length of a video is 2.36 minutes. Each video is labelled with 3.91 step segments, where each segment lasts 14.91 seconds on average. In total, the dataset contains videos of 476 hours, with 46,354 annotated segments.\", \"PwC License Name\": \"Various\", \"PwC License URL\": \"https://coin-dataset.github.io/\", \"S2 Citation Count (June 2024)\": \"\", \"S2 Date\": \"\"}, \"Bibtex\": \"@Article{Tang2019COINAL,\\n author = {Yansong Tang and Dajun Ding and Yongming Rao and Yu Zheng and Danyang Zhang and Lili Zhao and Jiwen Lu and Jie Zhou},\\n booktitle = {Computer Vision and Pattern Recognition},\\n journal = {2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},\\n pages = {1207-1216},\\n title = {COIN: A Large-Scale Dataset for Comprehensive Instructional Video Analysis},\\n year = {2019}\\n}\\n\", \"License Use (DataProvenance)\": \"academic-only\", \"License Attribution (DataProvenance)\": 1, \"License Share Alike (DataProvenance)\": 0, \"License Use (DataProvenance IgnoreOpenAI)\": \"academic-only\", \"License Attribution (DataProvenance IgnoreOpenAI)\": 1, \"License Share Alike (DataProvenance IgnoreOpenAI)\": 0, \"License Type\": \"Non-Commercial/Academic\"}, {\"Unique Dataset Identifier\": \"crosstask\", \"Collection\": \"crosstask\", \"Collection URL\": \"https://arxiv.org/abs/1903.08225\", \"Dataset Name\": \"CrossTask: weakly supervised learning from instructional videos (CVPR 2019)\", \"Paper Title\": \"CrossTask: weakly supervised learning from instructional videos (CVPR 2019)\", \"Paper URL\": \"https://arxiv.org/abs/1903.08225\", \"GitHub URL\": \"https://github.com/DmZhukov/CrossTask\", \"Hugging Face URL\": \"\", \"Papers with Code URL\": \"https://paperswithcode.com/dataset/crosstask\", \"ArXiv URL\": \"https://arxiv.org/abs/1903.08225\", \"Semantic Scholar Corpus ID\": 84187266, \"Year Released\": \"2019\", \"Text Sources\": [\"youtube\"], \"Licenses\": [{\"License\": \"Unspecified\", \"License URL\": \"\"}], \"Creators\": \"Research Group\", \"Countries\": [\"France\", \"Turkey\", \"United States of America\", \"Czech Republic\"], \"License Verified By\": \"Vivek Sharma\", \"Video Hours\": 376.0, \"Taken Down\": \"False\", \"Video Sources\": [\"youtube\"], \"Task Categories\": [\"Temporal Action Localization\"], \"Inferred Metadata\": {\"GitHub License\": \"BSD 3-Clause License\", \"GitHub Stars (June 2024)\": 83, \"GitHub Topics\": [], \"Github Date\": \"\", \"HF Config\": \"\", \"HF Config License\": \"\", \"HF Dataset\": \"\", \"HF Date\": \"\", \"HF Downloads (June 2024)\": \"\", \"HF Likes (June 2024)\": \"\", \"HF Yaml License\": \"\", \"PwC Date\": \"2019-01-01\", \"PwC Description\": \"CrossTask dataset contains instructional videos, collected for 83 different tasks. For each task an ordered list of steps with manual descriptions is provided. The dataset is divided in two parts: 18 primary and 65 related tasks. Videos for the primary tasks are collected manually and provided with annotations for temporal step boundaries. Videos for the related tasks are collected automatically and don't have annotations.\", \"PwC License Name\": \"Unspecified\", \"PwC License URL\": null, \"S2 Citation Count (June 2024)\": \"\", \"S2 Date\": \"\"}, \"Bibtex\": \"@Article{Zhukov2019CrossTaskWS,\\n author = {Dimitri Zhukov and Jean-Baptiste Alayrac and R. G. Cinbis and D. Fouhey and I. Laptev and Josef Sivic},\\n booktitle = {Computer Vision and Pattern Recognition},\\n journal = {2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},\\n pages = {3532-3540},\\n title = {Cross-Task Weakly Supervised Learning From Instructional Videos},\\n year = {2019}\\n}\\n\", \"License Use (DataProvenance)\": \"unspecified\", \"License Attribution (DataProvenance)\": 0, \"License Share Alike (DataProvenance)\": 0, \"License Use (DataProvenance IgnoreOpenAI)\": \"unspecified\", \"License Attribution (DataProvenance IgnoreOpenAI)\": 0, \"License Share Alike (DataProvenance IgnoreOpenAI)\": 0, \"License Type\": \"Unspecified\"}, {\"Unique Dataset Identifier\": \"hyu-vids\", \"Collection\": \"hyu-vids\", \"Collection URL\": \"https://arxiv.org/abs/1904.11451\", \"Dataset Name\": \"HVU: Large Scale Holistic Video Understanding (ECCV 2020)\", \"Paper Title\": \"HVU: Large Scale Holistic Video Understanding (ECCV 2020)\", \"Paper URL\": \"https://arxiv.org/abs/1904.11451\", \"GitHub URL\": \"https://github.com/holistic-video-understanding/HVU-Dataset\", \"Hugging Face URL\": \"\", \"Papers with Code URL\": \"https://paperswithcode.com/paper/holistic-large-scale-video-understanding\", \"ArXiv URL\": \"https://arxiv.org/abs/1904.11451\", \"Semantic Scholar Corpus ID\": 131777079, \"Year Released\": \"2020\", \"Text Sources\": [\"youtube\"], \"Licenses\": [{\"License\": \"Custom\", \"License URL\": \"https://github.com/holistic-video-understanding/HVU-Dataset\"}], \"Creators\": \"Academic\", \"Countries\": [\"Switzerland\", \"Belgium\", \"Germany\"], \"License Verified By\": \"Vivek Sharma\", \"Video Hours\": 96166.67, \"Taken Down\": \"False\", \"Video Sources\": [\"youtube\"], \"Task Categories\": [\"Video Classification\"], \"Inferred Metadata\": {\"GitHub License\": \"\", \"GitHub Stars (June 2024)\": 70, \"GitHub Topics\": [], \"Github Date\": \"\", \"HF Config\": \"\", \"HF Config License\": \"\", \"HF Dataset\": \"\", \"HF Date\": \"\", \"HF Downloads (June 2024)\": \"\", \"HF Likes (June 2024)\": \"\", \"HF Yaml License\": \"\", \"PwC Date\": \"\", \"PwC Description\": \"#11 best model for Action Recognition on UCF101 (3-fold Accuracy metric)\", \"PwC License Name\": \"\", \"PwC License URL\": \"\", \"S2 Citation Count (June 2024)\": \"\", \"S2 Date\": \"\"}, \"Bibtex\": \"@Article{Diba2019HolisticLS,\\n author = {Ali Diba and Mohsen Fayyaz and Vivek Sharma and Manohar Paluri and Juergen Gall and R. Stiefelhagen and L. Gool},\\n booktitle = {arXiv.org},\\n journal = {ArXiv},\\n title = {Holistic Large Scale Video Understanding},\\n volume = {abs/1904.11451},\\n year = {2019}\\n}\\n\", \"License Use (DataProvenance)\": \"academic-only\", \"License Attribution (DataProvenance)\": 1, \"License Share Alike (DataProvenance)\": 0, \"License Use (DataProvenance IgnoreOpenAI)\": \"academic-only\", \"License Attribution (DataProvenance IgnoreOpenAI)\": 1, \"License Share Alike (DataProvenance IgnoreOpenAI)\": 0, \"License Type\": \"Non-Commercial/Academic\"}, {\"Unique Dataset Identifier\": \"rare-act-dataset\", \"Collection\": \"rare-act-dataset\", \"Collection URL\": \"https://arxiv.org/pdf/2008.01018.pdf\", \"Dataset Name\": \"RareAct: A video dataset of unusual interactions\", \"Paper Title\": \"RareAct: A video dataset of unusual interactions\", \"Paper URL\": \"https://arxiv.org/pdf/2008.01018.pdf\", \"GitHub URL\": \"https://github.com/antoine77340/RareAct\", \"Hugging Face URL\": \"\", \"Papers with Code URL\": \"https://paperswithcode.com/dataset/rareact\", \"ArXiv URL\": \"https://arxiv.org/pdf/2008.01018.pdf\", \"Semantic Scholar Corpus ID\": 220936243, \"Year Released\": \"2020\", \"Text Sources\": [\"undisclosed web\"], \"Licenses\": [{\"License\": \"Unspecified\", \"License URL\": \"\"}], \"Creators\": \"Academic\", \"Countries\": [\"United Kingdom\", \"France\", \"Czech Republic\"], \"License Verified By\": \"Vivek Sharma\", \"Video Hours\": 21.13, \"Taken Down\": \"False\", \"Video Sources\": [\"undisclosed web\"], \"Task Categories\": [\"Video Classification\"], \"Inferred Metadata\": {\"GitHub License\": \"Apache License 2.0\", \"GitHub Stars (June 2024)\": 32, \"GitHub Topics\": [], \"Github Date\": \"\", \"HF Config\": \"\", \"HF Config License\": \"\", \"HF Dataset\": \"\", \"HF Date\": \"\", \"HF Downloads (June 2024)\": \"\", \"HF Likes (June 2024)\": \"\", \"HF Yaml License\": \"\", \"PwC Date\": \"\", \"PwC Description\": \"RareAct is a video dataset of unusual actions, including actions like \\u201cblend phone\\u201d, \\u201ccut keyboard\\u201d and \\u201cmicrowave shoes\\u201d. It aims at evaluating the zero-shot and few-shot compositionality of action recognition models for unlikely compositions of common action verbs and object nouns. It contains 122 different actions which were obtained by combining verbs and nouns rarely co-occurring together in the large-scale textual corpus from HowTo100M, but that frequently appear separately.\", \"PwC License Name\": \"Unspecified\", \"PwC License URL\": null, \"S2 Citation Count (June 2024)\": \"\", \"S2 Date\": \"\"}, \"Bibtex\": \"@Article{Miech2020RareActAV,\\n author = {Antoine Miech and Jean-Baptiste Alayrac and I. Laptev and Josef Sivic and Andrew Zisserman},\\n booktitle = {arXiv.org},\\n journal = {ArXiv},\\n title = {RareAct: A video dataset of unusual interactions},\\n volume = {abs/2008.01018},\\n year = {2020}\\n}\\n\", \"License Use (DataProvenance)\": \"unspecified\", \"License Attribution (DataProvenance)\": 0, \"License Share Alike (DataProvenance)\": 0, \"License Use (DataProvenance IgnoreOpenAI)\": \"unspecified\", \"License Attribution (DataProvenance IgnoreOpenAI)\": 0, \"License Share Alike (DataProvenance IgnoreOpenAI)\": 0, \"License Type\": \"Unspecified\"}, {\"Unique Dataset Identifier\": \"lsmdc-ordering\", \"Collection\": \"lsmdc-ordering\", \"Collection URL\": \"https://arxiv.org/pdf/2004.02205\", \"Dataset Name\": \"LSMDC Ordering\", \"Paper Title\": \"LSMDC Ordering\", \"Paper URL\": \"https://arxiv.org/pdf/2004.02205\", \"GitHub URL\": \"https://github.com/vivoutlaw/TCBP\", \"Hugging Face URL\": \"\", \"Papers with Code URL\": \"https://paperswithcode.com/paper/deep-multimodal-feature-encoding-for-video\", \"ArXiv URL\": \"https://arxiv.org/pdf/2004.02205\", \"Semantic Scholar Corpus ID\": 214802821, \"Year Released\": \"2020\", \"Text Sources\": [\"movies\"], \"Licenses\": [{\"License\": \"MIT License\", \"License URL\": \"https://github.com/vivoutlaw/tcbp/blob/master/LICENSE\"}], \"Creators\": \"Academic\", \"Countries\": [\"Germany\", \"Canada\", \"United States of America\", \"France\"], \"License Verified By\": \"Vivek Sharma\", \"Video Hours\": 158.0, \"Taken Down\": \"False\", \"Video Sources\": [\"movies\"], \"Task Categories\": [\"Video Classification\"], \"Inferred Metadata\": {\"GitHub License\": \"MIT License\", \"GitHub Stars (June 2024)\": 11, \"GitHub Topics\": [], \"Github Date\": \"\", \"HF Config\": \"\", \"HF Config License\": \"\", \"HF Dataset\": \"\", \"HF Date\": \"\", \"HF Downloads (June 2024)\": \"\", \"HF Likes (June 2024)\": \"\", \"HF Yaml License\": \"\", \"PwC Date\": \"\", \"PwC Description\": \"1 code implementation in PyTorch. True understanding of videos comes from a joint analysis of all its modalities: the video frames, the audio track, and any accompanying text such as closed captions. We present a way to learn a compact multimodal feature representation that encodes all these modalities. Our model parameters are learned through a proxy task of inferring the temporal ordering of a set of unordered videos in a timeline. To this end, we create a new multimodal dataset for temporal ordering that consists of approximately 30K scenes (2-6 clips per scene) based on the \\\"Large Scale Movie Description Challenge\\\". We analyze and evaluate the individual and joint modalities on three challenging tasks: (i) inferring the temporal ordering of a set of videos; and (ii) action recognition. We demonstrate empirically that multimodal representations are indeed complementary, and can play a key role in improving the performance of many applications.\", \"PwC License Name\": \"\", \"PwC License URL\": \"\", \"S2 Citation Count (June 2024)\": \"\", \"S2 Date\": \"\"}, \"Bibtex\": \"@Article{Sharma2020DeepMF,\\n author = {Vivek Sharma and Makarand Tapaswi and R. Stiefelhagen},\\n booktitle = {arXiv.org},\\n journal = {ArXiv},\\n title = {Deep Multimodal Feature Encoding for Video Ordering},\\n volume = {abs/2004.02205},\\n year = {2020}\\n}\\n\", \"License Use (DataProvenance)\": \"commercial\", \"License Attribution (DataProvenance)\": 1, \"License Share Alike (DataProvenance)\": 0, \"License Use (DataProvenance IgnoreOpenAI)\": \"commercial\", \"License Attribution (DataProvenance IgnoreOpenAI)\": 1, \"License Share Alike (DataProvenance IgnoreOpenAI)\": 0, \"License Type\": \"Commercial\"}, {\"Unique Dataset Identifier\": \"condensed-movies\", \"Collection\": \"condensed-movies\", \"Collection URL\": \"https://arxiv.org/pdf/2005.04208\", \"Dataset Name\": \"Condensed Movies\", \"Paper Title\": \"Condensed Movies\", \"Paper URL\": \"https://arxiv.org/pdf/2005.04208\", \"GitHub URL\": \"https://github.com/m-bain/CondensedMovies\", \"Hugging Face URL\": \"\", \"Papers with Code URL\": \"https://paperswithcode.com/dataset/condensed-movies\", \"ArXiv URL\": \"https://arxiv.org/pdf/2005.04208\", \"Semantic Scholar Corpus ID\": 218571391, \"Year Released\": \"2020\", \"Text Sources\": [\"movies\"], \"Licenses\": [{\"License\": \"CC BY 4.0\", \"License URL\": \"https://www.robots.ox.ac.uk/~vgg/data/condensed-movies/#download\"}], \"Creators\": \"Academic\", \"Countries\": [\"United Kingdom\"], \"License Verified By\": \"Vivek Sharma\", \"Video Hours\": 1270.0, \"Taken Down\": \"False\", \"Video Sources\": [\"movies\"], \"Task Categories\": [\"Video Summarization\"], \"Inferred Metadata\": {\"GitHub License\": \"\", \"GitHub Stars (June 2024)\": 153, \"GitHub Topics\": [\"dataset\", \"precomputed-features\", \"retrieval\", \"source-videos\", \"video-text-retrieval\"], \"Github Date\": \"\", \"HF Config\": \"\", \"HF Config License\": \"\", \"HF Dataset\": \"\", \"HF Date\": \"\", \"HF Downloads (June 2024)\": \"\", \"HF Likes (June 2024)\": \"\", \"HF Yaml License\": \"\", \"PwC Date\": \"2020-05-08\", \"PwC Description\": \"A large-scale video dataset, featuring clips from movies with detailed captions.\", \"PwC License Name\": \"Unspecified\", \"PwC License URL\": null, \"S2 Citation Count (June 2024)\": \"\", \"S2 Date\": \"\"}, \"Bibtex\": \"@Article{Bain2020CondensedMS,\\n author = {Max Bain and Arsha Nagrani and A. Brown and Andrew Zisserman},\\n booktitle = {Asian Conference on Computer Vision},\\n journal = {ArXiv},\\n title = {Condensed Movies: Story Based Retrieval with Contextual Embeddings},\\n volume = {abs/2005.04208},\\n year = {2020}\\n}\\n\", \"License Use (DataProvenance)\": \"commercial\", \"License Attribution (DataProvenance)\": 1, \"License Share Alike (DataProvenance)\": 0, \"License Use (DataProvenance IgnoreOpenAI)\": \"commercial\", \"License Attribution (DataProvenance IgnoreOpenAI)\": 1, \"License Share Alike (DataProvenance IgnoreOpenAI)\": 0, \"License Type\": \"Commercial\"}, {\"Unique Dataset Identifier\": \"titan\", \"Collection\": \"titan\", \"Collection URL\": \"https://openaccess.thecvf.com/content_CVPR_2020/papers/Malla_TITAN_Future_Forecast_Using_Action_Priors_CVPR_2020_paper.pdf\", \"Dataset Name\": \"TITAN: Future Forecast using Action Priors (CVPR 2020)\", \"Paper Title\": \"TITAN: Future Forecast using Action Priors (CVPR 2020)\", \"Paper URL\": \"https://openaccess.thecvf.com/content_CVPR_2020/papers/Malla_TITAN_Future_Forecast_Using_Action_Priors_CVPR_2020_paper.pdf\", \"GitHub URL\": \"\", \"Hugging Face URL\": \"\", \"Papers with Code URL\": \"https://paperswithcode.com/paper/titan-future-forecast-using-action-priors\", \"ArXiv URL\": \"https://openaccess.thecvf.com/content_CVPR_2020/papers/Malla_TITAN_Future_Forecast_Using_Action_Priors_CVPR_2020_paper.pdf\", \"Semantic Scholar Corpus ID\": 214727763, \"Year Released\": \"2020\", \"Text Sources\": [\"crowdsourced\"], \"Licenses\": [{\"License\": \"Non Commercial\", \"License URL\": \"https://usa.honda-ri.com/titan\"}], \"Creators\": \"Industry Lab\", \"Countries\": [\"United States of America\"], \"License Verified By\": \"Vivek Sharma\", \"Video Hours\": 2.91, \"Taken Down\": \"False\", \"Video Sources\": [\"crowdsourced\"], \"Task Categories\": [\"Video Classification\"], \"Inferred Metadata\": {\"GitHub License\": \"\", \"GitHub Stars (June 2024)\": \"\", \"GitHub Topics\": \"\", \"Github Date\": \"\", \"HF Config\": \"\", \"HF Config License\": \"\", \"HF Dataset\": \"\", \"HF Date\": \"\", \"HF Downloads (June 2024)\": \"\", \"HF Likes (June 2024)\": \"\", \"HF Yaml License\": \"\", \"PwC Date\": \"\", \"PwC Description\": \"We consider the problem of predicting the future trajectory of scene agents from egocentric views obtained from a moving platform. This problem is important in a variety of domains, particularly for autonomous systems making reactive or strategic decisions in navigation. In an attempt to address this problem, we introduce TITAN (Trajectory Inference using Targeted Action priors Network), a new model that incorporates prior positions, actions, and context to forecast future trajectory of agents and future ego-motion. In the absence of an appropriate dataset for this task, we created the TITAN dataset that consists of 700 labeled video-clips (with odometry) captured from a moving vehicle on highly interactive urban traffic scenes in Tokyo. Our dataset includes 50 labels including vehicle states and actions, pedestrian age groups, and targeted pedestrian action attributes that are organized hierarchically corresponding to atomic, simple/complex-contextual, transportive, and communicative actions. To evaluate our model, we conducted extensive experiments on the TITAN dataset, revealing significant performance improvement against baselines and state-of-the-art algorithms. We also report promising results from our Agent Importance Mechanism (AIM), a module which provides insight into assessment of perceived risk by calculating the relative influence of each agent on the future ego-trajectory. The dataset is available at https://usa.honda-ri.com/titan\", \"PwC License Name\": \"\", \"PwC License URL\": \"\", \"S2 Citation Count (June 2024)\": \"\", \"S2 Date\": \"\"}, \"Bibtex\": \"@Article{Malla2020TITANFF,\\n author = {Srikanth Malla and B. Dariush and Chiho Choi},\\n booktitle = {Computer Vision and Pattern Recognition},\\n journal = {2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},\\n pages = {11183-11193},\\n title = {TITAN: Future Forecast Using Action Priors},\\n year = {2020}\\n}\\n\", \"License Use (DataProvenance)\": \"non-commercial\", \"License Attribution (DataProvenance)\": 0, \"License Share Alike (DataProvenance)\": 0, \"License Use (DataProvenance IgnoreOpenAI)\": \"non-commercial\", \"License Attribution (DataProvenance IgnoreOpenAI)\": 0, \"License Share Alike (DataProvenance IgnoreOpenAI)\": 0, \"License Type\": \"Non-Commercial/Academic\"}, {\"Unique Dataset Identifier\": \"lsmdc-ordering\", \"Collection\": \"lsmdc-ordering\", \"Collection URL\": \"https://arxiv.org/pdf/2004.02205\", \"Dataset Name\": \"LSMDC Ordering\", \"Paper Title\": \"LSMDC Ordering\", \"Paper URL\": \"https://arxiv.org/pdf/2004.02205\", \"GitHub URL\": \"https://github.com/vivoutlaw/TCBP\", \"Hugging Face URL\": \"\", \"Papers with Code URL\": \"https://paperswithcode.com/paper/deep-multimodal-feature-encoding-for-video\", \"ArXiv URL\": \"https://arxiv.org/pdf/2004.02205\", \"Semantic Scholar Corpus ID\": 214802821, \"Year Released\": \"2020\", \"Text Sources\": [\"movies\"], \"Licenses\": [{\"License\": \"MIT License\", \"License URL\": \"https://github.com/vivoutlaw/tcbp/blob/master/LICENSE\"}], \"Creators\": \"Academic\", \"Countries\": [\"Germany\", \"Canada\", \"United States of America\", \"France\"], \"License Verified By\": \"Vivek Sharma\", \"Video Hours\": 158.0, \"Taken Down\": \"False\", \"Video Sources\": [\"movies\"], \"Task Categories\": [\"Video Classification\"], \"Inferred Metadata\": {\"GitHub License\": \"MIT License\", \"GitHub Stars (June 2024)\": 11, \"GitHub Topics\": [], \"Github Date\": \"\", \"HF Config\": \"\", \"HF Config License\": \"\", \"HF Dataset\": \"\", \"HF Date\": \"\", \"HF Downloads (June 2024)\": \"\", \"HF Likes (June 2024)\": \"\", \"HF Yaml License\": \"\", \"PwC Date\": \"\", \"PwC Description\": \"1 code implementation in PyTorch. True understanding of videos comes from a joint analysis of all its modalities: the video frames, the audio track, and any accompanying text such as closed captions. We present a way to learn a compact multimodal feature representation that encodes all these modalities. Our model parameters are learned through a proxy task of inferring the temporal ordering of a set of unordered videos in a timeline. To this end, we create a new multimodal dataset for temporal ordering that consists of approximately 30K scenes (2-6 clips per scene) based on the \\\"Large Scale Movie Description Challenge\\\". We analyze and evaluate the individual and joint modalities on three challenging tasks: (i) inferring the temporal ordering of a set of videos; and (ii) action recognition. We demonstrate empirically that multimodal representations are indeed complementary, and can play a key role in improving the performance of many applications.\", \"PwC License Name\": \"\", \"PwC License URL\": \"\", \"S2 Citation Count (June 2024)\": \"\", \"S2 Date\": \"\"}, \"Bibtex\": \"@Article{Sharma2020DeepMF,\\n author = {Vivek Sharma and Makarand Tapaswi and R. Stiefelhagen},\\n booktitle = {arXiv.org},\\n journal = {ArXiv},\\n title = {Deep Multimodal Feature Encoding for Video Ordering},\\n volume = {abs/2004.02205},\\n year = {2020}\\n}\\n\", \"License Use (DataProvenance)\": \"commercial\", \"License Attribution (DataProvenance)\": 1, \"License Share Alike (DataProvenance)\": 0, \"License Use (DataProvenance IgnoreOpenAI)\": \"commercial\", \"License Attribution (DataProvenance IgnoreOpenAI)\": 1, \"License Share Alike (DataProvenance IgnoreOpenAI)\": 0, \"License Type\": \"Commercial\"}, {\"Unique Dataset Identifier\": \"hyu-vids\", \"Collection\": \"hyu-vids\", \"Collection URL\": \"https://arxiv.org/abs/1904.11451\", \"Dataset Name\": \"HVU: Large Scale Holistic Video Understanding (ECCV 2020)\", \"Paper Title\": \"HVU: Large Scale Holistic Video Understanding (ECCV 2020)\", \"Paper URL\": \"https://arxiv.org/abs/1904.11451\", \"GitHub URL\": \"https://github.com/holistic-video-understanding/HVU-Dataset\", \"Hugging Face URL\": \"\", \"Papers with Code URL\": \"https://paperswithcode.com/paper/holistic-large-scale-video-understanding\", \"ArXiv URL\": \"https://arxiv.org/abs/1904.11451\", \"Semantic Scholar Corpus ID\": 131777079, \"Year Released\": \"2020\", \"Text Sources\": [\"youtube\"], \"Licenses\": [{\"License\": \"Custom\", \"License URL\": \"https://github.com/holistic-video-understanding/HVU-Dataset\"}], \"Creators\": \"Other\", \"Countries\": [\"Switzerland\", \"Belgium\", \"Germany\"], \"License Verified By\": \"Vivek Sharma\", \"Video Hours\": 96166.67, \"Taken Down\": \"False\", \"Video Sources\": [\"youtube\"], \"Task Categories\": [\"Video Classification\"], \"Inferred Metadata\": {\"GitHub License\": \"\", \"GitHub Stars (June 2024)\": 70, \"GitHub Topics\": [], \"Github Date\": \"\", \"HF Config\": \"\", \"HF Config License\": \"\", \"HF Dataset\": \"\", \"HF Date\": \"\", \"HF Downloads (June 2024)\": \"\", \"HF Likes (June 2024)\": \"\", \"HF Yaml License\": \"\", \"PwC Date\": \"\", \"PwC Description\": \"#11 best model for Action Recognition on UCF101 (3-fold Accuracy metric)\", \"PwC License Name\": \"\", \"PwC License URL\": \"\", \"S2 Citation Count (June 2024)\": \"\", \"S2 Date\": \"\"}, \"Bibtex\": \"@Article{Diba2019HolisticLS,\\n author = {Ali Diba and Mohsen Fayyaz and Vivek Sharma and Manohar Paluri and Juergen Gall and R. Stiefelhagen and L. Gool},\\n booktitle = {arXiv.org},\\n journal = {ArXiv},\\n title = {Holistic Large Scale Video Understanding},\\n volume = {abs/1904.11451},\\n year = {2019}\\n}\\n\", \"License Use (DataProvenance)\": \"academic-only\", \"License Attribution (DataProvenance)\": 1, \"License Share Alike (DataProvenance)\": 0, \"License Use (DataProvenance IgnoreOpenAI)\": \"academic-only\", \"License Attribution (DataProvenance IgnoreOpenAI)\": 1, \"License Share Alike (DataProvenance IgnoreOpenAI)\": 0, \"License Type\": \"Non-Commercial/Academic\"}, {\"Unique Dataset Identifier\": \"hyu-vids\", \"Collection\": \"hyu-vids\", \"Collection URL\": \"https://arxiv.org/abs/1904.11451\", \"Dataset Name\": \"HVU: Large Scale Holistic Video Understanding (ECCV 2020)\", \"Paper Title\": \"HVU: Large Scale Holistic Video Understanding (ECCV 2020)\", \"Paper URL\": \"https://arxiv.org/abs/1904.11451\", \"GitHub URL\": \"https://github.com/holistic-video-understanding/HVU-Dataset\", \"Hugging Face URL\": \"\", \"Papers with Code URL\": \"https://paperswithcode.com/paper/holistic-large-scale-video-understanding\", \"ArXiv URL\": \"https://arxiv.org/abs/1904.11451\", \"Semantic Scholar Corpus ID\": 131777079, \"Year Released\": \"2020\", \"Text Sources\": [\"youtube\"], \"Licenses\": [{\"License\": \"Custom\", \"License URL\": \"https://github.com/holistic-video-understanding/HVU-Dataset\"}], \"Creators\": \"Academic\", \"Countries\": [\"Switzerland\", \"Belgium\", \"Germany\"], \"License Verified By\": \"Vivek Sharma\", \"Video Hours\": 96166.67, \"Taken Down\": \"False\", \"Video Sources\": [\"youtube\"], \"Task Categories\": [\"Video Classification\"], \"Inferred Metadata\": {\"GitHub License\": \"\", \"GitHub Stars (June 2024)\": 70, \"GitHub Topics\": [], \"Github Date\": \"\", \"HF Config\": \"\", \"HF Config License\": \"\", \"HF Dataset\": \"\", \"HF Date\": \"\", \"HF Downloads (June 2024)\": \"\", \"HF Likes (June 2024)\": \"\", \"HF Yaml License\": \"\", \"PwC Date\": \"\", \"PwC Description\": \"#11 best model for Action Recognition on UCF101 (3-fold Accuracy metric)\", \"PwC License Name\": \"\", \"PwC License URL\": \"\", \"S2 Citation Count (June 2024)\": \"\", \"S2 Date\": \"\"}, \"Bibtex\": \"@Article{Diba2019HolisticLS,\\n author = {Ali Diba and Mohsen Fayyaz and Vivek Sharma and Manohar Paluri and Juergen Gall and R. Stiefelhagen and L. Gool},\\n booktitle = {arXiv.org},\\n journal = {ArXiv},\\n title = {Holistic Large Scale Video Understanding},\\n volume = {abs/1904.11451},\\n year = {2019}\\n}\\n\", \"License Use (DataProvenance)\": \"academic-only\", \"License Attribution (DataProvenance)\": 1, \"License Share Alike (DataProvenance)\": 0, \"License Use (DataProvenance IgnoreOpenAI)\": \"academic-only\", \"License Attribution (DataProvenance IgnoreOpenAI)\": 1, \"License Share Alike (DataProvenance IgnoreOpenAI)\": 0, \"License Type\": \"Non-Commercial/Academic\"}, {\"Unique Dataset Identifier\": \"eev-dataset\", \"Collection\": \"eev-dataset\", \"Collection URL\": \"https://arxiv.org/abs/2001.05488\", \"Dataset Name\": \"EEV: A Large-Scale Dataset for Studying Evoked Expressions from Video\", \"Paper Title\": \"EEV: A Large-Scale Dataset for Studying Evoked Expressions from Video\", \"Paper URL\": \"https://arxiv.org/abs/2001.05488\", \"GitHub URL\": \"https://github.com/google-research-datasets/eev\", \"Hugging Face URL\": \"\", \"Papers with Code URL\": \"\", \"ArXiv URL\": \"https://arxiv.org/abs/2001.05488\", \"Semantic Scholar Corpus ID\": 210701992, \"Year Released\": \"2020\", \"Text Sources\": [\"undisclosed web\"], \"Licenses\": [{\"License\": \"CC BY 4.0\", \"License URL\": \"https://github.com/google-research-datasets/eev?tab=readme-ov-file#license\"}], \"Creators\": \"Academic\", \"Countries\": [\"United States of America\"], \"License Verified By\": \"Vivek Sharma\", \"Video Hours\": 370.0, \"Taken Down\": \"False\", \"Video Sources\": [\"undisclosed web\"], \"Task Categories\": [\"Video Classification\"], \"Inferred Metadata\": {\"GitHub License\": \"\", \"GitHub Stars (June 2024)\": 34, \"GitHub Topics\": [], \"Github Date\": \"\", \"HF Config\": \"\", \"HF Config License\": \"\", \"HF Dataset\": \"\", \"HF Date\": \"\", \"HF Downloads (June 2024)\": \"\", \"HF Likes (June 2024)\": \"\", \"HF Yaml License\": \"\", \"PwC Date\": \"\", \"PwC Description\": \"\", \"PwC License Name\": \"\", \"PwC License URL\": \"\", \"S2 Citation Count (June 2024)\": \"\", \"S2 Date\": \"\"}, \"Bibtex\": \"@Article{Sun2020EEVDP,\\n author = {Jennifer J. Sun and Ting Liu and Alan S. Cowen and Florian Schroff and Hartwig Adam and Gautam Prasad},\\n booktitle = {arXiv.org},\\n journal = {ArXiv},\\n title = {EEV Dataset: Predicting Expressions Evoked by Diverse Videos},\\n volume = {abs/2001.05488},\\n year = {2020}\\n}\\n\", \"License Use (DataProvenance)\": \"commercial\", \"License Attribution (DataProvenance)\": 1, \"License Share Alike (DataProvenance)\": 0, \"License Use (DataProvenance IgnoreOpenAI)\": \"commercial\", \"License Attribution (DataProvenance IgnoreOpenAI)\": 1, \"License Share Alike (DataProvenance IgnoreOpenAI)\": 0, \"License Type\": \"Commercial\"}, {\"Unique Dataset Identifier\": \"omnisource-web-dataset\", \"Collection\": \"omnisource-web-dataset\", \"Collection URL\": \"https://arxiv.org/abs/2003.13042\", \"Dataset Name\": \"OmniSource Web Dataset\", \"Paper Title\": \"OmniSource Web Dataset\", \"Paper URL\": \"https://arxiv.org/abs/2003.13042\", \"GitHub URL\": \"https://github.com/open-mmlab/mmaction\", \"Hugging Face URL\": \"\", \"Papers with Code URL\": \"https://paperswithcode.com/paper/omni-sourced-webly-supervised-learning-for#code\", \"ArXiv URL\": \"https://arxiv.org/abs/2003.13042\", \"Semantic Scholar Corpus ID\": 214714240, \"Year Released\": \"2020\", \"Text Sources\": [\"google videos\", \"instagram\", \"youtube\"], \"Licenses\": [{\"License\": \"Apache License 2.0\", \"License URL\": \"https://github.com/open-mmlab/mmaction?tab=readme-ov-file#license\"}], \"Creators\": \"Academic\", \"Countries\": [\"China\"], \"License Verified By\": \"Vivek Sharma\", \"Video Hours\": 13333.0, \"Taken Down\": \"False\", \"Video Sources\": [\"google videos\", \"instagram\", \"youtube\"], \"Task Categories\": [\"Video Classification\"], \"Inferred Metadata\": {\"GitHub License\": \"Apache License 2.0\", \"GitHub Stars (June 2024)\": 1856, \"GitHub Topics\": [\"action-detection\", \"action-recognition\", \"pytorch\", \"spatial-temporal-action-detection\", \"temporal-action-detection\", \"temporal-action-localization\", \"video-understanding\"], \"Github Date\": \"\", \"HF Config\": \"\", \"HF Config License\": \"\", \"HF Dataset\": \"\", \"HF Date\": \"\", \"HF Downloads (June 2024)\": \"\", \"HF Likes (June 2024)\": \"\", \"HF Yaml License\": \"\", \"PwC Date\": \"\", \"PwC Description\": \"3 code implementations in PyTorch. We introduce OmniSource, a novel framework for leveraging web data to train video recognition models. OmniSource overcomes the barriers between data formats, such as images, short videos, and long untrimmed videos for webly-supervised learning. First, data samples with multiple formats, curated by task-specific data collection and automatically filtered by a teacher model, are transformed into a unified form. Then a joint-training strategy is proposed to deal with the domain gaps between multiple data sources and formats in webly-supervised learning. Several good practices, including data balancing, resampling, and cross-dataset mixup are adopted in joint training. Experiments show that by utilizing data from multiple sources and formats, OmniSource is more data-efficient in training. With only 3.5M images and 800K minutes videos crawled from the internet without human labeling (less than 2% of prior works), our models learned with OmniSource improve Top-1 accuracy of 2D- and 3D-ConvNet baseline models by 3.0% and 3.9%, respectively, on the Kinetics-400 benchmark. With OmniSource, we establish new records with different pretraining strategies for video recognition. Our best models achieve 80.4%, 80.5%, and 83.6 Top-1 accuracies on the Kinetics-400 benchmark respectively for training-from-scratch, ImageNet pre-training and IG-65M pre-training.\", \"PwC License Name\": \"\", \"PwC License URL\": \"\", \"S2 Citation Count (June 2024)\": \"\", \"S2 Date\": \"\"}, \"Bibtex\": \"@Article{Duan2020OmnisourcedWL,\\n author = {Haodong Duan and Yue Zhao and Yuanjun Xiong and Wentao Liu and Dahua Lin},\\n booktitle = {European Conference on Computer Vision},\\n journal = {ArXiv},\\n title = {Omni-sourced Webly-supervised Learning for Video Recognition},\\n volume = {abs/2003.13042},\\n year = {2020}\\n}\\n\", \"License Use (DataProvenance)\": \"commercial\", \"License Attribution (DataProvenance)\": 1, \"License Share Alike (DataProvenance)\": 0, \"License Use (DataProvenance IgnoreOpenAI)\": \"commercial\", \"License Attribution (DataProvenance IgnoreOpenAI)\": 1, \"License Share Alike (DataProvenance IgnoreOpenAI)\": 0, \"License Type\": \"Commercial\"}, {\"Unique Dataset Identifier\": \"finegym-dataset\", \"Collection\": \"finegym-dataset\", \"Collection URL\": \"https://arxiv.org/abs/2004.06704\", \"Dataset Name\": \"FineGym: A Hierarchical Video Dataset for Fine-grained Action Understanding (CVPR 2020)\", \"Paper Title\": \"FineGym: A Hierarchical Video Dataset for Fine-grained Action Understanding (CVPR 2020)\", \"Paper URL\": \"https://arxiv.org/abs/2004.06704\", \"GitHub URL\": \"https://github.com/SDOlivia/FineGym/\", \"Hugging Face URL\": \"\", \"Papers with Code URL\": \"https://paperswithcode.com/dataset/finegym\", \"ArXiv URL\": \"https://arxiv.org/abs/2004.06704\", \"Semantic Scholar Corpus ID\": 215754360, \"Year Released\": \"2020\", \"Text Sources\": [\"undisclosed web\"], \"Licenses\": [{\"License\": \"CC BY-NC 4.0\", \"License URL\": \"https://sdolivia.github.io/FineGym/\"}], \"Creators\": \"Academic\", \"Countries\": [\"China\"], \"License Verified By\": \"Vivek Sharma\", \"Video Hours\": 708.0, \"Taken Down\": \"False\", \"Video Sources\": [\"undisclosed web\"], \"Task Categories\": [\"Video Classification\"], \"Inferred Metadata\": {\"GitHub License\": \"\", \"GitHub Stars (June 2024)\": 124, \"GitHub Topics\": [], \"Github Date\": \"\", \"HF Config\": \"\", \"HF Config License\": \"\", \"HF Dataset\": \"\", \"HF Date\": \"\", \"HF Downloads (June 2024)\": \"\", \"HF Likes (June 2024)\": \"\", \"HF Yaml License\": \"\", \"PwC Date\": \"2020-04-14\", \"PwC Description\": \"FineGym is an action recognition dataset build on top of gymnasium videos. Compared to existing action recognition datasets, FineGym is distinguished in richness, quality, and diversity. In particular, it provides temporal annotations at both action and sub-action levels with a three-level semantic hierarchy. For example, a \\\"balance beam\\\" event will be annotated as a sequence of elementary sub-actions derived from five sets: \\\"leap-jumphop\\\", \\\"beam-turns\\\", \\\"flight-salto\\\", \\\"flight-handspring\\\", and \\\"dismount\\\", where the sub-action in each set will be further annotated with finely defined class labels. This new level of granularity presents significant challenges for action recognition, e.g. how to parse the temporal structures from a coherent action, and how to distinguish between subtly different action classes.\", \"PwC License Name\": \"CC BY-NC 4.0\", \"PwC License URL\": \"https://creativecommons.org/licenses/by-nc/4.0/\", \"S2 Citation Count (June 2024)\": \"\", \"S2 Date\": \"\"}, \"Bibtex\": \"@Article{Shao2020FineGymAH,\\n author = {Dian Shao and Yue Zhao and Bo Dai and Dahua Lin},\\n booktitle = {Computer Vision and Pattern Recognition},\\n journal = {2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},\\n pages = {2613-2622},\\n title = {FineGym: A Hierarchical Video Dataset for Fine-Grained Action Understanding},\\n year = {2020}\\n}\\n\", \"License Use (DataProvenance)\": \"non-commercial\", \"License Attribution (DataProvenance)\": 1, \"License Share Alike (DataProvenance)\": 0, \"License Use (DataProvenance IgnoreOpenAI)\": \"non-commercial\", \"License Attribution (DataProvenance IgnoreOpenAI)\": 1, \"License Share Alike (DataProvenance IgnoreOpenAI)\": 0, \"License Type\": \"Non-Commercial/Academic\"}, {\"Unique Dataset Identifier\": \"lsmdc-ordering\", \"Collection\": \"lsmdc-ordering\", \"Collection URL\": \"https://arxiv.org/pdf/2004.02205\", \"Dataset Name\": \"LSMDC Ordering\", \"Paper Title\": \"LSMDC Ordering\", \"Paper URL\": \"https://arxiv.org/pdf/2004.02205\", \"GitHub URL\": \"https://github.com/vivoutlaw/TCBP\", \"Hugging Face URL\": \"\", \"Papers with Code URL\": \"https://paperswithcode.com/paper/deep-multimodal-feature-encoding-for-video\", \"ArXiv URL\": \"https://arxiv.org/pdf/2004.02205\", \"Semantic Scholar Corpus ID\": 214802821, \"Year Released\": \"2020\", \"Text Sources\": [\"movies\"], \"Licenses\": [{\"License\": \"MIT License\", \"License URL\": \"https://github.com/vivoutlaw/tcbp/blob/master/LICENSE\"}], \"Creators\": \"Academic\", \"Countries\": [\"Germany\", \"Canada\", \"United States of America\", \"France\"], \"License Verified By\": \"Vivek Sharma\", \"Video Hours\": 158.0, \"Taken Down\": \"False\", \"Video Sources\": [\"movies\"], \"Task Categories\": [\"Video Classification\"], \"Inferred Metadata\": {\"GitHub License\": \"MIT License\", \"GitHub Stars (June 2024)\": 11, \"GitHub Topics\": [], \"Github Date\": \"\", \"HF Config\": \"\", \"HF Config License\": \"\", \"HF Dataset\": \"\", \"HF Date\": \"\", \"HF Downloads (June 2024)\": \"\", \"HF Likes (June 2024)\": \"\", \"HF Yaml License\": \"\", \"PwC Date\": \"\", \"PwC Description\": \"1 code implementation in PyTorch. True understanding of videos comes from a joint analysis of all its modalities: the video frames, the audio track, and any accompanying text such as closed captions. We present a way to learn a compact multimodal feature representation that encodes all these modalities. Our model parameters are learned through a proxy task of inferring the temporal ordering of a set of unordered videos in a timeline. To this end, we create a new multimodal dataset for temporal ordering that consists of approximately 30K scenes (2-6 clips per scene) based on the \\\"Large Scale Movie Description Challenge\\\". We analyze and evaluate the individual and joint modalities on three challenging tasks: (i) inferring the temporal ordering of a set of videos; and (ii) action recognition. We demonstrate empirically that multimodal representations are indeed complementary, and can play a key role in improving the performance of many applications.\", \"PwC License Name\": \"\", \"PwC License URL\": \"\", \"S2 Citation Count (June 2024)\": \"\", \"S2 Date\": \"\"}, \"Bibtex\": \"@Article{Sharma2020DeepMF,\\n author = {Vivek Sharma and Makarand Tapaswi and R. Stiefelhagen},\\n booktitle = {arXiv.org},\\n journal = {ArXiv},\\n title = {Deep Multimodal Feature Encoding for Video Ordering},\\n volume = {abs/2004.02205},\\n year = {2020}\\n}\\n\", \"License Use (DataProvenance)\": \"commercial\", \"License Attribution (DataProvenance)\": 1, \"License Share Alike (DataProvenance)\": 0, \"License Use (DataProvenance IgnoreOpenAI)\": \"commercial\", \"License Attribution (DataProvenance IgnoreOpenAI)\": 1, \"License Share Alike (DataProvenance IgnoreOpenAI)\": 0, \"License Type\": \"Commercial\"}, {\"Unique Dataset Identifier\": \"lsmdc-ordering\", \"Collection\": \"lsmdc-ordering\", \"Collection URL\": \"https://arxiv.org/pdf/2004.02205\", \"Dataset Name\": \"LSMDC Ordering\", \"Paper Title\": \"LSMDC Ordering\", \"Paper URL\": \"https://arxiv.org/pdf/2004.02205\", \"GitHub URL\": \"https://github.com/vivoutlaw/TCBP\", \"Hugging Face URL\": \"\", \"Papers with Code URL\": \"https://paperswithcode.com/paper/deep-multimodal-feature-encoding-for-video\", \"ArXiv URL\": \"https://arxiv.org/pdf/2004.02205\", \"Semantic Scholar Corpus ID\": 214802821, \"Year Released\": \"2020\", \"Text Sources\": [\"movies\"], \"Licenses\": [{\"License\": \"MIT License\", \"License URL\": \"https://github.com/vivoutlaw/tcbp/blob/master/LICENSE\"}], \"Creators\": \"Research Group\", \"Countries\": [\"Germany\", \"Canada\", \"United States of America\", \"France\"], \"License Verified By\": \"Vivek Sharma\", \"Video Hours\": 158.0, \"Taken Down\": \"False\", \"Video Sources\": [\"movies\"], \"Task Categories\": [\"Video Classification\"], \"Inferred Metadata\": {\"GitHub License\": \"MIT License\", \"GitHub Stars (June 2024)\": 11, \"GitHub Topics\": [], \"Github Date\": \"\", \"HF Config\": \"\", \"HF Config License\": \"\", \"HF Dataset\": \"\", \"HF Date\": \"\", \"HF Downloads (June 2024)\": \"\", \"HF Likes (June 2024)\": \"\", \"HF Yaml License\": \"\", \"PwC Date\": \"\", \"PwC Description\": \"1 code implementation in PyTorch. True understanding of videos comes from a joint analysis of all its modalities: the video frames, the audio track, and any accompanying text such as closed captions. We present a way to learn a compact multimodal feature representation that encodes all these modalities. Our model parameters are learned through a proxy task of inferring the temporal ordering of a set of unordered videos in a timeline. To this end, we create a new multimodal dataset for temporal ordering that consists of approximately 30K scenes (2-6 clips per scene) based on the \\\"Large Scale Movie Description Challenge\\\". We analyze and evaluate the individual and joint modalities on three challenging tasks: (i) inferring the temporal ordering of a set of videos; and (ii) action recognition. We demonstrate empirically that multimodal representations are indeed complementary, and can play a key role in improving the performance of many applications.\", \"PwC License Name\": \"\", \"PwC License URL\": \"\", \"S2 Citation Count (June 2024)\": \"\", \"S2 Date\": \"\"}, \"Bibtex\": \"@Article{Sharma2020DeepMF,\\n author = {Vivek Sharma and Makarand Tapaswi and R. Stiefelhagen},\\n booktitle = {arXiv.org},\\n journal = {ArXiv},\\n title = {Deep Multimodal Feature Encoding for Video Ordering},\\n volume = {abs/2004.02205},\\n year = {2020}\\n}\\n\", \"License Use (DataProvenance)\": \"commercial\", \"License Attribution (DataProvenance)\": 1, \"License Share Alike (DataProvenance)\": 0, \"License Use (DataProvenance IgnoreOpenAI)\": \"commercial\", \"License Attribution (DataProvenance IgnoreOpenAI)\": 1, \"License Share Alike (DataProvenance IgnoreOpenAI)\": 0, \"License Type\": \"Commercial\"}, {\"Unique Dataset Identifier\": \"oops-dataset\", \"Collection\": \"oops-dataset\", \"Collection URL\": \"https://openaccess.thecvf.com/content_CVPR_2020/papers/Epstein_Oops_Predicting_Unintentional_Action_in_Video_CVPR_2020_paper.pdf\", \"Dataset Name\": \"Oops!: Predicting Unintentional Action in Video (CVPR 2020)\", \"Paper Title\": \"Oops!: Predicting Unintentional Action in Video (CVPR 2020)\", \"Paper URL\": \"https://openaccess.thecvf.com/content_CVPR_2020/papers/Epstein_Oops_Predicting_Unintentional_Action_in_Video_CVPR_2020_paper.pdf\", \"GitHub URL\": \"https://github.com/cvlab-columbia/oops\", \"Hugging Face URL\": \"\", \"Papers with Code URL\": \"https://paperswithcode.com/paper/oops-predicting-unintentional-action-in-video\", \"ArXiv URL\": \"https://openaccess.thecvf.com/content_CVPR_2020/papers/Epstein_Oops_Predicting_Unintentional_Action_in_Video_CVPR_2020_paper.pdf\", \"Semantic Scholar Corpus ID\": 208291335, \"Year Released\": \"2020\", \"Text Sources\": [\"youtube\"], \"Licenses\": [{\"License\": \"CC BY-NC-SA 4.0\", \"License URL\": \"https://oops.cs.columbia.edu/data/\"}], \"Creators\": \"Academic\", \"Countries\": [\"United States of America\"], \"License Verified By\": \"Vivek Sharma\", \"Video Hours\": 50.0, \"Taken Down\": \"False\", \"Video Sources\": [\"youtube\"], \"Task Categories\": [\"Video Classification\"], \"Inferred Metadata\": {\"GitHub License\": \"MIT License\", \"GitHub Stars (June 2024)\": 77, \"GitHub Topics\": [], \"Github Date\": \"\", \"HF Config\": \"\", \"HF Config License\": \"\", \"HF Dataset\": \"\", \"HF Date\": \"\", \"HF Downloads (June 2024)\": \"\", \"HF Likes (June 2024)\": \"\", \"HF Yaml License\": \"\", \"PwC Date\": \"\", \"PwC Description\": \"Implemented in one code library.\", \"PwC License Name\": \"\", \"PwC License URL\": \"\", \"S2 Citation Count (June 2024)\": \"\", \"S2 Date\": \"\"}, \"Bibtex\": \"@Article{Epstein2019OopsPU,\\n author = {Dave Epstein and Boyuan Chen and Carl Vondrick},\\n booktitle = {Computer Vision and Pattern Recognition},\\n journal = {2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},\\n pages = {916-926},\\n title = {Oops! Predicting Unintentional Action in Video},\\n year = {2019}\\n}\\n\", \"License Use (DataProvenance)\": \"non-commercial\", \"License Attribution (DataProvenance)\": 1, \"License Share Alike (DataProvenance)\": 1, \"License Use (DataProvenance IgnoreOpenAI)\": \"non-commercial\", \"License Attribution (DataProvenance IgnoreOpenAI)\": 1, \"License Share Alike (DataProvenance IgnoreOpenAI)\": 1, \"License Type\": \"Non-Commercial/Academic\"}, {\"Unique Dataset Identifier\": \"eev-dataset\", \"Collection\": \"eev-dataset\", \"Collection URL\": \"https://arxiv.org/abs/2001.05488\", \"Dataset Name\": \"EEV: A Large-Scale Dataset for Studying Evoked Expressions from Video\", \"Paper Title\": \"EEV: A Large-Scale Dataset for Studying Evoked Expressions from Video\", \"Paper URL\": \"https://arxiv.org/abs/2001.05488\", \"GitHub URL\": \"https://github.com/google-research-datasets/eev\", \"Hugging Face URL\": \"\", \"Papers with Code URL\": \"\", \"ArXiv URL\": \"https://arxiv.org/abs/2001.05488\", \"Semantic Scholar Corpus ID\": 210701992, \"Year Released\": \"2020\", \"Text Sources\": [\"undisclosed web\"], \"Licenses\": [{\"License\": \"CC BY 4.0\", \"License URL\": \"https://github.com/google-research-datasets/eev?tab=readme-ov-file#license\"}], \"Creators\": \"Industry Lab\", \"Countries\": [\"United States of America\"], \"License Verified By\": \"Vivek Sharma\", \"Video Hours\": 370.0, \"Taken Down\": \"False\", \"Video Sources\": [\"undisclosed web\"], \"Task Categories\": [\"Video Classification\"], \"Inferred Metadata\": {\"GitHub License\": \"\", \"GitHub Stars (June 2024)\": 34, \"GitHub Topics\": [], \"Github Date\": \"\", \"HF Config\": \"\", \"HF Config License\": \"\", \"HF Dataset\": \"\", \"HF Date\": \"\", \"HF Downloads (June 2024)\": \"\", \"HF Likes (June 2024)\": \"\", \"HF Yaml License\": \"\", \"PwC Date\": \"\", \"PwC Description\": \"\", \"PwC License Name\": \"\", \"PwC License URL\": \"\", \"S2 Citation Count (June 2024)\": \"\", \"S2 Date\": \"\"}, \"Bibtex\": \"@Article{Sun2020EEVDP,\\n author = {Jennifer J. Sun and Ting Liu and Alan S. Cowen and Florian Schroff and Hartwig Adam and Gautam Prasad},\\n booktitle = {arXiv.org},\\n journal = {ArXiv},\\n title = {EEV Dataset: Predicting Expressions Evoked by Diverse Videos},\\n volume = {abs/2001.05488},\\n year = {2020}\\n}\\n\", \"License Use (DataProvenance)\": \"commercial\", \"License Attribution (DataProvenance)\": 1, \"License Share Alike (DataProvenance)\": 0, \"License Use (DataProvenance IgnoreOpenAI)\": \"commercial\", \"License Attribution (DataProvenance IgnoreOpenAI)\": 1, \"License Share Alike (DataProvenance IgnoreOpenAI)\": 0, \"License Type\": \"Commercial\"}, {\"Unique Dataset Identifier\": \"rare-act-dataset\", \"Collection\": \"rare-act-dataset\", \"Collection URL\": \"https://arxiv.org/pdf/2008.01018.pdf\", \"Dataset Name\": \"RareAct: A video dataset of unusual interactions\", \"Paper Title\": \"RareAct: A video dataset of unusual interactions\", \"Paper URL\": \"https://arxiv.org/pdf/2008.01018.pdf\", \"GitHub URL\": \"https://github.com/antoine77340/RareAct\", \"Hugging Face URL\": \"\", \"Papers with Code URL\": \"https://paperswithcode.com/dataset/rareact\", \"ArXiv URL\": \"https://arxiv.org/pdf/2008.01018.pdf\", \"Semantic Scholar Corpus ID\": 220936243, \"Year Released\": \"2020\", \"Text Sources\": [\"undisclosed web\"], \"Licenses\": [{\"License\": \"Unspecified\", \"License URL\": \"\"}], \"Creators\": \"Research Group\", \"Countries\": [\"United Kingdom\", \"France\", \"Czech Republic\"], \"License Verified By\": \"Vivek Sharma\", \"Video Hours\": 21.13, \"Taken Down\": \"False\", \"Video Sources\": [\"undisclosed web\"], \"Task Categories\": [\"Video Classification\"], \"Inferred Metadata\": {\"GitHub License\": \"Apache License 2.0\", \"GitHub Stars (June 2024)\": 32, \"GitHub Topics\": [], \"Github Date\": \"\", \"HF Config\": \"\", \"HF Config License\": \"\", \"HF Dataset\": \"\", \"HF Date\": \"\", \"HF Downloads (June 2024)\": \"\", \"HF Likes (June 2024)\": \"\", \"HF Yaml License\": \"\", \"PwC Date\": \"\", \"PwC Description\": \"RareAct is a video dataset of unusual actions, including actions like \\u201cblend phone\\u201d, \\u201ccut keyboard\\u201d and \\u201cmicrowave shoes\\u201d. It aims at evaluating the zero-shot and few-shot compositionality of action recognition models for unlikely compositions of common action verbs and object nouns. It contains 122 different actions which were obtained by combining verbs and nouns rarely co-occurring together in the large-scale textual corpus from HowTo100M, but that frequently appear separately.\", \"PwC License Name\": \"Unspecified\", \"PwC License URL\": null, \"S2 Citation Count (June 2024)\": \"\", \"S2 Date\": \"\"}, \"Bibtex\": \"@Article{Miech2020RareActAV,\\n author = {Antoine Miech and Jean-Baptiste Alayrac and I. Laptev and Josef Sivic and Andrew Zisserman},\\n booktitle = {arXiv.org},\\n journal = {ArXiv},\\n title = {RareAct: A video dataset of unusual interactions},\\n volume = {abs/2008.01018},\\n year = {2020}\\n}\\n\", \"License Use (DataProvenance)\": \"unspecified\", \"License Attribution (DataProvenance)\": 0, \"License Share Alike (DataProvenance)\": 0, \"License Use (DataProvenance IgnoreOpenAI)\": \"unspecified\", \"License Attribution (DataProvenance IgnoreOpenAI)\": 0, \"License Share Alike (DataProvenance IgnoreOpenAI)\": 0, \"License Type\": \"Unspecified\"}, {\"Unique Dataset Identifier\": \"100doh\", \"Collection\": \"100doh\", \"Collection URL\": \"https://arxiv.org/abs/2006.06669\", \"Dataset Name\": \"100DOH: Understanding Human Hands in Contact at Internet Scale (CVPR 2020)\", \"Paper Title\": \"100DOH: Understanding Human Hands in Contact at Internet Scale (CVPR 2020)\", \"Paper URL\": \"https://arxiv.org/abs/2006.06669\", \"GitHub URL\": \"https://github.com/ddshan/hand_object_detector\", \"Hugging Face URL\": \"\", \"Papers with Code URL\": \"https://paperswithcode.com/paper/understanding-human-hands-in-contact-at-1\", \"ArXiv URL\": \"https://arxiv.org/abs/2006.06669\", \"Semantic Scholar Corpus ID\": 215413188, \"Year Released\": \"2020\", \"Text Sources\": [\"youtube\"], \"Licenses\": [{\"License\": \"Custom\", \"License URL\": \"https://fouheylab.eecs.umich.edu/~dandans/projects/100DOH/download.html\"}], \"Creators\": \"Academic\", \"Countries\": [\"United States of America\"], \"License Verified By\": \"Vivek Sharma\", \"Video Hours\": 4577.3, \"Taken Down\": \"False\", \"Video Sources\": [\"youtube\"], \"Task Categories\": [\"Misc (Hand/Object Detection)\"], \"Inferred Metadata\": {\"GitHub License\": \"MIT License\", \"GitHub Stars (June 2024)\": 213, \"GitHub Topics\": [\"cvpr2020\", \"dataset\", \"handobjectdetection\", \"interactiondetection\"], \"Github Date\": \"\", \"HF Config\": \"\", \"HF Config License\": \"\", \"HF Dataset\": \"\", \"HF Date\": \"\", \"HF Downloads (June 2024)\": \"\", \"HF Likes (June 2024)\": \"\", \"HF Yaml License\": \"\", \"PwC Date\": \"\", \"PwC Description\": \"Implemented in one code library.\", \"PwC License Name\": \"\", \"PwC License URL\": \"\", \"S2 Citation Count (June 2024)\": \"\", \"S2 Date\": \"\"}, \"Bibtex\": \"@Article{Shan2020UnderstandingHH,\\n author = {Dandan Shan and Jiaqi Geng and Michelle Shu and D. Fouhey},\\n booktitle = {Computer Vision and Pattern Recognition},\\n journal = {2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},\\n pages = {9866-9875},\\n title = {Understanding Human Hands in Contact at Internet Scale},\\n year = {2020}\\n}\\n\", \"License Use (DataProvenance)\": \"academic-only\", \"License Attribution (DataProvenance)\": 1, \"License Share Alike (DataProvenance)\": 0, \"License Use (DataProvenance IgnoreOpenAI)\": \"academic-only\", \"License Attribution (DataProvenance IgnoreOpenAI)\": 1, \"License Share Alike (DataProvenance IgnoreOpenAI)\": 0, \"License Type\": \"Non-Commercial/Academic\"}, {\"Unique Dataset Identifier\": \"rare-act-dataset\", \"Collection\": \"rare-act-dataset\", \"Collection URL\": \"https://arxiv.org/pdf/2008.01018.pdf\", \"Dataset Name\": \"RareAct: A video dataset of unusual interactions\", \"Paper Title\": \"RareAct: A video dataset of unusual interactions\", \"Paper URL\": \"https://arxiv.org/pdf/2008.01018.pdf\", \"GitHub URL\": \"https://github.com/antoine77340/RareAct\", \"Hugging Face URL\": \"\", \"Papers with Code URL\": \"https://paperswithcode.com/dataset/rareact\", \"ArXiv URL\": \"https://arxiv.org/pdf/2008.01018.pdf\", \"Semantic Scholar Corpus ID\": 220936243, \"Year Released\": \"2020\", \"Text Sources\": [\"undisclosed web\"], \"Licenses\": [{\"License\": \"Unspecified\", \"License URL\": \"\"}], \"Creators\": \"Other\", \"Countries\": [\"United Kingdom\", \"France\", \"Czech Republic\"], \"License Verified By\": \"Vivek Sharma\", \"Video Hours\": 21.13, \"Taken Down\": \"False\", \"Video Sources\": [\"undisclosed web\"], \"Task Categories\": [\"Video Classification\"], \"Inferred Metadata\": {\"GitHub License\": \"Apache License 2.0\", \"GitHub Stars (June 2024)\": 32, \"GitHub Topics\": [], \"Github Date\": \"\", \"HF Config\": \"\", \"HF Config License\": \"\", \"HF Dataset\": \"\", \"HF Date\": \"\", \"HF Downloads (June 2024)\": \"\", \"HF Likes (June 2024)\": \"\", \"HF Yaml License\": \"\", \"PwC Date\": \"\", \"PwC Description\": \"RareAct is a video dataset of unusual actions, including actions like \\u201cblend phone\\u201d, \\u201ccut keyboard\\u201d and \\u201cmicrowave shoes\\u201d. It aims at evaluating the zero-shot and few-shot compositionality of action recognition models for unlikely compositions of common action verbs and object nouns. It contains 122 different actions which were obtained by combining verbs and nouns rarely co-occurring together in the large-scale textual corpus from HowTo100M, but that frequently appear separately.\", \"PwC License Name\": \"Unspecified\", \"PwC License URL\": null, \"S2 Citation Count (June 2024)\": \"\", \"S2 Date\": \"\"}, \"Bibtex\": \"@Article{Miech2020RareActAV,\\n author = {Antoine Miech and Jean-Baptiste Alayrac and I. Laptev and Josef Sivic and Andrew Zisserman},\\n booktitle = {arXiv.org},\\n journal = {ArXiv},\\n title = {RareAct: A video dataset of unusual interactions},\\n volume = {abs/2008.01018},\\n year = {2020}\\n}\\n\", \"License Use (DataProvenance)\": \"unspecified\", \"License Attribution (DataProvenance)\": 0, \"License Share Alike (DataProvenance)\": 0, \"License Use (DataProvenance IgnoreOpenAI)\": \"unspecified\", \"License Attribution (DataProvenance IgnoreOpenAI)\": 0, \"License Share Alike (DataProvenance IgnoreOpenAI)\": 0, \"License Type\": \"Unspecified\"}, {\"Unique Dataset Identifier\": \"rare-act-dataset\", \"Collection\": \"rare-act-dataset\", \"Collection URL\": \"https://arxiv.org/pdf/2008.01018.pdf\", \"Dataset Name\": \"RareAct: A video dataset of unusual interactions\", \"Paper Title\": \"RareAct: A video dataset of unusual interactions\", \"Paper URL\": \"https://arxiv.org/pdf/2008.01018.pdf\", \"GitHub URL\": \"https://github.com/antoine77340/RareAct\", \"Hugging Face URL\": \"\", \"Papers with Code URL\": \"https://paperswithcode.com/dataset/rareact\", \"ArXiv URL\": \"https://arxiv.org/pdf/2008.01018.pdf\", \"Semantic Scholar Corpus ID\": 220936243, \"Year Released\": \"2020\", \"Text Sources\": [\"undisclosed web\"], \"Licenses\": [{\"License\": \"Unspecified\", \"License URL\": \"\"}], \"Creators\": \"Academic\", \"Countries\": [\"United Kingdom\", \"France\", \"Czech Republic\"], \"License Verified By\": \"Vivek Sharma\", \"Video Hours\": 21.13, \"Taken Down\": \"False\", \"Video Sources\": [\"undisclosed web\"], \"Task Categories\": [\"Video Classification\"], \"Inferred Metadata\": {\"GitHub License\": \"Apache License 2.0\", \"GitHub Stars (June 2024)\": 32, \"GitHub Topics\": [], \"Github Date\": \"\", \"HF Config\": \"\", \"HF Config License\": \"\", \"HF Dataset\": \"\", \"HF Date\": \"\", \"HF Downloads (June 2024)\": \"\", \"HF Likes (June 2024)\": \"\", \"HF Yaml License\": \"\", \"PwC Date\": \"\", \"PwC Description\": \"RareAct is a video dataset of unusual actions, including actions like \\u201cblend phone\\u201d, \\u201ccut keyboard\\u201d and \\u201cmicrowave shoes\\u201d. It aims at evaluating the zero-shot and few-shot compositionality of action recognition models for unlikely compositions of common action verbs and object nouns. It contains 122 different actions which were obtained by combining verbs and nouns rarely co-occurring together in the large-scale textual corpus from HowTo100M, but that frequently appear separately.\", \"PwC License Name\": \"Unspecified\", \"PwC License URL\": null, \"S2 Citation Count (June 2024)\": \"\", \"S2 Date\": \"\"}, \"Bibtex\": \"@Article{Miech2020RareActAV,\\n author = {Antoine Miech and Jean-Baptiste Alayrac and I. Laptev and Josef Sivic and Andrew Zisserman},\\n booktitle = {arXiv.org},\\n journal = {ArXiv},\\n title = {RareAct: A video dataset of unusual interactions},\\n volume = {abs/2008.01018},\\n year = {2020}\\n}\\n\", \"License Use (DataProvenance)\": \"unspecified\", \"License Attribution (DataProvenance)\": 0, \"License Share Alike (DataProvenance)\": 0, \"License Use (DataProvenance IgnoreOpenAI)\": \"unspecified\", \"License Attribution (DataProvenance IgnoreOpenAI)\": 0, \"License Share Alike (DataProvenance IgnoreOpenAI)\": 0, \"License Type\": \"Unspecified\"}, {\"Unique Dataset Identifier\": \"lemma-dataset\", \"Collection\": \"lemma-dataset\", \"Collection URL\": \"https://arxiv.org/abs/2007.15781\", \"Dataset Name\": \"LEMMA: A Multi-view Dataset for LEarning Multi-agent Multi-task Activities (ECCV 2020)\", \"Paper Title\": \"LEMMA: A Multi-view Dataset for LEarning Multi-agent Multi-task Activities (ECCV 2020)\", \"Paper URL\": \"https://arxiv.org/abs/2007.15781\", \"GitHub URL\": \"https://github.com/Buzz-Beater/LEMMA\", \"Hugging Face URL\": \"\", \"Papers with Code URL\": \"https://paperswithcode.com/dataset/lemma\", \"ArXiv URL\": \"https://arxiv.org/abs/2007.15781\", \"Semantic Scholar Corpus ID\": 220634784, \"Year Released\": \"2020\", \"Text Sources\": [\"crowdsourced\", \"human\"], \"Licenses\": [{\"License\": \"Unspecified\", \"License URL\": \"\"}], \"Creators\": \"Academic\", \"Countries\": [\"United States of America\"], \"License Verified By\": \"Vivek Sharma\", \"Video Hours\": 10.8, \"Taken Down\": \"False\", \"Video Sources\": [\"crowdsourced\", \"human\"], \"Task Categories\": [\"Video Classification\"], \"Inferred Metadata\": {\"GitHub License\": \"\", \"GitHub Stars (June 2024)\": 27, \"GitHub Topics\": [], \"Github Date\": \"\", \"HF Config\": \"\", \"HF Config License\": \"\", \"HF Dataset\": \"\", \"HF Date\": \"\", \"HF Downloads (June 2024)\": \"\", \"HF Likes (June 2024)\": \"\", \"HF Yaml License\": \"\", \"PwC Date\": \"\", \"PwC Description\": \"The LEMMA dataset aims to explore the essence of complex human activities in a goal-directed, multi-agent, multi-task setting with ground-truth labels of compositional atomic-actions and their associated tasks. By quantifying the scenarios to up to two multi-step tasks with two agents, the authors strive to address human multi-task and multi-agent interactions in four scenarios: single-agent single-task (1 x 1), single-agent multi-task (1 x 2), multi-agent single-task (2 x 1), and multi-agent multi-task (2 x 2). Task instructions are only given to one agent in the 2 x 1 setting to resemble the robot-helping scenario, hoping that the learned perception models could be applied in robotic tasks (especially in HRI) in the near future.\\n\\nBoth the third-person views (TPVs) and the first-person views (FPVs) were recorded to account for different perspectives of the same activities. The authors densely annotate atomic-actions (in the form of compositional verb-noun pairs) and tasks of each atomic-action, as well as the spatial location of each participating agent (bounding boxes) to facilitate the learning of multi-agent multi-task task scheduling and assignment.\", \"PwC License Name\": \"Unspecified\", \"PwC License URL\": null, \"S2 Citation Count (June 2024)\": \"\", \"S2 Date\": \"\"}, \"Bibtex\": \"@Article{Jia2020LEMMAAM,\\n author = {Baoxiong Jia and Yixin Chen and Siyuan Huang and Yixin Zhu and Song-Chun Zhu},\\n booktitle = {European Conference on Computer Vision},\\n journal = {ArXiv},\\n title = {LEMMA: A Multi-view Dataset for Learning Multi-agent Multi-task Activities},\\n volume = {abs/2007.15781},\\n year = {2020}\\n}\\n\", \"License Use (DataProvenance)\": \"unspecified\", \"License Attribution (DataProvenance)\": 0, \"License Share Alike (DataProvenance)\": 0, \"License Use (DataProvenance IgnoreOpenAI)\": \"unspecified\", \"License Attribution (DataProvenance IgnoreOpenAI)\": 0, \"License Share Alike (DataProvenance IgnoreOpenAI)\": 0, \"License Type\": \"Unspecified\"}, {\"Unique Dataset Identifier\": \"rare-act-dataset\", \"Collection\": \"rare-act-dataset\", \"Collection URL\": \"https://arxiv.org/pdf/2008.01018.pdf\", \"Dataset Name\": \"RareAct: A video dataset of unusual interactions\", \"Paper Title\": \"RareAct: A video dataset of unusual interactions\", \"Paper URL\": \"https://arxiv.org/pdf/2008.01018.pdf\", \"GitHub URL\": \"https://github.com/antoine77340/RareAct\", \"Hugging Face URL\": \"\", \"Papers with Code URL\": \"https://paperswithcode.com/dataset/rareact\", \"ArXiv URL\": \"https://arxiv.org/pdf/2008.01018.pdf\", \"Semantic Scholar Corpus ID\": 220936243, \"Year Released\": \"2020\", \"Text Sources\": [\"undisclosed web\"], \"Licenses\": [{\"License\": \"Unspecified\", \"License URL\": \"\"}], \"Creators\": \"Academic\", \"Countries\": [\"United Kingdom\", \"France\", \"Czech Republic\"], \"License Verified By\": \"Vivek Sharma\", \"Video Hours\": 21.13, \"Taken Down\": \"False\", \"Video Sources\": [\"undisclosed web\"], \"Task Categories\": [\"Video Classification\"], \"Inferred Metadata\": {\"GitHub License\": \"Apache License 2.0\", \"GitHub Stars (June 2024)\": 32, \"GitHub Topics\": [], \"Github Date\": \"\", \"HF Config\": \"\", \"HF Config License\": \"\", \"HF Dataset\": \"\", \"HF Date\": \"\", \"HF Downloads (June 2024)\": \"\", \"HF Likes (June 2024)\": \"\", \"HF Yaml License\": \"\", \"PwC Date\": \"\", \"PwC Description\": \"RareAct is a video dataset of unusual actions, including actions like \\u201cblend phone\\u201d, \\u201ccut keyboard\\u201d and \\u201cmicrowave shoes\\u201d. It aims at evaluating the zero-shot and few-shot compositionality of action recognition models for unlikely compositions of common action verbs and object nouns. It contains 122 different actions which were obtained by combining verbs and nouns rarely co-occurring together in the large-scale textual corpus from HowTo100M, but that frequently appear separately.\", \"PwC License Name\": \"Unspecified\", \"PwC License URL\": null, \"S2 Citation Count (June 2024)\": \"\", \"S2 Date\": \"\"}, \"Bibtex\": \"@Article{Miech2020RareActAV,\\n author = {Antoine Miech and Jean-Baptiste Alayrac and I. Laptev and Josef Sivic and Andrew Zisserman},\\n booktitle = {arXiv.org},\\n journal = {ArXiv},\\n title = {RareAct: A video dataset of unusual interactions},\\n volume = {abs/2008.01018},\\n year = {2020}\\n}\\n\", \"License Use (DataProvenance)\": \"unspecified\", \"License Attribution (DataProvenance)\": 0, \"License Share Alike (DataProvenance)\": 0, \"License Use (DataProvenance IgnoreOpenAI)\": \"unspecified\", \"License Attribution (DataProvenance IgnoreOpenAI)\": 0, \"License Share Alike (DataProvenance IgnoreOpenAI)\": 0, \"License Type\": \"Unspecified\"}, {\"Unique Dataset Identifier\": \"tiny-virat\", \"Collection\": \"tiny-virat\", \"Collection URL\": \"https://arxiv.org/abs/2007.07355\", \"Dataset Name\": \"TinyVIRAT: Low-resolution Video Action Recognition\", \"Paper Title\": \"TinyVIRAT: Low-resolution Video Action Recognition\", \"Paper URL\": \"https://arxiv.org/abs/2007.07355\", \"GitHub URL\": \"https://github.com/UgurDemir/Tiny-VIRAT\", \"Hugging Face URL\": \"\", \"Papers with Code URL\": \"https://paperswithcode.com/dataset/tinyvirat\", \"ArXiv URL\": \"https://arxiv.org/abs/2007.07355\", \"Semantic Scholar Corpus ID\": 220525685, \"Year Released\": \"2020\", \"Text Sources\": [\"crowdsourced\"], \"Licenses\": [{\"License\": \"Unspecified\", \"License URL\": \"\"}], \"Creators\": \"Academic\", \"Countries\": [\"United States of America\"], \"License Verified By\": \"Vivek Sharma\", \"Video Hours\": 10.83, \"Taken Down\": \"False\", \"Video Sources\": [\"crowdsourced\"], \"Task Categories\": [\"Video Classification\"], \"Inferred Metadata\": {\"GitHub License\": \"\", \"GitHub Stars (June 2024)\": 16, \"GitHub Topics\": [], \"Github Date\": \"\", \"HF Config\": \"\", \"HF Config License\": \"\", \"HF Dataset\": \"\", \"HF Date\": \"\", \"HF Downloads (June 2024)\": \"\", \"HF Likes (June 2024)\": \"\", \"HF Yaml License\": \"\", \"PwC Date\": \"\", \"PwC Description\": \"TinyVIRAT contains natural low-resolution activities. The actions in TinyVIRAT videos have multiple labels and they are extracted from surveillance videos which makes them realistic and more challenging.\", \"PwC License Name\": \"Unspecified\", \"PwC License URL\": null, \"S2 Citation Count (June 2024)\": \"\", \"S2 Date\": \"\"}, \"Bibtex\": \"@Article{Demir2020TinyVIRATLV,\\n author = {Ugur Demir and Y. Rawat and M. Shah},\\n booktitle = {International Conference on Pattern Recognition},\\n journal = {2020 25th International Conference on Pattern Recognition (ICPR)},\\n pages = {7387-7394},\\n title = {TinyVIRAT: Low-resolution Video Action Recognition},\\n year = {2020}\\n}\\n\", \"License Use (DataProvenance)\": \"unspecified\", \"License Attribution (DataProvenance)\": 0, \"License Share Alike (DataProvenance)\": 0, \"License Use (DataProvenance IgnoreOpenAI)\": \"unspecified\", \"License Attribution (DataProvenance IgnoreOpenAI)\": 0, \"License Share Alike (DataProvenance IgnoreOpenAI)\": 0, \"License Type\": \"Unspecified\"}, {\"Unique Dataset Identifier\": \"movie-net\", \"Collection\": \"movie-net\", \"Collection URL\": \"https://arxiv.org/abs/2007.10937\", \"Dataset Name\": \"MovieNet\", \"Paper Title\": \"MovieNet\", \"Paper URL\": \"https://arxiv.org/abs/2007.10937\", \"GitHub URL\": \"https://github.com/movienet/movienet-tools\", \"Hugging Face URL\": \"\", \"Papers with Code URL\": \"https://paperswithcode.com/dataset/movienet\", \"ArXiv URL\": \"https://arxiv.org/abs/2007.10937\", \"Semantic Scholar Corpus ID\": 220665753, \"Year Released\": \"2020\", \"Text Sources\": [\"movies\"], \"Licenses\": [{\"License\": \"Unspecified\", \"License URL\": \"\"}], \"Creators\": \"Academic\", \"Countries\": [\"China\"], \"License Verified By\": \"Vivek Sharma\", \"Video Hours\": 3000.0, \"Taken Down\": \"False\", \"Video Sources\": [\"movies\"], \"Task Categories\": [\"Video Summarization\", \"Misc\"], \"Inferred Metadata\": {\"GitHub License\": \"\", \"GitHub Stars (June 2024)\": 256, \"GitHub Topics\": [\"action-recognition\", \"computer-vision\", \"cross-modality\", \"deep-learning\", \"movie\", \"person-analysis\", \"shot-detection\", \"video-understanding\", \"vision-language\"], \"Github Date\": \"\", \"HF Config\": \"\", \"HF Config License\": \"\", \"HF Dataset\": \"\", \"HF Date\": \"\", \"HF Downloads (June 2024)\": \"\", \"HF Likes (June 2024)\": \"\", \"HF Yaml License\": \"\", \"PwC Date\": \"2020-07-21\", \"PwC Description\": \"MovieNet is a holistic dataset for movie understanding. MovieNet contains 1,100 movies with a large amount of multi-modal data, e.g. trailers, photos, plot descriptions, etc.. Besides, different aspects of manual annotations are provided in MovieNet, including 1.1M characters with bounding boxes and identities, 42K scene boundaries, 2.5K aligned description sentences, 65K tags of place and action, and 92 K tags of cinematic style.\", \"PwC License Name\": \"Unspecified\", \"PwC License URL\": null, \"S2 Citation Count (June 2024)\": \"\", \"S2 Date\": \"\"}, \"Bibtex\": \"@Article{Huang2020MovieNetAH,\\n author = {Qingqiu Huang and Yu Xiong and Anyi Rao and Jiaze Wang and Dahua Lin},\\n booktitle = {European Conference on Computer Vision},\\n pages = {709-727},\\n title = {MovieNet: A Holistic Dataset for Movie Understanding},\\n year = {2020}\\n}\\n\", \"License Use (DataProvenance)\": \"unspecified\", \"License Attribution (DataProvenance)\": 0, \"License Share Alike (DataProvenance)\": 0, \"License Use (DataProvenance IgnoreOpenAI)\": \"unspecified\", \"License Attribution (DataProvenance IgnoreOpenAI)\": 0, \"License Share Alike (DataProvenance IgnoreOpenAI)\": 0, \"License Type\": \"Unspecified\"}, {\"Unique Dataset Identifier\": \"kinetics-700\", \"Collection\": \"kinetics-700\", \"Collection URL\": \"https://arxiv.org/pdf/2010.10864.pdf\", \"Dataset Name\": \"Kinetics-700\", \"Paper Title\": \"Kinetics-700\", \"Paper URL\": \"https://arxiv.org/pdf/2010.10864.pdf\", \"GitHub URL\": \"\", \"Hugging Face URL\": \"\", \"Papers with Code URL\": \"https://paperswithcode.com/dataset/kinetics-700\", \"ArXiv URL\": \"https://arxiv.org/pdf/2010.10864.pdf\", \"Semantic Scholar Corpus ID\": 196831809, \"Year Released\": \"2020\", \"Text Sources\": [\"youtube\"], \"Licenses\": [{\"License\": \"Unspecified\", \"License URL\": \"\"}], \"Creators\": \"Industry Lab\", \"Countries\": [\"United Kingdom\"], \"License Verified By\": \"Vivek Sharma\", \"Video Hours\": 1805.56, \"Taken Down\": \"False\", \"Video Sources\": [\"youtube\"], \"Task Categories\": [\"Video Classification\"], \"Inferred Metadata\": {\"GitHub License\": \"\", \"GitHub Stars (June 2024)\": \"\", \"GitHub Topics\": \"\", \"Github Date\": \"\", \"HF Config\": \"\", \"HF Config License\": \"\", \"HF Dataset\": \"\", \"HF Date\": \"\", \"HF Downloads (June 2024)\": \"\", \"HF Likes (June 2024)\": \"\", \"HF Yaml License\": \"\", \"PwC Date\": \"2019-07-15\", \"PwC Description\": \"Kinetics-700 is a video dataset of 650,000 clips that covers 700 human action classes. The videos include human-object interactions such as playing instruments, as well as human-human interactions such as shaking hands and hugging. Each action class has at least 700 video clips. Each clip is annotated with an action class and lasts approximately 10 seconds.\", \"PwC License Name\": \"CC BY 4.0\", \"PwC License URL\": \"https://creativecommons.org/licenses/by/4.0/\", \"S2 Citation Count (June 2024)\": \"\", \"S2 Date\": \"\"}, \"Bibtex\": \"@Article{Carreira2019ASN,\\n author = {Jo\\u00e3o Carreira and Eric Noland and Chloe Hillier and Andrew Zisserman},\\n booktitle = {arXiv.org},\\n journal = {ArXiv},\\n title = {A Short Note on the Kinetics-700 Human Action Dataset},\\n volume = {abs/1907.06987},\\n year = {2019}\\n}\\n\", \"License Use (DataProvenance)\": \"unspecified\", \"License Attribution (DataProvenance)\": 0, \"License Share Alike (DataProvenance)\": 0, \"License Use (DataProvenance IgnoreOpenAI)\": \"unspecified\", \"License Attribution (DataProvenance IgnoreOpenAI)\": 0, \"License Share Alike (DataProvenance IgnoreOpenAI)\": 0, \"License Type\": \"Unspecified\"}, {\"Unique Dataset Identifier\": \"haa500-dataset\", \"Collection\": \"haa500-dataset\", \"Collection URL\": \"https://arxiv.org/abs/2009.05224\", \"Dataset Name\": \"HAA500: Human-Centric Atomic Action Dataset with Curated Videos (ICCV 2021)\", \"Paper Title\": \"HAA500: Human-Centric Atomic Action Dataset with Curated Videos (ICCV 2021)\", \"Paper URL\": \"https://arxiv.org/abs/2009.05224\", \"GitHub URL\": \"\", \"Hugging Face URL\": \"\", \"Papers with Code URL\": \"https://paperswithcode.com/paper/haa500-human-centric-atomic-action-dataset\", \"ArXiv URL\": \"https://arxiv.org/abs/2009.05224\", \"Semantic Scholar Corpus ID\": 221640805, \"Year Released\": \"2020\", \"Text Sources\": [\"human\"], \"Licenses\": [{\"License\": \"Unspecified\", \"License URL\": \"\"}], \"Creators\": \"Academic\", \"Countries\": [\"China\", \"United States of America\"], \"License Verified By\": \"Vivek Sharma\", \"Video Hours\": 5.48, \"Taken Down\": \"False\", \"Video Sources\": [\"human\"], \"Task Categories\": [\"Video Classification\"], \"Inferred Metadata\": {\"GitHub License\": \"\", \"GitHub Stars (June 2024)\": \"\", \"GitHub Topics\": \"\", \"Github Date\": \"\", \"HF Config\": \"\", \"HF Config License\": \"\", \"HF Dataset\": \"\", \"HF Date\": \"\", \"HF Downloads (June 2024)\": \"\", \"HF Likes (June 2024)\": \"\", \"HF Yaml License\": \"\", \"PwC Date\": \"\", \"PwC Description\": \"\\ud83c\\udfc6 SOTA for Action Recognition on HAA500 (Top-1 (%) metric)\", \"PwC License Name\": \"\", \"PwC License URL\": \"\", \"S2 Citation Count (June 2024)\": \"\", \"S2 Date\": \"\"}, \"Bibtex\": \"@Article{Chung2020HAA500HA,\\n author = {Jihoon Chung and Cheng-hsin Wuu and Hsuan-ru Yang and Yu-Wing Tai and Chi-Keung Tang},\\n booktitle = {IEEE International Conference on Computer Vision},\\n journal = {2021 IEEE/CVF International Conference on Computer Vision (ICCV)},\\n pages = {13445-13454},\\n title = {HAA500: Human-Centric Atomic Action Dataset with Curated Videos},\\n year = {2020}\\n}\\n\", \"License Use (DataProvenance)\": \"unspecified\", \"License Attribution (DataProvenance)\": 0, \"License Share Alike (DataProvenance)\": 0, \"License Use (DataProvenance IgnoreOpenAI)\": \"unspecified\", \"License Attribution (DataProvenance IgnoreOpenAI)\": 0, \"License Share Alike (DataProvenance IgnoreOpenAI)\": 0, \"License Type\": \"Unspecified\"}, {\"Unique Dataset Identifier\": \"haa500-dataset\", \"Collection\": \"haa500-dataset\", \"Collection URL\": \"https://arxiv.org/abs/2009.05224\", \"Dataset Name\": \"HAA500: Human-Centric Atomic Action Dataset with Curated Videos (ICCV 2021)\", \"Paper Title\": \"HAA500: Human-Centric Atomic Action Dataset with Curated Videos (ICCV 2021)\", \"Paper URL\": \"https://arxiv.org/abs/2009.05224\", \"GitHub URL\": \"\", \"Hugging Face URL\": \"\", \"Papers with Code URL\": \"https://paperswithcode.com/paper/haa500-human-centric-atomic-action-dataset\", \"ArXiv URL\": \"https://arxiv.org/abs/2009.05224\", \"Semantic Scholar Corpus ID\": 221640805, \"Year Released\": \"2020\", \"Text Sources\": [\"human\"], \"Licenses\": [{\"License\": \"Unspecified\", \"License URL\": \"\"}], \"Creators\": \"Academic\", \"Countries\": [\"China\", \"United States of America\"], \"License Verified By\": \"Vivek Sharma\", \"Video Hours\": 5.48, \"Taken Down\": \"False\", \"Video Sources\": [\"human\"], \"Task Categories\": [\"Video Classification\"], \"Inferred Metadata\": {\"GitHub License\": \"\", \"GitHub Stars (June 2024)\": \"\", \"GitHub Topics\": \"\", \"Github Date\": \"\", \"HF Config\": \"\", \"HF Config License\": \"\", \"HF Dataset\": \"\", \"HF Date\": \"\", \"HF Downloads (June 2024)\": \"\", \"HF Likes (June 2024)\": \"\", \"HF Yaml License\": \"\", \"PwC Date\": \"\", \"PwC Description\": \"\\ud83c\\udfc6 SOTA for Action Recognition on HAA500 (Top-1 (%) metric)\", \"PwC License Name\": \"\", \"PwC License URL\": \"\", \"S2 Citation Count (June 2024)\": \"\", \"S2 Date\": \"\"}, \"Bibtex\": \"@Article{Chung2020HAA500HA,\\n author = {Jihoon Chung and Cheng-hsin Wuu and Hsuan-ru Yang and Yu-Wing Tai and Chi-Keung Tang},\\n booktitle = {IEEE International Conference on Computer Vision},\\n journal = {2021 IEEE/CVF International Conference on Computer Vision (ICCV)},\\n pages = {13445-13454},\\n title = {HAA500: Human-Centric Atomic Action Dataset with Curated Videos},\\n year = {2020}\\n}\\n\", \"License Use (DataProvenance)\": \"unspecified\", \"License Attribution (DataProvenance)\": 0, \"License Share Alike (DataProvenance)\": 0, \"License Use (DataProvenance IgnoreOpenAI)\": \"unspecified\", \"License Attribution (DataProvenance IgnoreOpenAI)\": 0, \"License Share Alike (DataProvenance IgnoreOpenAI)\": 0, \"License Type\": \"Unspecified\"}, {\"Unique Dataset Identifier\": \"haa500-dataset\", \"Collection\": \"haa500-dataset\", \"Collection URL\": \"https://arxiv.org/abs/2009.05224\", \"Dataset Name\": \"HAA500: Human-Centric Atomic Action Dataset with Curated Videos (ICCV 2021)\", \"Paper Title\": \"HAA500: Human-Centric Atomic Action Dataset with Curated Videos (ICCV 2021)\", \"Paper URL\": \"https://arxiv.org/abs/2009.05224\", \"GitHub URL\": \"\", \"Hugging Face URL\": \"\", \"Papers with Code URL\": \"https://paperswithcode.com/paper/haa500-human-centric-atomic-action-dataset\", \"ArXiv URL\": \"https://arxiv.org/abs/2009.05224\", \"Semantic Scholar Corpus ID\": 221640805, \"Year Released\": \"2020\", \"Text Sources\": [\"human\"], \"Licenses\": [{\"License\": \"Unspecified\", \"License URL\": \"\"}], \"Creators\": \"Academic\", \"Countries\": [\"China\", \"United States of America\"], \"License Verified By\": \"Vivek Sharma\", \"Video Hours\": 5.48, \"Taken Down\": \"False\", \"Video Sources\": [\"human\"], \"Task Categories\": [\"Video Classification\"], \"Inferred Metadata\": {\"GitHub License\": \"\", \"GitHub Stars (June 2024)\": \"\", \"GitHub Topics\": \"\", \"Github Date\": \"\", \"HF Config\": \"\", \"HF Config License\": \"\", \"HF Dataset\": \"\", \"HF Date\": \"\", \"HF Downloads (June 2024)\": \"\", \"HF Likes (June 2024)\": \"\", \"HF Yaml License\": \"\", \"PwC Date\": \"\", \"PwC Description\": \"\\ud83c\\udfc6 SOTA for Action Recognition on HAA500 (Top-1 (%) metric)\", \"PwC License Name\": \"\", \"PwC License URL\": \"\", \"S2 Citation Count (June 2024)\": \"\", \"S2 Date\": \"\"}, \"Bibtex\": \"@Article{Chung2020HAA500HA,\\n author = {Jihoon Chung and Cheng-hsin Wuu and Hsuan-ru Yang and Yu-Wing Tai and Chi-Keung Tang},\\n booktitle = {IEEE International Conference on Computer Vision},\\n journal = {2021 IEEE/CVF International Conference on Computer Vision (ICCV)},\\n pages = {13445-13454},\\n title = {HAA500: Human-Centric Atomic Action Dataset with Curated Videos},\\n year = {2020}\\n}\\n\", \"License Use (DataProvenance)\": \"unspecified\", \"License Attribution (DataProvenance)\": 0, \"License Share Alike (DataProvenance)\": 0, \"License Use (DataProvenance IgnoreOpenAI)\": \"unspecified\", \"License Attribution (DataProvenance IgnoreOpenAI)\": 0, \"License Share Alike (DataProvenance IgnoreOpenAI)\": 0, \"License Type\": \"Unspecified\"}, {\"Unique Dataset Identifier\": \"haa500-dataset\", \"Collection\": \"haa500-dataset\", \"Collection URL\": \"https://arxiv.org/abs/2009.05224\", \"Dataset Name\": \"HAA500: Human-Centric Atomic Action Dataset with Curated Videos (ICCV 2021)\", \"Paper Title\": \"HAA500: Human-Centric Atomic Action Dataset with Curated Videos (ICCV 2021)\", \"Paper URL\": \"https://arxiv.org/abs/2009.05224\", \"GitHub URL\": \"\", \"Hugging Face URL\": \"\", \"Papers with Code URL\": \"https://paperswithcode.com/paper/haa500-human-centric-atomic-action-dataset\", \"ArXiv URL\": \"https://arxiv.org/abs/2009.05224\", \"Semantic Scholar Corpus ID\": 221640805, \"Year Released\": \"2020\", \"Text Sources\": [\"human\"], \"Licenses\": [{\"License\": \"Unspecified\", \"License URL\": \"\"}], \"Creators\": \"Corporation\", \"Countries\": [\"China\", \"United States of America\"], \"License Verified By\": \"Vivek Sharma\", \"Video Hours\": 5.48, \"Taken Down\": \"False\", \"Video Sources\": [\"human\"], \"Task Categories\": [\"Video Classification\"], \"Inferred Metadata\": {\"GitHub License\": \"\", \"GitHub Stars (June 2024)\": \"\", \"GitHub Topics\": \"\", \"Github Date\": \"\", \"HF Config\": \"\", \"HF Config License\": \"\", \"HF Dataset\": \"\", \"HF Date\": \"\", \"HF Downloads (June 2024)\": \"\", \"HF Likes (June 2024)\": \"\", \"HF Yaml License\": \"\", \"PwC Date\": \"\", \"PwC Description\": \"\\ud83c\\udfc6 SOTA for Action Recognition on HAA500 (Top-1 (%) metric)\", \"PwC License Name\": \"\", \"PwC License URL\": \"\", \"S2 Citation Count (June 2024)\": \"\", \"S2 Date\": \"\"}, \"Bibtex\": \"@Article{Chung2020HAA500HA,\\n author = {Jihoon Chung and Cheng-hsin Wuu and Hsuan-ru Yang and Yu-Wing Tai and Chi-Keung Tang},\\n booktitle = {IEEE International Conference on Computer Vision},\\n journal = {2021 IEEE/CVF International Conference on Computer Vision (ICCV)},\\n pages = {13445-13454},\\n title = {HAA500: Human-Centric Atomic Action Dataset with Curated Videos},\\n year = {2020}\\n}\\n\", \"License Use (DataProvenance)\": \"unspecified\", \"License Attribution (DataProvenance)\": 0, \"License Share Alike (DataProvenance)\": 0, \"License Use (DataProvenance IgnoreOpenAI)\": \"unspecified\", \"License Attribution (DataProvenance IgnoreOpenAI)\": 0, \"License Share Alike (DataProvenance IgnoreOpenAI)\": 0, \"License Type\": \"Unspecified\"}, {\"Unique Dataset Identifier\": \"moviescenes\", \"Collection\": \"moviescenes\", \"Collection URL\": \"https://arxiv.org/abs/2004.02678\", \"Dataset Name\": \"MovieScenes\", \"Paper Title\": \"MovieScenes\", \"Paper URL\": \"https://arxiv.org/abs/2004.02678\", \"GitHub URL\": \"https://github.com/AnyiRao/SceneSeg\", \"Hugging Face URL\": \"\", \"Papers with Code URL\": \"https://paperswithcode.com/paper/a-local-to-global-approach-to-multi-modal\", \"ArXiv URL\": \"https://arxiv.org/abs/2004.02678\", \"Semantic Scholar Corpus ID\": 214802984, \"Year Released\": \"2020\", \"Text Sources\": [\"movies\"], \"Licenses\": [{\"License\": \"Unspecified\", \"License URL\": \"\"}], \"Creators\": \"Academic\", \"Countries\": [\"China\", \"United States of America\"], \"License Verified By\": \"Vivek Sharma\", \"Video Hours\": 250.0, \"Taken Down\": \"False\", \"Video Sources\": [\"movies\"], \"Task Categories\": [\"Misc (Scene Segmentation)\"], \"Inferred Metadata\": {\"GitHub License\": \"\", \"GitHub Stars (June 2024)\": 213, \"GitHub Topics\": [\"boundary-detection\", \"scene\", \"segmentation\", \"video-analysis\"], \"Github Date\": \"\", \"HF Config\": \"\", \"HF Config License\": \"\", \"HF Dataset\": \"\", \"HF Date\": \"\", \"HF Downloads (June 2024)\": \"\", \"HF Likes (June 2024)\": \"\", \"HF Yaml License\": \"\", \"PwC Date\": \"\", \"PwC Description\": \"4 code implementations in PyTorch and TensorFlow. Scene, as the crucial unit of storytelling in movies, contains complex activities of actors and their interactions in a physical environment. Identifying the composition of scenes serves as a critical step towards semantic understanding of movies. This is very challenging -- compared to the videos studied in conventional vision problems, e.g. action recognition, as scenes in movies usually contain much richer temporal structures and more complex semantic information. Towards this goal, we scale up the scene segmentation task by building a large-scale video dataset MovieScenes, which contains 21K annotated scene segments from 150 movies. We further propose a local-to-global scene segmentation framework, which integrates multi-modal information across three levels, i.e. clip, segment, and movie. This framework is able to distill complex semantics from hierarchical temporal structures over a long movie, providing top-down guidance for scene segmentation. Our experiments show that the proposed network is able to segment a movie into scenes with high accuracy, consistently outperforming previous methods. We also found that pretraining on our MovieScenes can bring significant improvements to the existing approaches.\", \"PwC License Name\": \"\", \"PwC License URL\": \"\", \"S2 Citation Count (June 2024)\": \"\", \"S2 Date\": \"\"}, \"Bibtex\": \"@Article{Rao2020ALA,\\n author = {Anyi Rao and Linning Xu and Yu Xiong and Guodong Xu and Qingqiu Huang and Bolei Zhou and Dahua Lin},\\n booktitle = {Computer Vision and Pattern Recognition},\\n journal = {2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},\\n pages = {10143-10152},\\n title = {A Local-to-Global Approach to Multi-Modal Movie Scene Segmentation},\\n year = {2020}\\n}\\n\", \"License Use (DataProvenance)\": \"unspecified\", \"License Attribution (DataProvenance)\": 0, \"License Share Alike (DataProvenance)\": 0, \"License Use (DataProvenance IgnoreOpenAI)\": \"unspecified\", \"License Attribution (DataProvenance IgnoreOpenAI)\": 0, \"License Share Alike (DataProvenance IgnoreOpenAI)\": 0, \"License Type\": \"Unspecified\"}, {\"Unique Dataset Identifier\": \"moviescenes\", \"Collection\": \"moviescenes\", \"Collection URL\": \"https://arxiv.org/abs/2004.02678\", \"Dataset Name\": \"MovieScenes\", \"Paper Title\": \"MovieScenes\", \"Paper URL\": \"https://arxiv.org/abs/2004.02678\", \"GitHub URL\": \"https://github.com/AnyiRao/SceneSeg\", \"Hugging Face URL\": \"\", \"Papers with Code URL\": \"https://paperswithcode.com/paper/a-local-to-global-approach-to-multi-modal\", \"ArXiv URL\": \"https://arxiv.org/abs/2004.02678\", \"Semantic Scholar Corpus ID\": 214802984, \"Year Released\": \"2020\", \"Text Sources\": [\"movies\"], \"Licenses\": [{\"License\": \"Unspecified\", \"License URL\": \"\"}], \"Creators\": \"Academic\", \"Countries\": [\"China\", \"United States of America\"], \"License Verified By\": \"Vivek Sharma\", \"Video Hours\": 250.0, \"Taken Down\": \"False\", \"Video Sources\": [\"movies\"], \"Task Categories\": [\"Misc (Scene Segmentation)\"], \"Inferred Metadata\": {\"GitHub License\": \"\", \"GitHub Stars (June 2024)\": 213, \"GitHub Topics\": [\"boundary-detection\", \"scene\", \"segmentation\", \"video-analysis\"], \"Github Date\": \"\", \"HF Config\": \"\", \"HF Config License\": \"\", \"HF Dataset\": \"\", \"HF Date\": \"\", \"HF Downloads (June 2024)\": \"\", \"HF Likes (June 2024)\": \"\", \"HF Yaml License\": \"\", \"PwC Date\": \"\", \"PwC Description\": \"4 code implementations in PyTorch and TensorFlow. Scene, as the crucial unit of storytelling in movies, contains complex activities of actors and their interactions in a physical environment. Identifying the composition of scenes serves as a critical step towards semantic understanding of movies. This is very challenging -- compared to the videos studied in conventional vision problems, e.g. action recognition, as scenes in movies usually contain much richer temporal structures and more complex semantic information. Towards this goal, we scale up the scene segmentation task by building a large-scale video dataset MovieScenes, which contains 21K annotated scene segments from 150 movies. We further propose a local-to-global scene segmentation framework, which integrates multi-modal information across three levels, i.e. clip, segment, and movie. This framework is able to distill complex semantics from hierarchical temporal structures over a long movie, providing top-down guidance for scene segmentation. Our experiments show that the proposed network is able to segment a movie into scenes with high accuracy, consistently outperforming previous methods. We also found that pretraining on our MovieScenes can bring significant improvements to the existing approaches.\", \"PwC License Name\": \"\", \"PwC License URL\": \"\", \"S2 Citation Count (June 2024)\": \"\", \"S2 Date\": \"\"}, \"Bibtex\": \"@Article{Rao2020ALA,\\n author = {Anyi Rao and Linning Xu and Yu Xiong and Guodong Xu and Qingqiu Huang and Bolei Zhou and Dahua Lin},\\n booktitle = {Computer Vision and Pattern Recognition},\\n journal = {2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},\\n pages = {10143-10152},\\n title = {A Local-to-Global Approach to Multi-Modal Movie Scene Segmentation},\\n year = {2020}\\n}\\n\", \"License Use (DataProvenance)\": \"unspecified\", \"License Attribution (DataProvenance)\": 0, \"License Share Alike (DataProvenance)\": 0, \"License Use (DataProvenance IgnoreOpenAI)\": \"unspecified\", \"License Attribution (DataProvenance IgnoreOpenAI)\": 0, \"License Share Alike (DataProvenance IgnoreOpenAI)\": 0, \"License Type\": \"Unspecified\"}, {\"Unique Dataset Identifier\": \"violin\", \"Collection\": \"violin\", \"Collection URL\": \"https://arxiv.org/abs/2003.11618\", \"Dataset Name\": \"VIOLIN\", \"Paper Title\": \"VIOLIN\", \"Paper URL\": \"https://arxiv.org/abs/2003.11618\", \"GitHub URL\": \"https://github.com/jimmy646/violin\", \"Hugging Face URL\": \"\", \"Papers with Code URL\": \"https://paperswithcode.com/dataset/violin\", \"ArXiv URL\": \"https://arxiv.org/abs/2003.11618\", \"Semantic Scholar Corpus ID\": 214668012, \"Year Released\": \"2020\", \"Text Sources\": [\"youtube\"], \"Licenses\": [{\"License\": \"Unspecified\", \"License URL\": \"\"}], \"Creators\": \"Industry Lab\", \"Countries\": [\"United States of America\"], \"License Verified By\": \"Vivek Sharma\", \"Video Hours\": 582.0, \"Taken Down\": \"False\", \"Video Sources\": [\"youtube\"], \"Task Categories\": [\"Video Captioning\"], \"Inferred Metadata\": {\"GitHub License\": \"MIT License\", \"GitHub Stars (June 2024)\": 156, \"GitHub Topics\": [], \"Github Date\": \"\", \"HF Config\": \"\", \"HF Config License\": \"\", \"HF Dataset\": \"\", \"HF Date\": \"\", \"HF Downloads (June 2024)\": \"\", \"HF Likes (June 2024)\": \"\", \"HF Yaml License\": \"\", \"PwC Date\": \"\", \"PwC Description\": \"Video-and-Language Inference is the task of joint multimodal understanding of video and text. Given a video clip with aligned subtitles as premise, paired with a natural language hypothesis based on the video content, a model needs to infer whether the hypothesis is entailed or contradicted by the given video clip. The Violin dataset is a dataset for this task which consists of 95,322 video-hypothesis pairs from 15,887 video clips, spanning over 582 hours of video. These video clips contain rich content with diverse temporal dynamics, event shifts, and people interactions, collected from two sources: (i) popular TV shows, and (ii) movie clips from YouTube channels.\", \"PwC License Name\": \"Unspecified\", \"PwC License URL\": null, \"S2 Citation Count (June 2024)\": \"\", \"S2 Date\": \"\"}, \"Bibtex\": \"@Article{Liu2020ViolinAL,\\n author = {J. Liu and Wenhu Chen and Yu Cheng and Zhe Gan and Licheng Yu and Yiming Yang and Jingjing Liu},\\n booktitle = {Computer Vision and Pattern Recognition},\\n journal = {2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},\\n pages = {10897-10907},\\n title = {Violin: A Large-Scale Dataset for Video-and-Language Inference},\\n year = {2020}\\n}\\n\", \"License Use (DataProvenance)\": \"unspecified\", \"License Attribution (DataProvenance)\": 0, \"License Share Alike (DataProvenance)\": 0, \"License Use (DataProvenance IgnoreOpenAI)\": \"unspecified\", \"License Attribution (DataProvenance IgnoreOpenAI)\": 0, \"License Share Alike (DataProvenance IgnoreOpenAI)\": 0, \"License Type\": \"Unspecified\"}, {\"Unique Dataset Identifier\": \"hyu-vids\", \"Collection\": \"hyu-vids\", \"Collection URL\": \"https://arxiv.org/abs/1904.11451\", \"Dataset Name\": \"HVU: Large Scale Holistic Video Understanding (ECCV 2020)\", \"Paper Title\": \"HVU: Large Scale Holistic Video Understanding (ECCV 2020)\", \"Paper URL\": \"https://arxiv.org/abs/1904.11451\", \"GitHub URL\": \"https://github.com/holistic-video-understanding/HVU-Dataset\", \"Hugging Face URL\": \"\", \"Papers with Code URL\": \"https://paperswithcode.com/paper/holistic-large-scale-video-understanding\", \"ArXiv URL\": \"https://arxiv.org/abs/1904.11451\", \"Semantic Scholar Corpus ID\": 131777079, \"Year Released\": \"2020\", \"Text Sources\": [\"youtube\"], \"Licenses\": [{\"License\": \"Custom\", \"License URL\": \"https://github.com/holistic-video-understanding/HVU-Dataset\"}], \"Creators\": \"Academic\", \"Countries\": [\"Switzerland\", \"Belgium\", \"Germany\"], \"License Verified By\": \"Vivek Sharma\", \"Video Hours\": 96166.67, \"Taken Down\": \"False\", \"Video Sources\": [\"youtube\"], \"Task Categories\": [\"Video Classification\"], \"Inferred Metadata\": {\"GitHub License\": \"\", \"GitHub Stars (June 2024)\": 70, \"GitHub Topics\": [], \"Github Date\": \"\", \"HF Config\": \"\", \"HF Config License\": \"\", \"HF Dataset\": \"\", \"HF Date\": \"\", \"HF Downloads (June 2024)\": \"\", \"HF Likes (June 2024)\": \"\", \"HF Yaml License\": \"\", \"PwC Date\": \"\", \"PwC Description\": \"#11 best model for Action Recognition on UCF101 (3-fold Accuracy metric)\", \"PwC License Name\": \"\", \"PwC License URL\": \"\", \"S2 Citation Count (June 2024)\": \"\", \"S2 Date\": \"\"}, \"Bibtex\": \"@Article{Diba2019HolisticLS,\\n author = {Ali Diba and Mohsen Fayyaz and Vivek Sharma and Manohar Paluri and Juergen Gall and R. Stiefelhagen and L. Gool},\\n booktitle = {arXiv.org},\\n journal = {ArXiv},\\n title = {Holistic Large Scale Video Understanding},\\n volume = {abs/1904.11451},\\n year = {2019}\\n}\\n\", \"License Use (DataProvenance)\": \"academic-only\", \"License Attribution (DataProvenance)\": 1, \"License Share Alike (DataProvenance)\": 0, \"License Use (DataProvenance IgnoreOpenAI)\": \"academic-only\", \"License Attribution (DataProvenance IgnoreOpenAI)\": 1, \"License Share Alike (DataProvenance IgnoreOpenAI)\": 0, \"License Type\": \"Non-Commercial/Academic\"}, {\"Unique Dataset Identifier\": \"100doh\", \"Collection\": \"100doh\", \"Collection URL\": \"https://arxiv.org/abs/2006.06669\", \"Dataset Name\": \"100DOH: Understanding Human Hands in Contact at Internet Scale (CVPR 2020)\", \"Paper Title\": \"100DOH: Understanding Human Hands in Contact at Internet Scale (CVPR 2020)\", \"Paper URL\": \"https://arxiv.org/abs/2006.06669\", \"GitHub URL\": \"https://github.com/ddshan/hand_object_detector\", \"Hugging Face URL\": \"\", \"Papers with Code URL\": \"https://paperswithcode.com/paper/understanding-human-hands-in-contact-at-1\", \"ArXiv URL\": \"https://arxiv.org/abs/2006.06669\", \"Semantic Scholar Corpus ID\": 215413188, \"Year Released\": \"2020\", \"Text Sources\": [\"youtube\"], \"Licenses\": [{\"License\": \"Custom\", \"License URL\": \"https://fouheylab.eecs.umich.edu/~dandans/projects/100DOH/download.html\"}], \"Creators\": \"Academic\", \"Countries\": [\"United States of America\"], \"License Verified By\": \"Vivek Sharma\", \"Video Hours\": 4577.3, \"Taken Down\": \"False\", \"Video Sources\": [\"youtube\"], \"Task Categories\": [\"Misc (Hand/Object Detection)\"], \"Inferred Metadata\": {\"GitHub License\": \"MIT License\", \"GitHub Stars (June 2024)\": 213, \"GitHub Topics\": [\"cvpr2020\", \"dataset\", \"handobjectdetection\", \"interactiondetection\"], \"Github Date\": \"\", \"HF Config\": \"\", \"HF Config License\": \"\", \"HF Dataset\": \"\", \"HF Date\": \"\", \"HF Downloads (June 2024)\": \"\", \"HF Likes (June 2024)\": \"\", \"HF Yaml License\": \"\", \"PwC Date\": \"\", \"PwC Description\": \"Implemented in one code library.\", \"PwC License Name\": \"\", \"PwC License URL\": \"\", \"S2 Citation Count (June 2024)\": \"\", \"S2 Date\": \"\"}, \"Bibtex\": \"@Article{Shan2020UnderstandingHH,\\n author = {Dandan Shan and Jiaqi Geng and Michelle Shu and D. Fouhey},\\n booktitle = {Computer Vision and Pattern Recognition},\\n journal = {2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},\\n pages = {9866-9875},\\n title = {Understanding Human Hands in Contact at Internet Scale},\\n year = {2020}\\n}\\n\", \"License Use (DataProvenance)\": \"academic-only\", \"License Attribution (DataProvenance)\": 1, \"License Share Alike (DataProvenance)\": 0, \"License Use (DataProvenance IgnoreOpenAI)\": \"academic-only\", \"License Attribution (DataProvenance IgnoreOpenAI)\": 1, \"License Share Alike (DataProvenance IgnoreOpenAI)\": 0, \"License Type\": \"Non-Commercial/Academic\"}, {\"Unique Dataset Identifier\": \"hyu-vids\", \"Collection\": \"hyu-vids\", \"Collection URL\": \"https://arxiv.org/abs/1904.11451\", \"Dataset Name\": \"HVU: Large Scale Holistic Video Understanding (ECCV 2020)\", \"Paper Title\": \"HVU: Large Scale Holistic Video Understanding (ECCV 2020)\", \"Paper URL\": \"https://arxiv.org/abs/1904.11451\", \"GitHub URL\": \"https://github.com/holistic-video-understanding/HVU-Dataset\", \"Hugging Face URL\": \"\", \"Papers with Code URL\": \"https://paperswithcode.com/paper/holistic-large-scale-video-understanding\", \"ArXiv URL\": \"https://arxiv.org/abs/1904.11451\", \"Semantic Scholar Corpus ID\": 131777079, \"Year Released\": \"2020\", \"Text Sources\": [\"youtube\"], \"Licenses\": [{\"License\": \"Custom\", \"License URL\": \"https://github.com/holistic-video-understanding/HVU-Dataset\"}], \"Creators\": \"Academic\", \"Countries\": [\"Switzerland\", \"Belgium\", \"Germany\"], \"License Verified By\": \"Vivek Sharma\", \"Video Hours\": 96166.67, \"Taken Down\": \"False\", \"Video Sources\": [\"youtube\"], \"Task Categories\": [\"Video Classification\"], \"Inferred Metadata\": {\"GitHub License\": \"\", \"GitHub Stars (June 2024)\": 70, \"GitHub Topics\": [], \"Github Date\": \"\", \"HF Config\": \"\", \"HF Config License\": \"\", \"HF Dataset\": \"\", \"HF Date\": \"\", \"HF Downloads (June 2024)\": \"\", \"HF Likes (June 2024)\": \"\", \"HF Yaml License\": \"\", \"PwC Date\": \"\", \"PwC Description\": \"#11 best model for Action Recognition on UCF101 (3-fold Accuracy metric)\", \"PwC License Name\": \"\", \"PwC License URL\": \"\", \"S2 Citation Count (June 2024)\": \"\", \"S2 Date\": \"\"}, \"Bibtex\": \"@Article{Diba2019HolisticLS,\\n author = {Ali Diba and Mohsen Fayyaz and Vivek Sharma and Manohar Paluri and Juergen Gall and R. Stiefelhagen and L. Gool},\\n booktitle = {arXiv.org},\\n journal = {ArXiv},\\n title = {Holistic Large Scale Video Understanding},\\n volume = {abs/1904.11451},\\n year = {2019}\\n}\\n\", \"License Use (DataProvenance)\": \"academic-only\", \"License Attribution (DataProvenance)\": 1, \"License Share Alike (DataProvenance)\": 0, \"License Use (DataProvenance IgnoreOpenAI)\": \"academic-only\", \"License Attribution (DataProvenance IgnoreOpenAI)\": 1, \"License Share Alike (DataProvenance IgnoreOpenAI)\": 0, \"License Type\": \"Non-Commercial/Academic\"}, {\"Unique Dataset Identifier\": \"mimetics-dataset\", \"Collection\": \"mimetics-dataset\", \"Collection URL\": \"https://arxiv.org/abs/1912.07249\", \"Dataset Name\": \"Mimetics: Towards Understanding Human Actions Out of Context\", \"Paper Title\": \"Mimetics: Towards Understanding Human Actions Out of Context\", \"Paper URL\": \"https://arxiv.org/abs/1912.07249\", \"GitHub URL\": \"\", \"Hugging Face URL\": \"\", \"Papers with Code URL\": \"https://paperswithcode.com/paper/mimetics-towards-understanding-human-actions\", \"ArXiv URL\": \"https://arxiv.org/abs/1912.07249\", \"Semantic Scholar Corpus ID\": 209376248, \"Year Released\": \"2021\", \"Text Sources\": [\"youtube\"], \"Licenses\": [{\"License\": \"Unspecified\", \"License URL\": \"\"}], \"Creators\": \"Corporation\", \"Countries\": [\"South Korea\"], \"License Verified By\": \"Vivek Sharma\", \"Video Hours\": 0.99, \"Taken Down\": \"False\", \"Video Sources\": [\"youtube\"], \"Task Categories\": [\"Video Classification\"], \"Inferred Metadata\": {\"GitHub License\": \"\", \"GitHub Stars (June 2024)\": \"\", \"GitHub Topics\": \"\", \"Github Date\": \"\", \"HF Config\": \"\", \"HF Config License\": \"\", \"HF Dataset\": \"\", \"HF Date\": \"\", \"HF Downloads (June 2024)\": \"\", \"HF Likes (June 2024)\": \"\", \"HF Yaml License\": \"\", \"PwC Date\": \"\", \"PwC Description\": \"No code available yet.\", \"PwC License Name\": \"\", \"PwC License URL\": \"\", \"S2 Citation Count (June 2024)\": \"\", \"S2 Date\": \"\"}, \"Bibtex\": \"@Article{Weinzaepfel2019MimeticsTU,\\n author = {Philippe Weinzaepfel and Gr\\u00e9gory Rogez},\\n booktitle = {International Journal of Computer Vision},\\n journal = {International Journal of Computer Vision},\\n pages = {1675 - 1690},\\n title = {Mimetics: Towards Understanding Human Actions Out of Context},\\n volume = {129},\\n year = {2019}\\n}\\n\", \"License Use (DataProvenance)\": \"unspecified\", \"License Attribution (DataProvenance)\": 0, \"License Share Alike (DataProvenance)\": 0, \"License Use (DataProvenance IgnoreOpenAI)\": \"unspecified\", \"License Attribution (DataProvenance IgnoreOpenAI)\": 0, \"License Share Alike (DataProvenance IgnoreOpenAI)\": 0, \"License Type\": \"Unspecified\"}, {\"Unique Dataset Identifier\": \"homage\", \"Collection\": \"homage\", \"Collection URL\": \"https://arxiv.org/abs/2105.05226\", \"Dataset Name\": \"HOMAGE: Home Action Genome: Cooperative Compositional Action Understanding (CVPR 2021)\", \"Paper Title\": \"HOMAGE: Home Action Genome: Cooperative Compositional Action Understanding (CVPR 2021)\", \"Paper URL\": \"https://arxiv.org/abs/2105.05226\", \"GitHub URL\": \"https://github.com/nishantrai18/homage\", \"Hugging Face URL\": \"\", \"Papers with Code URL\": \"https://paperswithcode.com/paper/home-action-genome-cooperative-compositional\", \"ArXiv URL\": \"https://arxiv.org/abs/2105.05226\", \"Semantic Scholar Corpus ID\": 234357543, \"Year Released\": \"2021\", \"Text Sources\": [\"human\"], \"Licenses\": [{\"License\": \"Unspecified\", \"License URL\": \"\"}], \"Creators\": \"Academic\", \"Countries\": [\"United States of America\"], \"License Verified By\": \"Vivek Sharma\", \"Video Hours\": 30.0, \"Taken Down\": \"False\", \"Video Sources\": [\"human\"], \"Task Categories\": [\"Video Classification\", \"Temporal Action Segmentation\"], \"Inferred Metadata\": {\"GitHub License\": \"MIT License\", \"GitHub Stars (June 2024)\": 17, \"GitHub Topics\": [], \"Github Date\": \"\", \"HF Config\": \"\", \"HF Config License\": \"\", \"HF Dataset\": \"\", \"HF Date\": \"\", \"HF Downloads (June 2024)\": \"\", \"HF Likes (June 2024)\": \"\", \"HF Yaml License\": \"\", \"PwC Date\": \"\", \"PwC Description\": \"\\ud83c\\udfc6 SOTA for Video Classification on Home Action Genome (Accuracy (%) metric)\", \"PwC License Name\": \"\", \"PwC License URL\": \"\", \"S2 Citation Count (June 2024)\": \"\", \"S2 Date\": \"\"}, \"Bibtex\": \"@Article{Rai2021HomeAG,\\n author = {Nishant Rai and Haofeng Chen and Jingwei Ji and Rishi Desai and K. Kozuka and Shun Ishizaka and E. Adeli and Juan Carlos Niebles},\\n booktitle = {Computer Vision and Pattern Recognition},\\n journal = {2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},\\n pages = {11179-11188},\\n title = {Home Action Genome: Cooperative Compositional Action Understanding},\\n year = {2021}\\n}\\n\", \"License Use (DataProvenance)\": \"unspecified\", \"License Attribution (DataProvenance)\": 0, \"License Share Alike (DataProvenance)\": 0, \"License Use (DataProvenance IgnoreOpenAI)\": \"unspecified\", \"License Attribution (DataProvenance IgnoreOpenAI)\": 0, \"License Share Alike (DataProvenance IgnoreOpenAI)\": 0, \"License Type\": \"Unspecified\"}, {\"Unique Dataset Identifier\": \"homage\", \"Collection\": \"homage\", \"Collection URL\": \"https://arxiv.org/abs/2105.05226\", \"Dataset Name\": \"HOMAGE: Home Action Genome: Cooperative Compositional Action Understanding (CVPR 2021)\", \"Paper Title\": \"HOMAGE: Home Action Genome: Cooperative Compositional Action Understanding (CVPR 2021)\", \"Paper URL\": \"https://arxiv.org/abs/2105.05226\", \"GitHub URL\": \"https://github.com/nishantrai18/homage\", \"Hugging Face URL\": \"\", \"Papers with Code URL\": \"https://paperswithcode.com/paper/home-action-genome-cooperative-compositional\", \"ArXiv URL\": \"https://arxiv.org/abs/2105.05226\", \"Semantic Scholar Corpus ID\": 234357543, \"Year Released\": \"2021\", \"Text Sources\": [\"human\"], \"Licenses\": [{\"License\": \"Unspecified\", \"License URL\": \"\"}], \"Creators\": \"Corporation\", \"Countries\": [\"United States of America\"], \"License Verified By\": \"Vivek Sharma\", \"Video Hours\": 30.0, \"Taken Down\": \"False\", \"Video Sources\": [\"human\"], \"Task Categories\": [\"Video Classification\", \"Temporal Action Segmentation\"], \"Inferred Metadata\": {\"GitHub License\": \"MIT License\", \"GitHub Stars (June 2024)\": 17, \"GitHub Topics\": [], \"Github Date\": \"\", \"HF Config\": \"\", \"HF Config License\": \"\", \"HF Dataset\": \"\", \"HF Date\": \"\", \"HF Downloads (June 2024)\": \"\", \"HF Likes (June 2024)\": \"\", \"HF Yaml License\": \"\", \"PwC Date\": \"\", \"PwC Description\": \"\\ud83c\\udfc6 SOTA for Video Classification on Home Action Genome (Accuracy (%) metric)\", \"PwC License Name\": \"\", \"PwC License URL\": \"\", \"S2 Citation Count (June 2024)\": \"\", \"S2 Date\": \"\"}, \"Bibtex\": \"@Article{Rai2021HomeAG,\\n author = {Nishant Rai and Haofeng Chen and Jingwei Ji and Rishi Desai and K. Kozuka and Shun Ishizaka and E. Adeli and Juan Carlos Niebles},\\n booktitle = {Computer Vision and Pattern Recognition},\\n journal = {2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},\\n pages = {11179-11188},\\n title = {Home Action Genome: Cooperative Compositional Action Understanding},\\n year = {2021}\\n}\\n\", \"License Use (DataProvenance)\": \"unspecified\", \"License Attribution (DataProvenance)\": 0, \"License Share Alike (DataProvenance)\": 0, \"License Use (DataProvenance IgnoreOpenAI)\": \"unspecified\", \"License Attribution (DataProvenance IgnoreOpenAI)\": 0, \"License Share Alike (DataProvenance IgnoreOpenAI)\": 0, \"License Type\": \"Unspecified\"}, {\"Unique Dataset Identifier\": \"WebVid\", \"Collection\": \"WebVid\", \"Collection URL\": \"https://arxiv.org/abs/2104.00650\", \"Dataset Name\": \"WebVid\", \"Paper Title\": \"WebVid\", \"Paper URL\": \"https://arxiv.org/abs/2104.00650\", \"GitHub URL\": \"https://github.com/m-bain/webvid\", \"Hugging Face URL\": \"https://huggingface.co/datasets/TempoFunk/webvid-10M\", \"Papers with Code URL\": \"https://paperswithcode.com/dataset/webvid\", \"ArXiv URL\": \"https://arxiv.org/abs/2104.00650\", \"Semantic Scholar Corpus ID\": 232478955, \"Year Released\": \"2021\", \"Text Sources\": [\"undisclosed web\"], \"Licenses\": [{\"License\": \"Custom\", \"License URL\": \"https://github.com/m-bain/webvid/blob/main/TERMS.md\"}], \"Creators\": \"Academic\", \"Countries\": [\"United Kingdom\", \"France\"], \"License Verified By\": \"Vivek Sharma\", \"Video Hours\": 13000.0, \"Taken Down\": \"True\", \"Video Sources\": [\"undisclosed web\"], \"Task Categories\": [\"Video Captioning\"], \"Inferred Metadata\": {\"GitHub License\": \"\", \"GitHub Stars (June 2024)\": 528, \"GitHub Topics\": [], \"Github Date\": \"\", \"HF Config\": \"default\", \"HF Config License\": \"\", \"HF Dataset\": \"TempoFunk/webvid-10M\", \"HF Date\": \"2023-06-16\", \"HF Downloads (June 2024)\": 360, \"HF Likes (June 2024)\": 28, \"HF Yaml License\": \"GNU General Public License v3.0\", \"PwC Date\": \"2021-04-01\", \"PwC Description\": \"WebVid contains 10 million video clips with captions, sourced from the web. The videos are diverse and rich in their content.\\n\\nBoth the full 10M set and a 2.5M subset is available for download:\\nhttps://github.com/m-bain/webvid-dataset\", \"PwC License Name\": \"Various\", \"PwC License URL\": \"https://github.com/m-bain/webvid/blob/main/TERMS.md\", \"S2 Citation Count (June 2024)\": \"\", \"S2 Date\": \"\"}, \"Bibtex\": \"@Article{Bain2021FrozenIT,\\n author = {Max Bain and Arsha Nagrani and G\\u00fcl Varol and Andrew Zisserman},\\n booktitle = {IEEE International Conference on Computer Vision},\\n journal = {2021 IEEE/CVF International Conference on Computer Vision (ICCV)},\\n pages = {1708-1718},\\n title = {Frozen in Time: A Joint Video and Image Encoder for End-to-End Retrieval},\\n year = {2021}\\n}\\n\", \"License Use (DataProvenance)\": \"academic-only\", \"License Attribution (DataProvenance)\": 0, \"License Share Alike (DataProvenance)\": 0, \"License Use (DataProvenance IgnoreOpenAI)\": \"academic-only\", \"License Attribution (DataProvenance IgnoreOpenAI)\": 0, \"License Share Alike (DataProvenance IgnoreOpenAI)\": 0, \"License Type\": \"Non-Commercial/Academic\"}, {\"Unique Dataset Identifier\": \"multi-moments-in-time-dataset\", \"Collection\": \"multi-moments-in-time-dataset\", \"Collection URL\": \"https://arxiv.org/pdf/1911.00232.pdf\", \"Dataset Name\": \"Multi-Moments in Time: Learning and Interpreting Models for Multi-Action Video Understanding\", \"Paper Title\": \"Multi-Moments in Time: Learning and Interpreting Models for Multi-Action Video Understanding\", \"Paper URL\": \"https://arxiv.org/pdf/1911.00232.pdf\", \"GitHub URL\": \"\", \"Hugging Face URL\": \"\", \"Papers with Code URL\": \"\", \"ArXiv URL\": \"https://arxiv.org/pdf/1911.00232.pdf\", \"Semantic Scholar Corpus ID\": 207780280, \"Year Released\": \"2021\", \"Text Sources\": [\"crowdsourced\", \"undisclosed web\"], \"Licenses\": [{\"License\": \"Unspecified\", \"License URL\": \"\"}], \"Creators\": \"Academic\", \"Countries\": [\"United States of America\"], \"License Verified By\": \"Vivek Sharma\", \"Video Hours\": 833.0, \"Taken Down\": \"False\", \"Video Sources\": [\"crowdsourced\", \"undisclosed web\"], \"Task Categories\": [\"Video Classification\"], \"Inferred Metadata\": {\"GitHub License\": \"\", \"GitHub Stars (June 2024)\": \"\", \"GitHub Topics\": \"\", \"Github Date\": \"\", \"HF Config\": \"\", \"HF Config License\": \"\", \"HF Dataset\": \"\", \"HF Date\": \"\", \"HF Downloads (June 2024)\": \"\", \"HF Likes (June 2024)\": \"\", \"HF Yaml License\": \"\", \"PwC Date\": \"\", \"PwC Description\": \"\", \"PwC License Name\": \"\", \"PwC License URL\": \"\", \"S2 Citation Count (June 2024)\": \"\", \"S2 Date\": \"\"}, \"Bibtex\": \"@Article{Monfort2019MultiMomentsIT,\\n author = {Mathew Monfort and K. Ramakrishnan and A. Andonian and Barry A. McNamara and A. Lascelles and Bowen Pan and Quanfu Fan and Dan Gutfreund and R. Feris and A. Oliva},\\n booktitle = {IEEE Transactions on Pattern Analysis and Machine Intelligence},\\n journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},\\n pages = {1-1},\\n title = {Multi-Moments in Time: Learning and Interpreting Models for Multi-Action Video Understanding},\\n volume = {PP},\\n year = {2019}\\n}\\n\", \"License Use (DataProvenance)\": \"unspecified\", \"License Attribution (DataProvenance)\": 0, \"License Share Alike (DataProvenance)\": 0, \"License Use (DataProvenance IgnoreOpenAI)\": \"unspecified\", \"License Attribution (DataProvenance IgnoreOpenAI)\": 0, \"License Share Alike (DataProvenance IgnoreOpenAI)\": 0, \"License Type\": \"Unspecified\"}, {\"Unique Dataset Identifier\": \"apes\", \"Collection\": \"apes\", \"Collection URL\": \"https://arxiv.org/pdf/2106.01667\", \"Dataset Name\": \"Apes\", \"Paper Title\": \"Apes\", \"Paper URL\": \"https://arxiv.org/pdf/2106.01667\", \"GitHub URL\": \"https://github.com/fuankarion/audiovisual-person-search\", \"Hugging Face URL\": \"\", \"Papers with Code URL\": \"https://paperswithcode.com/paper/apes-audiovisual-person-search-in-untrimmed/review/\", \"ArXiv URL\": \"https://arxiv.org/pdf/2106.01667\", \"Semantic Scholar Corpus ID\": 235313698, \"Year Released\": \"2021\", \"Text Sources\": [\"movies\"], \"Licenses\": [{\"License\": \"Unspecified\", \"License URL\": \"\"}], \"Creators\": \"Academic\", \"Countries\": [\"Chile\", \"United States of America\", \"Saudi Arabia\"], \"License Verified By\": \"Vivek Sharma\", \"Video Hours\": 36.0, \"Taken Down\": \"False\", \"Video Sources\": [\"movies\"], \"Task Categories\": [\"Video Classification\"], \"Inferred Metadata\": {\"GitHub License\": \"\", \"GitHub Stars (June 2024)\": 4, \"GitHub Topics\": [], \"Github Date\": \"\", \"HF Config\": \"\", \"HF Config License\": \"\", \"HF Dataset\": \"\", \"HF Date\": \"\", \"HF Downloads (June 2024)\": \"\", \"HF Likes (June 2024)\": \"\", \"HF Yaml License\": \"\", \"PwC Date\": \"\", \"PwC Description\": \"Paper tables with annotated results for APES: Audiovisual Person Search in Untrimmed Video\", \"PwC License Name\": \"\", \"PwC License URL\": \"\", \"S2 Citation Count (June 2024)\": \"\", \"S2 Date\": \"\"}, \"Bibtex\": \"@Article{Alcazar2021APESAP,\\n author = {Juan Leon Alcazar and Long Mai and Federico Perazzi and Joon-Young Lee and Pablo Arbel\\u00e1ez and Bernard Ghanem and Fabian Caba Heilbron},\\n booktitle = {2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)},\\n journal = {2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)},\\n pages = {1720-1729},\\n title = {APES: Audiovisual Person Search in Untrimmed Video},\\n year = {2021}\\n}\\n\", \"License Use (DataProvenance)\": \"unspecified\", \"License Attribution (DataProvenance)\": 0, \"License Share Alike (DataProvenance)\": 0, \"License Use (DataProvenance IgnoreOpenAI)\": \"unspecified\", \"License Attribution (DataProvenance IgnoreOpenAI)\": 0, \"License Share Alike (DataProvenance IgnoreOpenAI)\": 0, \"License Type\": \"Unspecified\"}, {\"Unique Dataset Identifier\": \"spoken-moments\", \"Collection\": \"spoken-moments\", \"Collection URL\": \"https://arxiv.org/abs/2105.04489\", \"Dataset Name\": \"Spoken Moments\", \"Paper Title\": \"Spoken Moments\", \"Paper URL\": \"https://arxiv.org/abs/2105.04489\", \"GitHub URL\": \"\", \"Hugging Face URL\": \"\", \"Papers with Code URL\": \"https://paperswithcode.com/paper/spoken-moments-learning-joint-audio-visual\", \"ArXiv URL\": \"https://arxiv.org/abs/2105.04489\", \"Semantic Scholar Corpus ID\": 233992735, \"Year Released\": \"2021\", \"Text Sources\": [\"youtube\", \"flickr\", \"vine\", \"metacafe\", \"peeks\", \"vimeo\", \"videoblocks\", \"bing videos\", \"giphy\", \"weather channel\", \"getty-images\"], \"Licenses\": [{\"License\": \"Custom\", \"License URL\": \"https://docs.google.com/forms/d/e/1FAIpQLSdUOScon6uWkslgcA2-BCkAhMp86SihgKtMqX53zRf-E2fIHw/formResponse\"}], \"Creators\": \"Academic\", \"Countries\": [\"United States of America\"], \"License Verified By\": \"Vivek Sharma\", \"Video Hours\": 416.67, \"Taken Down\": \"False\", \"Video Sources\": [\"youtube\", \"flickr\", \"vine\", \"metacafe\", \"peeks\", \"vimeo\", \"videoblocks\", \"bing videos\", \"giphy\", \"weather channel\", \"getty-images\"], \"Task Categories\": [\"Video Captioning\"], \"Inferred Metadata\": {\"GitHub License\": \"\", \"GitHub Stars (June 2024)\": \"\", \"GitHub Topics\": \"\", \"Github Date\": \"\", \"HF Config\": \"\", \"HF Config License\": \"\", \"HF Dataset\": \"\", \"HF Date\": \"\", \"HF Downloads (June 2024)\": \"\", \"HF Likes (June 2024)\": \"\", \"HF Yaml License\": \"\", \"PwC Date\": \"\", \"PwC Description\": \"No code available yet.\", \"PwC License Name\": \"\", \"PwC License URL\": \"\", \"S2 Citation Count (June 2024)\": \"\", \"S2 Date\": \"\"}, \"Bibtex\": \"@Article{Monfort2021SpokenML,\\n author = {Mathew Monfort and SouYoung Jin},\\n booktitle = {Computer Vision and Pattern Recognition},\\n journal = {2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},\\n pages = {14866-14876},\\n title = {Spoken Moments: Learning Joint Audio-Visual Representations from Video Descriptions},\\n year = {2021}\\n}\\n\", \"License Use (DataProvenance)\": \"commercial\", \"License Attribution (DataProvenance)\": 0, \"License Share Alike (DataProvenance)\": 0, \"License Use (DataProvenance IgnoreOpenAI)\": \"commercial\", \"License Attribution (DataProvenance IgnoreOpenAI)\": 0, \"License Share Alike (DataProvenance IgnoreOpenAI)\": 0, \"License Type\": \"Commercial\"}, {\"Unique Dataset Identifier\": \"spoken-moments\", \"Collection\": \"spoken-moments\", \"Collection URL\": \"https://arxiv.org/abs/2105.04489\", \"Dataset Name\": \"Spoken Moments\", \"Paper Title\": \"Spoken Moments\", \"Paper URL\": \"https://arxiv.org/abs/2105.04489\", \"GitHub URL\": \"\", \"Hugging Face URL\": \"\", \"Papers with Code URL\": \"https://paperswithcode.com/paper/spoken-moments-learning-joint-audio-visual\", \"ArXiv URL\": \"https://arxiv.org/abs/2105.04489\", \"Semantic Scholar Corpus ID\": 233992735, \"Year Released\": \"2021\", \"Text Sources\": [\"youtube\", \"flickr\", \"vine\", \"metacafe\", \"peeks\", \"vimeo\", \"videoblocks\", \"bing videos\", \"giphy\", \"weather channel\", \"getty-images\"], \"Licenses\": [{\"License\": \"Custom\", \"License URL\": \"https://docs.google.com/forms/d/e/1FAIpQLSdUOScon6uWkslgcA2-BCkAhMp86SihgKtMqX53zRf-E2fIHw/formResponse\"}], \"Creators\": \"Corporation\", \"Countries\": [\"United States of America\"], \"License Verified By\": \"Vivek Sharma\", \"Video Hours\": 416.67, \"Taken Down\": \"False\", \"Video Sources\": [\"youtube\", \"flickr\", \"vine\", \"metacafe\", \"peeks\", \"vimeo\", \"videoblocks\", \"bing videos\", \"giphy\", \"weather channel\", \"getty-images\"], \"Task Categories\": [\"Video Captioning\"], \"Inferred Metadata\": {\"GitHub License\": \"\", \"GitHub Stars (June 2024)\": \"\", \"GitHub Topics\": \"\", \"Github Date\": \"\", \"HF Config\": \"\", \"HF Config License\": \"\", \"HF Dataset\": \"\", \"HF Date\": \"\", \"HF Downloads (June 2024)\": \"\", \"HF Likes (June 2024)\": \"\", \"HF Yaml License\": \"\", \"PwC Date\": \"\", \"PwC Description\": \"No code available yet.\", \"PwC License Name\": \"\", \"PwC License URL\": \"\", \"S2 Citation Count (June 2024)\": \"\", \"S2 Date\": \"\"}, \"Bibtex\": \"@Article{Monfort2021SpokenML,\\n author = {Mathew Monfort and SouYoung Jin},\\n booktitle = {Computer Vision and Pattern Recognition},\\n journal = {2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},\\n pages = {14866-14876},\\n title = {Spoken Moments: Learning Joint Audio-Visual Representations from Video Descriptions},\\n year = {2021}\\n}\\n\", \"License Use (DataProvenance)\": \"commercial\", \"License Attribution (DataProvenance)\": 0, \"License Share Alike (DataProvenance)\": 0, \"License Use (DataProvenance IgnoreOpenAI)\": \"commercial\", \"License Attribution (DataProvenance IgnoreOpenAI)\": 0, \"License Share Alike (DataProvenance IgnoreOpenAI)\": 0, \"License Type\": \"Commercial\"}, {\"Unique Dataset Identifier\": \"spoken-moments\", \"Collection\": \"spoken-moments\", \"Collection URL\": \"https://arxiv.org/abs/2105.04489\", \"Dataset Name\": \"Spoken Moments\", \"Paper Title\": \"Spoken Moments\", \"Paper URL\": \"https://arxiv.org/abs/2105.04489\", \"GitHub URL\": \"\", \"Hugging Face URL\": \"\", \"Papers with Code URL\": \"https://paperswithcode.com/paper/spoken-moments-learning-joint-audio-visual\", \"ArXiv URL\": \"https://arxiv.org/abs/2105.04489\", \"Semantic Scholar Corpus ID\": 233992735, \"Year Released\": \"2021\", \"Text Sources\": [\"youtube\", \"flickr\", \"vine\", \"metacafe\", \"peeks\", \"vimeo\", \"videoblocks\", \"bing videos\", \"giphy\", \"weather channel\", \"getty-images\"], \"Licenses\": [{\"License\": \"Custom\", \"License URL\": \"https://docs.google.com/forms/d/e/1FAIpQLSdUOScon6uWkslgcA2-BCkAhMp86SihgKtMqX53zRf-E2fIHw/formResponse\"}], \"Creators\": \"Academic\", \"Countries\": [\"United States of America\"], \"License Verified By\": \"Vivek Sharma\", \"Video Hours\": 416.67, \"Taken Down\": \"False\", \"Video Sources\": [\"youtube\", \"flickr\", \"vine\", \"metacafe\", \"peeks\", \"vimeo\", \"videoblocks\", \"bing videos\", \"giphy\", \"weather channel\", \"getty-images\"], \"Task Categories\": [\"Video Captioning\"], \"Inferred Metadata\": {\"GitHub License\": \"\", \"GitHub Stars (June 2024)\": \"\", \"GitHub Topics\": \"\", \"Github Date\": \"\", \"HF Config\": \"\", \"HF Config License\": \"\", \"HF Dataset\": \"\", \"HF Date\": \"\", \"HF Downloads (June 2024)\": \"\", \"HF Likes (June 2024)\": \"\", \"HF Yaml License\": \"\", \"PwC Date\": \"\", \"PwC Description\": \"No code available yet.\", \"PwC License Name\": \"\", \"PwC License URL\": \"\", \"S2 Citation Count (June 2024)\": \"\", \"S2 Date\": \"\"}, \"Bibtex\": \"@Article{Monfort2021SpokenML,\\n author = {Mathew Monfort and SouYoung Jin},\\n booktitle = {Computer Vision and Pattern Recognition},\\n journal = {2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},\\n pages = {14866-14876},\\n title = {Spoken Moments: Learning Joint Audio-Visual Representations from Video Descriptions},\\n year = {2021}\\n}\\n\", \"License Use (DataProvenance)\": \"commercial\", \"License Attribution (DataProvenance)\": 0, \"License Share Alike (DataProvenance)\": 0, \"License Use (DataProvenance IgnoreOpenAI)\": \"commercial\", \"License Attribution (DataProvenance IgnoreOpenAI)\": 0, \"License Share Alike (DataProvenance IgnoreOpenAI)\": 0, \"License Type\": \"Commercial\"}, {\"Unique Dataset Identifier\": \"videolt-dataset\", \"Collection\": \"videolt-dataset\", \"Collection URL\": \"https://arxiv.org/abs/2105.02668\", \"Dataset Name\": \"VideoLT: Large-scale Long-tailed Video Recognition\", \"Paper Title\": \"VideoLT: Large-scale Long-tailed Video Recognition\", \"Paper URL\": \"https://arxiv.org/abs/2105.02668\", \"GitHub URL\": \"https://github.com/17Skye17/VideoLT\", \"Hugging Face URL\": \"\", \"Papers with Code URL\": \"https://paperswithcode.com/dataset/videolt\", \"ArXiv URL\": \"https://arxiv.org/abs/2105.02668\", \"Semantic Scholar Corpus ID\": 233864776, \"Year Released\": \"2021\", \"Text Sources\": [\"youtube\"], \"Licenses\": [{\"License\": \"Non Commercial\", \"License URL\": \"https://github.com/17Skye17/VideoLT?tab=readme-ov-file#data-preparation\"}], \"Creators\": \"Academic\", \"Countries\": [\"China\", \"UAE\"], \"License Verified By\": \"Vivek Sharma\", \"Video Hours\": 13664.96, \"Taken Down\": \"False\", \"Video Sources\": [\"youtube\"], \"Task Categories\": [\"Video Classification\"], \"Inferred Metadata\": {\"GitHub License\": \"\", \"GitHub Stars (June 2024)\": 33, \"GitHub Topics\": [\"long-tailed-recognition\", \"video-classification\", \"video-dataset\"], \"Github Date\": \"\", \"HF Config\": \"\", \"HF Config License\": \"\", \"HF Dataset\": \"\", \"HF Date\": \"\", \"HF Downloads (June 2024)\": \"\", \"HF Likes (June 2024)\": \"\", \"HF Yaml License\": \"\", \"PwC Date\": \"2021-05-06\", \"PwC Description\": \"VideoLT is a large-scale long-tailed video recognition dataset that contains 256,218 untrimmed videos, annotated into 1,004 classes with a long-tailed distribution.\", \"PwC License Name\": \"Unspecified\", \"PwC License URL\": null, \"S2 Citation Count (June 2024)\": \"\", \"S2 Date\": \"\"}, \"Bibtex\": \"@Article{Zhang2021VideoLTLL,\\n author = {Xing Zhang and Zuxuan Wu and Zejia Weng and H. Fu and Jingjing Chen and Yu-Gang Jiang and Larry S. Davis},\\n booktitle = {IEEE International Conference on Computer Vision},\\n journal = {2021 IEEE/CVF International Conference on Computer Vision (ICCV)},\\n pages = {7940-7949},\\n title = {VideoLT: Large-scale Long-tailed Video Recognition},\\n year = {2021}\\n}\\n\", \"License Use (DataProvenance)\": \"non-commercial\", \"License Attribution (DataProvenance)\": 0, \"License Share Alike (DataProvenance)\": 0, \"License Use (DataProvenance IgnoreOpenAI)\": \"non-commercial\", \"License Attribution (DataProvenance IgnoreOpenAI)\": 0, \"License Share Alike (DataProvenance IgnoreOpenAI)\": 0, \"License Type\": \"Non-Commercial/Academic\"}, {\"Unique Dataset Identifier\": \"videolt-dataset\", \"Collection\": \"videolt-dataset\", \"Collection URL\": \"https://arxiv.org/abs/2105.02668\", \"Dataset Name\": \"VideoLT: Large-scale Long-tailed Video Recognition\", \"Paper Title\": \"VideoLT: Large-scale Long-tailed Video Recognition\", \"Paper URL\": \"https://arxiv.org/abs/2105.02668\", \"GitHub URL\": \"https://github.com/17Skye17/VideoLT\", \"Hugging Face URL\": \"\", \"Papers with Code URL\": \"https://paperswithcode.com/dataset/videolt\", \"ArXiv URL\": \"https://arxiv.org/abs/2105.02668\", \"Semantic Scholar Corpus ID\": 233864776, \"Year Released\": \"2021\", \"Text Sources\": [\"youtube\"], \"Licenses\": [{\"License\": \"Non Commercial\", \"License URL\": \"https://github.com/17Skye17/VideoLT?tab=readme-ov-file#data-preparation\"}], \"Creators\": \"Research Group\", \"Countries\": [\"China\", \"UAE\"], \"License Verified By\": \"Vivek Sharma\", \"Video Hours\": 13664.96, \"Taken Down\": \"False\", \"Video Sources\": [\"youtube\"], \"Task Categories\": [\"Video Classification\"], \"Inferred Metadata\": {\"GitHub License\": \"\", \"GitHub Stars (June 2024)\": 33, \"GitHub Topics\": [\"long-tailed-recognition\", \"video-classification\", \"video-dataset\"], \"Github Date\": \"\", \"HF Config\": \"\", \"HF Config License\": \"\", \"HF Dataset\": \"\", \"HF Date\": \"\", \"HF Downloads (June 2024)\": \"\", \"HF Likes (June 2024)\": \"\", \"HF Yaml License\": \"\", \"PwC Date\": \"2021-05-06\", \"PwC Description\": \"VideoLT is a large-scale long-tailed video recognition dataset that contains 256,218 untrimmed videos, annotated into 1,004 classes with a long-tailed distribution.\", \"PwC License Name\": \"Unspecified\", \"PwC License URL\": null, \"S2 Citation Count (June 2024)\": \"\", \"S2 Date\": \"\"}, \"Bibtex\": \"@Article{Zhang2021VideoLTLL,\\n author = {Xing Zhang and Zuxuan Wu and Zejia Weng and H. Fu and Jingjing Chen and Yu-Gang Jiang and Larry S. Davis},\\n booktitle = {IEEE International Conference on Computer Vision},\\n journal = {2021 IEEE/CVF International Conference on Computer Vision (ICCV)},\\n pages = {7940-7949},\\n title = {VideoLT: Large-scale Long-tailed Video Recognition},\\n year = {2021}\\n}\\n\", \"License Use (DataProvenance)\": \"non-commercial\", \"License Attribution (DataProvenance)\": 0, \"License Share Alike (DataProvenance)\": 0, \"License Use (DataProvenance IgnoreOpenAI)\": \"non-commercial\", \"License Attribution (DataProvenance IgnoreOpenAI)\": 0, \"License Share Alike (DataProvenance IgnoreOpenAI)\": 0, \"License Type\": \"Non-Commercial/Academic\"}, {\"Unique Dataset Identifier\": \"videolt-dataset\", \"Collection\": \"videolt-dataset\", \"Collection URL\": \"https://arxiv.org/abs/2105.02668\", \"Dataset Name\": \"VideoLT: Large-scale Long-tailed Video Recognition\", \"Paper Title\": \"VideoLT: Large-scale Long-tailed Video Recognition\", \"Paper URL\": \"https://arxiv.org/abs/2105.02668\", \"GitHub URL\": \"https://github.com/17Skye17/VideoLT\", \"Hugging Face URL\": \"\", \"Papers with Code URL\": \"https://paperswithcode.com/dataset/videolt\", \"ArXiv URL\": \"https://arxiv.org/abs/2105.02668\", \"Semantic Scholar Corpus ID\": 233864776, \"Year Released\": \"2021\", \"Text Sources\": [\"youtube\"], \"Licenses\": [{\"License\": \"Non Commercial\", \"License URL\": \"https://github.com/17Skye17/VideoLT?tab=readme-ov-file#data-preparation\"}], \"Creators\": \"Other\", \"Countries\": [\"China\", \"UAE\"], \"License Verified By\": \"Vivek Sharma\", \"Video Hours\": 13664.96, \"Taken Down\": \"False\", \"Video Sources\": [\"youtube\"], \"Task Categories\": [\"Video Classification\"], \"Inferred Metadata\": {\"GitHub License\": \"\", \"GitHub Stars (June 2024)\": 33, \"GitHub Topics\": [\"long-tailed-recognition\", \"video-classification\", \"video-dataset\"], \"Github Date\": \"\", \"HF Config\": \"\", \"HF Config License\": \"\", \"HF Dataset\": \"\", \"HF Date\": \"\", \"HF Downloads (June 2024)\": \"\", \"HF Likes (June 2024)\": \"\", \"HF Yaml License\": \"\", \"PwC Date\": \"2021-05-06\", \"PwC Description\": \"VideoLT is a large-scale long-tailed video recognition dataset that contains 256,218 untrimmed videos, annotated into 1,004 classes with a long-tailed distribution.\", \"PwC License Name\": \"Unspecified\", \"PwC License URL\": null, \"S2 Citation Count (June 2024)\": \"\", \"S2 Date\": \"\"}, \"Bibtex\": \"@Article{Zhang2021VideoLTLL,\\n author = {Xing Zhang and Zuxuan Wu and Zejia Weng and H. Fu and Jingjing Chen and Yu-Gang Jiang and Larry S. Davis},\\n booktitle = {IEEE International Conference on Computer Vision},\\n journal = {2021 IEEE/CVF International Conference on Computer Vision (ICCV)},\\n pages = {7940-7949},\\n title = {VideoLT: Large-scale Long-tailed Video Recognition},\\n year = {2021}\\n}\\n\", \"License Use (DataProvenance)\": \"non-commercial\", \"License Attribution (DataProvenance)\": 0, \"License Share Alike (DataProvenance)\": 0, \"License Use (DataProvenance IgnoreOpenAI)\": \"non-commercial\", \"License Attribution (DataProvenance IgnoreOpenAI)\": 0, \"License Share Alike (DataProvenance IgnoreOpenAI)\": 0, \"License Type\": \"Non-Commercial/Academic\"}, {\"Unique Dataset Identifier\": \"videolt-dataset\", \"Collection\": \"videolt-dataset\", \"Collection URL\": \"https://arxiv.org/abs/2105.02668\", \"Dataset Name\": \"VideoLT: Large-scale Long-tailed Video Recognition\", \"Paper Title\": \"VideoLT: Large-scale Long-tailed Video Recognition\", \"Paper URL\": \"https://arxiv.org/abs/2105.02668\", \"GitHub URL\": \"https://github.com/17Skye17/VideoLT\", \"Hugging Face URL\": \"\", \"Papers with Code URL\": \"https://paperswithcode.com/dataset/videolt\", \"ArXiv URL\": \"https://arxiv.org/abs/2105.02668\", \"Semantic Scholar Corpus ID\": 233864776, \"Year Released\": \"2021\", \"Text Sources\": [\"youtube\"], \"Licenses\": [{\"License\": \"Non Commercial\", \"License URL\": \"https://github.com/17Skye17/VideoLT?tab=readme-ov-file#data-preparation\"}], \"Creators\": \"Academic\", \"Countries\": [\"China\", \"UAE\"], \"License Verified By\": \"Vivek Sharma\", \"Video Hours\": 13664.96, \"Taken Down\": \"False\", \"Video Sources\": [\"youtube\"], \"Task Categories\": [\"Video Classification\"], \"Inferred Metadata\": {\"GitHub License\": \"\", \"GitHub Stars (June 2024)\": 33, \"GitHub Topics\": [\"long-tailed-recognition\", \"video-classification\", \"video-dataset\"], \"Github Date\": \"\", \"HF Config\": \"\", \"HF Config License\": \"\", \"HF Dataset\": \"\", \"HF Date\": \"\", \"HF Downloads (June 2024)\": \"\", \"HF Likes (June 2024)\": \"\", \"HF Yaml License\": \"\", \"PwC Date\": \"2021-05-06\", \"PwC Description\": \"VideoLT is a large-scale long-tailed video recognition dataset that contains 256,218 untrimmed videos, annotated into 1,004 classes with a long-tailed distribution.\", \"PwC License Name\": \"Unspecified\", \"PwC License URL\": null, \"S2 Citation Count (June 2024)\": \"\", \"S2 Date\": \"\"}, \"Bibtex\": \"@Article{Zhang2021VideoLTLL,\\n author = {Xing Zhang and Zuxuan Wu and Zejia Weng and H. Fu and Jingjing Chen and Yu-Gang Jiang and Larry S. Davis},\\n booktitle = {IEEE International Conference on Computer Vision},\\n journal = {2021 IEEE/CVF International Conference on Computer Vision (ICCV)},\\n pages = {7940-7949},\\n title = {VideoLT: Large-scale Long-tailed Video Recognition},\\n year = {2021}\\n}\\n\", \"License Use (DataProvenance)\": \"non-commercial\", \"License Attribution (DataProvenance)\": 0, \"License Share Alike (DataProvenance)\": 0, \"License Use (DataProvenance IgnoreOpenAI)\": \"non-commercial\", \"License Attribution (DataProvenance IgnoreOpenAI)\": 0, \"License Share Alike (DataProvenance IgnoreOpenAI)\": 0, \"License Type\": \"Non-Commercial/Academic\"}, {\"Unique Dataset Identifier\": \"apes\", \"Collection\": \"apes\", \"Collection URL\": \"https://arxiv.org/pdf/2106.01667\", \"Dataset Name\": \"Apes\", \"Paper Title\": \"Apes\", \"Paper URL\": \"https://arxiv.org/pdf/2106.01667\", \"GitHub URL\": \"https://github.com/fuankarion/audiovisual-person-search\", \"Hugging Face URL\": \"\", \"Papers with Code URL\": \"https://paperswithcode.com/paper/apes-audiovisual-person-search-in-untrimmed/review/\", \"ArXiv URL\": \"https://arxiv.org/pdf/2106.01667\", \"Semantic Scholar Corpus ID\": 235313698, \"Year Released\": \"2021\", \"Text Sources\": [\"movies\"], \"Licenses\": [{\"License\": \"Unspecified\", \"License URL\": \"\"}], \"Creators\": \"Corporation\", \"Countries\": [\"Chile\", \"United States of America\", \"Saudi Arabia\"], \"License Verified By\": \"Vivek Sharma\", \"Video Hours\": 36.0, \"Taken Down\": \"False\", \"Video Sources\": [\"movies\"], \"Task Categories\": [\"Video Classification\"], \"Inferred Metadata\": {\"GitHub License\": \"\", \"GitHub Stars (June 2024)\": 4, \"GitHub Topics\": [], \"Github Date\": \"\", \"HF Config\": \"\", \"HF Config License\": \"\", \"HF Dataset\": \"\", \"HF Date\": \"\", \"HF Downloads (June 2024)\": \"\", \"HF Likes (June 2024)\": \"\", \"HF Yaml License\": \"\", \"PwC Date\": \"\", \"PwC Description\": \"Paper tables with annotated results for APES: Audiovisual Person Search in Untrimmed Video\", \"PwC License Name\": \"\", \"PwC License URL\": \"\", \"S2 Citation Count (June 2024)\": \"\", \"S2 Date\": \"\"}, \"Bibtex\": \"@Article{Alcazar2021APESAP,\\n author = {Juan Leon Alcazar and Long Mai and Federico Perazzi and Joon-Young Lee and Pablo Arbel\\u00e1ez and Bernard Ghanem and Fabian Caba Heilbron},\\n booktitle = {2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)},\\n journal = {2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)},\\n pages = {1720-1729},\\n title = {APES: Audiovisual Person Search in Untrimmed Video},\\n year = {2021}\\n}\\n\", \"License Use (DataProvenance)\": \"unspecified\", \"License Attribution (DataProvenance)\": 0, \"License Share Alike (DataProvenance)\": 0, \"License Use (DataProvenance IgnoreOpenAI)\": \"unspecified\", \"License Attribution (DataProvenance IgnoreOpenAI)\": 0, \"License Share Alike (DataProvenance IgnoreOpenAI)\": 0, \"License Type\": \"Unspecified\"}, {\"Unique Dataset Identifier\": \"hd-vila-100m\", \"Collection\": \"hd-vila-100m\", \"Collection URL\": \"https://arxiv.org/abs/2111.10337\", \"Dataset Name\": \"Advancing High-Resolution Video-Language Representation with Large-Scale Video Transcriptions\", \"Paper Title\": \"Advancing High-Resolution Video-Language Representation with Large-Scale Video Transcriptions\", \"Paper URL\": \"https://arxiv.org/abs/2111.10337\", \"GitHub URL\": \"https://github.com/microsoft/XPretrain/blob/main/hd-vila-100m/README.md\", \"Hugging Face URL\": \"\", \"Papers with Code URL\": \"https://paperswithcode.com/paper/advancing-high-resolution-video-language/review/\", \"ArXiv URL\": \"https://arxiv.org/abs/2111.10337\", \"Semantic Scholar Corpus ID\": 244462849, \"Year Released\": \"2021\", \"Text Sources\": [\"youtube\"], \"Licenses\": [{\"License\": \"Custom\", \"License URL\": \"https://github.com/microsoft/XPretrain/blob/main/hd-vila-100m/README.md\"}], \"Creators\": \"Industry Lab\", \"Countries\": [\"United States of America\"], \"License Verified By\": \"Vivek Sharma\", \"Video Hours\": 371.5, \"Taken Down\": \"False\", \"Video Sources\": [\"youtube\"], \"Task Categories\": [\"Video Captioning\"], \"Inferred Metadata\": {\"GitHub License\": \"\", \"GitHub Stars (June 2024)\": 0, \"GitHub Topics\": [\"\"], \"Github Date\": \"\", \"HF Config\": \"\", \"HF Config License\": \"\", \"HF Dataset\": \"\", \"HF Date\": \"\", \"HF Downloads (June 2024)\": \"\", \"HF Likes (June 2024)\": \"\", \"HF Yaml License\": \"\", \"PwC Date\": \"\", \"PwC Description\": \"Paper tables with annotated results for Advancing High-Resolution Video-Language Representation with Large-Scale Video Transcriptions\", \"PwC License Name\": \"\", \"PwC License URL\": \"\", \"S2 Citation Count (June 2024)\": \"\", \"S2 Date\": \"\"}, \"Bibtex\": \"@Article{Xue2021AdvancingHV,\\n author = {Hongwei Xue and Tiankai Hang and Yanhong Zeng and Yuchong Sun and Bei Liu and Huan Yang and Jianlong Fu and B. Guo},\\n booktitle = {Computer Vision and Pattern Recognition},\\n journal = {2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},\\n pages = {5026-5035},\\n title = {Advancing High-Resolution Video-Language Representation with Large-Scale Video Transcriptions},\\n year = {2021}\\n}\\n\", \"License Use (DataProvenance)\": \"academic-only\", \"License Attribution (DataProvenance)\": 1, \"License Share Alike (DataProvenance)\": 0, \"License Use (DataProvenance IgnoreOpenAI)\": \"academic-only\", \"License Attribution (DataProvenance IgnoreOpenAI)\": 1, \"License Share Alike (DataProvenance IgnoreOpenAI)\": 0, \"License Type\": \"Non-Commercial/Academic\"}, {\"Unique Dataset Identifier\": \"YT-Temporal-180m\", \"Collection\": \"YT-Temporal-180m\", \"Collection URL\": \"https://arxiv.org/pdf/2106.02636\", \"Dataset Name\": \"MERLOT: Multimodal Neural Script Knowledge Models\", \"Paper Title\": \"MERLOT: Multimodal Neural Script Knowledge Models\", \"Paper URL\": \"https://arxiv.org/pdf/2106.02636\", \"GitHub URL\": \"https://github.com/rowanz/merlot\", \"Hugging Face URL\": \"https://huggingface.co/datasets/HuggingFaceM4/yttemporal180m\", \"Papers with Code URL\": \"https://paperswithcode.com/paper/merlot-multimodal-neural-script-knowledge\", \"ArXiv URL\": \"https://arxiv.org/pdf/2106.02636\", \"Semantic Scholar Corpus ID\": 235352775, \"Year Released\": \"2021\", \"Text Sources\": [\"youtube\"], \"Licenses\": [{\"License\": \"MIT License\", \"License URL\": \"https://github.com/rowanz/merlot/blob/main/LICENSE\"}], \"Creators\": \"Academic\", \"Countries\": [\"United States of America\"], \"License Verified By\": \"Vivek Sharma\", \"Video Hours\": 1515.0, \"Taken Down\": \"False\", \"Video Sources\": [\"youtube\"], \"Task Categories\": [\"Video Q&A\"], \"Inferred Metadata\": null, \"Bibtex\": null, \"License Use (DataProvenance)\": \"commercial\", \"License Attribution (DataProvenance)\": 1, \"License Share Alike (DataProvenance)\": 0, \"License Use (DataProvenance IgnoreOpenAI)\": \"commercial\", \"License Attribution (DataProvenance IgnoreOpenAI)\": 1, \"License Share Alike (DataProvenance IgnoreOpenAI)\": 0, \"License Type\": \"Commercial\"}, {\"Unique Dataset Identifier\": \"queryd\", \"Collection\": \"queryd\", \"Collection URL\": \"https://arxiv.org/abs/2011.11071\", \"Dataset Name\": \"QuerYD\", \"Paper Title\": \"QuerYD\", \"Paper URL\": \"https://arxiv.org/abs/2011.11071\", \"GitHub URL\": \"\", \"Hugging Face URL\": \"\", \"Papers with Code URL\": \"https://paperswithcode.com/dataset/queryd\", \"ArXiv URL\": \"https://arxiv.org/abs/2011.11071\", \"Semantic Scholar Corpus ID\": 261006321, \"Year Released\": \"2021\", \"Text Sources\": [\"youtube\", \"youdescribe\"], \"Licenses\": [{\"License\": \"Unspecified\", \"License URL\": \"\"}], \"Creators\": \"Academic\", \"Countries\": [\"United Kingdom\"], \"License Verified By\": \"Vivek Sharma\", \"Video Hours\": 207.0, \"Taken Down\": \"False\", \"Video Sources\": [\"youtube\", \"youdescribe\"], \"Task Categories\": [\"Video Captioning\"], \"Inferred Metadata\": {\"GitHub License\": \"\", \"GitHub Stars (June 2024)\": \"\", \"GitHub Topics\": \"\", \"Github Date\": \"\", \"HF Config\": \"\", \"HF Config License\": \"\", \"HF Dataset\": \"\", \"HF Date\": \"\", \"HF Downloads (June 2024)\": \"\", \"HF Likes (June 2024)\": \"\", \"HF Yaml License\": \"\", \"PwC Date\": \"\", \"PwC Description\": \"A large-scale dataset for retrieval and event localisation in video. A unique feature of the dataset is the availability of two audio tracks for each video: the original audio, and a high-quality spoken description of the visual content.\", \"PwC License Name\": \"Unspecified\", \"PwC License URL\": null, \"S2 Citation Count (June 2024)\": \"\", \"S2 Date\": \"\"}, \"Bibtex\": \"@Article{Oncescu2021QUERYDAV,\\n author = {Andreea-Maria Oncescu and Jo\\u00e3o F. Henriques and Yang Liu and Andrew Zisserman and Samuel Albanie},\\n booktitle = {IEEE International Conference on Acoustics, Speech, and Signal Processing},\\n journal = {ICASSP 2021 - 2021 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},\\n pages = {2265-2269},\\n title = {QUERYD: A Video Dataset with High-Quality Text and Audio Narrations},\\n year = {2021}\\n}\\n\", \"License Use (DataProvenance)\": \"unspecified\", \"License Attribution (DataProvenance)\": 0, \"License Share Alike (DataProvenance)\": 0, \"License Use (DataProvenance IgnoreOpenAI)\": \"unspecified\", \"License Attribution (DataProvenance IgnoreOpenAI)\": 0, \"License Share Alike (DataProvenance IgnoreOpenAI)\": 0, \"License Type\": \"Unspecified\"}, {\"Unique Dataset Identifier\": \"apes\", \"Collection\": \"apes\", \"Collection URL\": \"https://arxiv.org/pdf/2106.01667\", \"Dataset Name\": \"Apes\", \"Paper Title\": \"Apes\", \"Paper URL\": \"https://arxiv.org/pdf/2106.01667\", \"GitHub URL\": \"https://github.com/fuankarion/audiovisual-person-search\", \"Hugging Face URL\": \"\", \"Papers with Code URL\": \"https://paperswithcode.com/paper/apes-audiovisual-person-search-in-untrimmed/review/\", \"ArXiv URL\": \"https://arxiv.org/pdf/2106.01667\", \"Semantic Scholar Corpus ID\": 235313698, \"Year Released\": \"2021\", \"Text Sources\": [\"movies\"], \"Licenses\": [{\"License\": \"Unspecified\", \"License URL\": \"\"}], \"Creators\": \"Academic\", \"Countries\": [\"Chile\", \"United States of America\", \"Saudi Arabia\"], \"License Verified By\": \"Vivek Sharma\", \"Video Hours\": 36.0, \"Taken Down\": \"False\", \"Video Sources\": [\"movies\"], \"Task Categories\": [\"Video Classification\"], \"Inferred Metadata\": {\"GitHub License\": \"\", \"GitHub Stars (June 2024)\": 4, \"GitHub Topics\": [], \"Github Date\": \"\", \"HF Config\": \"\", \"HF Config License\": \"\", \"HF Dataset\": \"\", \"HF Date\": \"\", \"HF Downloads (June 2024)\": \"\", \"HF Likes (June 2024)\": \"\", \"HF Yaml License\": \"\", \"PwC Date\": \"\", \"PwC Description\": \"Paper tables with annotated results for APES: Audiovisual Person Search in Untrimmed Video\", \"PwC License Name\": \"\", \"PwC License URL\": \"\", \"S2 Citation Count (June 2024)\": \"\", \"S2 Date\": \"\"}, \"Bibtex\": \"@Article{Alcazar2021APESAP,\\n author = {Juan Leon Alcazar and Long Mai and Federico Perazzi and Joon-Young Lee and Pablo Arbel\\u00e1ez and Bernard Ghanem and Fabian Caba Heilbron},\\n booktitle = {2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)},\\n journal = {2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)},\\n pages = {1720-1729},\\n title = {APES: Audiovisual Person Search in Untrimmed Video},\\n year = {2021}\\n}\\n\", \"License Use (DataProvenance)\": \"unspecified\", \"License Attribution (DataProvenance)\": 0, \"License Share Alike (DataProvenance)\": 0, \"License Use (DataProvenance IgnoreOpenAI)\": \"unspecified\", \"License Attribution (DataProvenance IgnoreOpenAI)\": 0, \"License Share Alike (DataProvenance IgnoreOpenAI)\": 0, \"License Type\": \"Unspecified\"}, {\"Unique Dataset Identifier\": \"uav-human\", \"Collection\": \"uav-human\", \"Collection URL\": \"https://arxiv.org/abs/2104.00946\", \"Dataset Name\": \"UAV-Human: A Large Benchmark for Human Behavior Understanding with Unmanned Aerial Vehicles\", \"Paper Title\": \"UAV-Human: A Large Benchmark for Human Behavior Understanding with Unmanned Aerial Vehicles\", \"Paper URL\": \"https://arxiv.org/abs/2104.00946\", \"GitHub URL\": \"https://github.com/sutdcv/UAV-Human\", \"Hugging Face URL\": \"\", \"Papers with Code URL\": \"https://paperswithcode.com/paper/uav-human-a-large-benchmark-for-human\", \"ArXiv URL\": \"https://arxiv.org/abs/2104.00946\", \"Semantic Scholar Corpus ID\": 233004700, \"Year Released\": \"2021\", \"Text Sources\": [\"human\"], \"Licenses\": [{\"License\": \"Custom\", \"License URL\": \"https://sutdcv.github.io/uav-human-web/\"}], \"Creators\": \"Academic\", \"Countries\": [\"Singapore\", \"China\"], \"License Verified By\": \"Vivek Sharma\", \"Video Hours\": 18.34, \"Taken Down\": \"False\", \"Video Sources\": [\"human\"], \"Task Categories\": [\"Video Classification\"], \"Inferred Metadata\": {\"GitHub License\": \"\", \"GitHub Stars (June 2024)\": 180, \"GitHub Topics\": [\"action-recognition\", \"dataset\", \"uav\"], \"Github Date\": \"\", \"HF Config\": \"\", \"HF Config License\": \"\", \"HF Dataset\": \"\", \"HF Date\": \"\", \"HF Downloads (June 2024)\": \"\", \"HF Likes (June 2024)\": \"\", \"HF Yaml License\": \"\", \"PwC Date\": \"\", \"PwC Description\": \"Implemented in 2 code libraries.\", \"PwC License Name\": \"\", \"PwC License URL\": \"\", \"S2 Citation Count (June 2024)\": \"\", \"S2 Date\": \"\"}, \"Bibtex\": \"@Article{Li2021UAVHumanAL,\\n author = {Tianjiao Li and Jun Liu and Wei Zhang and Yun Ni and Wenqian Wang and Zhiheng Li},\\n booktitle = {Computer Vision and Pattern Recognition},\\n journal = {2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},\\n pages = {16261-16270},\\n title = {UAV-Human: A Large Benchmark for Human Behavior Understanding with Unmanned Aerial Vehicles},\\n year = {2021}\\n}\\n\", \"License Use (DataProvenance)\": \"academic-only\", \"License Attribution (DataProvenance)\": 1, \"License Share Alike (DataProvenance)\": 0, \"License Use (DataProvenance IgnoreOpenAI)\": \"academic-only\", \"License Attribution (DataProvenance IgnoreOpenAI)\": 1, \"License Share Alike (DataProvenance IgnoreOpenAI)\": 0, \"License Type\": \"Non-Commercial/Academic\"}, {\"Unique Dataset Identifier\": \"YT-Temporal-180m\", \"Collection\": \"YT-Temporal-180m\", \"Collection URL\": \"https://arxiv.org/pdf/2106.02636\", \"Dataset Name\": \"MERLOT: Multimodal Neural Script Knowledge Models\", \"Paper Title\": \"MERLOT: Multimodal Neural Script Knowledge Models\", \"Paper URL\": \"https://arxiv.org/pdf/2106.02636\", \"GitHub URL\": \"https://github.com/rowanz/merlot\", \"Hugging Face URL\": \"https://huggingface.co/datasets/HuggingFaceM4/yttemporal180m\", \"Papers with Code URL\": \"https://paperswithcode.com/paper/merlot-multimodal-neural-script-knowledge\", \"ArXiv URL\": \"https://arxiv.org/pdf/2106.02636\", \"Semantic Scholar Corpus ID\": 235352775, \"Year Released\": \"2021\", \"Text Sources\": [\"youtube\"], \"Licenses\": [{\"License\": \"MIT License\", \"License URL\": \"https://github.com/rowanz/merlot/blob/main/LICENSE\"}], \"Creators\": \"Academic\", \"Countries\": [\"United States of America\"], \"License Verified By\": \"Vivek Sharma\", \"Video Hours\": 1515.0, \"Taken Down\": \"False\", \"Video Sources\": [\"youtube\"], \"Task Categories\": [\"Video Q&A\"], \"Inferred Metadata\": null, \"Bibtex\": null, \"License Use (DataProvenance)\": \"commercial\", \"License Attribution (DataProvenance)\": 1, \"License Share Alike (DataProvenance)\": 0, \"License Use (DataProvenance IgnoreOpenAI)\": \"commercial\", \"License Attribution (DataProvenance IgnoreOpenAI)\": 1, \"License Share Alike (DataProvenance IgnoreOpenAI)\": 0, \"License Type\": \"Commercial\"}, {\"Unique Dataset Identifier\": \"uav-human\", \"Collection\": \"uav-human\", \"Collection URL\": \"https://arxiv.org/abs/2104.00946\", \"Dataset Name\": \"UAV-Human: A Large Benchmark for Human Behavior Understanding with Unmanned Aerial Vehicles\", \"Paper Title\": \"UAV-Human: A Large Benchmark for Human Behavior Understanding with Unmanned Aerial Vehicles\", \"Paper URL\": \"https://arxiv.org/abs/2104.00946\", \"GitHub URL\": \"https://github.com/sutdcv/UAV-Human\", \"Hugging Face URL\": \"\", \"Papers with Code URL\": \"https://paperswithcode.com/paper/uav-human-a-large-benchmark-for-human\", \"ArXiv URL\": \"https://arxiv.org/abs/2104.00946\", \"Semantic Scholar Corpus ID\": 233004700, \"Year Released\": \"2021\", \"Text Sources\": [\"human\"], \"Licenses\": [{\"License\": \"Custom\", \"License URL\": \"https://sutdcv.github.io/uav-human-web/\"}], \"Creators\": \"Academic\", \"Countries\": [\"Singapore\", \"China\"], \"License Verified By\": \"Vivek Sharma\", \"Video Hours\": 18.34, \"Taken Down\": \"False\", \"Video Sources\": [\"human\"], \"Task Categories\": [\"Video Classification\"], \"Inferred Metadata\": {\"GitHub License\": \"\", \"GitHub Stars (June 2024)\": 180, \"GitHub Topics\": [\"action-recognition\", \"dataset\", \"uav\"], \"Github Date\": \"\", \"HF Config\": \"\", \"HF Config License\": \"\", \"HF Dataset\": \"\", \"HF Date\": \"\", \"HF Downloads (June 2024)\": \"\", \"HF Likes (June 2024)\": \"\", \"HF Yaml License\": \"\", \"PwC Date\": \"\", \"PwC Description\": \"Implemented in 2 code libraries.\", \"PwC License Name\": \"\", \"PwC License URL\": \"\", \"S2 Citation Count (June 2024)\": \"\", \"S2 Date\": \"\"}, \"Bibtex\": \"@Article{Li2021UAVHumanAL,\\n author = {Tianjiao Li and Jun Liu and Wei Zhang and Yun Ni and Wenqian Wang and Zhiheng Li},\\n booktitle = {Computer Vision and Pattern Recognition},\\n journal = {2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},\\n pages = {16261-16270},\\n title = {UAV-Human: A Large Benchmark for Human Behavior Understanding with Unmanned Aerial Vehicles},\\n year = {2021}\\n}\\n\", \"License Use (DataProvenance)\": \"academic-only\", \"License Attribution (DataProvenance)\": 1, \"License Share Alike (DataProvenance)\": 0, \"License Use (DataProvenance IgnoreOpenAI)\": \"academic-only\", \"License Attribution (DataProvenance IgnoreOpenAI)\": 1, \"License Share Alike (DataProvenance IgnoreOpenAI)\": 0, \"License Type\": \"Non-Commercial/Academic\"}, {\"Unique Dataset Identifier\": \"WebVid\", \"Collection\": \"WebVid\", \"Collection URL\": \"https://arxiv.org/abs/2104.00650\", \"Dataset Name\": \"WebVid\", \"Paper Title\": \"WebVid\", \"Paper URL\": \"https://arxiv.org/abs/2104.00650\", \"GitHub URL\": \"https://github.com/m-bain/webvid\", \"Hugging Face URL\": \"https://huggingface.co/datasets/TempoFunk/webvid-10M\", \"Papers with Code URL\": \"https://paperswithcode.com/dataset/webvid\", \"ArXiv URL\": \"https://arxiv.org/abs/2104.00650\", \"Semantic Scholar Corpus ID\": 232478955, \"Year Released\": \"2021\", \"Text Sources\": [\"undisclosed web\"], \"Licenses\": [{\"License\": \"Custom\", \"License URL\": \"https://github.com/m-bain/webvid/blob/main/TERMS.md\"}], \"Creators\": \"Academic\", \"Countries\": [\"United Kingdom\", \"France\"], \"License Verified By\": \"Vivek Sharma\", \"Video Hours\": 13000.0, \"Taken Down\": \"True\", \"Video Sources\": [\"undisclosed web\"], \"Task Categories\": [\"Video Captioning\"], \"Inferred Metadata\": {\"GitHub License\": \"\", \"GitHub Stars (June 2024)\": 528, \"GitHub Topics\": [], \"Github Date\": \"\", \"HF Config\": \"default\", \"HF Config License\": \"\", \"HF Dataset\": \"TempoFunk/webvid-10M\", \"HF Date\": \"2023-06-16\", \"HF Downloads (June 2024)\": 360, \"HF Likes (June 2024)\": 28, \"HF Yaml License\": \"GNU General Public License v3.0\", \"PwC Date\": \"2021-04-01\", \"PwC Description\": \"WebVid contains 10 million video clips with captions, sourced from the web. The videos are diverse and rich in their content.\\n\\nBoth the full 10M set and a 2.5M subset is available for download:\\nhttps://github.com/m-bain/webvid-dataset\", \"PwC License Name\": \"Various\", \"PwC License URL\": \"https://github.com/m-bain/webvid/blob/main/TERMS.md\", \"S2 Citation Count (June 2024)\": \"\", \"S2 Date\": \"\"}, \"Bibtex\": \"@Article{Bain2021FrozenIT,\\n author = {Max Bain and Arsha Nagrani and G\\u00fcl Varol and Andrew Zisserman},\\n booktitle = {IEEE International Conference on Computer Vision},\\n journal = {2021 IEEE/CVF International Conference on Computer Vision (ICCV)},\\n pages = {1708-1718},\\n title = {Frozen in Time: A Joint Video and Image Encoder for End-to-End Retrieval},\\n year = {2021}\\n}\\n\", \"License Use (DataProvenance)\": \"academic-only\", \"License Attribution (DataProvenance)\": 0, \"License Share Alike (DataProvenance)\": 0, \"License Use (DataProvenance IgnoreOpenAI)\": \"academic-only\", \"License Attribution (DataProvenance IgnoreOpenAI)\": 0, \"License Share Alike (DataProvenance IgnoreOpenAI)\": 0, \"License Type\": \"Non-Commercial/Academic\"}, {\"Unique Dataset Identifier\": \"YT-Temporal-1B\", \"Collection\": \"YT-Temporal-1B\", \"Collection URL\": \"https://arxiv.org/pdf/2201.02639\", \"Dataset Name\": \"MERLOT Reserve: Multimodal Neural Script Knowledge through Vision and Language and Sound\", \"Paper Title\": \"MERLOT Reserve: Multimodal Neural Script Knowledge through Vision and Language and Sound\", \"Paper URL\": \"https://arxiv.org/pdf/2201.02639\", \"GitHub URL\": \"\", \"Hugging Face URL\": \"\", \"Papers with Code URL\": \"https://paperswithcode.com/paper/merlot-reserve-neural-script-knowledge\", \"ArXiv URL\": \"https://arxiv.org/pdf/2201.02639\", \"Semantic Scholar Corpus ID\": 245837609, \"Year Released\": \"2022\", \"Text Sources\": [\"youtube\"], \"Licenses\": [{\"License\": \"MIT License\", \"License URL\": \"https://github.com/rowanz/merlot_reserve/blob/main/LICENSE\"}], \"Creators\": \"Academic\", \"Countries\": [\"United States of America\", \"Scotland\"], \"License Verified By\": \"Vivek Sharma\", \"Video Hours\": 55555.0, \"Taken Down\": \"False\", \"Video Sources\": [\"youtube\"], \"Task Categories\": [\"Video Q&A\"], \"Inferred Metadata\": null, \"Bibtex\": null, \"License Use (DataProvenance)\": \"commercial\", \"License Attribution (DataProvenance)\": 1, \"License Share Alike (DataProvenance)\": 0, \"License Use (DataProvenance IgnoreOpenAI)\": \"commercial\", \"License Attribution (DataProvenance IgnoreOpenAI)\": 1, \"License Share Alike (DataProvenance IgnoreOpenAI)\": 0, \"License Type\": \"Commercial\"}, {\"Unique Dataset Identifier\": \"YT-Temporal-1B\", \"Collection\": \"YT-Temporal-1B\", \"Collection URL\": \"https://arxiv.org/pdf/2201.02639\", \"Dataset Name\": \"MERLOT Reserve: Multimodal Neural Script Knowledge through Vision and Language and Sound\", \"Paper Title\": \"MERLOT Reserve: Multimodal Neural Script Knowledge through Vision and Language and Sound\", \"Paper URL\": \"https://arxiv.org/pdf/2201.02639\", \"GitHub URL\": \"\", \"Hugging Face URL\": \"\", \"Papers with Code URL\": \"https://paperswithcode.com/paper/merlot-reserve-neural-script-knowledge\", \"ArXiv URL\": \"https://arxiv.org/pdf/2201.02639\", \"Semantic Scholar Corpus ID\": 245837609, \"Year Released\": \"2022\", \"Text Sources\": [\"youtube\"], \"Licenses\": [{\"License\": \"MIT License\", \"License URL\": \"https://github.com/rowanz/merlot_reserve/blob/main/LICENSE\"}], \"Creators\": \"Academic\", \"Countries\": [\"United States of America\", \"Scotland\"], \"License Verified By\": \"Vivek Sharma\", \"Video Hours\": 55555.0, \"Taken Down\": \"False\", \"Video Sources\": [\"youtube\"], \"Task Categories\": [\"Video Q&A\"], \"Inferred Metadata\": null, \"Bibtex\": null, \"License Use (DataProvenance)\": \"commercial\", \"License Attribution (DataProvenance)\": 1, \"License Share Alike (DataProvenance)\": 0, \"License Use (DataProvenance IgnoreOpenAI)\": \"commercial\", \"License Attribution (DataProvenance IgnoreOpenAI)\": 1, \"License Share Alike (DataProvenance IgnoreOpenAI)\": 0, \"License Type\": \"Commercial\"}, {\"Unique Dataset Identifier\": \"cacd\", \"Collection\": \"cacd\", \"Collection URL\": \"https://openaccess.thecvf.com/content/CVPR2022W/L3D-IVU/papers/Xiang_CDAD_A_Common_Daily_Action_Dataset_With_Collected_Hard_Negative_CVPRW_2022_paper.pdf\", \"Dataset Name\": \"CDAD: A Common Daily Action Dataset with Collected Hard Negative Samples (CVPR 2022)\", \"Paper Title\": \"CDAD: A Common Daily Action Dataset with Collected Hard Negative Samples (CVPR 2022)\", \"Paper URL\": \"https://openaccess.thecvf.com/content/CVPR2022W/L3D-IVU/papers/Xiang_CDAD_A_Common_Daily_Action_Dataset_With_Collected_Hard_Negative_CVPRW_2022_paper.pdf\", \"GitHub URL\": \"https://github.com/MartinXM/CDAD\", \"Hugging Face URL\": \"\", \"Papers with Code URL\": \"\", \"ArXiv URL\": \"https://openaccess.thecvf.com/content/CVPR2022W/L3D-IVU/papers/Xiang_CDAD_A_Common_Daily_Action_Dataset_With_Collected_Hard_Negative_CVPRW_2022_paper.pdf\", \"Semantic Scholar Corpus ID\": 251035434, \"Year Released\": \"2022\", \"Text Sources\": [\"crowdsourced\"], \"Licenses\": [{\"License\": \"Unspecified\", \"License URL\": \"\"}], \"Creators\": \"Corporation\", \"Countries\": [\"China\"], \"License Verified By\": \"Vivek Sharma\", \"Video Hours\": 215.0, \"Taken Down\": \"False\", \"Video Sources\": [\"crowdsourced\"], \"Task Categories\": [\"Video Classification\"], \"Inferred Metadata\": {\"GitHub License\": \"\", \"GitHub Stars (June 2024)\": 6, \"GitHub Topics\": [], \"Github Date\": \"\", \"HF Config\": \"\", \"HF Config License\": \"\", \"HF Dataset\": \"\", \"HF Date\": \"\", \"HF Downloads (June 2024)\": \"\", \"HF Likes (June 2024)\": \"\", \"HF Yaml License\": \"\", \"PwC Date\": \"\", \"PwC Description\": \"\", \"PwC License Name\": \"\", \"PwC License URL\": \"\", \"S2 Citation Count (June 2024)\": \"\", \"S2 Date\": \"\"}, \"Bibtex\": \"@Article{Xiang2022CDADAC,\\n author = {Wangmeng Xiang and C. Li and Ke Li and Biao Wang and Xiangpei Hua and Lei Zhang},\\n booktitle = {2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)},\\n journal = {2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)},\\n pages = {3920-3929},\\n title = {CDAD: A Common Daily Action Dataset with Collected Hard Negative Samples},\\n year = {2022}\\n}\\n\", \"License Use (DataProvenance)\": \"unspecified\", \"License Attribution (DataProvenance)\": 0, \"License Share Alike (DataProvenance)\": 0, \"License Use (DataProvenance IgnoreOpenAI)\": \"unspecified\", \"License Attribution (DataProvenance IgnoreOpenAI)\": 0, \"License Share Alike (DataProvenance IgnoreOpenAI)\": 0, \"License Type\": \"Unspecified\"}, {\"Unique Dataset Identifier\": \"cacd\", \"Collection\": \"cacd\", \"Collection URL\": \"https://openaccess.thecvf.com/content/CVPR2022W/L3D-IVU/papers/Xiang_CDAD_A_Common_Daily_Action_Dataset_With_Collected_Hard_Negative_CVPRW_2022_paper.pdf\", \"Dataset Name\": \"CDAD: A Common Daily Action Dataset with Collected Hard Negative Samples (CVPR 2022)\", \"Paper Title\": \"CDAD: A Common Daily Action Dataset with Collected Hard Negative Samples (CVPR 2022)\", \"Paper URL\": \"https://openaccess.thecvf.com/content/CVPR2022W/L3D-IVU/papers/Xiang_CDAD_A_Common_Daily_Action_Dataset_With_Collected_Hard_Negative_CVPRW_2022_paper.pdf\", \"GitHub URL\": \"https://github.com/MartinXM/CDAD\", \"Hugging Face URL\": \"\", \"Papers with Code URL\": \"\", \"ArXiv URL\": \"https://openaccess.thecvf.com/content/CVPR2022W/L3D-IVU/papers/Xiang_CDAD_A_Common_Daily_Action_Dataset_With_Collected_Hard_Negative_CVPRW_2022_paper.pdf\", \"Semantic Scholar Corpus ID\": 251035434, \"Year Released\": \"2022\", \"Text Sources\": [\"crowdsourced\"], \"Licenses\": [{\"License\": \"Unspecified\", \"License URL\": \"\"}], \"Creators\": \"Academic\", \"Countries\": [\"China\"], \"License Verified By\": \"Vivek Sharma\", \"Video Hours\": 215.0, \"Taken Down\": \"False\", \"Video Sources\": [\"crowdsourced\"], \"Task Categories\": [\"Video Classification\"], \"Inferred Metadata\": {\"GitHub License\": \"\", \"GitHub Stars (June 2024)\": 6, \"GitHub Topics\": [], \"Github Date\": \"\", \"HF Config\": \"\", \"HF Config License\": \"\", \"HF Dataset\": \"\", \"HF Date\": \"\", \"HF Downloads (June 2024)\": \"\", \"HF Likes (June 2024)\": \"\", \"HF Yaml License\": \"\", \"PwC Date\": \"\", \"PwC Description\": \"\", \"PwC License Name\": \"\", \"PwC License URL\": \"\", \"S2 Citation Count (June 2024)\": \"\", \"S2 Date\": \"\"}, \"Bibtex\": \"@Article{Xiang2022CDADAC,\\n author = {Wangmeng Xiang and C. Li and Ke Li and Biao Wang and Xiangpei Hua and Lei Zhang},\\n booktitle = {2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)},\\n journal = {2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)},\\n pages = {3920-3929},\\n title = {CDAD: A Common Daily Action Dataset with Collected Hard Negative Samples},\\n year = {2022}\\n}\\n\", \"License Use (DataProvenance)\": \"unspecified\", \"License Attribution (DataProvenance)\": 0, \"License Share Alike (DataProvenance)\": 0, \"License Use (DataProvenance IgnoreOpenAI)\": \"unspecified\", \"License Attribution (DataProvenance IgnoreOpenAI)\": 0, \"License Share Alike (DataProvenance IgnoreOpenAI)\": 0, \"License Type\": \"Unspecified\"}, {\"Unique Dataset Identifier\": \"mad\", \"Collection\": \"mad\", \"Collection URL\": \"https://arxiv.org/abs/2112.00431\", \"Dataset Name\": \"MAD: A Scalable Dataset for Language Grounding in Videos from Movie Audio Descriptions\", \"Paper Title\": \"MAD: A Scalable Dataset for Language Grounding in Videos from Movie Audio Descriptions\", \"Paper URL\": \"https://arxiv.org/abs/2112.00431\", \"GitHub URL\": \"https://github.com/Soldelli/MAD\", \"Hugging Face URL\": \"\", \"Papers with Code URL\": \"https://paperswithcode.com/dataset/mad\", \"ArXiv URL\": \"https://arxiv.org/abs/2112.00431\", \"Semantic Scholar Corpus ID\": 244773187, \"Year Released\": \"2022\", \"Text Sources\": [\"undisclosed web\"], \"Licenses\": [{\"License\": \"Custom\", \"License URL\": \"https://docs.google.com/forms/d/e/1FAIpQLSdtUV3uweS0u7AHAMIJAL_dRRdZ5MHpJS3fdZVbhnVt-Yb4NA/viewform\"}], \"Creators\": \"Academic\", \"Countries\": [\"Saudi Arabia\"], \"License Verified By\": \"Vivek Sharma\", \"Video Hours\": 1207.3, \"Taken Down\": \"False\", \"Video Sources\": [\"undisclosed web\"], \"Task Categories\": [\"Video Captioning\"], \"Inferred Metadata\": {\"GitHub License\": \"MIT License\", \"GitHub Stars (June 2024)\": 138, \"GitHub Topics\": [], \"Github Date\": \"\", \"HF Config\": \"\", \"HF Config License\": \"\", \"HF Dataset\": \"\", \"HF Date\": \"\", \"HF Downloads (June 2024)\": \"\", \"HF Likes (June 2024)\": \"\", \"HF Yaml License\": \"\", \"PwC Date\": \"2021-12-01\", \"PwC Description\": \"MAD (Movie Audio Descriptions) is an automatically curated large-scale dataset for the task of natural language grounding in videos or natural language moment retrieval.\\nMAD exploits available audio descriptions of mainstream movies. Such audio descriptions are redacted for visually impaired audiences and are therefore highly descriptive of the visual content being displayed. \\nMAD contains over 384,000 natural language sentences grounded in over 1,200 hours of video, and provides a unique setup for video grounding as the visual stream is truly untrimmed with an average video duration of 110 minutes. 2 orders of magnitude longer than legacy datasets. \\n\\nTake a look at the paper for additional information.\\n\\nFrom the authors on availability: \\\"Due to copyright constraints, MAD\\u2019s videos will not be publicly released. However, we will provide all necessary features for our experiments\\u2019 reproducibility and promote future research in this direction\\\"\", \"PwC License Name\": \"Unspecified\", \"PwC License URL\": null, \"S2 Citation Count (June 2024)\": \"\", \"S2 Date\": \"\"}, \"Bibtex\": \"@Article{Soldan2021MADAS,\\n author = {Mattia Soldan and A. Pardo and Juan Le'on Alc'azar and Fabian Caba Heilbron and Chen Zhao and Silvio Giancola and Bernard Ghanem},\\n booktitle = {Computer Vision and Pattern Recognition},\\n journal = {2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},\\n pages = {5016-5025},\\n title = {MAD: A Scalable Dataset for Language Grounding in Videos from Movie Audio Descriptions},\\n year = {2021}\\n}\\n\", \"License Use (DataProvenance)\": \"academic-only\", \"License Attribution (DataProvenance)\": 1, \"License Share Alike (DataProvenance)\": 0, \"License Use (DataProvenance IgnoreOpenAI)\": \"academic-only\", \"License Attribution (DataProvenance IgnoreOpenAI)\": 1, \"License Share Alike (DataProvenance IgnoreOpenAI)\": 0, \"License Type\": \"Non-Commercial/Academic\"}, {\"Unique Dataset Identifier\": \"ferv39k-dataset\", \"Collection\": \"ferv39k-dataset\", \"Collection URL\": \"https://arxiv.org/abs/2203.09463\", \"Dataset Name\": \"FERV39k: A Large-Scale Multi-Scene Dataset for Facial Expression Recognition in Videos (CVPR 2022)\", \"Paper Title\": \"FERV39k: A Large-Scale Multi-Scene Dataset for Facial Expression Recognition in Videos (CVPR 2022)\", \"Paper URL\": \"https://arxiv.org/abs/2203.09463\", \"GitHub URL\": \"https://github.com/wangyanckxx/FERV39k\", \"Hugging Face URL\": \"\", \"Papers with Code URL\": \"https://paperswithcode.com/paper/ferv39k-a-large-scale-multi-scene-dataset-for\", \"ArXiv URL\": \"https://arxiv.org/abs/2203.09463\", \"Semantic Scholar Corpus ID\": 247518747, \"Year Released\": \"2022\", \"Text Sources\": [\"undisclosed web\"], \"Licenses\": [{\"License\": \"CC BY-NC 4.0\", \"License URL\": \"https://wangyanckxx.github.io/Proj_CVPR2022_FERV39k.html\"}], \"Creators\": \"Academic\", \"Countries\": [\"China\"], \"License Verified By\": \"Vivek Sharma\", \"Video Hours\": 16.47, \"Taken Down\": \"False\", \"Video Sources\": [\"undisclosed web\"], \"Task Categories\": [\"Video Classification\"], \"Inferred Metadata\": {\"GitHub License\": \"\", \"GitHub Stars (June 2024)\": 52, \"GitHub Topics\": [], \"Github Date\": \"\", \"HF Config\": \"\", \"HF Config License\": \"\", \"HF Dataset\": \"\", \"HF Date\": \"\", \"HF Downloads (June 2024)\": \"\", \"HF Likes (June 2024)\": \"\", \"HF Yaml License\": \"\", \"PwC Date\": \"\", \"PwC Description\": \"No code available yet.\", \"PwC License Name\": \"\", \"PwC License URL\": \"\", \"S2 Citation Count (June 2024)\": \"\", \"S2 Date\": \"\"}, \"Bibtex\": \"@Article{Wang2022FERV39kAL,\\n author = {Yan Wang and Yixuan Sun and Yiwen Huang and Zhongying Liu and Shuyong Gao and Wei Zhang and Weifeng Ge and Wenqiang Zhang},\\n booktitle = {Computer Vision and Pattern Recognition},\\n journal = {2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},\\n pages = {20890-20899},\\n title = {FERV39k: A Large-Scale Multi-Scene Dataset for Facial Expression Recognition in Videos},\\n year = {2022}\\n}\\n\", \"License Use (DataProvenance)\": \"non-commercial\", \"License Attribution (DataProvenance)\": 1, \"License Share Alike (DataProvenance)\": 0, \"License Use (DataProvenance IgnoreOpenAI)\": \"non-commercial\", \"License Attribution (DataProvenance IgnoreOpenAI)\": 1, \"License Share Alike (DataProvenance IgnoreOpenAI)\": 0, \"License Type\": \"Non-Commercial/Academic\"}, {\"Unique Dataset Identifier\": \"ego-4d\", \"Collection\": \"ego-4d\", \"Collection URL\": \"https://arxiv.org/abs/2110.07058\", \"Dataset Name\": \"Ego4D: Around the World in 3,000 Hours of Egocentric Video\", \"Paper Title\": \"Ego4D: Around the World in 3,000 Hours of Egocentric Video\", \"Paper URL\": \"https://arxiv.org/abs/2110.07058\", \"GitHub URL\": \"https://github.com/EGO4D/forecasting\", \"Hugging Face URL\": \"\", \"Papers with Code URL\": \"https://paperswithcode.com/paper/ego4d-around-the-world-in-3000-hours-of\", \"ArXiv URL\": \"https://arxiv.org/abs/2110.07058\", \"Semantic Scholar Corpus ID\": 238856888, \"Year Released\": \"2022\", \"Text Sources\": [\"human\"], \"Licenses\": [{\"License\": \"Custom\", \"License URL\": \"https://ego4d-data.org/pdfs/Ego4D-Licenses-Draft.pdf\"}], \"Creators\": \"Industry Lab\", \"Countries\": [\"United States of America\"], \"License Verified By\": \"Vivek Sharma\", \"Video Hours\": 3670.0, \"Taken Down\": \"False\", \"Video Sources\": [\"human\"], \"Task Categories\": [\"Video Classification\", \"Temporal Action Segmentation\", \"Video Captioning\", \"Misc\"], \"Inferred Metadata\": {\"GitHub License\": \"MIT License\", \"GitHub Stars (June 2024)\": 63, \"GitHub Topics\": [], \"Github Date\": \"\", \"HF Config\": \"\", \"HF Config License\": \"\", \"HF Dataset\": \"\", \"HF Date\": \"\", \"HF Downloads (June 2024)\": \"\", \"HF Likes (June 2024)\": \"\", \"HF Yaml License\": \"\", \"PwC Date\": \"\", \"PwC Description\": \"Implemented in 8 code libraries.\", \"PwC License Name\": \"\", \"PwC License URL\": \"\", \"S2 Citation Count (June 2024)\": \"\", \"S2 Date\": \"\"}, \"Bibtex\": \"@Article{Grauman2021Ego4DAT,\\n author = {K. Grauman and Andrew Westbury and Eugene Byrne and Zachary Chavis and Antonino Furnari and Rohit Girdhar and Jackson Hamburger and Hao Jiang and Miao Liu and Xingyu Liu and Miguel Martin and Tushar Nagarajan and Ilija Radosavovic and Santhosh K. Ramakrishnan and Fiona Ryan and J. Sharma and Michael Wray and Mengmeng Xu and Eric Z. Xu and Chen Zhao and Siddhant Bansal and Dhruv Batra and Vincent Cartillier and S. Crane and Tien Do and Morrie Doulaty and Akshay Erapalli and Christoph Feichtenhofer and A. Fragomeni and Qichen Fu and Christian Fuegen and A. Gebreselasie and Cristina Gonz\\u00e1lez and James M. Hillis and Xuhua Huang and Yifei Huang and Wenqi Jia and Weslie Khoo and J. Kol\\u00e1r and Satwik Kottur and Anurag Kumar and F. Landini and Chao Li and Yanghao Li and Zhenqiang Li and K. Mangalam and Raghava Modhugu and Jonathan Munro and Tullie Murrell and Takumi Nishiyasu and Will Price and Paola Ruiz Puentes and Merey Ramazanova and Leda Sari and K. Somasundaram and Audrey Southerland and Yusuke Sugano and Ruijie Tao and Minh Vo and Yuchen Wang and Xindi Wu and Takuma Yagi and Yunyi Zhu and P. Arbel\\u00e1ez and David J. Crandall and D. Damen and G. Farinella and Bernard Ghanem and V. Ithapu and C. V. Jawahar and H. Joo and Kris Kitani and Haizhou Li and Richard A. Newcombe and A. Oliva and H. Park and James M. Rehg and Yoichi Sato and Jianbo Shi and Mike Zheng Shou and A. Torralba and L. Torresani and Mingfei Yan and J. Malik},\\n booktitle = {Computer Vision and Pattern Recognition},\\n journal = {2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},\\n pages = {18973-18990},\\n title = {Ego4D: Around the World in 3,000 Hours of Egocentric Video},\\n year = {2021}\\n}\\n\", \"License Use (DataProvenance)\": \"commercial\", \"License Attribution (DataProvenance)\": 1, \"License Share Alike (DataProvenance)\": 0, \"License Use (DataProvenance IgnoreOpenAI)\": \"commercial\", \"License Attribution (DataProvenance IgnoreOpenAI)\": 1, \"License Share Alike (DataProvenance IgnoreOpenAI)\": 0, \"License Type\": \"Commercial\"}, {\"Unique Dataset Identifier\": \"YT-Temporal-1B\", \"Collection\": \"YT-Temporal-1B\", \"Collection URL\": \"https://arxiv.org/pdf/2201.02639\", \"Dataset Name\": \"MERLOT Reserve: Multimodal Neural Script Knowledge through Vision and Language and Sound\", \"Paper Title\": \"MERLOT Reserve: Multimodal Neural Script Knowledge through Vision and Language and Sound\", \"Paper URL\": \"https://arxiv.org/pdf/2201.02639\", \"GitHub URL\": \"\", \"Hugging Face URL\": \"\", \"Papers with Code URL\": \"https://paperswithcode.com/paper/merlot-reserve-neural-script-knowledge\", \"ArXiv URL\": \"https://arxiv.org/pdf/2201.02639\", \"Semantic Scholar Corpus ID\": 245837609, \"Year Released\": \"2022\", \"Text Sources\": [\"youtube\"], \"Licenses\": [{\"License\": \"MIT License\", \"License URL\": \"https://github.com/rowanz/merlot_reserve/blob/main/LICENSE\"}], \"Creators\": \"Academic\", \"Countries\": [\"United States of America\", \"Scotland\"], \"License Verified By\": \"Vivek Sharma\", \"Video Hours\": 55555.0, \"Taken Down\": \"False\", \"Video Sources\": [\"youtube\"], \"Task Categories\": [\"Video Q&A\"], \"Inferred Metadata\": null, \"Bibtex\": null, \"License Use (DataProvenance)\": \"commercial\", \"License Attribution (DataProvenance)\": 1, \"License Share Alike (DataProvenance)\": 0, \"License Use (DataProvenance IgnoreOpenAI)\": \"commercial\", \"License Attribution (DataProvenance IgnoreOpenAI)\": 1, \"License Share Alike (DataProvenance IgnoreOpenAI)\": 0, \"License Type\": \"Commercial\"}, {\"Unique Dataset Identifier\": \"project-aria-digital-twin-dataset\", \"Collection\": \"project-aria-digital-twin-dataset\", \"Collection URL\": \"https://arxiv.org/abs/2306.06362\", \"Dataset Name\": \"Aria Digital Twin\", \"Paper Title\": \"Aria Digital Twin\", \"Paper URL\": \"https://arxiv.org/abs/2306.06362\", \"GitHub URL\": \"https://github.com/facebookresearch/projectaria_tools\", \"Hugging Face URL\": \"\", \"Papers with Code URL\": \"https://paperswithcode.com/paper/aria-digital-twin-a-new-benchmark-dataset-for\", \"ArXiv URL\": \"https://arxiv.org/abs/2306.06362\", \"Semantic Scholar Corpus ID\": 261243365, \"Year Released\": \"2023\", \"Text Sources\": [\"human\"], \"Licenses\": [{\"License\": \"Apache License 2.0\", \"License URL\": \"https://github.com/facebookresearch/projectaria_tools/blob/main/LICENSE\"}], \"Creators\": \"Corporation\", \"Countries\": [\"United States of America\"], \"License Verified By\": \"Vivek Sharma\", \"Video Hours\": 6.6, \"Taken Down\": \"False\", \"Video Sources\": [\"human\"], \"Task Categories\": [\"Video Object Detection\", \"Video Segmentation\"], \"Inferred Metadata\": {\"GitHub License\": \"Apache License 2.0\", \"GitHub Stars (June 2024)\": 361, \"GitHub Topics\": [], \"Github Date\": \"\", \"HF Config\": \"\", \"HF Config License\": \"\", \"HF Dataset\": \"\", \"HF Date\": \"\", \"HF Downloads (June 2024)\": \"\", \"HF Likes (June 2024)\": \"\", \"HF Yaml License\": \"\", \"PwC Date\": \"\", \"PwC Description\": \"Implemented in one code library.\", \"PwC License Name\": \"\", \"PwC License URL\": \"\", \"S2 Citation Count (June 2024)\": \"\", \"S2 Date\": \"\"}, \"Bibtex\": \"@Article{Somasundaram2023ProjectAA,\\n author = {K. Somasundaram and Jing Dong and Huixuan Tang and Julian Straub and Mingfei Yan and M. Goesele and Jakob J. Engel and R. D. Nardi and Richard A. Newcombe},\\n booktitle = {arXiv.org},\\n journal = {ArXiv},\\n title = {Project Aria: A New Tool for Egocentric Multi-Modal AI Research},\\n volume = {abs/2308.13561},\\n year = {2023}\\n}\\n\", \"License Use (DataProvenance)\": \"commercial\", \"License Attribution (DataProvenance)\": 1, \"License Share Alike (DataProvenance)\": 0, \"License Use (DataProvenance IgnoreOpenAI)\": \"commercial\", \"License Attribution (DataProvenance IgnoreOpenAI)\": 1, \"License Share Alike (DataProvenance IgnoreOpenAI)\": 0, \"License Type\": \"Commercial\"}, {\"Unique Dataset Identifier\": \"ego-exo4D\", \"Collection\": \"ego-exo4D\", \"Collection URL\": \"https://arxiv.org/abs/2311.18259\", \"Dataset Name\": \"Ego-Exo4D\", \"Paper Title\": \"Ego-Exo4D\", \"Paper URL\": \"https://arxiv.org/abs/2311.18259\", \"GitHub URL\": \"https://github.com/facebookresearch/Ego4d\", \"Hugging Face URL\": \"\", \"Papers with Code URL\": \"https://paperswithcode.com/paper/ego-exo4d-understanding-skilled-human/review/\", \"ArXiv URL\": \"https://arxiv.org/abs/2311.18259\", \"Semantic Scholar Corpus ID\": 265506384, \"Year Released\": \"2023\", \"Text Sources\": [\"human\"], \"Licenses\": [{\"License\": \"MIT License\", \"License URL\": \"https://github.com/facebookresearch/Ego4d/blob/main/LICENSE\"}], \"Creators\": \"Corporation\", \"Countries\": [\"United States of America\"], \"License Verified By\": \"Vivek Sharma\", \"Video Hours\": 1422.0, \"Taken Down\": \"False\", \"Video Sources\": [\"human\"], \"Task Categories\": [\"Pose Estimation\", \"Video Classification\", \"Video Captioning\", \"Misc\"], \"Inferred Metadata\": {\"GitHub License\": \"MIT License\", \"GitHub Stars (June 2024)\": 297, \"GitHub Topics\": [\"computer-vision\", \"dataset\", \"feature-extraction\", \"video\", \"visuzalization\"], \"Github Date\": \"\", \"HF Config\": \"\", \"HF Config License\": \"\", \"HF Dataset\": \"\", \"HF Date\": \"\", \"HF Downloads (June 2024)\": \"\", \"HF Likes (June 2024)\": \"\", \"HF Yaml License\": \"\", \"PwC Date\": \"\", \"PwC Description\": \"Paper tables with annotated results for Ego-Exo4D: Understanding Skilled Human Activity from First- and Third-Person Perspectives\", \"PwC License Name\": \"\", \"PwC License URL\": \"\", \"S2 Citation Count (June 2024)\": \"\", \"S2 Date\": \"\"}, \"Bibtex\": \"@Article{Grauman2023EgoExo4DUS,\\n author = {K. Grauman and Andrew Westbury and L. Torresani and Kris Kitani and Jitendra Malik and Triantafyllos Afouras and Kumar Ashutosh and Vijay Baiyya and Siddhant Bansal and Bikram Boote and Eugene Byrne and Zachary Chavis and Joya Chen and Feng Cheng and Fu-Jen Chu and Sean Crane and Avijit Dasgupta and Jing Dong and Mar\\u00eda Escobar and Cristhian Forigua and A. Gebreselasie and S. Haresh and Jing Huang and Md Mohaiminul Islam and S. Jain and Rawal Khirodkar and Devansh Kukreja and Kevin J Liang and Jia-Wei Liu and Sagnik Majumder and Yongsen Mao and Miguel Martin and E. Mavroudi and Tushar Nagarajan and Francesco Ragusa and Santhosh K. Ramakrishnan and Luigi Seminara and Arjun Somayazulu and Yale Song and Shan Su and Zihui Xue and Edward Zhang and Jinxu Zhang and Angela Castillo and Changan Chen and Xinzhu Fu and Ryosuke Furuta and Cristina Gonzalez and Prince Gupta and Jiabo Hu and Yifei Huang and Yiming Huang and Weslie Khoo and Anush Kumar and Robert Kuo and Sach Lakhavani and Miao Liu and M. Luo and Zhengyi Luo and Brighid Meredith and Austin Miller and Oluwatumininu Oguntola and Xiaqing Pan and Penny Peng and Shraman Pramanick and Merey Ramazanova and Fiona Ryan and Wei Shan and Kiran Somasundaram and Chenan Song and Audrey Southerland and Masatoshi Tateno and Huiyu Wang and Yuchen Wang and Takuma Yagi and Mingfei Yan and Xitong Yang and Zecheng Yu and S. Zha and Chen Zhao and Ziwei Zhao and Zhifan Zhu and Jeff Zhuo and Pablo Arbel\\u00e1ez and Gedas Bertasius and David J. Crandall and D. Damen and J. Engel and G. Farinella and Antonino Furnari and Bernard Ghanem and Judy Hoffman and C. V. Jawahar and Richard A. Newcombe and Hyun Soo Park and James M. Rehg and Yoichi Sato and M. Savva and Jianbo Shi and Mike Zheng Shou and Michael Wray},\\n booktitle = {arXiv.org},\\n journal = {ArXiv},\\n title = {Ego-Exo4D: Understanding Skilled Human Activity from First- and Third-Person Perspectives},\\n volume = {abs/2311.18259},\\n year = {2023}\\n}\\n\", \"License Use (DataProvenance)\": \"commercial\", \"License Attribution (DataProvenance)\": 1, \"License Share Alike (DataProvenance)\": 0, \"License Use (DataProvenance IgnoreOpenAI)\": \"commercial\", \"License Attribution (DataProvenance IgnoreOpenAI)\": 1, \"License Share Alike (DataProvenance IgnoreOpenAI)\": 0, \"License Type\": \"Commercial\"}, {\"Unique Dataset Identifier\": \"egoschema\", \"Collection\": \"egoschema\", \"Collection URL\": \"https://arxiv.org/pdf/2308.09126\", \"Dataset Name\": \"EgoSchema: A Diagnostic Benchmark for Very Long-form Video Language Understanding\", \"Paper Title\": \"EgoSchema: A Diagnostic Benchmark for Very Long-form Video Language Understanding\", \"Paper URL\": \"https://arxiv.org/pdf/2308.09126\", \"GitHub URL\": \"\", \"Hugging Face URL\": \"\", \"Papers with Code URL\": \"https://paperswithcode.com/paper/egoschema-a-diagnostic-benchmark-for-very-1\", \"ArXiv URL\": \"https://arxiv.org/pdf/2308.09126\", \"Semantic Scholar Corpus ID\": 261031047, \"Year Released\": \"2024\", \"Text Sources\": [\"human\"], \"Licenses\": [{\"License\": \"Unspecified\", \"License URL\": \"\"}], \"Creators\": \"Academic\", \"Countries\": [\"United States of America\"], \"License Verified By\": \"Vivek Sharma\", \"Video Hours\": 250.0, \"Taken Down\": \"False\", \"Video Sources\": [\"human\"], \"Task Categories\": [\"Video Q&A\"], \"Inferred Metadata\": null, \"Bibtex\": null, \"License Use (DataProvenance)\": \"unspecified\", \"License Attribution (DataProvenance)\": 0, \"License Share Alike (DataProvenance)\": 0, \"License Use (DataProvenance IgnoreOpenAI)\": \"unspecified\", \"License Attribution (DataProvenance IgnoreOpenAI)\": 0, \"License Share Alike (DataProvenance IgnoreOpenAI)\": 0, \"License Type\": \"Unspecified\"}, {\"Unique Dataset Identifier\": \"egopet\", \"Collection\": \"egopet\", \"Collection URL\": \"https://arxiv.org/pdf/2404.09991\", \"Dataset Name\": \"EgoPet: Egomotion and Interaction Data from an Animal's Perspective\", \"Paper Title\": \"EgoPet: Egomotion and Interaction Data from an Animal's Perspective\", \"Paper URL\": \"https://arxiv.org/pdf/2404.09991\", \"GitHub URL\": \"\", \"Hugging Face URL\": \"\", \"Papers with Code URL\": \"https://paperswithcode.com/paper/egopet-egomotion-and-interaction-data-from-an\", \"ArXiv URL\": \"https://arxiv.org/pdf/2404.09991\", \"Semantic Scholar Corpus ID\": 269148727, \"Year Released\": \"2024\", \"Text Sources\": [\"tiktok\", \"youtube\"], \"Licenses\": [{\"License\": \"CC BY-NC 4.0\", \"License URL\": \"https://github.com/DannyTran123/egopet/blob/main/LICENSE\"}], \"Creators\": \"Academic\", \"Countries\": [\"United States of America\", \"Israel\"], \"License Verified By\": \"Vivek Sharma\", \"Video Hours\": 84.0, \"Taken Down\": \"False\", \"Video Sources\": [\"tiktok\", \"youtube\"], \"Task Categories\": [\"Misc (Locomotion Prediction\", \"Visual Interaction Prediction\", \"Vision to Proprioception Prediction)\"], \"Inferred Metadata\": null, \"Bibtex\": null, \"License Use (DataProvenance)\": \"non-commercial\", \"License Attribution (DataProvenance)\": 1, \"License Share Alike (DataProvenance)\": 0, \"License Use (DataProvenance IgnoreOpenAI)\": \"non-commercial\", \"License Attribution (DataProvenance IgnoreOpenAI)\": 1, \"License Share Alike (DataProvenance IgnoreOpenAI)\": 0, \"License Type\": \"Non-Commercial/Academic\"}, {\"Unique Dataset Identifier\": \"project-aria-dataset\", \"Collection\": \"project-aria-dataset\", \"Collection URL\": \"https://arxiv.org/pdf/2402.13349\", \"Dataset Name\": \"Aria Everyday Activities Dataset\", \"Paper Title\": \"Aria Everyday Activities Dataset\", \"Paper URL\": \"https://arxiv.org/pdf/2402.13349\", \"GitHub URL\": \"https://github.com/facebookresearch/projectaria_tools\", \"Hugging Face URL\": \"\", \"Papers with Code URL\": \"https://paperswithcode.com/paper/aria-everyday-activities-dataset\", \"ArXiv URL\": \"https://arxiv.org/pdf/2402.13349\", \"Semantic Scholar Corpus ID\": 267770215, \"Year Released\": \"2024\", \"Text Sources\": [\"human\"], \"Licenses\": [{\"License\": \"Apache License 2.0\", \"License URL\": \"https://github.com/facebookresearch/Aria_data_tools/blob/main/LICENSE\"}], \"Creators\": \"Corporation\", \"Countries\": [\"United States of America\"], \"License Verified By\": \"Vivek Sharma\", \"Video Hours\": 1400.0, \"Taken Down\": \"False\", \"Video Sources\": [\"human\"], \"Task Categories\": [\"Misc (Scene Reconstruction)\"], \"Inferred Metadata\": {\"GitHub License\": \"Apache License 2.0\", \"GitHub Stars (June 2024)\": 361, \"GitHub Topics\": [], \"Github Date\": \"\", \"HF Config\": \"\", \"HF Config License\": \"\", \"HF Dataset\": \"\", \"HF Date\": \"\", \"HF Downloads (June 2024)\": \"\", \"HF Likes (June 2024)\": \"\", \"HF Yaml License\": \"\", \"PwC Date\": \"\", \"PwC Description\": \"Implemented in one code library.\", \"PwC License Name\": \"\", \"PwC License URL\": \"\", \"S2 Citation Count (June 2024)\": \"\", \"S2 Date\": \"\"}, \"Bibtex\": \"@Article{Lv2024AriaEA,\\n author = {Zhaoyang Lv and Nickolas Charron and Pierre Moulon and Alexander Gamino and Cheng Peng and Chris Sweeney and Edward Miller and Huixuan Tang and Jeff Meissner and Jing Dong and Kiran Somasundaram and Luis Pesqueira and Mark Schwesinger and Omkar M. Parkhi and Qiao Gu and R. D. Nardi and Shangyi Cheng and Steve Saarinen and Vijay Baiyya and Yuyang Zou and Richard A. Newcombe and J. Engel and Xiaqing Pan and Carl Ren},\\n booktitle = {arXiv.org},\\n journal = {ArXiv},\\n title = {Aria Everyday Activities Dataset},\\n volume = {abs/2402.13349},\\n year = {2024}\\n}\\n\", \"License Use (DataProvenance)\": \"commercial\", \"License Attribution (DataProvenance)\": 1, \"License Share Alike (DataProvenance)\": 0, \"License Use (DataProvenance IgnoreOpenAI)\": \"commercial\", \"License Attribution (DataProvenance IgnoreOpenAI)\": 1, \"License Share Alike (DataProvenance IgnoreOpenAI)\": 0, \"License Type\": \"Commercial\"}, {\"Unique Dataset Identifier\": \"cinepile\", \"Collection\": \"cinepile\", \"Collection URL\": \"https://arxiv.org/pdf/2405.08813\", \"Dataset Name\": \"CinePile: A Long Video Question Answering Dataset and Benchmark\", \"Paper Title\": \"CinePile: A Long Video Question Answering Dataset and Benchmark\", \"Paper URL\": \"https://arxiv.org/pdf/2405.08813\", \"GitHub URL\": \"\", \"Hugging Face URL\": \"https://huggingface.co/datasets/tomg-group-umd/cinepile\", \"Papers with Code URL\": \"https://paperswithcode.com/dataset/cinepile\", \"ArXiv URL\": \"https://arxiv.org/pdf/2405.08813\", \"Semantic Scholar Corpus ID\": 269761335, \"Year Released\": \"2024\", \"Text Sources\": [\"youtube\", \"movies\"], \"Licenses\": [{\"License\": \"CC BY 4.0\", \"License URL\": \"https://creativecommons.org/licenses/by/4.0/\"}], \"Creators\": \"Academic\", \"Countries\": [\"United States of America\", \"Israel\"], \"License Verified By\": \"Vivek Sharma\", \"Video Hours\": 417.6, \"Taken Down\": \"False\", \"Video Sources\": [\"youtube\", \"movies\"], \"Task Categories\": [\"Video Q&A\"], \"Inferred Metadata\": null, \"Bibtex\": null, \"License Use (DataProvenance)\": \"commercial\", \"License Attribution (DataProvenance)\": 1, \"License Share Alike (DataProvenance)\": 0, \"License Use (DataProvenance IgnoreOpenAI)\": \"commercial\", \"License Attribution (DataProvenance IgnoreOpenAI)\": 1, \"License Share Alike (DataProvenance IgnoreOpenAI)\": 0, \"License Type\": \"Commercial\"}, {\"Unique Dataset Identifier\": \"cinepile\", \"Collection\": \"cinepile\", \"Collection URL\": \"https://arxiv.org/pdf/2405.08813\", \"Dataset Name\": \"CinePile: A Long Video Question Answering Dataset and Benchmark\", \"Paper Title\": \"CinePile: A Long Video Question Answering Dataset and Benchmark\", \"Paper URL\": \"https://arxiv.org/pdf/2405.08813\", \"GitHub URL\": \"\", \"Hugging Face URL\": \"https://huggingface.co/datasets/tomg-group-umd/cinepile\", \"Papers with Code URL\": \"https://paperswithcode.com/dataset/cinepile\", \"ArXiv URL\": \"https://arxiv.org/pdf/2405.08813\", \"Semantic Scholar Corpus ID\": 269761335, \"Year Released\": \"2024\", \"Text Sources\": [\"youtube\", \"movies\"], \"Licenses\": [{\"License\": \"CC BY 4.0\", \"License URL\": \"https://creativecommons.org/licenses/by/4.0/\"}], \"Creators\": \"Academic\", \"Countries\": [\"United States of America\", \"Israel\"], \"License Verified By\": \"Vivek Sharma\", \"Video Hours\": 417.6, \"Taken Down\": \"False\", \"Video Sources\": [\"youtube\", \"movies\"], \"Task Categories\": [\"Video Q&A\"], \"Inferred Metadata\": null, \"Bibtex\": null, \"License Use (DataProvenance)\": \"commercial\", \"License Attribution (DataProvenance)\": 1, \"License Share Alike (DataProvenance)\": 0, \"License Use (DataProvenance IgnoreOpenAI)\": \"commercial\", \"License Attribution (DataProvenance IgnoreOpenAI)\": 1, \"License Share Alike (DataProvenance IgnoreOpenAI)\": 0, \"License Type\": \"Commercial\"}, {\"Unique Dataset Identifier\": \"egopet\", \"Collection\": \"egopet\", \"Collection URL\": \"https://arxiv.org/pdf/2404.09991\", \"Dataset Name\": \"EgoPet: Egomotion and Interaction Data from an Animal's Perspective\", \"Paper Title\": \"EgoPet: Egomotion and Interaction Data from an Animal's Perspective\", \"Paper URL\": \"https://arxiv.org/pdf/2404.09991\", \"GitHub URL\": \"\", \"Hugging Face URL\": \"\", \"Papers with Code URL\": \"https://paperswithcode.com/paper/egopet-egomotion-and-interaction-data-from-an\", \"ArXiv URL\": \"https://arxiv.org/pdf/2404.09991\", \"Semantic Scholar Corpus ID\": 269148727, \"Year Released\": \"2024\", \"Text Sources\": [\"tiktok\", \"youtube\"], \"Licenses\": [{\"License\": \"CC BY-NC 4.0\", \"License URL\": \"https://github.com/DannyTran123/egopet/blob/main/LICENSE\"}], \"Creators\": \"Academic\", \"Countries\": [\"United States of America\", \"Israel\"], \"License Verified By\": \"Vivek Sharma\", \"Video Hours\": 84.0, \"Taken Down\": \"False\", \"Video Sources\": [\"tiktok\", \"youtube\"], \"Task Categories\": [\"Misc (Locomotion Prediction\", \"Visual Interaction Prediction\", \"Vision to Proprioception Prediction)\"], \"Inferred Metadata\": null, \"Bibtex\": null, \"License Use (DataProvenance)\": \"non-commercial\", \"License Attribution (DataProvenance)\": 1, \"License Share Alike (DataProvenance)\": 0, \"License Use (DataProvenance IgnoreOpenAI)\": \"non-commercial\", \"License Attribution (DataProvenance IgnoreOpenAI)\": 1, \"License Share Alike (DataProvenance IgnoreOpenAI)\": 0, \"License Type\": \"Non-Commercial/Academic\"}, {\"Unique Dataset Identifier\": \"egopet\", \"Collection\": \"egopet\", \"Collection URL\": \"https://arxiv.org/pdf/2404.09991\", \"Dataset Name\": \"EgoPet: Egomotion and Interaction Data from an Animal's Perspective\", \"Paper Title\": \"EgoPet: Egomotion and Interaction Data from an Animal's Perspective\", \"Paper URL\": \"https://arxiv.org/pdf/2404.09991\", \"GitHub URL\": \"\", \"Hugging Face URL\": \"\", \"Papers with Code URL\": \"https://paperswithcode.com/paper/egopet-egomotion-and-interaction-data-from-an\", \"ArXiv URL\": \"https://arxiv.org/pdf/2404.09991\", \"Semantic Scholar Corpus ID\": 269148727, \"Year Released\": \"2024\", \"Text Sources\": [\"tiktok\", \"youtube\"], \"Licenses\": [{\"License\": \"CC BY-NC 4.0\", \"License URL\": \"https://github.com/DannyTran123/egopet/blob/main/LICENSE\"}], \"Creators\": \"Academic\", \"Countries\": [\"United States of America\", \"Israel\"], \"License Verified By\": \"Vivek Sharma\", \"Video Hours\": 84.0, \"Taken Down\": \"False\", \"Video Sources\": [\"tiktok\", \"youtube\"], \"Task Categories\": [\"Misc (Locomotion Prediction\", \"Visual Interaction Prediction\", \"Vision to Proprioception Prediction)\"], \"Inferred Metadata\": null, \"Bibtex\": null, \"License Use (DataProvenance)\": \"non-commercial\", \"License Attribution (DataProvenance)\": 1, \"License Share Alike (DataProvenance)\": 0, \"License Use (DataProvenance IgnoreOpenAI)\": \"non-commercial\", \"License Attribution (DataProvenance IgnoreOpenAI)\": 1, \"License Share Alike (DataProvenance IgnoreOpenAI)\": 0, \"License Type\": \"Non-Commercial/Academic\"}]}}, {\"mode\": \"vega-lite\"});\n",
       "</script>"
      ],
      "text/plain": [
       "alt.LayerChart(...)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_creatoryear = alt.Chart(\n",
    "    df_videocreatoryears\n",
    ").mark_bar().encode(\n",
    "    x=alt.X(\n",
    "        \"Year Released:N\",\n",
    "        title=\"Year Released\",\n",
    "        sort=YEARS_ORDER,\n",
    "        axis=alt.Axis(labelAngle=-30)\n",
    "    ),\n",
    "    y=alt.Y(\n",
    "        \"count():Q\",\n",
    "        stack=\"normalize\",\n",
    "        axis=alt.Axis(format=\"%\"),\n",
    "        title=\"Pct. Datasets\"\n",
    "    ),\n",
    "    color=alt.Color(\n",
    "        \"Creators:N\",\n",
    "        title=\"Video Creator Cateogies\"\n",
    "    )\n",
    ").properties(\n",
    "    width=600,\n",
    "    height=160\n",
    ")\n",
    "\n",
    "text_creatoryear = alt.Chart(df_videocreatoryears).mark_text(\n",
    "    dy=-90,\n",
    "    align=\"center\",\n",
    "    baseline=\"top\",\n",
    "    fontSize=12\n",
    ").encode(\n",
    "    x=alt.X(\n",
    "        \"Year Released:N\",\n",
    "        title=\"Year Released\",\n",
    "        sort=YEARS_ORDER\n",
    "    ),\n",
    "    text=\"count():Q\"\n",
    ")\n",
    "\n",
    "chart_creatoryear = (base_creatoryear + text_creatoryear).configure_axis(\n",
    "    labelFontSize=FONT_SIZE,\n",
    "    titleFontSize=FONT_SIZE\n",
    ").configure_legend(\n",
    "    labelFontSize=FONT_SIZE,\n",
    "    titleFontSize=FONT_SIZE,\n",
    "    orient=LEGEND_POSITION,\n",
    "    columns=4,\n",
    "    labelLimit=MAX_LABELLIMIT\n",
    ")\n",
    "\n",
    "\n",
    "if PLOT_TOFILE:\n",
    "    chart_creatoryear.save(\n",
    "        os.path.join(PLOT_DIR, \"video_creatorcategories-years.png\"),\n",
    "        ppi=PLOT_PPI\n",
    "    )\n",
    "\n",
    "chart_creatoryear"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Total Hours by Source Category (Cumulative)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "INCLUDE_TOP_N_CATEGORIES = 6\n",
    "\n",
    "df_speechsourceyears = df_video.explode(\"Video Sources\")\n",
    "df_speechsourceyears = reduce_categories_to_topk(df_speechsourceyears, \"Video Sources\", INCLUDE_TOP_N_CATEGORIES)\n",
    "\n",
    "df_speechsourceyearscumulativehours = df_speechsourceyears.groupby(\n",
    "    [\"Year Released\", \"Video Sources\"]\n",
    ")[\"Video Hours\"].sum().groupby(\n",
    "    \"Video Sources\"\n",
    ").cumsum().reset_index(name=\"Cumulative Hours\")\n",
    "\n",
    "df_speechsourceyearscumulativehours = df_speechsourceyearscumulativehours.sort_values(by=\"Year Released\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "  #altair-viz-26bd7bd731664e55ac4aa1ff4d845590.vega-embed {\n",
       "    width: 100%;\n",
       "    display: flex;\n",
       "  }\n",
       "\n",
       "  #altair-viz-26bd7bd731664e55ac4aa1ff4d845590.vega-embed details,\n",
       "  #altair-viz-26bd7bd731664e55ac4aa1ff4d845590.vega-embed details summary {\n",
       "    position: relative;\n",
       "  }\n",
       "</style>\n",
       "<div id=\"altair-viz-26bd7bd731664e55ac4aa1ff4d845590\"></div>\n",
       "<script type=\"text/javascript\">\n",
       "  var VEGA_DEBUG = (typeof VEGA_DEBUG == \"undefined\") ? {} : VEGA_DEBUG;\n",
       "  (function(spec, embedOpt){\n",
       "    let outputDiv = document.currentScript.previousElementSibling;\n",
       "    if (outputDiv.id !== \"altair-viz-26bd7bd731664e55ac4aa1ff4d845590\") {\n",
       "      outputDiv = document.getElementById(\"altair-viz-26bd7bd731664e55ac4aa1ff4d845590\");\n",
       "    }\n",
       "    const paths = {\n",
       "      \"vega\": \"https://cdn.jsdelivr.net/npm/vega@5?noext\",\n",
       "      \"vega-lib\": \"https://cdn.jsdelivr.net/npm/vega-lib?noext\",\n",
       "      \"vega-lite\": \"https://cdn.jsdelivr.net/npm/vega-lite@5.20.1?noext\",\n",
       "      \"vega-embed\": \"https://cdn.jsdelivr.net/npm/vega-embed@6?noext\",\n",
       "    };\n",
       "\n",
       "    function maybeLoadScript(lib, version) {\n",
       "      var key = `${lib.replace(\"-\", \"\")}_version`;\n",
       "      return (VEGA_DEBUG[key] == version) ?\n",
       "        Promise.resolve(paths[lib]) :\n",
       "        new Promise(function(resolve, reject) {\n",
       "          var s = document.createElement('script');\n",
       "          document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
       "          s.async = true;\n",
       "          s.onload = () => {\n",
       "            VEGA_DEBUG[key] = version;\n",
       "            return resolve(paths[lib]);\n",
       "          };\n",
       "          s.onerror = () => reject(`Error loading script: ${paths[lib]}`);\n",
       "          s.src = paths[lib];\n",
       "        });\n",
       "    }\n",
       "\n",
       "    function showError(err) {\n",
       "      outputDiv.innerHTML = `<div class=\"error\" style=\"color:red;\">${err}</div>`;\n",
       "      throw err;\n",
       "    }\n",
       "\n",
       "    function displayChart(vegaEmbed) {\n",
       "      vegaEmbed(outputDiv, spec, embedOpt)\n",
       "        .catch(err => showError(`Javascript Error: ${err.message}<br>This usually means there's a typo in your chart specification. See the javascript console for the full traceback.`));\n",
       "    }\n",
       "\n",
       "    if(typeof define === \"function\" && define.amd) {\n",
       "      requirejs.config({paths});\n",
       "      require([\"vega-embed\"], displayChart, err => showError(`Error loading script: ${err.message}`));\n",
       "    } else {\n",
       "      maybeLoadScript(\"vega\", \"5\")\n",
       "        .then(() => maybeLoadScript(\"vega-lite\", \"5.20.1\"))\n",
       "        .then(() => maybeLoadScript(\"vega-embed\", \"6\"))\n",
       "        .catch(showError)\n",
       "        .then(() => displayChart(vegaEmbed));\n",
       "    }\n",
       "  })({\"config\": {\"title\": {\"font\": \"Times New Roman\"}, \"axis\": {\"labelFont\": \"Times New Roman\", \"titleFont\": \"Times New Roman\"}, \"header\": {\"labelFont\": \"Times New Roman\", \"titleFont\": \"Times New Roman\"}, \"legend\": {\"labelFont\": \"Times New Roman\", \"titleFont\": \"Times New Roman\"}, \"text\": {\"font\": \"Times New Roman\"}}, \"data\": {\"name\": \"data-ad31ef507ef239cdd64fdbe14568e0d2\"}, \"mark\": {\"type\": \"line\"}, \"encoding\": {\"color\": {\"field\": \"Video Sources\", \"title\": \"Video Sources\", \"type\": \"nominal\"}, \"x\": {\"axis\": {\"labelAngle\": -30}, \"field\": \"Year Released\", \"sort\": [\"<2004\", \"2004\", \"2005\", \"2006\", \"2007\", \"2008\", \"2009\", \"2010\", \"2011\", \"2012\", \"2013\", \"2014\", \"2015\", \"2016\", \"2017\", \"2018\", \"2019\", \"2020\", \"2021\", \"2022\", \"2023\", \"2024\"], \"title\": \"Year Released\", \"type\": \"nominal\"}, \"y\": {\"axis\": {\"labelExpr\": \"datum.value >= 1000000 ? datum.value / 1000000 + 'M' : datum.value >= 1000 ? datum.value / 1000 + 'K' : datum.value\", \"values\": [0, 1, 1000, 10000, 100000, 1000000]}, \"field\": \"Cumulative Hours\", \"scale\": {\"constant\": 1000, \"domain\": [1, 1000000], \"type\": \"symlog\"}, \"title\": \"Cumulative Hours\", \"type\": \"quantitative\"}}, \"height\": 160, \"width\": 400, \"$schema\": \"https://vega.github.io/schema/vega-lite/v5.20.1.json\", \"datasets\": {\"data-ad31ef507ef239cdd64fdbe14568e0d2\": [{\"Year Released\": \"<2004\", \"Video Sources\": \"Other\", \"Cumulative Hours\": 0.0}, {\"Year Released\": \"<2004\", \"Video Sources\": \"crowdsourced\", \"Cumulative Hours\": 0.0}, {\"Year Released\": \"<2004\", \"Video Sources\": \"flickr\", \"Cumulative Hours\": 0.0}, {\"Year Released\": \"<2004\", \"Video Sources\": \"human\", \"Cumulative Hours\": 0.0}, {\"Year Released\": \"<2004\", \"Video Sources\": \"movies\", \"Cumulative Hours\": 0.0}, {\"Year Released\": \"<2004\", \"Video Sources\": \"undisclosed web\", \"Cumulative Hours\": 0.0}, {\"Year Released\": \"<2004\", \"Video Sources\": \"youtube\", \"Cumulative Hours\": 0.0}, {\"Year Released\": \"2004\", \"Video Sources\": \"youtube\", \"Cumulative Hours\": 0.0}, {\"Year Released\": \"2004\", \"Video Sources\": \"undisclosed web\", \"Cumulative Hours\": 0.0}, {\"Year Released\": \"2004\", \"Video Sources\": \"movies\", \"Cumulative Hours\": 0.0}, {\"Year Released\": \"2004\", \"Video Sources\": \"crowdsourced\", \"Cumulative Hours\": 0.0}, {\"Year Released\": \"2004\", \"Video Sources\": \"flickr\", \"Cumulative Hours\": 0.0}, {\"Year Released\": \"2004\", \"Video Sources\": \"Other\", \"Cumulative Hours\": 0.0}, {\"Year Released\": \"2004\", \"Video Sources\": \"human\", \"Cumulative Hours\": 0.0}, {\"Year Released\": \"2005\", \"Video Sources\": \"undisclosed web\", \"Cumulative Hours\": 0.0}, {\"Year Released\": \"2005\", \"Video Sources\": \"youtube\", \"Cumulative Hours\": 0.0}, {\"Year Released\": \"2005\", \"Video Sources\": \"human\", \"Cumulative Hours\": 0.0}, {\"Year Released\": \"2005\", \"Video Sources\": \"movies\", \"Cumulative Hours\": 0.0}, {\"Year Released\": \"2005\", \"Video Sources\": \"crowdsourced\", \"Cumulative Hours\": 0.0}, {\"Year Released\": \"2005\", \"Video Sources\": \"Other\", \"Cumulative Hours\": 0.0}, {\"Year Released\": \"2005\", \"Video Sources\": \"flickr\", \"Cumulative Hours\": 0.0}, {\"Year Released\": \"2006\", \"Video Sources\": \"youtube\", \"Cumulative Hours\": 0.0}, {\"Year Released\": \"2006\", \"Video Sources\": \"movies\", \"Cumulative Hours\": 0.0}, {\"Year Released\": \"2006\", \"Video Sources\": \"human\", \"Cumulative Hours\": 0.0}, {\"Year Released\": \"2006\", \"Video Sources\": \"undisclosed web\", \"Cumulative Hours\": 0.0}, {\"Year Released\": \"2006\", \"Video Sources\": \"crowdsourced\", \"Cumulative Hours\": 0.0}, {\"Year Released\": \"2006\", \"Video Sources\": \"Other\", \"Cumulative Hours\": 0.0}, {\"Year Released\": \"2006\", \"Video Sources\": \"flickr\", \"Cumulative Hours\": 0.0}, {\"Year Released\": \"2007\", \"Video Sources\": \"undisclosed web\", \"Cumulative Hours\": 0.0}, {\"Year Released\": \"2007\", \"Video Sources\": \"Other\", \"Cumulative Hours\": 0.0}, {\"Year Released\": \"2007\", \"Video Sources\": \"crowdsourced\", \"Cumulative Hours\": 0.0}, {\"Year Released\": \"2007\", \"Video Sources\": \"flickr\", \"Cumulative Hours\": 0.0}, {\"Year Released\": \"2007\", \"Video Sources\": \"human\", \"Cumulative Hours\": 0.0}, {\"Year Released\": \"2007\", \"Video Sources\": \"movies\", \"Cumulative Hours\": 0.0}, {\"Year Released\": \"2007\", \"Video Sources\": \"youtube\", \"Cumulative Hours\": 0.0}, {\"Year Released\": \"2008\", \"Video Sources\": \"undisclosed web\", \"Cumulative Hours\": 0.0}, {\"Year Released\": \"2008\", \"Video Sources\": \"youtube\", \"Cumulative Hours\": 0.0}, {\"Year Released\": \"2008\", \"Video Sources\": \"movies\", \"Cumulative Hours\": 0.0}, {\"Year Released\": \"2008\", \"Video Sources\": \"crowdsourced\", \"Cumulative Hours\": 0.0}, {\"Year Released\": \"2008\", \"Video Sources\": \"flickr\", \"Cumulative Hours\": 0.0}, {\"Year Released\": \"2008\", \"Video Sources\": \"human\", \"Cumulative Hours\": 0.0}, {\"Year Released\": \"2008\", \"Video Sources\": \"Other\", \"Cumulative Hours\": 0.0}, {\"Year Released\": \"2009\", \"Video Sources\": \"Other\", \"Cumulative Hours\": 0.0}, {\"Year Released\": \"2009\", \"Video Sources\": \"crowdsourced\", \"Cumulative Hours\": 0.0}, {\"Year Released\": \"2009\", \"Video Sources\": \"flickr\", \"Cumulative Hours\": 0.0}, {\"Year Released\": \"2009\", \"Video Sources\": \"human\", \"Cumulative Hours\": 0.1}, {\"Year Released\": \"2009\", \"Video Sources\": \"movies\", \"Cumulative Hours\": 20.1}, {\"Year Released\": \"2009\", \"Video Sources\": \"undisclosed web\", \"Cumulative Hours\": 0.0}, {\"Year Released\": \"2009\", \"Video Sources\": \"youtube\", \"Cumulative Hours\": 0.0}, {\"Year Released\": \"2010\", \"Video Sources\": \"undisclosed web\", \"Cumulative Hours\": 0.0}, {\"Year Released\": \"2010\", \"Video Sources\": \"movies\", \"Cumulative Hours\": 20.1}, {\"Year Released\": \"2010\", \"Video Sources\": \"human\", \"Cumulative Hours\": 0.1}, {\"Year Released\": \"2010\", \"Video Sources\": \"youtube\", \"Cumulative Hours\": 0.0}, {\"Year Released\": \"2010\", \"Video Sources\": \"crowdsourced\", \"Cumulative Hours\": 0.0}, {\"Year Released\": \"2010\", \"Video Sources\": \"Other\", \"Cumulative Hours\": 0.0}, {\"Year Released\": \"2010\", \"Video Sources\": \"flickr\", \"Cumulative Hours\": 0.0}, {\"Year Released\": \"2011\", \"Video Sources\": \"undisclosed web\", \"Cumulative Hours\": 7000.0}, {\"Year Released\": \"2011\", \"Video Sources\": \"movies\", \"Cumulative Hours\": 7020.1}, {\"Year Released\": \"2011\", \"Video Sources\": \"human\", \"Cumulative Hours\": 0.1}, {\"Year Released\": \"2011\", \"Video Sources\": \"youtube\", \"Cumulative Hours\": 7000.0}, {\"Year Released\": \"2011\", \"Video Sources\": \"crowdsourced\", \"Cumulative Hours\": 0.0}, {\"Year Released\": \"2011\", \"Video Sources\": \"Other\", \"Cumulative Hours\": 14000.0}, {\"Year Released\": \"2011\", \"Video Sources\": \"flickr\", \"Cumulative Hours\": 0.0}, {\"Year Released\": \"2012\", \"Video Sources\": \"Other\", \"Cumulative Hours\": 14000.0}, {\"Year Released\": \"2012\", \"Video Sources\": \"crowdsourced\", \"Cumulative Hours\": 0.0}, {\"Year Released\": \"2012\", \"Video Sources\": \"flickr\", \"Cumulative Hours\": 0.0}, {\"Year Released\": \"2012\", \"Video Sources\": \"human\", \"Cumulative Hours\": 0.1}, {\"Year Released\": \"2012\", \"Video Sources\": \"movies\", \"Cumulative Hours\": 7020.1}, {\"Year Released\": \"2012\", \"Video Sources\": \"undisclosed web\", \"Cumulative Hours\": 7000.0}, {\"Year Released\": \"2012\", \"Video Sources\": \"youtube\", \"Cumulative Hours\": 7026.0}, {\"Year Released\": \"2013\", \"Video Sources\": \"movies\", \"Cumulative Hours\": 7020.1}, {\"Year Released\": \"2013\", \"Video Sources\": \"undisclosed web\", \"Cumulative Hours\": 7000.0}, {\"Year Released\": \"2013\", \"Video Sources\": \"human\", \"Cumulative Hours\": 40.1}, {\"Year Released\": \"2013\", \"Video Sources\": \"youtube\", \"Cumulative Hours\": 8026.0}, {\"Year Released\": \"2013\", \"Video Sources\": \"crowdsourced\", \"Cumulative Hours\": 0.0}, {\"Year Released\": \"2013\", \"Video Sources\": \"flickr\", \"Cumulative Hours\": 0.0}, {\"Year Released\": \"2013\", \"Video Sources\": \"Other\", \"Cumulative Hours\": 14000.0}, {\"Year Released\": \"2014\", \"Video Sources\": \"youtube\", \"Cumulative Hours\": 114784.41}, {\"Year Released\": \"2014\", \"Video Sources\": \"Other\", \"Cumulative Hours\": 14000.0}, {\"Year Released\": \"2014\", \"Video Sources\": \"crowdsourced\", \"Cumulative Hours\": 1.11}, {\"Year Released\": \"2014\", \"Video Sources\": \"flickr\", \"Cumulative Hours\": 0.0}, {\"Year Released\": \"2014\", \"Video Sources\": \"human\", \"Cumulative Hours\": 117.1}, {\"Year Released\": \"2014\", \"Video Sources\": \"movies\", \"Cumulative Hours\": 7036.150000000001}, {\"Year Released\": \"2014\", \"Video Sources\": \"undisclosed web\", \"Cumulative Hours\": 7000.0}, {\"Year Released\": \"2015\", \"Video Sources\": \"youtube\", \"Cumulative Hours\": 114788.01000000001}, {\"Year Released\": \"2015\", \"Video Sources\": \"undisclosed web\", \"Cumulative Hours\": 7849.0}, {\"Year Released\": \"2015\", \"Video Sources\": \"movies\", \"Cumulative Hours\": 7417.150000000001}, {\"Year Released\": \"2015\", \"Video Sources\": \"Other\", \"Cumulative Hours\": 14000.0}, {\"Year Released\": \"2015\", \"Video Sources\": \"flickr\", \"Cumulative Hours\": 0.0}, {\"Year Released\": \"2015\", \"Video Sources\": \"crowdsourced\", \"Cumulative Hours\": 1.11}, {\"Year Released\": \"2015\", \"Video Sources\": \"human\", \"Cumulative Hours\": 144.1}, {\"Year Released\": \"2016\", \"Video Sources\": \"movies\", \"Cumulative Hours\": 7417.150000000001}, {\"Year Released\": \"2016\", \"Video Sources\": \"undisclosed web\", \"Cumulative Hours\": 8103.4}, {\"Year Released\": \"2016\", \"Video Sources\": \"youtube\", \"Cumulative Hours\": 464795.01}, {\"Year Released\": \"2016\", \"Video Sources\": \"human\", \"Cumulative Hours\": 253.64}, {\"Year Released\": \"2016\", \"Video Sources\": \"crowdsourced\", \"Cumulative Hours\": 75.21}, {\"Year Released\": \"2016\", \"Video Sources\": \"Other\", \"Cumulative Hours\": 14086.1}, {\"Year Released\": \"2016\", \"Video Sources\": \"flickr\", \"Cumulative Hours\": 0.0}, {\"Year Released\": \"2017\", \"Video Sources\": \"undisclosed web\", \"Cumulative Hours\": 8112.7}, {\"Year Released\": \"2017\", \"Video Sources\": \"movies\", \"Cumulative Hours\": 7631.650000000001}, {\"Year Released\": \"2017\", \"Video Sources\": \"human\", \"Cumulative Hours\": 1104.32}, {\"Year Released\": \"2017\", \"Video Sources\": \"youtube\", \"Cumulative Hours\": 467108.11}, {\"Year Released\": \"2017\", \"Video Sources\": \"crowdsourced\", \"Cumulative Hours\": 266.66999999999996}, {\"Year Released\": \"2017\", \"Video Sources\": \"Other\", \"Cumulative Hours\": 14086.1}, {\"Year Released\": \"2017\", \"Video Sources\": \"flickr\", \"Cumulative Hours\": 0.0}, {\"Year Released\": \"2018\", \"Video Sources\": \"Other\", \"Cumulative Hours\": 15647.2}, {\"Year Released\": \"2018\", \"Video Sources\": \"crowdsourced\", \"Cumulative Hours\": 266.66999999999996}, {\"Year Released\": \"2018\", \"Video Sources\": \"flickr\", \"Cumulative Hours\": 275.0}, {\"Year Released\": \"2018\", \"Video Sources\": \"human\", \"Cumulative Hours\": 1273.6499999999999}, {\"Year Released\": \"2018\", \"Video Sources\": \"movies\", \"Cumulative Hours\": 7725.55}, {\"Year Released\": \"2018\", \"Video Sources\": \"undisclosed web\", \"Cumulative Hours\": 8112.7}, {\"Year Released\": \"2018\", \"Video Sources\": \"youtube\", \"Cumulative Hours\": 471120.63}, {\"Year Released\": \"2019\", \"Video Sources\": \"youtube\", \"Cumulative Hours\": 608225.21}, {\"Year Released\": \"2019\", \"Video Sources\": \"undisclosed web\", \"Cumulative Hours\": 9112.7}, {\"Year Released\": \"2019\", \"Video Sources\": \"movies\", \"Cumulative Hours\": 8241.550000000001}, {\"Year Released\": \"2019\", \"Video Sources\": \"crowdsourced\", \"Cumulative Hours\": 279.66999999999996}, {\"Year Released\": \"2019\", \"Video Sources\": \"flickr\", \"Cumulative Hours\": 1108.0}, {\"Year Released\": \"2019\", \"Video Sources\": \"Other\", \"Cumulative Hours\": 24182.7}, {\"Year Released\": \"2019\", \"Video Sources\": \"human\", \"Cumulative Hours\": 1642.23}, {\"Year Released\": \"2020\", \"Video Sources\": \"undisclosed web\", \"Cumulative Hours\": 10211.83}, {\"Year Released\": \"2020\", \"Video Sources\": \"movies\", \"Cumulative Hours\": 12919.55}, {\"Year Released\": \"2020\", \"Video Sources\": \"human\", \"Cumulative Hours\": 1658.51}, {\"Year Released\": \"2020\", \"Video Sources\": \"youtube\", \"Cumulative Hours\": 724739.74}, {\"Year Released\": \"2020\", \"Video Sources\": \"crowdsourced\", \"Cumulative Hours\": 304.21}, {\"Year Released\": \"2020\", \"Video Sources\": \"Other\", \"Cumulative Hours\": 50848.7}, {\"Year Released\": \"2020\", \"Video Sources\": \"flickr\", \"Cumulative Hours\": 1108.0}, {\"Year Released\": \"2021\", \"Video Sources\": \"Other\", \"Cumulative Hours\": 54805.73}, {\"Year Released\": \"2021\", \"Video Sources\": \"crowdsourced\", \"Cumulative Hours\": 1137.21}, {\"Year Released\": \"2021\", \"Video Sources\": \"flickr\", \"Cumulative Hours\": 1524.67}, {\"Year Released\": \"2021\", \"Video Sources\": \"human\", \"Cumulative Hours\": 1706.85}, {\"Year Released\": \"2021\", \"Video Sources\": \"movies\", \"Cumulative Hours\": 12955.55}, {\"Year Released\": \"2021\", \"Video Sources\": \"undisclosed web\", \"Cumulative Hours\": 24044.83}, {\"Year Released\": \"2021\", \"Video Sources\": \"youtube\", \"Cumulative Hours\": 740915.86}, {\"Year Released\": \"2022\", \"Video Sources\": \"youtube\", \"Cumulative Hours\": 796470.86}, {\"Year Released\": \"2022\", \"Video Sources\": \"undisclosed web\", \"Cumulative Hours\": 25268.6}, {\"Year Released\": \"2022\", \"Video Sources\": \"movies\", \"Cumulative Hours\": 12955.55}, {\"Year Released\": \"2022\", \"Video Sources\": \"crowdsourced\", \"Cumulative Hours\": 1352.21}, {\"Year Released\": \"2022\", \"Video Sources\": \"flickr\", \"Cumulative Hours\": 1524.67}, {\"Year Released\": \"2022\", \"Video Sources\": \"Other\", \"Cumulative Hours\": 54805.73}, {\"Year Released\": \"2022\", \"Video Sources\": \"human\", \"Cumulative Hours\": 5376.85}, {\"Year Released\": \"2023\", \"Video Sources\": \"youtube\", \"Cumulative Hours\": 796470.86}, {\"Year Released\": \"2023\", \"Video Sources\": \"movies\", \"Cumulative Hours\": 12955.55}, {\"Year Released\": \"2023\", \"Video Sources\": \"human\", \"Cumulative Hours\": 6805.45}, {\"Year Released\": \"2023\", \"Video Sources\": \"undisclosed web\", \"Cumulative Hours\": 25268.6}, {\"Year Released\": \"2023\", \"Video Sources\": \"crowdsourced\", \"Cumulative Hours\": 1352.21}, {\"Year Released\": \"2023\", \"Video Sources\": \"Other\", \"Cumulative Hours\": 54805.73}, {\"Year Released\": \"2023\", \"Video Sources\": \"flickr\", \"Cumulative Hours\": 1524.67}, {\"Year Released\": \"2024\", \"Video Sources\": \"undisclosed web\", \"Cumulative Hours\": 25268.6}, {\"Year Released\": \"2024\", \"Video Sources\": \"Other\", \"Cumulative Hours\": 54889.73}, {\"Year Released\": \"2024\", \"Video Sources\": \"crowdsourced\", \"Cumulative Hours\": 1352.21}, {\"Year Released\": \"2024\", \"Video Sources\": \"flickr\", \"Cumulative Hours\": 1524.67}, {\"Year Released\": \"2024\", \"Video Sources\": \"human\", \"Cumulative Hours\": 8455.45}, {\"Year Released\": \"2024\", \"Video Sources\": \"movies\", \"Cumulative Hours\": 13373.15}, {\"Year Released\": \"2024\", \"Video Sources\": \"youtube\", \"Cumulative Hours\": 796972.46}]}}, {\"mode\": \"vega-lite\"});\n",
       "</script>"
      ],
      "text/plain": [
       "alt.Chart(...)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chart_sourceyearhours = alt.Chart(\n",
    "    df_speechsourceyearscumulativehours\n",
    ").mark_line().encode(\n",
    "    x=alt.X(\n",
    "        \"Year Released:N\",\n",
    "        title=\"Year Released\",\n",
    "        sort=YEARS_ORDER,\n",
    "        axis=alt.Axis(labelAngle=-30)\n",
    "    ),\n",
    "    y=alt.Y(\n",
    "        \"Cumulative Hours:Q\",\n",
    "        title=\"Cumulative Hours\",\n",
    "        scale=alt.Scale(\n",
    "            type=\"symlog\",\n",
    "            constant=1000,\n",
    "            domain=[1, 1000000]\n",
    "        ),\n",
    "        axis=alt.Axis(\n",
    "            values=[0, 1, 1000, 10000, 100000, 1000000],\n",
    "            labelExpr=\"datum.value >= 1000000 ? datum.value / 1000000 + 'M' : datum.value >= 1000 ? datum.value / 1000 + 'K' : datum.value\"\n",
    "        )\n",
    "    ),\n",
    "    color=alt.Color(\n",
    "        \"Video Sources:N\",\n",
    "        title=\"Video Sources\"\n",
    "    )\n",
    ").properties(\n",
    "    width=400,\n",
    "    height=160\n",
    ")\n",
    "\n",
    "if PLOT_TOFILE:\n",
    "    chart_sourceyearhours.save(\n",
    "        os.path.join(PLOT_DIR, \"video_sourcecategories-cumulativehours.png\"),\n",
    "        ppi=PLOT_PPI\n",
    "    )\n",
    "\n",
    "chart_sourceyearhours"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Combine Source Years-Based Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "  #altair-viz-49c6897901294111ba5b4e7239ad7767.vega-embed {\n",
       "    width: 100%;\n",
       "    display: flex;\n",
       "  }\n",
       "\n",
       "  #altair-viz-49c6897901294111ba5b4e7239ad7767.vega-embed details,\n",
       "  #altair-viz-49c6897901294111ba5b4e7239ad7767.vega-embed details summary {\n",
       "    position: relative;\n",
       "  }\n",
       "</style>\n",
       "<div id=\"altair-viz-49c6897901294111ba5b4e7239ad7767\"></div>\n",
       "<script type=\"text/javascript\">\n",
       "  var VEGA_DEBUG = (typeof VEGA_DEBUG == \"undefined\") ? {} : VEGA_DEBUG;\n",
       "  (function(spec, embedOpt){\n",
       "    let outputDiv = document.currentScript.previousElementSibling;\n",
       "    if (outputDiv.id !== \"altair-viz-49c6897901294111ba5b4e7239ad7767\") {\n",
       "      outputDiv = document.getElementById(\"altair-viz-49c6897901294111ba5b4e7239ad7767\");\n",
       "    }\n",
       "    const paths = {\n",
       "      \"vega\": \"https://cdn.jsdelivr.net/npm/vega@5?noext\",\n",
       "      \"vega-lib\": \"https://cdn.jsdelivr.net/npm/vega-lib?noext\",\n",
       "      \"vega-lite\": \"https://cdn.jsdelivr.net/npm/vega-lite@5.20.1?noext\",\n",
       "      \"vega-embed\": \"https://cdn.jsdelivr.net/npm/vega-embed@6?noext\",\n",
       "    };\n",
       "\n",
       "    function maybeLoadScript(lib, version) {\n",
       "      var key = `${lib.replace(\"-\", \"\")}_version`;\n",
       "      return (VEGA_DEBUG[key] == version) ?\n",
       "        Promise.resolve(paths[lib]) :\n",
       "        new Promise(function(resolve, reject) {\n",
       "          var s = document.createElement('script');\n",
       "          document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
       "          s.async = true;\n",
       "          s.onload = () => {\n",
       "            VEGA_DEBUG[key] = version;\n",
       "            return resolve(paths[lib]);\n",
       "          };\n",
       "          s.onerror = () => reject(`Error loading script: ${paths[lib]}`);\n",
       "          s.src = paths[lib];\n",
       "        });\n",
       "    }\n",
       "\n",
       "    function showError(err) {\n",
       "      outputDiv.innerHTML = `<div class=\"error\" style=\"color:red;\">${err}</div>`;\n",
       "      throw err;\n",
       "    }\n",
       "\n",
       "    function displayChart(vegaEmbed) {\n",
       "      vegaEmbed(outputDiv, spec, embedOpt)\n",
       "        .catch(err => showError(`Javascript Error: ${err.message}<br>This usually means there's a typo in your chart specification. See the javascript console for the full traceback.`));\n",
       "    }\n",
       "\n",
       "    if(typeof define === \"function\" && define.amd) {\n",
       "      requirejs.config({paths});\n",
       "      require([\"vega-embed\"], displayChart, err => showError(`Error loading script: ${err.message}`));\n",
       "    } else {\n",
       "      maybeLoadScript(\"vega\", \"5\")\n",
       "        .then(() => maybeLoadScript(\"vega-lite\", \"5.20.1\"))\n",
       "        .then(() => maybeLoadScript(\"vega-embed\", \"6\"))\n",
       "        .catch(showError)\n",
       "        .then(() => displayChart(vegaEmbed));\n",
       "    }\n",
       "  })({\"config\": {\"title\": {\"font\": \"Times New Roman\", \"fontSize\": 20}, \"axis\": {\"labelFont\": \"Times New Roman\", \"titleFont\": \"Times New Roman\", \"grid\": false, \"labelFontSize\": 20, \"titleFontSize\": 20}, \"header\": {\"labelFont\": \"Times New Roman\", \"titleFont\": \"Times New Roman\", \"labelFontSize\": 20, \"titleFontSize\": 20}, \"legend\": {\"labelFont\": \"Times New Roman\", \"titleFont\": \"Times New Roman\", \"labelFontSize\": 20, \"labelLimit\": 1000, \"titleFontSize\": 20}, \"text\": {\"font\": \"Times New Roman\"}}, \"hconcat\": [{\"data\": {\"name\": \"data-1e9feeadaef45a8f9a62e99c28759dee\"}, \"mark\": {\"type\": \"bar\"}, \"encoding\": {\"color\": {\"field\": \"Video Sources\", \"title\": \"Video Sources\", \"type\": \"nominal\"}, \"x\": {\"axis\": {\"labelAngle\": -30}, \"field\": \"Year Released\", \"sort\": [\"<2004\", \"2004\", \"2005\", \"2006\", \"2007\", \"2008\", \"2009\", \"2010\", \"2011\", \"2012\", \"2013\", \"2014\", \"2015\", \"2016\", \"2017\", \"2018\", \"2019\", \"2020\", \"2021\", \"2022\", \"2023\", \"2024\"], \"title\": \"Year Released\", \"type\": \"nominal\"}, \"y\": {\"aggregate\": \"count\", \"axis\": {\"format\": \"%\"}, \"stack\": \"normalize\", \"title\": \"Pct. Datasets\", \"type\": \"quantitative\"}}, \"height\": 160, \"width\": 600}, {\"data\": {\"name\": \"data-ad31ef507ef239cdd64fdbe14568e0d2\"}, \"mark\": {\"type\": \"line\"}, \"encoding\": {\"color\": {\"field\": \"Video Sources\", \"title\": \"Video Sources\", \"type\": \"nominal\"}, \"x\": {\"axis\": {\"labelAngle\": -30}, \"field\": \"Year Released\", \"sort\": [\"<2004\", \"2004\", \"2005\", \"2006\", \"2007\", \"2008\", \"2009\", \"2010\", \"2011\", \"2012\", \"2013\", \"2014\", \"2015\", \"2016\", \"2017\", \"2018\", \"2019\", \"2020\", \"2021\", \"2022\", \"2023\", \"2024\"], \"title\": \"Year Released\", \"type\": \"nominal\"}, \"y\": {\"axis\": {\"labelExpr\": \"datum.value >= 1000000 ? datum.value / 1000000 + 'M' : datum.value >= 1000 ? datum.value / 1000 + 'K' : datum.value\", \"values\": [0, 1, 1000, 10000, 100000, 1000000]}, \"field\": \"Cumulative Hours\", \"scale\": {\"constant\": 1000, \"domain\": [1, 1000000], \"type\": \"symlog\"}, \"title\": \"Cumulative Hours\", \"type\": \"quantitative\"}}, \"height\": 160, \"width\": 400}], \"resolve\": {\"scale\": {\"x\": \"independent\", \"y\": \"independent\"}}, \"$schema\": \"https://vega.github.io/schema/vega-lite/v5.20.1.json\", \"datasets\": {\"data-1e9feeadaef45a8f9a62e99c28759dee\": [{\"Unique Dataset Identifier\": \"collective\", \"Collection\": \"collective\", \"Collection URL\": \"https://cvgl.stanford.edu/papers/Wongun_CollectiveActivityRecognition09.pdf\", \"Dataset Name\": \"Collective: What are they doing? : Collective activity classification using spatio-temporal relationship among people\", \"Paper Title\": \"Collective: What are they doing? : Collective activity classification using spatio-temporal relationship among people\", \"Paper URL\": \"https://cvgl.stanford.edu/papers/Wongun_CollectiveActivityRecognition09.pdf\", \"GitHub URL\": \"\", \"Hugging Face URL\": \"\", \"Papers with Code URL\": \"\", \"ArXiv URL\": \"https://cvgl.stanford.edu/papers/Wongun_CollectiveActivityRecognition09.pdf\", \"Semantic Scholar Corpus ID\": 5925915, \"Year Released\": \"2009\", \"Text Sources\": [\"human\"], \"Licenses\": [{\"License\": \"Unspecified\", \"License URL\": \"\"}], \"Creators\": [\"University of Michigan\"], \"Countries\": [\"United States of America\"], \"License Verified By\": \"Vivek Sharma\", \"Video Hours\": 0.1, \"Taken Down\": \"False\", \"Video Sources\": \"human\", \"Task Categories\": [\"Group Activity Recognition\"], \"Inferred Metadata\": {\"GitHub License\": \"\", \"GitHub Stars (June 2024)\": \"\", \"GitHub Topics\": \"\", \"Github Date\": \"\", \"HF Config\": \"\", \"HF Config License\": \"\", \"HF Dataset\": \"\", \"HF Date\": \"\", \"HF Downloads (June 2024)\": \"\", \"HF Likes (June 2024)\": \"\", \"HF Yaml License\": \"\", \"PwC Date\": \"\", \"PwC Description\": \"\", \"PwC License Name\": \"\", \"PwC License URL\": \"\", \"S2 Citation Count (June 2024)\": \"\", \"S2 Date\": \"\"}, \"Bibtex\": \"@Article{Choi2009WhatAT,\\n author = {Wongun Choi and Khuram Shahid and S. Savarese},\\n booktitle = {2009 IEEE 12th International Conference on Computer Vision Workshops, ICCV Workshops},\\n journal = {2009 IEEE 12th International Conference on Computer Vision Workshops, ICCV Workshops},\\n pages = {1282-1289},\\n title = {What are they doing? : Collective activity classification using spatio-temporal relationship among people},\\n year = {2009}\\n}\\n\", \"License Use (DataProvenance)\": \"unspecified\", \"License Attribution (DataProvenance)\": 0, \"License Share Alike (DataProvenance)\": 0, \"License Use (DataProvenance IgnoreOpenAI)\": \"unspecified\", \"License Attribution (DataProvenance IgnoreOpenAI)\": 0, \"License Share Alike (DataProvenance IgnoreOpenAI)\": 0, \"License Type\": \"Unspecified\"}, {\"Unique Dataset Identifier\": \"hollywood2-dataset\", \"Collection\": \"hollywood2-dataset\", \"Collection URL\": \"https://www.irisa.fr/vista/Papers/2009_cvpr_marszalek.pdf\", \"Dataset Name\": \"HOLLYWOOD2: Actions in Context (CVPR 2009)\", \"Paper Title\": \"HOLLYWOOD2: Actions in Context (CVPR 2009)\", \"Paper URL\": \"https://www.irisa.fr/vista/Papers/2009_cvpr_marszalek.pdf\", \"GitHub URL\": \"\", \"Hugging Face URL\": \"\", \"Papers with Code URL\": \"\", \"ArXiv URL\": \"https://www.irisa.fr/vista/Papers/2009_cvpr_marszalek.pdf\", \"Semantic Scholar Corpus ID\": 3155054, \"Year Released\": \"2009\", \"Text Sources\": [\"movies\"], \"Licenses\": [{\"License\": \"Unspecified\", \"License URL\": \"\"}], \"Creators\": [\"Inria\"], \"Countries\": [\"France\"], \"License Verified By\": \"Vivek Sharma\", \"Video Hours\": 20.1, \"Taken Down\": \"False\", \"Video Sources\": \"movies\", \"Task Categories\": [\"Video Classification\"], \"Inferred Metadata\": {\"GitHub License\": \"\", \"GitHub Stars (June 2024)\": \"\", \"GitHub Topics\": \"\", \"Github Date\": \"\", \"HF Config\": \"\", \"HF Config License\": \"\", \"HF Dataset\": \"\", \"HF Date\": \"\", \"HF Downloads (June 2024)\": \"\", \"HF Likes (June 2024)\": \"\", \"HF Yaml License\": \"\", \"PwC Date\": \"\", \"PwC Description\": \"\", \"PwC License Name\": \"\", \"PwC License URL\": \"\", \"S2 Citation Count (June 2024)\": \"\", \"S2 Date\": \"\"}, \"Bibtex\": \"@Article{Marszalek2009ActionsIC,\\n author = {Marcin Marszalek and I. Laptev and C. Schmid},\\n booktitle = {2009 IEEE Conference on Computer Vision and Pattern Recognition},\\n journal = {2009 IEEE Conference on Computer Vision and Pattern Recognition},\\n pages = {2929-2936},\\n title = {Actions in context},\\n year = {2009}\\n}\\n\", \"License Use (DataProvenance)\": \"unspecified\", \"License Attribution (DataProvenance)\": 0, \"License Share Alike (DataProvenance)\": 0, \"License Use (DataProvenance IgnoreOpenAI)\": \"unspecified\", \"License Attribution (DataProvenance IgnoreOpenAI)\": 0, \"License Share Alike (DataProvenance IgnoreOpenAI)\": 0, \"License Type\": \"Unspecified\"}, {\"Unique Dataset Identifier\": \"hmdb-dataset\", \"Collection\": \"hmdb-dataset\", \"Collection URL\": \"https://ieeexplore.ieee.org/document/6126543\", \"Dataset Name\": \"HMDB: A Large Video Database for Human Motion Recognition (ICCV 2011)\", \"Paper Title\": \"HMDB: A Large Video Database for Human Motion Recognition (ICCV 2011)\", \"Paper URL\": \"https://ieeexplore.ieee.org/document/6126543\", \"GitHub URL\": \"\", \"Hugging Face URL\": \"\", \"Papers with Code URL\": \"https://paperswithcode.com/dataset/hmdb51\", \"ArXiv URL\": \"https://ieeexplore.ieee.org/document/6126543\", \"Semantic Scholar Corpus ID\": 206769852, \"Year Released\": \"2011\", \"Text Sources\": [\"movies\", \"prelinger archive\", \"undisclosed web\", \"youtube\", \"google videos\"], \"Licenses\": [{\"License\": \"CC BY 4.0\", \"License URL\": \"https://serre-lab.clps.brown.edu/resource/hmdb-a-large-human-motion-database/\"}], \"Creators\": [\"Karlsruhe Institute of Technology\", \"Massachusetts Institute of Technology\", \"Brown University\"], \"Countries\": [\"United States of America\", \"Germany\"], \"License Verified By\": \"Vivek Sharma\", \"Video Hours\": 7000.0, \"Taken Down\": \"False\", \"Video Sources\": \"youtube\", \"Task Categories\": [\"Video Classification\"], \"Inferred Metadata\": {\"GitHub License\": \"\", \"GitHub Stars (June 2024)\": \"\", \"GitHub Topics\": \"\", \"Github Date\": \"\", \"HF Config\": \"\", \"HF Config License\": \"\", \"HF Dataset\": \"\", \"HF Date\": \"\", \"HF Downloads (June 2024)\": \"\", \"HF Likes (June 2024)\": \"\", \"HF Yaml License\": \"\", \"PwC Date\": \"2011-01-01\", \"PwC Description\": \"The HMDB51 dataset is a large collection of realistic videos from various sources, including movies and web videos. The dataset is composed of 6,766 video clips from 51 action categories (such as \\u201cjump\\u201d, \\u201ckiss\\u201d and \\u201claugh\\u201d), with each category containing at least 101 clips. The original evaluation scheme uses three different training/testing splits. In each split, each action class has 70 clips for training and 30 clips for testing. The average accuracy over these three splits is used to measure the final performance.\", \"PwC License Name\": \"CC BY 4.0\", \"PwC License URL\": \"https://creativecommons.org/licenses/by/4.0/\", \"S2 Citation Count (June 2024)\": \"\", \"S2 Date\": \"\"}, \"Bibtex\": \"@Article{Kuehne2011HMDBAL,\\n author = {Hilde Kuehne and Hueihan Jhuang and Est\\u00edbaliz Garrote and T. Poggio and Thomas Serre},\\n booktitle = {Vision},\\n journal = {2011 International Conference on Computer Vision},\\n pages = {2556-2563},\\n title = {HMDB: A large video database for human motion recognition},\\n year = {2011}\\n}\\n\", \"License Use (DataProvenance)\": \"commercial\", \"License Attribution (DataProvenance)\": 1, \"License Share Alike (DataProvenance)\": 0, \"License Use (DataProvenance IgnoreOpenAI)\": \"commercial\", \"License Attribution (DataProvenance IgnoreOpenAI)\": 1, \"License Share Alike (DataProvenance IgnoreOpenAI)\": 0, \"License Type\": \"Commercial\"}, {\"Unique Dataset Identifier\": \"hmdb-dataset\", \"Collection\": \"hmdb-dataset\", \"Collection URL\": \"https://ieeexplore.ieee.org/document/6126543\", \"Dataset Name\": \"HMDB: A Large Video Database for Human Motion Recognition (ICCV 2011)\", \"Paper Title\": \"HMDB: A Large Video Database for Human Motion Recognition (ICCV 2011)\", \"Paper URL\": \"https://ieeexplore.ieee.org/document/6126543\", \"GitHub URL\": \"\", \"Hugging Face URL\": \"\", \"Papers with Code URL\": \"https://paperswithcode.com/dataset/hmdb51\", \"ArXiv URL\": \"https://ieeexplore.ieee.org/document/6126543\", \"Semantic Scholar Corpus ID\": 206769852, \"Year Released\": \"2011\", \"Text Sources\": [\"movies\", \"prelinger archive\", \"undisclosed web\", \"youtube\", \"google videos\"], \"Licenses\": [{\"License\": \"CC BY 4.0\", \"License URL\": \"https://serre-lab.clps.brown.edu/resource/hmdb-a-large-human-motion-database/\"}], \"Creators\": [\"Karlsruhe Institute of Technology\", \"Massachusetts Institute of Technology\", \"Brown University\"], \"Countries\": [\"United States of America\", \"Germany\"], \"License Verified By\": \"Vivek Sharma\", \"Video Hours\": 7000.0, \"Taken Down\": \"False\", \"Video Sources\": \"undisclosed web\", \"Task Categories\": [\"Video Classification\"], \"Inferred Metadata\": {\"GitHub License\": \"\", \"GitHub Stars (June 2024)\": \"\", \"GitHub Topics\": \"\", \"Github Date\": \"\", \"HF Config\": \"\", \"HF Config License\": \"\", \"HF Dataset\": \"\", \"HF Date\": \"\", \"HF Downloads (June 2024)\": \"\", \"HF Likes (June 2024)\": \"\", \"HF Yaml License\": \"\", \"PwC Date\": \"2011-01-01\", \"PwC Description\": \"The HMDB51 dataset is a large collection of realistic videos from various sources, including movies and web videos. The dataset is composed of 6,766 video clips from 51 action categories (such as \\u201cjump\\u201d, \\u201ckiss\\u201d and \\u201claugh\\u201d), with each category containing at least 101 clips. The original evaluation scheme uses three different training/testing splits. In each split, each action class has 70 clips for training and 30 clips for testing. The average accuracy over these three splits is used to measure the final performance.\", \"PwC License Name\": \"CC BY 4.0\", \"PwC License URL\": \"https://creativecommons.org/licenses/by/4.0/\", \"S2 Citation Count (June 2024)\": \"\", \"S2 Date\": \"\"}, \"Bibtex\": \"@Article{Kuehne2011HMDBAL,\\n author = {Hilde Kuehne and Hueihan Jhuang and Est\\u00edbaliz Garrote and T. Poggio and Thomas Serre},\\n booktitle = {Vision},\\n journal = {2011 International Conference on Computer Vision},\\n pages = {2556-2563},\\n title = {HMDB: A large video database for human motion recognition},\\n year = {2011}\\n}\\n\", \"License Use (DataProvenance)\": \"commercial\", \"License Attribution (DataProvenance)\": 1, \"License Share Alike (DataProvenance)\": 0, \"License Use (DataProvenance IgnoreOpenAI)\": \"commercial\", \"License Attribution (DataProvenance IgnoreOpenAI)\": 1, \"License Share Alike (DataProvenance IgnoreOpenAI)\": 0, \"License Type\": \"Commercial\"}, {\"Unique Dataset Identifier\": \"hmdb-dataset\", \"Collection\": \"hmdb-dataset\", \"Collection URL\": \"https://ieeexplore.ieee.org/document/6126543\", \"Dataset Name\": \"HMDB: A Large Video Database for Human Motion Recognition (ICCV 2011)\", \"Paper Title\": \"HMDB: A Large Video Database for Human Motion Recognition (ICCV 2011)\", \"Paper URL\": \"https://ieeexplore.ieee.org/document/6126543\", \"GitHub URL\": \"\", \"Hugging Face URL\": \"\", \"Papers with Code URL\": \"https://paperswithcode.com/dataset/hmdb51\", \"ArXiv URL\": \"https://ieeexplore.ieee.org/document/6126543\", \"Semantic Scholar Corpus ID\": 206769852, \"Year Released\": \"2011\", \"Text Sources\": [\"movies\", \"prelinger archive\", \"undisclosed web\", \"youtube\", \"google videos\"], \"Licenses\": [{\"License\": \"CC BY 4.0\", \"License URL\": \"https://serre-lab.clps.brown.edu/resource/hmdb-a-large-human-motion-database/\"}], \"Creators\": [\"Karlsruhe Institute of Technology\", \"Massachusetts Institute of Technology\", \"Brown University\"], \"Countries\": [\"United States of America\", \"Germany\"], \"License Verified By\": \"Vivek Sharma\", \"Video Hours\": 7000.0, \"Taken Down\": \"False\", \"Video Sources\": \"Other\", \"Task Categories\": [\"Video Classification\"], \"Inferred Metadata\": {\"GitHub License\": \"\", \"GitHub Stars (June 2024)\": \"\", \"GitHub Topics\": \"\", \"Github Date\": \"\", \"HF Config\": \"\", \"HF Config License\": \"\", \"HF Dataset\": \"\", \"HF Date\": \"\", \"HF Downloads (June 2024)\": \"\", \"HF Likes (June 2024)\": \"\", \"HF Yaml License\": \"\", \"PwC Date\": \"2011-01-01\", \"PwC Description\": \"The HMDB51 dataset is a large collection of realistic videos from various sources, including movies and web videos. The dataset is composed of 6,766 video clips from 51 action categories (such as \\u201cjump\\u201d, \\u201ckiss\\u201d and \\u201claugh\\u201d), with each category containing at least 101 clips. The original evaluation scheme uses three different training/testing splits. In each split, each action class has 70 clips for training and 30 clips for testing. The average accuracy over these three splits is used to measure the final performance.\", \"PwC License Name\": \"CC BY 4.0\", \"PwC License URL\": \"https://creativecommons.org/licenses/by/4.0/\", \"S2 Citation Count (June 2024)\": \"\", \"S2 Date\": \"\"}, \"Bibtex\": \"@Article{Kuehne2011HMDBAL,\\n author = {Hilde Kuehne and Hueihan Jhuang and Est\\u00edbaliz Garrote and T. Poggio and Thomas Serre},\\n booktitle = {Vision},\\n journal = {2011 International Conference on Computer Vision},\\n pages = {2556-2563},\\n title = {HMDB: A large video database for human motion recognition},\\n year = {2011}\\n}\\n\", \"License Use (DataProvenance)\": \"commercial\", \"License Attribution (DataProvenance)\": 1, \"License Share Alike (DataProvenance)\": 0, \"License Use (DataProvenance IgnoreOpenAI)\": \"commercial\", \"License Attribution (DataProvenance IgnoreOpenAI)\": 1, \"License Share Alike (DataProvenance IgnoreOpenAI)\": 0, \"License Type\": \"Commercial\"}, {\"Unique Dataset Identifier\": \"hmdb-dataset\", \"Collection\": \"hmdb-dataset\", \"Collection URL\": \"https://ieeexplore.ieee.org/document/6126543\", \"Dataset Name\": \"HMDB: A Large Video Database for Human Motion Recognition (ICCV 2011)\", \"Paper Title\": \"HMDB: A Large Video Database for Human Motion Recognition (ICCV 2011)\", \"Paper URL\": \"https://ieeexplore.ieee.org/document/6126543\", \"GitHub URL\": \"\", \"Hugging Face URL\": \"\", \"Papers with Code URL\": \"https://paperswithcode.com/dataset/hmdb51\", \"ArXiv URL\": \"https://ieeexplore.ieee.org/document/6126543\", \"Semantic Scholar Corpus ID\": 206769852, \"Year Released\": \"2011\", \"Text Sources\": [\"movies\", \"prelinger archive\", \"undisclosed web\", \"youtube\", \"google videos\"], \"Licenses\": [{\"License\": \"CC BY 4.0\", \"License URL\": \"https://serre-lab.clps.brown.edu/resource/hmdb-a-large-human-motion-database/\"}], \"Creators\": [\"Karlsruhe Institute of Technology\", \"Massachusetts Institute of Technology\", \"Brown University\"], \"Countries\": [\"United States of America\", \"Germany\"], \"License Verified By\": \"Vivek Sharma\", \"Video Hours\": 7000.0, \"Taken Down\": \"False\", \"Video Sources\": \"movies\", \"Task Categories\": [\"Video Classification\"], \"Inferred Metadata\": {\"GitHub License\": \"\", \"GitHub Stars (June 2024)\": \"\", \"GitHub Topics\": \"\", \"Github Date\": \"\", \"HF Config\": \"\", \"HF Config License\": \"\", \"HF Dataset\": \"\", \"HF Date\": \"\", \"HF Downloads (June 2024)\": \"\", \"HF Likes (June 2024)\": \"\", \"HF Yaml License\": \"\", \"PwC Date\": \"2011-01-01\", \"PwC Description\": \"The HMDB51 dataset is a large collection of realistic videos from various sources, including movies and web videos. The dataset is composed of 6,766 video clips from 51 action categories (such as \\u201cjump\\u201d, \\u201ckiss\\u201d and \\u201claugh\\u201d), with each category containing at least 101 clips. The original evaluation scheme uses three different training/testing splits. In each split, each action class has 70 clips for training and 30 clips for testing. The average accuracy over these three splits is used to measure the final performance.\", \"PwC License Name\": \"CC BY 4.0\", \"PwC License URL\": \"https://creativecommons.org/licenses/by/4.0/\", \"S2 Citation Count (June 2024)\": \"\", \"S2 Date\": \"\"}, \"Bibtex\": \"@Article{Kuehne2011HMDBAL,\\n author = {Hilde Kuehne and Hueihan Jhuang and Est\\u00edbaliz Garrote and T. Poggio and Thomas Serre},\\n booktitle = {Vision},\\n journal = {2011 International Conference on Computer Vision},\\n pages = {2556-2563},\\n title = {HMDB: A large video database for human motion recognition},\\n year = {2011}\\n}\\n\", \"License Use (DataProvenance)\": \"commercial\", \"License Attribution (DataProvenance)\": 1, \"License Share Alike (DataProvenance)\": 0, \"License Use (DataProvenance IgnoreOpenAI)\": \"commercial\", \"License Attribution (DataProvenance IgnoreOpenAI)\": 1, \"License Share Alike (DataProvenance IgnoreOpenAI)\": 0, \"License Type\": \"Commercial\"}, {\"Unique Dataset Identifier\": \"hmdb-dataset\", \"Collection\": \"hmdb-dataset\", \"Collection URL\": \"https://ieeexplore.ieee.org/document/6126543\", \"Dataset Name\": \"HMDB: A Large Video Database for Human Motion Recognition (ICCV 2011)\", \"Paper Title\": \"HMDB: A Large Video Database for Human Motion Recognition (ICCV 2011)\", \"Paper URL\": \"https://ieeexplore.ieee.org/document/6126543\", \"GitHub URL\": \"\", \"Hugging Face URL\": \"\", \"Papers with Code URL\": \"https://paperswithcode.com/dataset/hmdb51\", \"ArXiv URL\": \"https://ieeexplore.ieee.org/document/6126543\", \"Semantic Scholar Corpus ID\": 206769852, \"Year Released\": \"2011\", \"Text Sources\": [\"movies\", \"prelinger archive\", \"undisclosed web\", \"youtube\", \"google videos\"], \"Licenses\": [{\"License\": \"CC BY 4.0\", \"License URL\": \"https://serre-lab.clps.brown.edu/resource/hmdb-a-large-human-motion-database/\"}], \"Creators\": [\"Karlsruhe Institute of Technology\", \"Massachusetts Institute of Technology\", \"Brown University\"], \"Countries\": [\"United States of America\", \"Germany\"], \"License Verified By\": \"Vivek Sharma\", \"Video Hours\": 7000.0, \"Taken Down\": \"False\", \"Video Sources\": \"Other\", \"Task Categories\": [\"Video Classification\"], \"Inferred Metadata\": {\"GitHub License\": \"\", \"GitHub Stars (June 2024)\": \"\", \"GitHub Topics\": \"\", \"Github Date\": \"\", \"HF Config\": \"\", \"HF Config License\": \"\", \"HF Dataset\": \"\", \"HF Date\": \"\", \"HF Downloads (June 2024)\": \"\", \"HF Likes (June 2024)\": \"\", \"HF Yaml License\": \"\", \"PwC Date\": \"2011-01-01\", \"PwC Description\": \"The HMDB51 dataset is a large collection of realistic videos from various sources, including movies and web videos. The dataset is composed of 6,766 video clips from 51 action categories (such as \\u201cjump\\u201d, \\u201ckiss\\u201d and \\u201claugh\\u201d), with each category containing at least 101 clips. The original evaluation scheme uses three different training/testing splits. In each split, each action class has 70 clips for training and 30 clips for testing. The average accuracy over these three splits is used to measure the final performance.\", \"PwC License Name\": \"CC BY 4.0\", \"PwC License URL\": \"https://creativecommons.org/licenses/by/4.0/\", \"S2 Citation Count (June 2024)\": \"\", \"S2 Date\": \"\"}, \"Bibtex\": \"@Article{Kuehne2011HMDBAL,\\n author = {Hilde Kuehne and Hueihan Jhuang and Est\\u00edbaliz Garrote and T. Poggio and Thomas Serre},\\n booktitle = {Vision},\\n journal = {2011 International Conference on Computer Vision},\\n pages = {2556-2563},\\n title = {HMDB: A large video database for human motion recognition},\\n year = {2011}\\n}\\n\", \"License Use (DataProvenance)\": \"commercial\", \"License Attribution (DataProvenance)\": 1, \"License Share Alike (DataProvenance)\": 0, \"License Use (DataProvenance IgnoreOpenAI)\": \"commercial\", \"License Attribution (DataProvenance IgnoreOpenAI)\": 1, \"License Share Alike (DataProvenance IgnoreOpenAI)\": 0, \"License Type\": \"Commercial\"}, {\"Unique Dataset Identifier\": \"ucf101-dataset\", \"Collection\": \"ucf101-dataset\", \"Collection URL\": \"https://arxiv.org/abs/1212.0402\", \"Dataset Name\": \"UCF101: A Dataset of 101 Human Actions Classes From Videos in The Wild\", \"Paper Title\": \"UCF101: A Dataset of 101 Human Actions Classes From Videos in The Wild\", \"Paper URL\": \"https://arxiv.org/abs/1212.0402\", \"GitHub URL\": \"\", \"Hugging Face URL\": \"\", \"Papers with Code URL\": \"https://paperswithcode.com/dataset/ucf101\", \"ArXiv URL\": \"https://arxiv.org/abs/1212.0402\", \"Semantic Scholar Corpus ID\": 7197134, \"Year Released\": \"2012\", \"Text Sources\": [\"youtube\"], \"Licenses\": [{\"License\": \"Unspecified\", \"License URL\": \"\"}], \"Creators\": [\"University of Central Florida\"], \"Countries\": [\"United States of America\"], \"License Verified By\": \"Vivek Sharma\", \"Video Hours\": 26.0, \"Taken Down\": \"False\", \"Video Sources\": \"youtube\", \"Task Categories\": [\"Video Classification\"], \"Inferred Metadata\": {\"GitHub License\": \"\", \"GitHub Stars (June 2024)\": \"\", \"GitHub Topics\": \"\", \"Github Date\": \"\", \"HF Config\": \"\", \"HF Config License\": \"\", \"HF Dataset\": \"\", \"HF Date\": \"\", \"HF Downloads (June 2024)\": \"\", \"HF Likes (June 2024)\": \"\", \"HF Yaml License\": \"\", \"PwC Date\": \"2012-12-03\", \"PwC Description\": \"UCF101 dataset is an extension of UCF50 and consists of 13,320 video clips, which are classified into 101 categories. These 101 categories can be classified into 5 types (Body motion, Human-human interactions, Human-object interactions, Playing musical instruments and Sports). The total length of these video clips is over 27 hours. All the videos are collected from YouTube and have a fixed frame rate of 25 FPS with the resolution of 320 \\u00d7 240.\", \"PwC License Name\": \"Unspecified\", \"PwC License URL\": null, \"S2 Citation Count (June 2024)\": \"\", \"S2 Date\": \"\"}, \"Bibtex\": \"@Article{Soomro2012UCF101AD,\\n author = {K. Soomro and Amir Zamir and M. Shah},\\n booktitle = {arXiv.org},\\n journal = {ArXiv},\\n title = {UCF101: A Dataset of 101 Human Actions Classes From Videos in The Wild},\\n volume = {abs/1212.0402},\\n year = {2012}\\n}\\n\", \"License Use (DataProvenance)\": \"unspecified\", \"License Attribution (DataProvenance)\": 0, \"License Share Alike (DataProvenance)\": 0, \"License Use (DataProvenance IgnoreOpenAI)\": \"unspecified\", \"License Attribution (DataProvenance IgnoreOpenAI)\": 0, \"License Share Alike (DataProvenance IgnoreOpenAI)\": 0, \"License Type\": \"Unspecified\"}, {\"Unique Dataset Identifier\": \"50salads\", \"Collection\": \"50salads\", \"Collection URL\": \"https://citeseerx.ist.psu.edu/document?repid=rep1&type=pdf&doi=b9410401cec076baef045e83953f3ff24f25d149\", \"Dataset Name\": \"50Salads\", \"Paper Title\": \"50Salads\", \"Paper URL\": \"https://citeseerx.ist.psu.edu/document?repid=rep1&type=pdf&doi=b9410401cec076baef045e83953f3ff24f25d149\", \"GitHub URL\": \"\", \"Hugging Face URL\": \"\", \"Papers with Code URL\": \"https://paperswithcode.com/dataset/50-salads\", \"ArXiv URL\": \"https://citeseerx.ist.psu.edu/document?repid=rep1&type=pdf&doi=b9410401cec076baef045e83953f3ff24f25d149\", \"Semantic Scholar Corpus ID\": 2333743, \"Year Released\": \"2013\", \"Text Sources\": [\"human\"], \"Licenses\": [{\"License\": \"CC BY-NC-SA 4.0\", \"License URL\": \"https://cvip.computing.dundee.ac.uk/datasets/foodpreparation/50salads/\"}], \"Creators\": [\"University of Dundee\"], \"Countries\": [\"United Kingdom\"], \"License Verified By\": \"Vivek Sharma\", \"Video Hours\": 40.0, \"Taken Down\": \"False\", \"Video Sources\": \"human\", \"Task Categories\": [\"Action Segmentation\", \"Action Localization\"], \"Inferred Metadata\": {\"GitHub License\": \"\", \"GitHub Stars (June 2024)\": \"\", \"GitHub Topics\": \"\", \"Github Date\": \"\", \"HF Config\": \"\", \"HF Config License\": \"\", \"HF Dataset\": \"\", \"HF Date\": \"\", \"HF Downloads (June 2024)\": \"\", \"HF Likes (June 2024)\": \"\", \"HF Yaml License\": \"\", \"PwC Date\": \"2013-09-08\", \"PwC Description\": \"Activity recognition research has shifted focus from distinguishing full-body motion patterns to recognizing complex interactions of multiple entities. Manipulative gestures \\u2013 characterized by interactions between hands, tools, and manipulable objects \\u2013 frequently occur in food preparation, manufacturing, and assembly tasks, and have a variety of applications including situational support, automated supervision, and skill assessment. With the aim to stimulate research on recognizing manipulative gestures we introduce the 50 Salads dataset. It captures 25 people preparing 2 mixed salads each and contains over 4h of annotated accelerometer and RGB-D video data. Including detailed annotations, multiple sensor types, and two sequences per participant, the 50 Salads dataset may be used for research in areas such as activity recognition, activity spotting, sequence analysis, progress tracking, sensor fusion, transfer learning, and user-adaptation.\\n\\nThe dataset includes\\n\\nRGB video data 640\\u00d7480 pixels at 30 Hz\\nDepth maps 640\\u00d7480 pixels at 30 Hz\\n3-axis accelerometer data at 50 Hz of devices attached to a knife, a mixing spoon, a small spoon, a peeler, a glass, an oil bottle, and a pepper dispenser.\\nSynchronization parameters for temporal alignment of video and accelerometer data\\nAnnotations as temporal intervals of pre- core- and post-phases of activities corresponding to steps in a recipe\", \"PwC License Name\": \"CC BY-NC-SA 4.0\", \"PwC License URL\": null, \"S2 Citation Count (June 2024)\": \"\", \"S2 Date\": \"\"}, \"Bibtex\": \"@Book{Stein2013CombiningEA,\\n author = {Sebastian Stein and S. McKenna},\\n booktitle = {Ubiquitous Computing},\\n journal = {Proceedings of the 2013 ACM international joint conference on Pervasive and ubiquitous computing},\\n title = {Combining embedded accelerometers with computer vision for recognizing food preparation activities},\\n year = {2013}\\n}\\n\", \"License Use (DataProvenance)\": \"non-commercial\", \"License Attribution (DataProvenance)\": 1, \"License Share Alike (DataProvenance)\": 1, \"License Use (DataProvenance IgnoreOpenAI)\": \"non-commercial\", \"License Attribution (DataProvenance IgnoreOpenAI)\": 1, \"License Share Alike (DataProvenance IgnoreOpenAI)\": 1, \"License Type\": \"Non-Commercial/Academic\"}, {\"Unique Dataset Identifier\": \"youcook\", \"Collection\": \"youcook\", \"Collection URL\": \"https://openaccess.thecvf.com/content_cvpr_2013/papers/Das_A_Thousand_Frames_2013_CVPR_paper.pdf\", \"Dataset Name\": \"YouCook: A Thousand Frames in Just a Few Words: Lingual Description of Videos through Latent Topics and Sparse Object Stitching (CVPR 2013)\", \"Paper Title\": \"YouCook: A Thousand Frames in Just a Few Words: Lingual Description of Videos through Latent Topics and Sparse Object Stitching (CVPR 2013)\", \"Paper URL\": \"https://openaccess.thecvf.com/content_cvpr_2013/papers/Das_A_Thousand_Frames_2013_CVPR_paper.pdf\", \"GitHub URL\": \"\", \"Hugging Face URL\": \"\", \"Papers with Code URL\": \"https://paperswithcode.com/dataset/youcook\", \"ArXiv URL\": \"https://openaccess.thecvf.com/content_cvpr_2013/papers/Das_A_Thousand_Frames_2013_CVPR_paper.pdf\", \"Semantic Scholar Corpus ID\": 12284555, \"Year Released\": \"2013\", \"Text Sources\": [\"youtube\"], \"Licenses\": [{\"License\": \"Unspecified\", \"License URL\": \"\"}], \"Creators\": [\"University at Buffalo\"], \"Countries\": [\"United States of America\"], \"License Verified By\": \"Vivek Sharma\", \"Video Hours\": 1000.0, \"Taken Down\": \"False\", \"Video Sources\": \"youtube\", \"Task Categories\": [\"Video Captioning\"], \"Inferred Metadata\": {\"GitHub License\": \"\", \"GitHub Stars (June 2024)\": \"\", \"GitHub Topics\": \"\", \"Github Date\": \"\", \"HF Config\": \"\", \"HF Config License\": \"\", \"HF Dataset\": \"\", \"HF Date\": \"\", \"HF Downloads (June 2024)\": \"\", \"HF Likes (June 2024)\": \"\", \"HF Yaml License\": \"\", \"PwC Date\": \"\", \"PwC Description\": \"This data set was prepared from 88 open-source YouTube cooking videos. The YouCook dataset contains videos of people cooking various recipes. The videos were downloaded from YouTube and are all in the third-person viewpoint; they represent a significantly more challenging visual problem than existing cooking and kitchen datasets (the background kitchen/scene is different for many and most videos have dynamic camera changes). In addition, frame-by-frame object and action annotations are provided for training data (as well as a number of precomputed low-level features). Finally, each video has a number of human provided natural language descriptions (on average, there are eight different descriptions per video). This dataset has been created to serve as a benchmark in describing complex real-world videos with natural language descriptions.\", \"PwC License Name\": \"Unspecified\", \"PwC License URL\": null, \"S2 Citation Count (June 2024)\": \"\", \"S2 Date\": \"\"}, \"Bibtex\": \"@Article{Das2013ATF,\\n author = {Pradipto Das and Chenliang Xu and Richard F. Doell and Jason J. Corso},\\n booktitle = {2013 IEEE Conference on Computer Vision and Pattern Recognition},\\n journal = {2013 IEEE Conference on Computer Vision and Pattern Recognition},\\n pages = {2634-2641},\\n title = {A Thousand Frames in Just a Few Words: Lingual Description of Videos through Latent Topics and Sparse Object Stitching},\\n year = {2013}\\n}\\n\", \"License Use (DataProvenance)\": \"unspecified\", \"License Attribution (DataProvenance)\": 0, \"License Share Alike (DataProvenance)\": 0, \"License Use (DataProvenance IgnoreOpenAI)\": \"unspecified\", \"License Attribution (DataProvenance IgnoreOpenAI)\": 0, \"License Share Alike (DataProvenance IgnoreOpenAI)\": 0, \"License Type\": \"Unspecified\"}, {\"Unique Dataset Identifier\": \"summe\", \"Collection\": \"summe\", \"Collection URL\": \"https://link.springer.com/chapter/10.1007/978-3-319-10584-0_33\", \"Dataset Name\": \"SumMe: Creating Summaries from User Videos (ECCV 2014)\", \"Paper Title\": \"SumMe: Creating Summaries from User Videos (ECCV 2014)\", \"Paper URL\": \"https://link.springer.com/chapter/10.1007/978-3-319-10584-0_33\", \"GitHub URL\": \"\", \"Hugging Face URL\": \"\", \"Papers with Code URL\": \"https://paperswithcode.com/dataset/summe\", \"ArXiv URL\": \"https://link.springer.com/chapter/10.1007/978-3-319-10584-0_33\", \"Semantic Scholar Corpus ID\": 2111093, \"Year Released\": \"2014\", \"Text Sources\": [\"crowdsourced\"], \"Licenses\": [{\"License\": \"Unspecified\", \"License URL\": \"\"}], \"Creators\": [\"ETH Z\\u00fcrich\", \"KU Leuven\", \"upicto GmbH\"], \"Countries\": [\"Belgium\", \"Switzerland\"], \"License Verified By\": \"Vivek Sharma\", \"Video Hours\": 1.11, \"Taken Down\": \"False\", \"Video Sources\": \"crowdsourced\", \"Task Categories\": [\"Video Summarization\"], \"Inferred Metadata\": {\"GitHub License\": \"\", \"GitHub Stars (June 2024)\": \"\", \"GitHub Topics\": \"\", \"Github Date\": \"\", \"HF Config\": \"\", \"HF Config License\": \"\", \"HF Dataset\": \"\", \"HF Date\": \"\", \"HF Downloads (June 2024)\": \"\", \"HF Likes (June 2024)\": \"\", \"HF Yaml License\": \"\", \"PwC Date\": \"2014-01-01\", \"PwC Description\": \"The SumMe dataset is a video summarization dataset consisting of 25 videos, each annotated with at least 15 human summaries (390 in total).\", \"PwC License Name\": \"Academic Research Purposes Only\", \"PwC License URL\": \"https://gyglim.github.io/me/vsum/index.html#benchmark\", \"S2 Citation Count (June 2024)\": \"\", \"S2 Date\": \"\"}, \"Bibtex\": \"@Article{Gygli2014CreatingSF,\\n author = {Michael Gygli and H. Grabner and Hayko Riemenschneider and L. Gool},\\n booktitle = {European Conference on Computer Vision},\\n pages = {505-520},\\n title = {Creating Summaries from User Videos},\\n year = {2014}\\n}\\n\", \"License Use (DataProvenance)\": \"unspecified\", \"License Attribution (DataProvenance)\": 0, \"License Share Alike (DataProvenance)\": 0, \"License Use (DataProvenance IgnoreOpenAI)\": \"unspecified\", \"License Attribution (DataProvenance IgnoreOpenAI)\": 0, \"License Share Alike (DataProvenance IgnoreOpenAI)\": 0, \"License Type\": \"Unspecified\"}, {\"Unique Dataset Identifier\": \"thumos-challenge\", \"Collection\": \"thumos-challenge\", \"Collection URL\": \"https://arxiv.org/pdf/1604.06182.pdf\", \"Dataset Name\": \"The THUMOS Challenge on Action Recognition for Videos in the Wild\", \"Paper Title\": \"The THUMOS Challenge on Action Recognition for Videos in the Wild\", \"Paper URL\": \"https://arxiv.org/pdf/1604.06182.pdf\", \"GitHub URL\": \"\", \"Hugging Face URL\": \"\", \"Papers with Code URL\": \"https://paperswithcode.com/dataset/thumos14-1\", \"ArXiv URL\": \"https://arxiv.org/pdf/1604.06182.pdf\", \"Semantic Scholar Corpus ID\": 14049355, \"Year Released\": \"2014\", \"Text Sources\": [\"youtube\"], \"Licenses\": [{\"License\": \"Custom\", \"License URL\": \"https://docs.google.com/forms/d/e/1FAIpQLScs9davISAtYQS7SEF5qQNu0jUpLzNH3aHmPfuqk2q1VYDkmw/viewform\"}], \"Creators\": [\"Stanford University\", \"University of Central Florida\", \"Fudan University\", \"Inria\"], \"Countries\": [\"United States of America\", \"China\"], \"License Verified By\": \"Vivek Sharma\", \"Video Hours\": 254.0, \"Taken Down\": \"False\", \"Video Sources\": \"youtube\", \"Task Categories\": [\"Video Classification\"], \"Inferred Metadata\": {\"GitHub License\": \"\", \"GitHub Stars (June 2024)\": \"\", \"GitHub Topics\": \"\", \"Github Date\": \"\", \"HF Config\": \"\", \"HF Config License\": \"\", \"HF Dataset\": \"\", \"HF Date\": \"\", \"HF Downloads (June 2024)\": \"\", \"HF Likes (June 2024)\": \"\", \"HF Yaml License\": \"\", \"PwC Date\": \"\", \"PwC Description\": \"The THUMOS14 (THUMOS 2014) dataset is a large-scale video dataset that includes 1,010 videos for validation and 1,574 videos for testing from 20 classes. Among all the videos, there are 220 and 212 videos with temporal annotations in validation and testing set, respectively.\", \"PwC License Name\": \"Unspecified\", \"PwC License URL\": null, \"S2 Citation Count (June 2024)\": \"\", \"S2 Date\": \"\"}, \"Bibtex\": \"@Misc{None,\\n author = {Haroon Idrees and Amir Zamir and Yu-Gang Jiang and Alexander N. Gorban and I. Laptev and R. Sukthankar and M. Shah},\\n title = {Computer Vision and Image Understanding}\\n}\\n\", \"License Use (DataProvenance)\": \"academic-only\", \"License Attribution (DataProvenance)\": 1, \"License Share Alike (DataProvenance)\": 0, \"License Use (DataProvenance IgnoreOpenAI)\": \"academic-only\", \"License Attribution (DataProvenance IgnoreOpenAI)\": 1, \"License Share Alike (DataProvenance IgnoreOpenAI)\": 0, \"License Type\": \"Non-Commercial/Academic\"}, {\"Unique Dataset Identifier\": \"breakfast\", \"Collection\": \"breakfast\", \"Collection URL\": \"https://openaccess.thecvf.com/content_cvpr_2014/html/Kuehne_The_Language_of_2014_CVPR_paper.html\", \"Dataset Name\": \"Breakfast\", \"Paper Title\": \"Breakfast\", \"Paper URL\": \"https://openaccess.thecvf.com/content_cvpr_2014/html/Kuehne_The_Language_of_2014_CVPR_paper.html\", \"GitHub URL\": \"\", \"Hugging Face URL\": \"\", \"Papers with Code URL\": \"https://paperswithcode.com/dataset/breakfast\", \"ArXiv URL\": \"https://openaccess.thecvf.com/content_cvpr_2014/html/Kuehne_The_Language_of_2014_CVPR_paper.html\", \"Semantic Scholar Corpus ID\": 9621856, \"Year Released\": \"2014\", \"Text Sources\": [\"human\"], \"Licenses\": [{\"License\": \"CC BY 4.0\", \"License URL\": \"https://serre-lab.clps.brown.edu/resource/breakfast-actions-dataset/\"}], \"Creators\": [\"Fraunhofer FKIE\", \"Brown University\"], \"Countries\": [\"United States of America\", \"Germany\"], \"License Verified By\": \"Vivek Sharma\", \"Video Hours\": 77.0, \"Taken Down\": \"False\", \"Video Sources\": \"human\", \"Task Categories\": [\"Video Captioning\", \"Action Segmentation\"], \"Inferred Metadata\": {\"GitHub License\": \"\", \"GitHub Stars (June 2024)\": \"\", \"GitHub Topics\": \"\", \"Github Date\": \"\", \"HF Config\": \"\", \"HF Config License\": \"\", \"HF Dataset\": \"\", \"HF Date\": \"\", \"HF Downloads (June 2024)\": \"\", \"HF Likes (June 2024)\": \"\", \"HF Yaml License\": \"\", \"PwC Date\": \"2014-01-01\", \"PwC Description\": \"The Breakfast Actions Dataset comprises of 10 actions related to breakfast preparation, performed by 52 different individuals in 18 different kitchens. The dataset is one of the largest fully annotated datasets available. The actions are recorded \\u201cin the wild\\u201d as opposed to a single controlled lab environment. It consists of over 77 hours of video recordings.\", \"PwC License Name\": \"CC BY 4.0\", \"PwC License URL\": \"https://creativecommons.org/licenses/by/4.0/\", \"S2 Citation Count (June 2024)\": \"\", \"S2 Date\": \"\"}, \"Bibtex\": \"@Article{Kuehne2014TheLO,\\n author = {Hilde Kuehne and A. B. Arslan and Thomas Serre},\\n booktitle = {2014 IEEE Conference on Computer Vision and Pattern Recognition},\\n journal = {2014 IEEE Conference on Computer Vision and Pattern Recognition},\\n pages = {780-787},\\n title = {The Language of Actions: Recovering the Syntax and Semantics of Goal-Directed Human Activities},\\n year = {2014}\\n}\\n\", \"License Use (DataProvenance)\": \"commercial\", \"License Attribution (DataProvenance)\": 1, \"License Share Alike (DataProvenance)\": 0, \"License Use (DataProvenance IgnoreOpenAI)\": \"commercial\", \"License Attribution (DataProvenance IgnoreOpenAI)\": 1, \"License Share Alike (DataProvenance IgnoreOpenAI)\": 0, \"License Type\": \"Commercial\"}, {\"Unique Dataset Identifier\": \"hollywood-extended\", \"Collection\": \"hollywood-extended\", \"Collection URL\": \"https://arxiv.org/pdf/1407.1208\", \"Dataset Name\": \"Hollywood Extended\", \"Paper Title\": \"Hollywood Extended\", \"Paper URL\": \"https://arxiv.org/pdf/1407.1208\", \"GitHub URL\": \"https://github.com/piotr-bojanowski/action-ordering\", \"Hugging Face URL\": \"\", \"Papers with Code URL\": \"https://paperswithcode.com/paper/weakly-supervised-action-labeling-in-videos\", \"ArXiv URL\": \"https://arxiv.org/pdf/1407.1208\", \"Semantic Scholar Corpus ID\": 9342651, \"Year Released\": \"2014\", \"Text Sources\": [\"movies\"], \"Licenses\": [{\"License\": \"MIT License\", \"License URL\": \"https://github.com/piotr-bojanowski/action-ordering/blob/master/LICENSE\"}], \"Creators\": [\"Inria\"], \"Countries\": [\"France\"], \"License Verified By\": \"Vivek Sharma\", \"Video Hours\": 8.75, \"Taken Down\": \"False\", \"Video Sources\": \"movies\", \"Task Categories\": [\"Temporal Action Detection\", \"Action Recognition\"], \"Inferred Metadata\": {\"GitHub License\": \"MIT License\", \"GitHub Stars (June 2024)\": 12, \"GitHub Topics\": [], \"Github Date\": \"\", \"HF Config\": \"\", \"HF Config License\": \"\", \"HF Dataset\": \"\", \"HF Date\": \"\", \"HF Downloads (June 2024)\": \"\", \"HF Likes (June 2024)\": \"\", \"HF Yaml License\": \"\", \"PwC Date\": \"\", \"PwC Description\": \"No code available yet.\", \"PwC License Name\": \"\", \"PwC License URL\": \"\", \"S2 Citation Count (June 2024)\": \"\", \"S2 Date\": \"\"}, \"Bibtex\": \"@Article{Bojanowski2014WeaklySA,\\n author = {Piotr Bojanowski and R\\u00e9mi Lajugie and F. Bach and I. Laptev and J. Ponce and C. Schmid and Josef Sivic},\\n booktitle = {European Conference on Computer Vision},\\n journal = {ArXiv},\\n title = {Weakly Supervised Action Labeling in Videos under Ordering Constraints},\\n volume = {abs/1407.1208},\\n year = {2014}\\n}\\n\", \"License Use (DataProvenance)\": \"commercial\", \"License Attribution (DataProvenance)\": 1, \"License Share Alike (DataProvenance)\": 0, \"License Use (DataProvenance IgnoreOpenAI)\": \"commercial\", \"License Attribution (DataProvenance IgnoreOpenAI)\": 1, \"License Share Alike (DataProvenance IgnoreOpenAI)\": 0, \"License Type\": \"Commercial\"}, {\"Unique Dataset Identifier\": \"stroygraphs\", \"Collection\": \"stroygraphs\", \"Collection URL\": \"https://openaccess.thecvf.com/content_cvpr_2014/papers/Tapaswi_StoryGraphs_Visualizing_Character_2014_CVPR_paper.pdf\", \"Dataset Name\": \"StoryGraphs\", \"Paper Title\": \"StoryGraphs\", \"Paper URL\": \"https://openaccess.thecvf.com/content_cvpr_2014/papers/Tapaswi_StoryGraphs_Visualizing_Character_2014_CVPR_paper.pdf\", \"GitHub URL\": \"https://github.com/makarandtapaswi/StoryGraphs_CVPR2014\", \"Hugging Face URL\": \"\", \"Papers with Code URL\": \"https://paperswithcode.com/paper/storygraphs-visualizing-character\", \"ArXiv URL\": \"https://openaccess.thecvf.com/content_cvpr_2014/papers/Tapaswi_StoryGraphs_Visualizing_Character_2014_CVPR_paper.pdf\", \"Semantic Scholar Corpus ID\": 1055956, \"Year Released\": \"2014\", \"Text Sources\": [\"movies\"], \"Licenses\": [{\"License\": \"Unspecified\", \"License URL\": \"\"}], \"Creators\": [\"Karlsruhe Institute of Technology\"], \"Countries\": [\"Germany\"], \"License Verified By\": \"Vivek Sharma\", \"Video Hours\": 7.3, \"Taken Down\": \"False\", \"Video Sources\": \"movies\", \"Task Categories\": [\"Misc\"], \"Inferred Metadata\": {\"GitHub License\": \"\", \"GitHub Stars (June 2024)\": 20, \"GitHub Topics\": [], \"Github Date\": \"\", \"HF Config\": \"\", \"HF Config License\": \"\", \"HF Dataset\": \"\", \"HF Date\": \"\", \"HF Downloads (June 2024)\": \"\", \"HF Likes (June 2024)\": \"\", \"HF Yaml License\": \"\", \"PwC Date\": \"\", \"PwC Description\": \"Implemented in one code library.\", \"PwC License Name\": \"\", \"PwC License URL\": \"\", \"S2 Citation Count (June 2024)\": \"\", \"S2 Date\": \"\"}, \"Bibtex\": \"@Article{Tapaswi2014StoryGraphsVC,\\n author = {Makarand Tapaswi and M. B\\u00e4uml and R. Stiefelhagen},\\n booktitle = {2014 IEEE Conference on Computer Vision and Pattern Recognition},\\n journal = {2014 IEEE Conference on Computer Vision and Pattern Recognition},\\n pages = {827-834},\\n title = {StoryGraphs: Visualizing Character Interactions as a Timeline},\\n year = {2014}\\n}\\n\", \"License Use (DataProvenance)\": \"unspecified\", \"License Attribution (DataProvenance)\": 0, \"License Share Alike (DataProvenance)\": 0, \"License Use (DataProvenance IgnoreOpenAI)\": \"unspecified\", \"License Attribution (DataProvenance IgnoreOpenAI)\": 0, \"License Share Alike (DataProvenance IgnoreOpenAI)\": 0, \"License Type\": \"Unspecified\"}, {\"Unique Dataset Identifier\": \"sports1M-dataset\", \"Collection\": \"sports1M-dataset\", \"Collection URL\": \"https://static.googleusercontent.com/media/research.google.com/en//pubs/archive/42455.pdf\", \"Dataset Name\": \"Sports-1M: Large-scale Video Classification with Convolutional Neural Networks\", \"Paper Title\": \"Sports-1M: Large-scale Video Classification with Convolutional Neural Networks\", \"Paper URL\": \"https://static.googleusercontent.com/media/research.google.com/en//pubs/archive/42455.pdf\", \"GitHub URL\": \"https://github.com/gtoderici/sports-1m-dataset\", \"Hugging Face URL\": \"\", \"Papers with Code URL\": \"https://paperswithcode.com/paper/large-scale-video-classification-with-1\", \"ArXiv URL\": \"https://static.googleusercontent.com/media/research.google.com/en//pubs/archive/42455.pdf\", \"Semantic Scholar Corpus ID\": 206592218, \"Year Released\": \"2014\", \"Text Sources\": [\"youtube\"], \"Licenses\": [{\"License\": \"CC BY 3.0\", \"License URL\": \"https://github.com/gtoderici/sports-1m-dataset\"}], \"Creators\": [\"Stanford University\"], \"Countries\": [\"United States of America\"], \"License Verified By\": \"Vivek Sharma\", \"Video Hours\": 105761.41, \"Taken Down\": \"False\", \"Video Sources\": \"youtube\", \"Task Categories\": [\"Video Classification\"], \"Inferred Metadata\": {\"GitHub License\": \"\", \"GitHub Stars (June 2024)\": 273, \"GitHub Topics\": [], \"Github Date\": \"\", \"HF Config\": \"\", \"HF Config License\": \"\", \"HF Dataset\": \"\", \"HF Date\": \"\", \"HF Downloads (June 2024)\": \"\", \"HF Likes (June 2024)\": \"\", \"HF Yaml License\": \"\", \"PwC Date\": \"\", \"PwC Description\": \"#9 best model for Action Recognition on Sports-1M (Video hit@1  metric)\", \"PwC License Name\": \"\", \"PwC License URL\": \"\", \"S2 Citation Count (June 2024)\": \"\", \"S2 Date\": \"\"}, \"Bibtex\": \"@Article{Karpathy2014LargeScaleVC,\\n author = {A. Karpathy and G. Toderici and Sanketh Shetty and Thomas Leung and R. Sukthankar and Li Fei-Fei},\\n booktitle = {2014 IEEE Conference on Computer Vision and Pattern Recognition},\\n journal = {2014 IEEE Conference on Computer Vision and Pattern Recognition},\\n pages = {1725-1732},\\n title = {Large-Scale Video Classification with Convolutional Neural Networks},\\n year = {2014}\\n}\\n\", \"License Use (DataProvenance)\": \"commercial\", \"License Attribution (DataProvenance)\": 1, \"License Share Alike (DataProvenance)\": 0, \"License Use (DataProvenance IgnoreOpenAI)\": \"commercial\", \"License Attribution (DataProvenance IgnoreOpenAI)\": 1, \"License Share Alike (DataProvenance IgnoreOpenAI)\": 0, \"License Type\": \"Commercial\"}, {\"Unique Dataset Identifier\": \"videostory\", \"Collection\": \"videostory\", \"Collection URL\": \"https://isis-data.science.uva.nl/cgmsnoek/pub/habibian-videostory-mm2014.pdf\", \"Dataset Name\": \"VideoStory: A New Multimedia Embedding for Few-Example Recognition and Translation of Events\", \"Paper Title\": \"VideoStory: A New Multimedia Embedding for Few-Example Recognition and Translation of Events\", \"Paper URL\": \"https://isis-data.science.uva.nl/cgmsnoek/pub/habibian-videostory-mm2014.pdf\", \"GitHub URL\": \"\", \"Hugging Face URL\": \"\", \"Papers with Code URL\": \"\", \"ArXiv URL\": \"https://isis-data.science.uva.nl/cgmsnoek/pub/habibian-videostory-mm2014.pdf\", \"Semantic Scholar Corpus ID\": 28203, \"Year Released\": \"2014\", \"Text Sources\": [\"youtube\"], \"Licenses\": [{\"License\": \"Unspecified\", \"License URL\": \"\"}], \"Creators\": [\"University of Amsterdam\"], \"Countries\": [\"Netherlands\"], \"License Verified By\": \"Vivek Sharma\", \"Video Hours\": 743.0, \"Taken Down\": \"False\", \"Video Sources\": \"youtube\", \"Task Categories\": [\"Video Captioning\"], \"Inferred Metadata\": {\"GitHub License\": \"\", \"GitHub Stars (June 2024)\": \"\", \"GitHub Topics\": \"\", \"Github Date\": \"\", \"HF Config\": \"\", \"HF Config License\": \"\", \"HF Dataset\": \"\", \"HF Date\": \"\", \"HF Downloads (June 2024)\": \"\", \"HF Likes (June 2024)\": \"\", \"HF Yaml License\": \"\", \"PwC Date\": \"\", \"PwC Description\": \"\", \"PwC License Name\": \"\", \"PwC License URL\": \"\", \"S2 Citation Count (June 2024)\": \"\", \"S2 Date\": \"\"}, \"Bibtex\": \"@Book{Habibian2014VideoStoryAN,\\n author = {A. Habibian and Thomas Mensink and Cees G. M. Snoek},\\n booktitle = {ACM Multimedia},\\n journal = {Proceedings of the 22nd ACM international conference on Multimedia},\\n title = {VideoStory: A New Multimedia Embedding for Few-Example Recognition and Translation of Events},\\n year = {2014}\\n}\\n\", \"License Use (DataProvenance)\": \"unspecified\", \"License Attribution (DataProvenance)\": 0, \"License Share Alike (DataProvenance)\": 0, \"License Use (DataProvenance IgnoreOpenAI)\": \"unspecified\", \"License Attribution (DataProvenance IgnoreOpenAI)\": 0, \"License Share Alike (DataProvenance IgnoreOpenAI)\": 0, \"License Type\": \"Unspecified\"}, {\"Unique Dataset Identifier\": \"tvsum\", \"Collection\": \"tvsum\", \"Collection URL\": \"https://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Song_TVSum_Summarizing_Web_2015_CVPR_paper.pdf\", \"Dataset Name\": \"TVSum: Summarizing web videos using titles (CVPR 2015)\", \"Paper Title\": \"TVSum: Summarizing web videos using titles (CVPR 2015)\", \"Paper URL\": \"https://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Song_TVSum_Summarizing_Web_2015_CVPR_paper.pdf\", \"GitHub URL\": \"https://github.com/yalesong/tvsum\", \"Hugging Face URL\": \"\", \"Papers with Code URL\": \"https://paperswithcode.com/dataset/tvsum-1\", \"ArXiv URL\": \"https://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Song_TVSum_Summarizing_Web_2015_CVPR_paper.pdf\", \"Semantic Scholar Corpus ID\": 7675635, \"Year Released\": \"2015\", \"Text Sources\": [\"youtube\"], \"Licenses\": [{\"License\": \"CC BY 3.0\", \"License URL\": \"https://github.com/yalesong/tvsum\"}], \"Creators\": [\"Yahoo Labs\"], \"Countries\": [\"United States of America\"], \"License Verified By\": \"Vivek Sharma\", \"Video Hours\": 3.5, \"Taken Down\": \"False\", \"Video Sources\": \"youtube\", \"Task Categories\": [\"Video Summarization\"], \"Inferred Metadata\": {\"GitHub License\": \"\", \"GitHub Stars (June 2024)\": 116, \"GitHub Topics\": [], \"Github Date\": \"\", \"HF Config\": \"\", \"HF Config License\": \"\", \"HF Dataset\": \"\", \"HF Date\": \"\", \"HF Downloads (June 2024)\": \"\", \"HF Likes (June 2024)\": \"\", \"HF Yaml License\": \"\", \"PwC Date\": \"2015-06-01\", \"PwC Description\": \"Title-based Video Summarization (TVSum) dataset serves as a benchmark to validate video summarization techniques. It contains 50 videos of various genres (e.g., news, how-to, documentary, vlog, egocentric) and 1,000 annotations of shot-level importance scores obtained via crowdsourcing (20 per video).\", \"PwC License Name\": \"Unspecified\", \"PwC License URL\": null, \"S2 Citation Count (June 2024)\": \"\", \"S2 Date\": \"\"}, \"Bibtex\": \"@Article{Song2015TVSumSW,\\n author = {Yale Song and Jordi Vallmitjana and Amanda Stent and A. Jaimes},\\n booktitle = {Computer Vision and Pattern Recognition},\\n journal = {2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},\\n pages = {5179-5187},\\n title = {TVSum: Summarizing web videos using titles},\\n year = {2015}\\n}\\n\", \"License Use (DataProvenance)\": \"commercial\", \"License Attribution (DataProvenance)\": 1, \"License Share Alike (DataProvenance)\": 0, \"License Use (DataProvenance IgnoreOpenAI)\": \"commercial\", \"License Attribution (DataProvenance IgnoreOpenAI)\": 1, \"License Share Alike (DataProvenance IgnoreOpenAI)\": 0, \"License Type\": \"Commercial\"}, {\"Unique Dataset Identifier\": \"mpii-cooking\", \"Collection\": \"mpii-cooking\", \"Collection URL\": \"https://arxiv.org/abs/1502.06648\", \"Dataset Name\": \"MPII-Cooking: Recognizing Fine-Grained and Composite Activities Using Hand-Centric Features and Script Data (IJCV 2015)\", \"Paper Title\": \"MPII-Cooking: Recognizing Fine-Grained and Composite Activities Using Hand-Centric Features and Script Data (IJCV 2015)\", \"Paper URL\": \"https://arxiv.org/abs/1502.06648\", \"GitHub URL\": \"\", \"Hugging Face URL\": \"\", \"Papers with Code URL\": \"https://paperswithcode.com/paper/recognizing-fine-grained-and-composite\", \"ArXiv URL\": \"https://arxiv.org/abs/1502.06648\", \"Semantic Scholar Corpus ID\": 14036544, \"Year Released\": \"2015\", \"Text Sources\": [\"human\"], \"Licenses\": [{\"License\": \"Custom\", \"License URL\": \"https://www.mpi-inf.mpg.de/departments/computer-vision-and-machine-learning/research/human-activity-recognition/mpii-cooking-activities-dataset\"}], \"Creators\": [\"Max Planck Institute for Informatics\"], \"Countries\": [\"Germany\"], \"License Verified By\": \"Vivek Sharma\", \"Video Hours\": 27.0, \"Taken Down\": \"False\", \"Video Sources\": \"human\", \"Task Categories\": [\"Video Classification\"], \"Inferred Metadata\": {\"GitHub License\": \"\", \"GitHub Stars (June 2024)\": \"\", \"GitHub Topics\": \"\", \"Github Date\": \"\", \"HF Config\": \"\", \"HF Config License\": \"\", \"HF Dataset\": \"\", \"HF Date\": \"\", \"HF Downloads (June 2024)\": \"\", \"HF Likes (June 2024)\": \"\", \"HF Yaml License\": \"\", \"PwC Date\": \"\", \"PwC Description\": \"No code available yet.\", \"PwC License Name\": \"\", \"PwC License URL\": \"\", \"S2 Citation Count (June 2024)\": \"\", \"S2 Date\": \"\"}, \"Bibtex\": \"@Article{Rohrbach2015RecognizingFA,\\n author = {Marcus Rohrbach and Anna Rohrbach and Michaela Regneri and S. Amin and Mykhaylo Andriluka and Manfred Pinkal and B. Schiele},\\n booktitle = {International Journal of Computer Vision},\\n journal = {International Journal of Computer Vision},\\n pages = {346 - 373},\\n title = {Recognizing Fine-Grained and Composite Activities Using Hand-Centric Features and Script Data},\\n volume = {119},\\n year = {2015}\\n}\\n\", \"License Use (DataProvenance)\": \"academic-only\", \"License Attribution (DataProvenance)\": 1, \"License Share Alike (DataProvenance)\": 0, \"License Use (DataProvenance IgnoreOpenAI)\": \"academic-only\", \"License Attribution (DataProvenance IgnoreOpenAI)\": 1, \"License Share Alike (DataProvenance IgnoreOpenAI)\": 0, \"License Type\": \"Non-Commercial/Academic\"}, {\"Unique Dataset Identifier\": \"activitynet\", \"Collection\": \"activitynet\", \"Collection URL\": \"https://ieeexplore.ieee.org/document/7298698\", \"Dataset Name\": \"ActivityNet\", \"Paper Title\": \"ActivityNet\", \"Paper URL\": \"https://ieeexplore.ieee.org/document/7298698\", \"GitHub URL\": \"\", \"Hugging Face URL\": \"https://huggingface.co/datasets/Leyo/ActivityNet_Captions\", \"Papers with Code URL\": \"https://paperswithcode.com/dataset/activitynet\", \"ArXiv URL\": \"https://ieeexplore.ieee.org/document/7298698\", \"Semantic Scholar Corpus ID\": 1710722, \"Year Released\": \"2015\", \"Text Sources\": [\"undisclosed web\"], \"Licenses\": [{\"License\": \"MIT License\", \"License URL\": \"https://github.com/facebookresearch/ActivityNet-Entities/blob/main/LICENSE\"}], \"Creators\": [\"Universidad del Norte\", \"King Abdullah University of Science and Technology (KAUST)\"], \"Countries\": [\"Saudi Arabia\", \"Colombia\"], \"License Verified By\": \"Vivek Sharma\", \"Video Hours\": 849.0, \"Taken Down\": \"False\", \"Video Sources\": \"undisclosed web\", \"Task Categories\": [\"Video Captioning\"], \"Inferred Metadata\": {\"GitHub License\": \"\", \"GitHub Stars (June 2024)\": \"\", \"GitHub Topics\": \"\", \"Github Date\": \"\", \"HF Config\": \"all\", \"HF Config License\": \"\", \"HF Dataset\": \"Leyo/ActivityNet_Captions\", \"HF Date\": \"2022-05-13\", \"HF Downloads (June 2024)\": 21, \"HF Likes (June 2024)\": 0, \"HF Yaml License\": \"Unspecified\", \"PwC Date\": \"2015-01-01\", \"PwC Description\": \"The ActivityNet dataset contains 200 different types of activities and a total of 849 hours of videos collected from YouTube. ActivityNet is the largest benchmark for temporal activity detection to date in terms of both the number of activity categories and number of videos, making the task particularly challenging. Version 1.3 of the dataset contains 19994 untrimmed videos in total and is divided into three disjoint subsets, training, validation, and testing by a ratio of 2:1:1. On average, each activity category has 137 untrimmed videos. Each video on average has 1.41 activities which are annotated with temporal boundaries. The ground-truth annotations of test videos are not public.\", \"PwC License Name\": \"Unspecified\", \"PwC License URL\": null, \"S2 Citation Count (June 2024)\": \"\", \"S2 Date\": \"\"}, \"Bibtex\": \"@Article{Heilbron2015ActivityNetAL,\\n author = {Fabian Caba Heilbron and Victor Escorcia and Bernard Ghanem and Juan Carlos Niebles},\\n booktitle = {Computer Vision and Pattern Recognition},\\n journal = {2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},\\n pages = {961-970},\\n title = {ActivityNet: A large-scale video benchmark for human activity understanding},\\n year = {2015}\\n}\\n\", \"License Use (DataProvenance)\": \"commercial\", \"License Attribution (DataProvenance)\": 1, \"License Share Alike (DataProvenance)\": 0, \"License Use (DataProvenance IgnoreOpenAI)\": \"commercial\", \"License Attribution (DataProvenance IgnoreOpenAI)\": 1, \"License Share Alike (DataProvenance IgnoreOpenAI)\": 0, \"License Type\": \"Commercial\"}, {\"Unique Dataset Identifier\": \"movieqa\", \"Collection\": \"movieqa\", \"Collection URL\": \"https://arxiv.org/abs/1512.02902\", \"Dataset Name\": \"MovieQA\", \"Paper Title\": \"MovieQA\", \"Paper URL\": \"https://arxiv.org/abs/1512.02902\", \"GitHub URL\": \"https://github.com/makarandtapaswi/MovieQA_CVPR2016/\", \"Hugging Face URL\": \"\", \"Papers with Code URL\": \"https://paperswithcode.com/dataset/movieqa\", \"ArXiv URL\": \"https://arxiv.org/abs/1512.02902\", \"Semantic Scholar Corpus ID\": 1017389, \"Year Released\": \"2015\", \"Text Sources\": [\"movies\"], \"Licenses\": [{\"License\": \"Unspecified\", \"License URL\": \"\"}], \"Creators\": [\"Karlsruhe Institute of Technology\", \"Massachusetts Institute of Technology\", \"University of Toronto\"], \"Countries\": [\"Germany\", \"Canada\", \"United States of America\"], \"License Verified By\": \"Vivek Sharma\", \"Video Hours\": 381.0, \"Taken Down\": \"True\", \"Video Sources\": \"movies\", \"Task Categories\": [\"Video Question Answering\"], \"Inferred Metadata\": {\"GitHub License\": \"\", \"GitHub Stars (June 2024)\": 80, \"GitHub Topics\": [], \"Github Date\": \"\", \"HF Config\": \"\", \"HF Config License\": \"\", \"HF Dataset\": \"\", \"HF Date\": \"\", \"HF Downloads (June 2024)\": \"\", \"HF Likes (June 2024)\": \"\", \"HF Yaml License\": \"\", \"PwC Date\": \"2016-01-01\", \"PwC Description\": \"The MovieQA dataset is a dataset for movie question answering. to evaluate automatic story comprehension from both video and text. The data set consists of almost 15,000 multiple choice question answers obtained from over 400 movies and features high semantic diversity. Each question comes with a set of five highly plausible answers; only one of which is correct. The questions can be answered using multiple sources of information: movie clips, plots, subtitles, and for a subset scripts and DVS.\", \"PwC License Name\": \"Unspecified\", \"PwC License URL\": null, \"S2 Citation Count (June 2024)\": \"\", \"S2 Date\": \"\"}, \"Bibtex\": \"@Article{Tapaswi2015MovieQAUS,\\n author = {Makarand Tapaswi and Yukun Zhu and R. Stiefelhagen and A. Torralba and R. Urtasun and S. Fidler},\\n booktitle = {Computer Vision and Pattern Recognition},\\n journal = {2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},\\n pages = {4631-4640},\\n title = {MovieQA: Understanding Stories in Movies through Question-Answering},\\n year = {2015}\\n}\\n\", \"License Use (DataProvenance)\": \"unspecified\", \"License Attribution (DataProvenance)\": 0, \"License Share Alike (DataProvenance)\": 0, \"License Use (DataProvenance IgnoreOpenAI)\": \"unspecified\", \"License Attribution (DataProvenance IgnoreOpenAI)\": 0, \"License Share Alike (DataProvenance IgnoreOpenAI)\": 0, \"License Type\": \"Unspecified\"}, {\"Unique Dataset Identifier\": \"volleyball-vids\", \"Collection\": \"volleyball-vids\", \"Collection URL\": \"https://arxiv.org/abs/1511.06040\", \"Dataset Name\": \"Volleyball: A Hierarchical Deep Temporal Model for Group Activity Recognition\", \"Paper Title\": \"Volleyball: A Hierarchical Deep Temporal Model for Group Activity Recognition\", \"Paper URL\": \"https://arxiv.org/abs/1511.06040\", \"GitHub URL\": \"https://github.com/mostafa-saad/deep-activity-rec#dataset\", \"Hugging Face URL\": \"\", \"Papers with Code URL\": \"https://paperswithcode.com/dataset/volleyball\", \"ArXiv URL\": \"https://arxiv.org/abs/1511.06040\", \"Semantic Scholar Corpus ID\": 8483403, \"Year Released\": \"2015\", \"Text Sources\": [\"youtube\"], \"Licenses\": [{\"License\": \"Unspecified\", \"License URL\": \"\"}], \"Creators\": [\"Simon Fraser University\"], \"Countries\": [\"Canada\"], \"License Verified By\": \"Vivek Sharma\", \"Video Hours\": 0.1, \"Taken Down\": \"False\", \"Video Sources\": \"youtube\", \"Task Categories\": [\"Group Activity Recognition\"], \"Inferred Metadata\": {\"GitHub License\": \"BSD 3-Clause License\", \"GitHub Stars (June 2024)\": 171, \"GitHub Topics\": [], \"Github Date\": \"\", \"HF Config\": \"\", \"HF Config License\": \"\", \"HF Dataset\": \"\", \"HF Date\": \"\", \"HF Downloads (June 2024)\": \"\", \"HF Likes (June 2024)\": \"\", \"HF Yaml License\": \"\", \"PwC Date\": \"2016-01-01\", \"PwC Description\": \"Volleyball is a video action recognition dataset. It has 4830 annotated frames that were handpicked from 55 videos with 9 player action labels and 8 team activity labels. It contains group activity annotations as well as individual activity annotations.\", \"PwC License Name\": \"Unspecified\", \"PwC License URL\": null, \"S2 Citation Count (June 2024)\": \"\", \"S2 Date\": \"\"}, \"Bibtex\": \"@Article{Ibrahim2016HierarchicalDT,\\n author = {Mostafa S. Ibrahim and S. Muralidharan and Zhiwei Deng and Arash Vahdat and Greg Mori},\\n booktitle = {arXiv.org},\\n journal = {ArXiv},\\n title = {Hierarchical Deep Temporal Models for Group Activity Recognition},\\n volume = {abs/1607.02643},\\n year = {2016}\\n}\\n\", \"License Use (DataProvenance)\": \"unspecified\", \"License Attribution (DataProvenance)\": 0, \"License Share Alike (DataProvenance)\": 0, \"License Use (DataProvenance IgnoreOpenAI)\": \"unspecified\", \"License Attribution (DataProvenance IgnoreOpenAI)\": 0, \"License Share Alike (DataProvenance IgnoreOpenAI)\": 0, \"License Type\": \"Unspecified\"}, {\"Unique Dataset Identifier\": \"msr-vtt\", \"Collection\": \"msr-vtt\", \"Collection URL\": \"https://ieeexplore.ieee.org/document/7780940\", \"Dataset Name\": \"MSR-VTT\", \"Paper Title\": \"MSR-VTT\", \"Paper URL\": \"https://ieeexplore.ieee.org/document/7780940\", \"GitHub URL\": \"\", \"Hugging Face URL\": \"\", \"Papers with Code URL\": \"https://paperswithcode.com/dataset/msr-vtt\", \"ArXiv URL\": \"https://ieeexplore.ieee.org/document/7780940\", \"Semantic Scholar Corpus ID\": 206594535, \"Year Released\": \"2016\", \"Text Sources\": [\"undisclosed web\"], \"Licenses\": [{\"License\": \"Unspecified\", \"License URL\": \"\"}], \"Creators\": [\"Microsoft Research\"], \"Countries\": [\"United States of America\"], \"License Verified By\": \"Vivek Sharma\", \"Video Hours\": 41.2, \"Taken Down\": \"False\", \"Video Sources\": \"undisclosed web\", \"Task Categories\": [\"Video Captioning\"], \"Inferred Metadata\": {\"GitHub License\": \"\", \"GitHub Stars (June 2024)\": \"\", \"GitHub Topics\": \"\", \"Github Date\": \"\", \"HF Config\": \"\", \"HF Config License\": \"\", \"HF Dataset\": \"\", \"HF Date\": \"\", \"HF Downloads (June 2024)\": \"\", \"HF Likes (June 2024)\": \"\", \"HF Yaml License\": \"\", \"PwC Date\": \"2016-01-01\", \"PwC Description\": \"MSR-VTT (Microsoft Research Video to Text) is a large-scale dataset for the open domain video captioning, which consists of 10,000 video clips from 20 categories, and each video clip is annotated with 20 English sentences by Amazon Mechanical Turks. There are about 29,000 unique words in all captions. The standard splits uses 6,513 clips for training, 497 clips for validation, and 2,990 clips for testing.\", \"PwC License Name\": \"Unspecified\", \"PwC License URL\": null, \"S2 Citation Count (June 2024)\": \"\", \"S2 Date\": \"\"}, \"Bibtex\": \"@Article{Xu2016MSRVTTAL,\\n author = {Jun Xu and Tao Mei and Ting Yao and Y. Rui},\\n booktitle = {Computer Vision and Pattern Recognition},\\n journal = {2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},\\n pages = {5288-5296},\\n title = {MSR-VTT: A Large Video Description Dataset for Bridging Video and Language},\\n year = {2016}\\n}\\n\", \"License Use (DataProvenance)\": \"unspecified\", \"License Attribution (DataProvenance)\": 0, \"License Share Alike (DataProvenance)\": 0, \"License Use (DataProvenance IgnoreOpenAI)\": \"unspecified\", \"License Attribution (DataProvenance IgnoreOpenAI)\": 0, \"License Share Alike (DataProvenance IgnoreOpenAI)\": 0, \"License Type\": \"Unspecified\"}, {\"Unique Dataset Identifier\": \"vtw\", \"Collection\": \"vtw\", \"Collection URL\": \"https://arxiv.org/abs/1608.07068\", \"Dataset Name\": \"VTW\", \"Paper Title\": \"VTW\", \"Paper URL\": \"https://arxiv.org/abs/1608.07068\", \"GitHub URL\": \"\", \"Hugging Face URL\": \"\", \"Papers with Code URL\": \"https://paperswithcode.com/paper/title-generation-for-user-generated-videos\", \"ArXiv URL\": \"https://arxiv.org/abs/1608.07068\", \"Semantic Scholar Corpus ID\": 6155397, \"Year Released\": \"2016\", \"Text Sources\": [\"undisclosed web\"], \"Licenses\": [{\"License\": \"Unspecified\", \"License URL\": \"\"}], \"Creators\": [\"Tsinghua University\", \"Stanford University\"], \"Countries\": [\"China\", \"United States of America\"], \"License Verified By\": \"Vivek Sharma\", \"Video Hours\": 213.2, \"Taken Down\": \"False\", \"Video Sources\": \"undisclosed web\", \"Task Categories\": [\"Video Captioning\"], \"Inferred Metadata\": {\"GitHub License\": \"\", \"GitHub Stars (June 2024)\": \"\", \"GitHub Topics\": \"\", \"Github Date\": \"\", \"HF Config\": \"\", \"HF Config License\": \"\", \"HF Dataset\": \"\", \"HF Date\": \"\", \"HF Downloads (June 2024)\": \"\", \"HF Likes (June 2024)\": \"\", \"HF Yaml License\": \"\", \"PwC Date\": \"\", \"PwC Description\": \"No code available yet.\", \"PwC License Name\": \"\", \"PwC License URL\": \"\", \"S2 Citation Count (June 2024)\": \"\", \"S2 Date\": \"\"}, \"Bibtex\": \"@Article{Zeng2016TitleGF,\\n author = {Kuo-Hao Zeng and Tseng-Hung Chen and Juan Carlos Niebles and Min Sun},\\n booktitle = {European Conference on Computer Vision},\\n pages = {609-625},\\n title = {Title Generation for User Generated Videos},\\n year = {2016}\\n}\\n\", \"License Use (DataProvenance)\": \"unspecified\", \"License Attribution (DataProvenance)\": 0, \"License Share Alike (DataProvenance)\": 0, \"License Use (DataProvenance IgnoreOpenAI)\": \"unspecified\", \"License Attribution (DataProvenance IgnoreOpenAI)\": 0, \"License Share Alike (DataProvenance IgnoreOpenAI)\": 0, \"License Type\": \"Unspecified\"}, {\"Unique Dataset Identifier\": \"narrated-instruction-vids\", \"Collection\": \"narrated-instruction-vids\", \"Collection URL\": \"https://arxiv.org/abs/1506.09215\", \"Dataset Name\": \"Narrated Instruction Videos\", \"Paper Title\": \"Narrated Instruction Videos\", \"Paper URL\": \"https://arxiv.org/abs/1506.09215\", \"GitHub URL\": \"https://github.com/jalayrac/instructionVideos\", \"Hugging Face URL\": \"\", \"Papers with Code URL\": \"https://paperswithcode.com/dataset/youtube-inria-instructional\", \"ArXiv URL\": \"https://arxiv.org/abs/1506.09215\", \"Semantic Scholar Corpus ID\": 2617244, \"Year Released\": \"2016\", \"Text Sources\": [\"youtube\"], \"Licenses\": [{\"License\": \"MIT License\", \"License URL\": \"https://github.com/jalayrac/instructionVideos?tab=readme-ov-file#license\"}], \"Creators\": [\"CNRS\", \"Ecole Normale Sup\\u00e9rieure\", \"Inria\", \"International Institute of Information Technology - Hyderabad\"], \"Countries\": [\"United States of America\", \"India\"], \"License Verified By\": \"Vivek Sharma\", \"Video Hours\": 7.0, \"Taken Down\": \"False\", \"Video Sources\": \"youtube\", \"Task Categories\": [\"Video Captioning\"], \"Inferred Metadata\": {\"GitHub License\": \"MIT License\", \"GitHub Stars (June 2024)\": 19, \"GitHub Topics\": [], \"Github Date\": \"\", \"HF Config\": \"\", \"HF Config License\": \"\", \"HF Dataset\": \"\", \"HF Date\": \"\", \"HF Downloads (June 2024)\": \"\", \"HF Likes (June 2024)\": \"\", \"HF Yaml License\": \"\", \"PwC Date\": \"\", \"PwC Description\": \"We address the problem of automatically learning the main steps to complete a certain task, such as changing a car tire, from a set of narrated instruction videos. The contributions of this paper are three-fold. First, we develop a new unsupervised learning approach that takes advantage of the complementary nature of the input video and the associated narration. The method solves two clustering problems, one in text and one in video, applied one after each other and linked by joint constraints to obtain a single coherent sequence of steps in both modalities. Second, we collect and annotate a new challenging dataset of real-world instruction videos from the Internet. The dataset contains about 800,000 frames for five different tasks (How to : change a car tire, perform CardioPulmonary resuscitation (CPR), jump cars, repot a plant and make coffee) that include complex interactions between people and objects, and are captured in a variety of indoor and outdoor settings. Third, we experimentally demonstrate that the proposed method can automatically discover, in an unsupervised manner , the main steps to achieve the task and locate the steps in the input videos.\\n\\nThis video presents our results of automatically discovering the scenario for the two following task : changing a tire and performing CardioPulmonary Resuscitation (CPR). At the bottom of the videos, there are three bars. The first one corresponds to our ground truth annotation. The second one corresponds to our time interval prediction in video. Finally the third one corresponds to the constraints that we obtain from the text domain. On the right, there is a list of label. They corresponds to the label recovered by our NLP method in an unsupervised manner.\", \"PwC License Name\": \"Unspecified\", \"PwC License URL\": null, \"S2 Citation Count (June 2024)\": \"\", \"S2 Date\": \"\"}, \"Bibtex\": \"@Article{Alayrac2015UnsupervisedLF,\\n author = {Jean-Baptiste Alayrac and Piotr Bojanowski and Nishant Agrawal and Josef Sivic and I. Laptev and Simon Lacoste-Julien},\\n booktitle = {Computer Vision and Pattern Recognition},\\n journal = {2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},\\n pages = {4575-4583},\\n title = {Unsupervised Learning from Narrated Instruction Videos},\\n year = {2015}\\n}\\n\", \"License Use (DataProvenance)\": \"commercial\", \"License Attribution (DataProvenance)\": 1, \"License Share Alike (DataProvenance)\": 0, \"License Use (DataProvenance IgnoreOpenAI)\": \"commercial\", \"License Attribution (DataProvenance IgnoreOpenAI)\": 1, \"License Share Alike (DataProvenance IgnoreOpenAI)\": 0, \"License Type\": \"Commercial\"}, {\"Unique Dataset Identifier\": \"mpii-cooking2\", \"Collection\": \"mpii-cooking2\", \"Collection URL\": \"https://arxiv.org/pdf/1502.06648.pdf\", \"Dataset Name\": \"MPII Cooking 2\", \"Paper Title\": \"MPII Cooking 2\", \"Paper URL\": \"https://arxiv.org/pdf/1502.06648.pdf\", \"GitHub URL\": \"\", \"Hugging Face URL\": \"\", \"Papers with Code URL\": \"https://paperswithcode.com/dataset/mpii-cooking-2-dataset\", \"ArXiv URL\": \"https://arxiv.org/pdf/1502.06648.pdf\", \"Semantic Scholar Corpus ID\": 14036544, \"Year Released\": \"2016\", \"Text Sources\": [\"human\"], \"Licenses\": [{\"License\": \"Custom\", \"License URL\": \"https://www.mpi-inf.mpg.de/departments/computer-vision-and-machine-learning/research/human-activity-recognition/mpii-cooking-2-dataset/\"}], \"Creators\": [\"Max Planck Institute for Informatics\", \"Saarland University\"], \"Countries\": [\"Germany\"], \"License Verified By\": \"Vivek Sharma\", \"Video Hours\": 27.0, \"Taken Down\": \"False\", \"Video Sources\": \"human\", \"Task Categories\": [\"Temporal Action Detection\"], \"Inferred Metadata\": {\"GitHub License\": \"\", \"GitHub Stars (June 2024)\": \"\", \"GitHub Topics\": \"\", \"Github Date\": \"\", \"HF Config\": \"\", \"HF Config License\": \"\", \"HF Dataset\": \"\", \"HF Date\": \"\", \"HF Downloads (June 2024)\": \"\", \"HF Likes (June 2024)\": \"\", \"HF Yaml License\": \"\", \"PwC Date\": \"\", \"PwC Description\": \"A dataset which provides detailed annotations for activity recognition.\", \"PwC License Name\": \"Unspecified\", \"PwC License URL\": null, \"S2 Citation Count (June 2024)\": \"\", \"S2 Date\": \"\"}, \"Bibtex\": \"@Article{Rohrbach2015RecognizingFA,\\n author = {Marcus Rohrbach and Anna Rohrbach and Michaela Regneri and S. Amin and Mykhaylo Andriluka and Manfred Pinkal and B. Schiele},\\n booktitle = {International Journal of Computer Vision},\\n journal = {International Journal of Computer Vision},\\n pages = {346 - 373},\\n title = {Recognizing Fine-Grained and Composite Activities Using Hand-Centric Features and Script Data},\\n volume = {119},\\n year = {2015}\\n}\\n\", \"License Use (DataProvenance)\": \"academic-only\", \"License Attribution (DataProvenance)\": 1, \"License Share Alike (DataProvenance)\": 0, \"License Use (DataProvenance IgnoreOpenAI)\": \"academic-only\", \"License Attribution (DataProvenance IgnoreOpenAI)\": 1, \"License Share Alike (DataProvenance IgnoreOpenAI)\": 0, \"License Type\": \"Non-Commercial/Academic\"}, {\"Unique Dataset Identifier\": \"tgif\", \"Collection\": \"tgif\", \"Collection URL\": \"https://arxiv.org/abs/1604.02748\", \"Dataset Name\": \"TGIF\", \"Paper Title\": \"TGIF\", \"Paper URL\": \"https://arxiv.org/abs/1604.02748\", \"GitHub URL\": \"https://github.com/raingo/TGIF-Release\", \"Hugging Face URL\": \"https://huggingface.co/datasets/HuggingFaceM4/TGIF\", \"Papers with Code URL\": \"https://paperswithcode.com/dataset/tgif\", \"ArXiv URL\": \"https://arxiv.org/abs/1604.02748\", \"Semantic Scholar Corpus ID\": 6262415, \"Year Released\": \"2016\", \"Text Sources\": [\"tumblr\"], \"Licenses\": [{\"License\": \"Custom\", \"License URL\": \"https://github.com/raingo/TGIF-Release\"}], \"Creators\": [\"University of Rochester\", \"Yahoo! Inc.\", \"AiCure\"], \"Countries\": [\"United States of America\"], \"License Verified By\": \"Vivek Sharma\", \"Video Hours\": 86.1, \"Taken Down\": \"False\", \"Video Sources\": \"Other\", \"Task Categories\": [\"Video Captioning\"], \"Inferred Metadata\": {\"GitHub License\": \"BSD 3-Clause License\", \"GitHub Stars (June 2024)\": 111, \"GitHub Topics\": [], \"Github Date\": \"\", \"HF Config\": \"all\", \"HF Config License\": \"\", \"HF Dataset\": \"HuggingFaceM4/TGIF\", \"HF Date\": \"2022-05-17\", \"HF Downloads (June 2024)\": 9, \"HF Likes (June 2024)\": 11, \"HF Yaml License\": \"Unspecified\", \"PwC Date\": \"2016-04-10\", \"PwC Description\": \"The Tumblr GIF (TGIF) dataset contains 100K animated GIFs and 120K sentences describing visual content of the animated GIFs. The animated GIFs have been collected from Tumblr, from randomly selected posts published between May and June of 2015. The dataset provides the URLs of animated GIFs. The sentences are collected via crowdsourcing, with a carefully designed annotation interface that ensures high quality dataset. There is one sentence per animated GIF for the training and validation splits, and three sentences per GIF for the test split. The dataset can be used to evaluate animated GIF/video description techniques.\", \"PwC License Name\": \"Academic Research Purposes Only\", \"PwC License URL\": \"https://github.com/raingo/TGIF-Release#license\", \"S2 Citation Count (June 2024)\": \"\", \"S2 Date\": \"\"}, \"Bibtex\": \"@Article{Li2016TGIFAN,\\n author = {Yuncheng Li and Yale Song and Liangliang Cao and Joel R. Tetreault and Larry Goldberg and A. Jaimes and Jiebo Luo},\\n booktitle = {Computer Vision and Pattern Recognition},\\n journal = {2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},\\n pages = {4641-4650},\\n title = {TGIF: A New Dataset and Benchmark on Animated GIF Description},\\n year = {2016}\\n}\\n\", \"License Use (DataProvenance)\": \"commercial\", \"License Attribution (DataProvenance)\": 1, \"License Share Alike (DataProvenance)\": 0, \"License Use (DataProvenance IgnoreOpenAI)\": \"commercial\", \"License Attribution (DataProvenance IgnoreOpenAI)\": 1, \"License Share Alike (DataProvenance IgnoreOpenAI)\": 0, \"License Type\": \"Commercial\"}, {\"Unique Dataset Identifier\": \"ntu-rgbd\", \"Collection\": \"ntu-rgbd\", \"Collection URL\": \"https://arxiv.org/pdf/1604.02808.pdf\", \"Dataset Name\": \"NTU RGB+D: A Large Scale Dataset for 3D Human Activity Analysis (CVPR 2016, TPAMI 2019)\", \"Paper Title\": \"NTU RGB+D: A Large Scale Dataset for 3D Human Activity Analysis (CVPR 2016, TPAMI 2019)\", \"Paper URL\": \"https://arxiv.org/pdf/1604.02808.pdf\", \"GitHub URL\": \"https://github.com/shahroudy/NTURGB-D\", \"Hugging Face URL\": \"\", \"Papers with Code URL\": \"https://paperswithcode.com/paper/ntu-rgbd-a-large-scale-dataset-for-3d-human\", \"ArXiv URL\": \"https://arxiv.org/pdf/1604.02808.pdf\", \"Semantic Scholar Corpus ID\": 15928602, \"Year Released\": \"2016\", \"Text Sources\": [\"crowdsourced\"], \"Licenses\": [{\"License\": \"Custom\", \"License URL\": \"https://rose1.ntu.edu.sg/dataset/actionRecognition/\"}], \"Creators\": [\"Nanyang Technological University\"], \"Countries\": [\"Singapore\"], \"License Verified By\": \"Vivek Sharma\", \"Video Hours\": 74.1, \"Taken Down\": \"False\", \"Video Sources\": \"crowdsourced\", \"Task Categories\": [\"Video Classification\"], \"Inferred Metadata\": {\"GitHub License\": \"\", \"GitHub Stars (June 2024)\": 715, \"GitHub Topics\": [], \"Github Date\": \"\", \"HF Config\": \"\", \"HF Config License\": \"\", \"HF Dataset\": \"\", \"HF Date\": \"\", \"HF Downloads (June 2024)\": \"\", \"HF Likes (June 2024)\": \"\", \"HF Yaml License\": \"\", \"PwC Date\": \"\", \"PwC Description\": \"#4 best model for Skeleton Based Action Recognition on Varying-view RGB-D Action-Skeleton (Accuracy (CS) metric)\", \"PwC License Name\": \"\", \"PwC License URL\": \"\", \"S2 Citation Count (June 2024)\": \"\", \"S2 Date\": \"\"}, \"Bibtex\": \"@Article{Shahroudy2016NTURA,\\n author = {Amir Shahroudy and Jun Liu and T. Ng and G. Wang},\\n booktitle = {Computer Vision and Pattern Recognition},\\n journal = {2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},\\n pages = {1010-1019},\\n title = {NTU RGB+D: A Large Scale Dataset for 3D Human Activity Analysis},\\n year = {2016}\\n}\\n\", \"License Use (DataProvenance)\": \"academic-only\", \"License Attribution (DataProvenance)\": 1, \"License Share Alike (DataProvenance)\": 0, \"License Use (DataProvenance IgnoreOpenAI)\": \"academic-only\", \"License Attribution (DataProvenance IgnoreOpenAI)\": 1, \"License Share Alike (DataProvenance IgnoreOpenAI)\": 0, \"License Type\": \"Non-Commercial/Academic\"}, {\"Unique Dataset Identifier\": \"charades\", \"Collection\": \"charades\", \"Collection URL\": \"https://arxiv.org/abs/1604.01753\", \"Dataset Name\": \"Charades\", \"Paper Title\": \"Charades\", \"Paper URL\": \"https://arxiv.org/abs/1604.01753\", \"GitHub URL\": \"\", \"Hugging Face URL\": \"https://huggingface.co/datasets/HuggingFaceM4/charades\", \"Papers with Code URL\": \"https://paperswithcode.com/dataset/charades\", \"ArXiv URL\": \"https://arxiv.org/abs/1604.01753\", \"Semantic Scholar Corpus ID\": 18061547, \"Year Released\": \"2016\", \"Text Sources\": [\"human\"], \"Licenses\": [{\"License\": \"Custom\", \"License URL\": \"https://huggingface.co/datasets/HuggingFaceM4/charades#licensing-information\"}], \"Creators\": [\"Carnegie Mellon University\", \"Inria\", \"University of Washington\", \"Allen Institute for AI\"], \"Countries\": [\"United States of America\", \"France\"], \"License Verified By\": \"Vivek Sharma\", \"Video Hours\": 82.3, \"Taken Down\": \"False\", \"Video Sources\": \"human\", \"Task Categories\": [\"Video Captioning\"], \"Inferred Metadata\": {\"GitHub License\": \"\", \"GitHub Stars (June 2024)\": \"\", \"GitHub Topics\": \"\", \"Github Date\": \"\", \"HF Config\": \"default\", \"HF Config License\": \"\", \"HF Dataset\": \"HuggingFaceM4/charades\", \"HF Date\": \"2022-05-11\", \"HF Downloads (June 2024)\": 7, \"HF Likes (June 2024)\": 2, \"HF Yaml License\": \"Unspecified\", \"PwC Date\": \"2016-01-01\", \"PwC Description\": \"The Charades dataset is composed of 9,848 videos of daily indoors activities with an average length of 30 seconds, involving interactions with 46 objects classes in 15 types of indoor scenes and containing a vocabulary of 30 verbs leading to 157 action classes. Each video in this dataset is annotated by multiple free-text descriptions, action labels, action intervals and classes of interacting objects. 267 different users were presented with a sentence, which includes objects and actions from a fixed vocabulary, and they recorded a video acting out the sentence. In total, the dataset contains 66,500 temporal annotations for 157 action classes, 41,104 labels for 46 object classes, and 27,847 textual descriptions of the videos. In the standard split there are7,986 training video and 1,863 validation video.\", \"PwC License Name\": \"Non Commercial\", \"PwC License URL\": \"http://vuchallenge.org/license-charades.txt\", \"S2 Citation Count (June 2024)\": \"\", \"S2 Date\": \"\"}, \"Bibtex\": \"@Article{Sigurdsson2016HollywoodIH,\\n author = {Gunnar A. Sigurdsson and G\\u00fcl Varol and X. Wang and Ali Farhadi and I. Laptev and A. Gupta},\\n booktitle = {European Conference on Computer Vision},\\n pages = {510-526},\\n title = {Hollywood in Homes: Crowdsourcing Data Collection for Activity Understanding},\\n year = {2016}\\n}\\n\", \"License Use (DataProvenance)\": \"commercial\", \"License Attribution (DataProvenance)\": 1, \"License Share Alike (DataProvenance)\": 1, \"License Use (DataProvenance IgnoreOpenAI)\": \"commercial\", \"License Attribution (DataProvenance IgnoreOpenAI)\": 1, \"License Share Alike (DataProvenance IgnoreOpenAI)\": 1, \"License Type\": \"Commercial\"}, {\"Unique Dataset Identifier\": \"mars\", \"Collection\": \"mars\", \"Collection URL\": \"https://link.springer.com/content/pdf/10.1007/978-3-319-46466-4_52.pdf\", \"Dataset Name\": \"Mars\", \"Paper Title\": \"Mars\", \"Paper URL\": \"https://link.springer.com/content/pdf/10.1007/978-3-319-46466-4_52.pdf\", \"GitHub URL\": \"\", \"Hugging Face URL\": \"\", \"Papers with Code URL\": \"https://paperswithcode.com/dataset/mars\", \"ArXiv URL\": \"https://link.springer.com/content/pdf/10.1007/978-3-319-46466-4_52.pdf\", \"Semantic Scholar Corpus ID\": 2214158, \"Year Released\": \"2016\", \"Text Sources\": [\"human\"], \"Licenses\": [{\"License\": \"Unspecified\", \"License URL\": \"\"}], \"Creators\": [\"Microsoft\", \"Tsinghua University\", \"The University of Texas at San Antonio\", \"Peking University\"], \"Countries\": [\"China\"], \"License Verified By\": \"Vivek Sharma\", \"Video Hours\": 0.24, \"Taken Down\": \"False\", \"Video Sources\": \"human\", \"Task Categories\": [\"Video Segmentation\", \"Video Classification\"], \"Inferred Metadata\": {\"GitHub License\": \"\", \"GitHub Stars (June 2024)\": \"\", \"GitHub Topics\": \"\", \"Github Date\": \"\", \"HF Config\": \"\", \"HF Config License\": \"\", \"HF Dataset\": \"\", \"HF Date\": \"\", \"HF Downloads (June 2024)\": \"\", \"HF Likes (June 2024)\": \"\", \"HF Yaml License\": \"\", \"PwC Date\": \"2016-01-01\", \"PwC Description\": \"MARS (Motion Analysis and Re-identification Set) is a large scale video based person reidentification dataset, an extension of the Market-1501 dataset. It has been collected from six near-synchronized cameras. It consists of 1,261 different pedestrians, who are captured by at least 2 cameras. The variations in poses, colors and illuminations of pedestrians, as well as the poor image quality, make it very difficult to yield high matching accuracy. Moreover, the dataset contains 3,248 distractors in order to make it more realistic. Deformable Part Model and GMMCP tracker were used to automatically generate the tracklets (mostly 25-50 frames long).\", \"PwC License Name\": \"Unspecified\", \"PwC License URL\": null, \"S2 Citation Count (June 2024)\": \"\", \"S2 Date\": \"\"}, \"Bibtex\": \"@Article{Zheng2016MARSAV,\\n author = {Liang Zheng and Zhi Bie and Yifan Sun and Jingdong Wang and Chi Su and Shengjin Wang and Q. Tian},\\n booktitle = {European Conference on Computer Vision},\\n pages = {868-884},\\n title = {MARS: A Video Benchmark for Large-Scale Person Re-Identification},\\n year = {2016}\\n}\\n\", \"License Use (DataProvenance)\": \"unspecified\", \"License Attribution (DataProvenance)\": 0, \"License Share Alike (DataProvenance)\": 0, \"License Use (DataProvenance IgnoreOpenAI)\": \"unspecified\", \"License Attribution (DataProvenance IgnoreOpenAI)\": 0, \"License Share Alike (DataProvenance IgnoreOpenAI)\": 0, \"License Type\": \"Unspecified\"}, {\"Unique Dataset Identifier\": \"youtube-8m\", \"Collection\": \"youtube-8m\", \"Collection URL\": \"https://arxiv.org/abs/1609.08675\", \"Dataset Name\": \"Youtube-8M: A Large-Scale Video Classification Benchmark\", \"Paper Title\": \"Youtube-8M: A Large-Scale Video Classification Benchmark\", \"Paper URL\": \"https://arxiv.org/abs/1609.08675\", \"GitHub URL\": \"\", \"Hugging Face URL\": \"\", \"Papers with Code URL\": \"https://paperswithcode.com/dataset/youtube-8m\", \"ArXiv URL\": \"https://arxiv.org/abs/1609.08675\", \"Semantic Scholar Corpus ID\": 11241677, \"Year Released\": \"2016\", \"Text Sources\": [\"youtube\"], \"Licenses\": [{\"License\": \"Unspecified\", \"License URL\": \"\"}], \"Creators\": [\"Google Research\"], \"Countries\": [\"United States of America\"], \"License Verified By\": \"Vivek Sharma\", \"Video Hours\": 350000.0, \"Taken Down\": \"False\", \"Video Sources\": \"youtube\", \"Task Categories\": [\"Video Classification\"], \"Inferred Metadata\": {\"GitHub License\": \"\", \"GitHub Stars (June 2024)\": \"\", \"GitHub Topics\": \"\", \"Github Date\": \"\", \"HF Config\": \"\", \"HF Config License\": \"\", \"HF Dataset\": \"\", \"HF Date\": \"\", \"HF Downloads (June 2024)\": \"\", \"HF Likes (June 2024)\": \"\", \"HF Yaml License\": \"\", \"PwC Date\": \"2016-01-01\", \"PwC Description\": \"The YouTube-8M dataset is a large scale video dataset, which includes more than 7 million videos with 4716 classes labeled by the annotation system. The dataset consists of three parts: training set, validate set, and test set. In the training set, each class contains at least 100 training videos. Features of these videos are extracted by the state-of-the-art popular pre-trained models and released for public use. Each video contains audio and visual modality. Based on the visual information, videos are divided into 24 topics, such as sports, game, arts & entertainment, etc\", \"PwC License Name\": \"CC BY 4.0\", \"PwC License URL\": \"https://creativecommons.org/licenses/by/4.0/\", \"S2 Citation Count (June 2024)\": \"\", \"S2 Date\": \"\"}, \"Bibtex\": \"@Article{Abu-El-Haija2016YouTube8MAL,\\n author = {Sami Abu-El-Haija and Nisarg Kothari and Joonseok Lee and A. Natsev and G. Toderici and Balakrishnan Varadarajan and Sudheendra Vijayanarasimhan},\\n booktitle = {arXiv.org},\\n journal = {ArXiv},\\n title = {YouTube-8M: A Large-Scale Video Classification Benchmark},\\n volume = {abs/1609.08675},\\n year = {2016}\\n}\\n\", \"License Use (DataProvenance)\": \"unspecified\", \"License Attribution (DataProvenance)\": 0, \"License Share Alike (DataProvenance)\": 0, \"License Use (DataProvenance IgnoreOpenAI)\": \"unspecified\", \"License Attribution (DataProvenance IgnoreOpenAI)\": 0, \"License Share Alike (DataProvenance IgnoreOpenAI)\": 0, \"License Type\": \"Unspecified\"}, {\"Unique Dataset Identifier\": \"pku-mmd-dataset\", \"Collection\": \"pku-mmd-dataset\", \"Collection URL\": \"https://arxiv.org/abs/1703.07475\", \"Dataset Name\": \"PKU-MMD: A Large Scale Benchmark for Continuous Multi-Modal Human Action Understanding (ACM Multimedia Workshop)\", \"Paper Title\": \"PKU-MMD: A Large Scale Benchmark for Continuous Multi-Modal Human Action Understanding (ACM Multimedia Workshop)\", \"Paper URL\": \"https://arxiv.org/abs/1703.07475\", \"GitHub URL\": \"https://struct002.github.io/PKUMMD/\", \"Hugging Face URL\": \"\", \"Papers with Code URL\": \"https://paperswithcode.com/paper/pku-mmd-a-large-scale-benchmark-for\", \"ArXiv URL\": \"https://arxiv.org/abs/1703.07475\", \"Semantic Scholar Corpus ID\": 1904265, \"Year Released\": \"2017\", \"Text Sources\": [\"crowdsourced\"], \"Licenses\": [{\"License\": \"Unspecified\", \"License URL\": \"\"}], \"Creators\": [\"Microsoft Research\", \"Peking University\"], \"Countries\": [\"China\"], \"License Verified By\": \"Vivek Sharma\", \"Video Hours\": 50.0, \"Taken Down\": \"False\", \"Video Sources\": \"crowdsourced\", \"Task Categories\": [\"Video Classification\"], \"Inferred Metadata\": {\"GitHub License\": \"\", \"GitHub Stars (June 2024)\": \"\", \"GitHub Topics\": \"\", \"Github Date\": \"\", \"HF Config\": \"\", \"HF Config License\": \"\", \"HF Dataset\": \"\", \"HF Date\": \"\", \"HF Downloads (June 2024)\": \"\", \"HF Likes (June 2024)\": \"\", \"HF Yaml License\": \"\", \"PwC Date\": \"\", \"PwC Description\": \"No code available yet.\", \"PwC License Name\": \"\", \"PwC License URL\": \"\", \"S2 Citation Count (June 2024)\": \"\", \"S2 Date\": \"\"}, \"Bibtex\": \"@Article{Liu2017PKUMMDAL,\\n author = {Chunhui Liu and Yueyu Hu and Yanghao Li and Sijie Song and Jiaying Liu},\\n booktitle = {arXiv.org},\\n journal = {ArXiv},\\n title = {PKU-MMD: A Large Scale Benchmark for Continuous Multi-Modal Human Action Understanding},\\n volume = {abs/1703.07475},\\n year = {2017}\\n}\\n\", \"License Use (DataProvenance)\": \"unspecified\", \"License Attribution (DataProvenance)\": 0, \"License Share Alike (DataProvenance)\": 0, \"License Use (DataProvenance IgnoreOpenAI)\": \"unspecified\", \"License Attribution (DataProvenance IgnoreOpenAI)\": 0, \"License Share Alike (DataProvenance IgnoreOpenAI)\": 0, \"License Type\": \"Unspecified\"}, {\"Unique Dataset Identifier\": \"mpii-md\", \"Collection\": \"mpii-md\", \"Collection URL\": \"https://arxiv.org/pdf/1501.02530.pdf\", \"Dataset Name\": \"MPII-MD: A Dataset for Movie Description\", \"Paper Title\": \"MPII-MD: A Dataset for Movie Description\", \"Paper URL\": \"https://arxiv.org/pdf/1501.02530.pdf\", \"GitHub URL\": \"\", \"Hugging Face URL\": \"\", \"Papers with Code URL\": \"\", \"ArXiv URL\": \"https://arxiv.org/pdf/1501.02530.pdf\", \"Semantic Scholar Corpus ID\": 15184723, \"Year Released\": \"2017\", \"Text Sources\": [\"movies\"], \"Licenses\": [{\"License\": \"Unspecified\", \"License URL\": \"\"}], \"Creators\": [\"Max Planck Institute for Informatics\"], \"Countries\": [\"Germany\"], \"License Verified By\": \"Vivek Sharma\", \"Video Hours\": 56.5, \"Taken Down\": \"False\", \"Video Sources\": \"movies\", \"Task Categories\": [\"Video Captioning\"], \"Inferred Metadata\": {\"GitHub License\": \"\", \"GitHub Stars (June 2024)\": \"\", \"GitHub Topics\": \"\", \"Github Date\": \"\", \"HF Config\": \"\", \"HF Config License\": \"\", \"HF Dataset\": \"\", \"HF Date\": \"\", \"HF Downloads (June 2024)\": \"\", \"HF Likes (June 2024)\": \"\", \"HF Yaml License\": \"\", \"PwC Date\": \"\", \"PwC Description\": \"\", \"PwC License Name\": \"\", \"PwC License URL\": \"\", \"S2 Citation Count (June 2024)\": \"\", \"S2 Date\": \"\"}, \"Bibtex\": \"@Article{Rohrbach2015ADF,\\n author = {Anna Rohrbach and Marcus Rohrbach and Niket Tandon and B. Schiele},\\n booktitle = {Computer Vision and Pattern Recognition},\\n journal = {2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},\\n pages = {3202-3212},\\n title = {A dataset for Movie Description},\\n year = {2015}\\n}\\n\", \"License Use (DataProvenance)\": \"unspecified\", \"License Attribution (DataProvenance)\": 0, \"License Share Alike (DataProvenance)\": 0, \"License Use (DataProvenance IgnoreOpenAI)\": \"unspecified\", \"License Attribution (DataProvenance IgnoreOpenAI)\": 0, \"License Share Alike (DataProvenance IgnoreOpenAI)\": 0, \"License Type\": \"Unspecified\"}, {\"Unique Dataset Identifier\": \"kinetics-400\", \"Collection\": \"kinetics-400\", \"Collection URL\": \"https://arxiv.org/abs/1705.06950\", \"Dataset Name\": \"Kinetics 400\", \"Paper Title\": \"Kinetics 400\", \"Paper URL\": \"https://arxiv.org/abs/1705.06950\", \"GitHub URL\": \"https://github.com/cvdfoundation/kinetics-dataset\", \"Hugging Face URL\": \"\", \"Papers with Code URL\": \"https://paperswithcode.com/dataset/kinetics\", \"ArXiv URL\": \"https://arxiv.org/abs/1705.06950\", \"Semantic Scholar Corpus ID\": 27300853, \"Year Released\": \"2017\", \"Text Sources\": [\"human\"], \"Licenses\": [{\"License\": \"Unspecified\", \"License URL\": \"\"}], \"Creators\": [\"DeepMind\"], \"Countries\": [\"United Kingdom\"], \"License Verified By\": \"Vivek Sharma\", \"Video Hours\": 850.68, \"Taken Down\": \"False\", \"Video Sources\": \"human\", \"Task Categories\": [\"Video Classification\"], \"Inferred Metadata\": {\"GitHub License\": \"\", \"GitHub Stars (June 2024)\": 704, \"GitHub Topics\": [], \"Github Date\": \"\", \"HF Config\": \"\", \"HF Config License\": \"\", \"HF Dataset\": \"\", \"HF Date\": \"\", \"HF Downloads (June 2024)\": \"\", \"HF Likes (June 2024)\": \"\", \"HF Yaml License\": \"\", \"PwC Date\": \"2017-05-19\", \"PwC Description\": \"The Kinetics dataset is a large-scale, high-quality dataset for human action recognition in videos. The dataset consists of around 500,000 video clips covering 600 human action classes with at least 600 video clips for each action class. Each video clip lasts around 10 seconds and is labeled with a single action class. The videos are collected from YouTube.\", \"PwC License Name\": \"CC BY 4.0\", \"PwC License URL\": \"https://creativecommons.org/licenses/by/4.0/\", \"S2 Citation Count (June 2024)\": \"\", \"S2 Date\": \"\"}, \"Bibtex\": \"@Article{Kay2017TheKH,\\n author = {W. Kay and Jo\\u00e3o Carreira and K. Simonyan and Brian Zhang and Chloe Hillier and Sudheendra Vijayanarasimhan and Fabio Viola and Tim Green and T. Back and A. Natsev and Mustafa Suleyman and Andrew Zisserman},\\n booktitle = {arXiv.org},\\n journal = {ArXiv},\\n title = {The Kinetics Human Action Video Dataset},\\n volume = {abs/1705.06950},\\n year = {2017}\\n}\\n\", \"License Use (DataProvenance)\": \"unspecified\", \"License Attribution (DataProvenance)\": 0, \"License Share Alike (DataProvenance)\": 0, \"License Use (DataProvenance IgnoreOpenAI)\": \"unspecified\", \"License Attribution (DataProvenance IgnoreOpenAI)\": 0, \"License Share Alike (DataProvenance IgnoreOpenAI)\": 0, \"License Type\": \"Unspecified\"}, {\"Unique Dataset Identifier\": \"multi-thumos-challenge\", \"Collection\": \"multi-thumos-challenge\", \"Collection URL\": \"https://arxiv.org/pdf/1507.05738v3.pdf\", \"Dataset Name\": \"MultiTHUMOS: Every Moment Counts: Dense Detailed Labeling of Actions in Complex Videos (IJCV 2017)\", \"Paper Title\": \"MultiTHUMOS: Every Moment Counts: Dense Detailed Labeling of Actions in Complex Videos (IJCV 2017)\", \"Paper URL\": \"https://arxiv.org/pdf/1507.05738v3.pdf\", \"GitHub URL\": \"\", \"Hugging Face URL\": \"\", \"Papers with Code URL\": \"https://paperswithcode.com/dataset/multithumos\", \"ArXiv URL\": \"https://arxiv.org/pdf/1507.05738v3.pdf\", \"Semantic Scholar Corpus ID\": 3337929, \"Year Released\": \"2017\", \"Text Sources\": [\"youtube\"], \"Licenses\": [{\"License\": \"CC BY 4.0\", \"License URL\": \"https://ai.stanford.edu/~syyeung/resources/multithumos.zip\"}], \"Creators\": [\"Stanford University\", \"Carnegie Mellon University\", \"Simon Fraser University\"], \"Countries\": [\"United States of America\", \"Canada\"], \"License Verified By\": \"Vivek Sharma\", \"Video Hours\": 30.0, \"Taken Down\": \"False\", \"Video Sources\": \"youtube\", \"Task Categories\": [\"Video Classification\"], \"Inferred Metadata\": {\"GitHub License\": \"\", \"GitHub Stars (June 2024)\": \"\", \"GitHub Topics\": \"\", \"Github Date\": \"\", \"HF Config\": \"\", \"HF Config License\": \"\", \"HF Dataset\": \"\", \"HF Date\": \"\", \"HF Downloads (June 2024)\": \"\", \"HF Likes (June 2024)\": \"\", \"HF Yaml License\": \"\", \"PwC Date\": \"2018-01-01\", \"PwC Description\": \"The MultiTHUMOS dataset contains dense, multilabel, frame-level action annotations for 30 hours across 400 videos in the THUMOS'14 action detection dataset. It consists of 38,690 annotations of 65 action classes, with an average of 1.5 labels per frame and 10.5 action classes per video.\", \"PwC License Name\": \"CC BY 4.0\", \"PwC License URL\": null, \"S2 Citation Count (June 2024)\": \"\", \"S2 Date\": \"\"}, \"Bibtex\": \"@Article{Yeung2015EveryMC,\\n author = {Serena Yeung and Olga Russakovsky and Ning Jin and Mykhaylo Andriluka and Greg Mori and Li Fei-Fei},\\n booktitle = {International Journal of Computer Vision},\\n journal = {International Journal of Computer Vision},\\n pages = {375 - 389},\\n title = {Every Moment Counts: Dense Detailed Labeling of Actions in Complex Videos},\\n volume = {126},\\n year = {2015}\\n}\\n\", \"License Use (DataProvenance)\": \"commercial\", \"License Attribution (DataProvenance)\": 1, \"License Share Alike (DataProvenance)\": 0, \"License Use (DataProvenance IgnoreOpenAI)\": \"commercial\", \"License Attribution (DataProvenance IgnoreOpenAI)\": 1, \"License Share Alike (DataProvenance IgnoreOpenAI)\": 0, \"License Type\": \"Commercial\"}, {\"Unique Dataset Identifier\": \"ava\", \"Collection\": \"ava\", \"Collection URL\": \"https://arxiv.org/pdf/1705.08421\", \"Dataset Name\": \"AVA\", \"Paper Title\": \"AVA\", \"Paper URL\": \"https://arxiv.org/pdf/1705.08421\", \"GitHub URL\": \"\", \"Hugging Face URL\": \"\", \"Papers with Code URL\": \"https://paperswithcode.com/paper/ava-a-video-dataset-of-spatio-temporally\", \"ArXiv URL\": \"https://arxiv.org/pdf/1705.08421\", \"Semantic Scholar Corpus ID\": 688013, \"Year Released\": \"2017\", \"Text Sources\": [\"youtube\"], \"Licenses\": [{\"License\": \"CC BY 4.0\", \"License URL\": \"https://research.google.com/ava/\"}], \"Creators\": [\"Google Research\"], \"Countries\": [\"United States of America\"], \"License Verified By\": \"Vivek Sharma\", \"Video Hours\": 107.5, \"Taken Down\": \"False\", \"Video Sources\": \"youtube\", \"Task Categories\": [\"Temporal Localization\", \"Video Classification\"], \"Inferred Metadata\": {\"GitHub License\": \"\", \"GitHub Stars (June 2024)\": \"\", \"GitHub Topics\": \"\", \"Github Date\": \"\", \"HF Config\": \"\", \"HF Config License\": \"\", \"HF Dataset\": \"\", \"HF Date\": \"\", \"HF Downloads (June 2024)\": \"\", \"HF Likes (June 2024)\": \"\", \"HF Yaml License\": \"\", \"PwC Date\": \"\", \"PwC Description\": \"#6 best model for Action Detection on UCF101-24 (Frame-mAP 0.5 metric)\", \"PwC License Name\": \"\", \"PwC License URL\": \"\", \"S2 Citation Count (June 2024)\": \"\", \"S2 Date\": \"\"}, \"Bibtex\": \"@Article{Gu2017AVAAV,\\n author = {Chunhui Gu and Chen Sun and Sudheendra Vijayanarasimhan and C. Pantofaru and David A. Ross and G. Toderici and Yeqing Li and Susanna Ricco and R. Sukthankar and C. Schmid and J. Malik},\\n booktitle = {2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition},\\n journal = {2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition},\\n pages = {6047-6056},\\n title = {AVA: A Video Dataset of Spatio-Temporally Localized Atomic Visual Actions},\\n year = {2017}\\n}\\n\", \"License Use (DataProvenance)\": \"commercial\", \"License Attribution (DataProvenance)\": 1, \"License Share Alike (DataProvenance)\": 0, \"License Use (DataProvenance IgnoreOpenAI)\": \"commercial\", \"License Attribution (DataProvenance IgnoreOpenAI)\": 1, \"License Share Alike (DataProvenance IgnoreOpenAI)\": 0, \"License Type\": \"Commercial\"}, {\"Unique Dataset Identifier\": \"20bn-something\", \"Collection\": \"20bn-something\", \"Collection URL\": \"https://arxiv.org/abs/1706.04261\", \"Dataset Name\": \"20BN-SOMETHING-SOMETHING: The \\\"something something\\\" video database for learning and evaluating visual common sense\", \"Paper Title\": \"20BN-SOMETHING-SOMETHING: The \\\"something something\\\" video database for learning and evaluating visual common sense\", \"Paper URL\": \"https://arxiv.org/abs/1706.04261\", \"GitHub URL\": \"\", \"Hugging Face URL\": \"\", \"Papers with Code URL\": \"https://paperswithcode.com/dataset/something-something-v2\", \"ArXiv URL\": \"https://arxiv.org/abs/1706.04261\", \"Semantic Scholar Corpus ID\": 834612, \"Year Released\": \"2017\", \"Text Sources\": [\"crowdsourced\"], \"Licenses\": [{\"License\": \"Custom\", \"License URL\": \"https://developer.qualcomm.com/software/ai-datasets/something-something\"}], \"Creators\": [\"Twenty Billion Neurons GmbH\"], \"Countries\": [\"Germany\"], \"License Verified By\": \"Vivek Sharma\", \"Video Hours\": 121.46, \"Taken Down\": \"False\", \"Video Sources\": \"crowdsourced\", \"Task Categories\": [\"Video Classification\"], \"Inferred Metadata\": {\"GitHub License\": \"\", \"GitHub Stars (June 2024)\": \"\", \"GitHub Topics\": \"\", \"Github Date\": \"\", \"HF Config\": \"\", \"HF Config License\": \"\", \"HF Dataset\": \"\", \"HF Date\": \"\", \"HF Downloads (June 2024)\": \"\", \"HF Likes (June 2024)\": \"\", \"HF Yaml License\": \"\", \"PwC Date\": \"2017-01-01\", \"PwC Description\": \"The 20BN-SOMETHING-SOMETHING V2 dataset is a large collection of labeled video clips that show humans performing pre-defined basic actions with everyday objects. The dataset was created by a large number of crowd workers. It allows machine learning models to develop fine-grained understanding of basic actions that occur in the physical world. It contains 220,847 videos, with 168,913 in the training set, 24,777 in the validation set and 27,157 in the test set. There are 174 labels.\\n\\nSource\\n\\nImage Source\", \"PwC License Name\": \"Various\", \"PwC License URL\": \"https://20bn.com/licensing/datasets/academic\", \"S2 Citation Count (June 2024)\": \"\", \"S2 Date\": \"\"}, \"Bibtex\": \"@Article{Goyal2017TheS,\\n author = {Raghav Goyal and Samira Ebrahimi Kahou and Vincent Michalski and Joanna Materzynska and S. Westphal and Heuna Kim and V. Haenel and Ingo Fr\\u00fcnd and P. Yianilos and Moritz Mueller-Freitag and F. Hoppe and Christian Thurau and Ingo Bax and R. Memisevic},\\n booktitle = {IEEE International Conference on Computer Vision},\\n journal = {2017 IEEE International Conference on Computer Vision (ICCV)},\\n pages = {5843-5851},\\n title = {The \\u201cSomething Something\\u201d Video Database for Learning and Evaluating Visual Common Sense},\\n year = {2017}\\n}\\n\", \"License Use (DataProvenance)\": \"commercial\", \"License Attribution (DataProvenance)\": 1, \"License Share Alike (DataProvenance)\": 1, \"License Use (DataProvenance IgnoreOpenAI)\": \"commercial\", \"License Attribution (DataProvenance IgnoreOpenAI)\": 1, \"License Share Alike (DataProvenance IgnoreOpenAI)\": 1, \"License Type\": \"Commercial\"}, {\"Unique Dataset Identifier\": \"davis\", \"Collection\": \"davis\", \"Collection URL\": \"https://ieeexplore.ieee.org/document/7780454\", \"Dataset Name\": \"Davis\", \"Paper Title\": \"Davis\", \"Paper URL\": \"https://ieeexplore.ieee.org/document/7780454\", \"GitHub URL\": \"https://github.com/fperazzi/davis\", \"Hugging Face URL\": \"\", \"Papers with Code URL\": \"https://paperswithcode.com/paper/a-benchmark-dataset-and-evaluation\", \"ArXiv URL\": \"https://ieeexplore.ieee.org/document/7780454\", \"Semantic Scholar Corpus ID\": 3619941, \"Year Released\": \"2017\", \"Text Sources\": [\"undisclosed web\"], \"Licenses\": [{\"License\": \"Custom\", \"License URL\": \"https://github.com/fperazzi/davis/blob/main/LICENSE\"}], \"Creators\": [\"ETH Z\\u00fcrich\", \"Disney Research\"], \"Countries\": [\"Switzerland\"], \"License Verified By\": \"Vivek Sharma\", \"Video Hours\": 0.04, \"Taken Down\": \"False\", \"Video Sources\": \"undisclosed web\", \"Task Categories\": [\"Video Segmentation\", \"Video Classification\"], \"Inferred Metadata\": {\"GitHub License\": \"\", \"GitHub Stars (June 2024)\": 6, \"GitHub Topics\": [], \"Github Date\": \"\", \"HF Config\": \"\", \"HF Config License\": \"\", \"HF Dataset\": \"\", \"HF Date\": \"\", \"HF Downloads (June 2024)\": \"\", \"HF Likes (June 2024)\": \"\", \"HF Yaml License\": \"\", \"PwC Date\": \"\", \"PwC Description\": \"Implemented in one code library.\", \"PwC License Name\": \"\", \"PwC License URL\": \"\", \"S2 Citation Count (June 2024)\": \"\", \"S2 Date\": \"\"}, \"Bibtex\": \"@Article{Pont-Tuset2017The2D,\\n author = {J. Pont-Tuset and Federico Perazzi and Sergi Caelles and Pablo Arbel\\u00e1ez and A. Sorkine-Hornung and L. Gool},\\n booktitle = {arXiv.org},\\n journal = {ArXiv},\\n title = {The 2017 DAVIS Challenge on Video Object Segmentation},\\n volume = {abs/1704.00675},\\n year = {2017}\\n}\\n\", \"License Use (DataProvenance)\": \"commercial\", \"License Attribution (DataProvenance)\": 1, \"License Share Alike (DataProvenance)\": 0, \"License Use (DataProvenance IgnoreOpenAI)\": \"commercial\", \"License Attribution (DataProvenance IgnoreOpenAI)\": 1, \"License Share Alike (DataProvenance IgnoreOpenAI)\": 0, \"License Type\": \"Commercial\"}, {\"Unique Dataset Identifier\": \"qfvs\", \"Collection\": \"qfvs\", \"Collection URL\": \"https://arxiv.org/abs/1707.04960\", \"Dataset Name\": \"QFVS: Query-Focused Video Summarization: Dataset, Evaluation, and A Memory Network Based Approach (CVPR 2017)\", \"Paper Title\": \"QFVS: Query-Focused Video Summarization: Dataset, Evaluation, and A Memory Network Based Approach (CVPR 2017)\", \"Paper URL\": \"https://arxiv.org/abs/1707.04960\", \"GitHub URL\": \"\", \"Hugging Face URL\": \"\", \"Papers with Code URL\": \"https://paperswithcode.com/dataset/query-focused-video-summarization-dataset\", \"ArXiv URL\": \"https://arxiv.org/abs/1707.04960\", \"Semantic Scholar Corpus ID\": 2774608, \"Year Released\": \"2017\", \"Text Sources\": [\"crowdsourced\"], \"Licenses\": [{\"License\": \"Unspecified\", \"License URL\": \"\"}], \"Creators\": [\"University of Central Florida\", \"University of Alabama\"], \"Countries\": [\"United States of America\"], \"License Verified By\": \"Vivek Sharma\", \"Video Hours\": 20.0, \"Taken Down\": \"False\", \"Video Sources\": \"crowdsourced\", \"Task Categories\": [\"Video Summarization\"], \"Inferred Metadata\": {\"GitHub License\": \"\", \"GitHub Stars (June 2024)\": \"\", \"GitHub Topics\": \"\", \"Github Date\": \"\", \"HF Config\": \"\", \"HF Config License\": \"\", \"HF Dataset\": \"\", \"HF Date\": \"\", \"HF Downloads (June 2024)\": \"\", \"HF Likes (June 2024)\": \"\", \"HF Yaml License\": \"\", \"PwC Date\": \"\", \"PwC Description\": \"Collects dense per-video-shot concept annotations.\", \"PwC License Name\": \"Unspecified\", \"PwC License URL\": null, \"S2 Citation Count (June 2024)\": \"\", \"S2 Date\": \"\"}, \"Bibtex\": \"@Article{Sharghi2017QueryFocusedVS,\\n author = {Aidean Sharghi and Jacob S. Laurel and Boqing Gong},\\n booktitle = {Computer Vision and Pattern Recognition},\\n journal = {2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},\\n pages = {2127-2136},\\n title = {Query-Focused Video Summarization: Dataset, Evaluation, and a Memory Network Based Approach},\\n year = {2017}\\n}\\n\", \"License Use (DataProvenance)\": \"unspecified\", \"License Attribution (DataProvenance)\": 0, \"License Share Alike (DataProvenance)\": 0, \"License Use (DataProvenance IgnoreOpenAI)\": \"unspecified\", \"License Attribution (DataProvenance IgnoreOpenAI)\": 0, \"License Share Alike (DataProvenance IgnoreOpenAI)\": 0, \"License Type\": \"Unspecified\"}, {\"Unique Dataset Identifier\": \"imagenet-vid\", \"Collection\": \"imagenet-vid\", \"Collection URL\": \"https://link.springer.com/article/10.1007/s11263-015-0816-y?sa_campaign=email/event/articleAuthor/onlineFirst#\", \"Dataset Name\": \"ImageNet VID\", \"Paper Title\": \"ImageNet VID\", \"Paper URL\": \"https://link.springer.com/article/10.1007/s11263-015-0816-y?sa_campaign=email/event/articleAuthor/onlineFirst#\", \"GitHub URL\": \"\", \"Hugging Face URL\": \"\", \"Papers with Code URL\": \"https://paperswithcode.com/sota/video-object-detection-on-imagenet-vid\", \"ArXiv URL\": \"https://link.springer.com/article/10.1007/s11263-015-0816-y?sa_campaign=email/event/articleAuthor/onlineFirst#\", \"Semantic Scholar Corpus ID\": 2930547, \"Year Released\": \"2017\", \"Text Sources\": [\"undisclosed web\"], \"Licenses\": [{\"License\": \"CC BY-NC 4.0\", \"License URL\": \"https://www.image-net.org/challenges/LSVRC/2017/index.php\"}], \"Creators\": [\"Stanford University\"], \"Countries\": [\"United States of America\"], \"License Verified By\": \"Vivek Sharma\", \"Video Hours\": 9.26, \"Taken Down\": \"False\", \"Video Sources\": \"undisclosed web\", \"Task Categories\": [\"Video Classification\"], \"Inferred Metadata\": {\"GitHub License\": \"\", \"GitHub Stars (June 2024)\": \"\", \"GitHub Topics\": \"\", \"Github Date\": \"\", \"HF Config\": \"\", \"HF Config License\": \"\", \"HF Dataset\": \"\", \"HF Date\": \"\", \"HF Downloads (June 2024)\": \"\", \"HF Likes (June 2024)\": \"\", \"HF Yaml License\": \"\", \"PwC Date\": \"\", \"PwC Description\": \"The current state-of-the-art on ImageNet VID is DiffusionVID (Swin-B). See a full comparison of 31 papers with code.\", \"PwC License Name\": \"\", \"PwC License URL\": \"\", \"S2 Citation Count (June 2024)\": \"\", \"S2 Date\": \"\"}, \"Bibtex\": \"@Article{Russakovsky2014ImageNetLS,\\n author = {Olga Russakovsky and Jia Deng and Hao Su and J. Krause and S. Satheesh and Sean Ma and Zhiheng Huang and A. Karpathy and A. Khosla and Michael S. Bernstein and A. Berg and Li Fei-Fei},\\n booktitle = {International Journal of Computer Vision},\\n journal = {International Journal of Computer Vision},\\n pages = {211 - 252},\\n title = {ImageNet Large Scale Visual Recognition Challenge},\\n volume = {115},\\n year = {2014}\\n}\\n\", \"License Use (DataProvenance)\": \"non-commercial\", \"License Attribution (DataProvenance)\": 1, \"License Share Alike (DataProvenance)\": 0, \"License Use (DataProvenance IgnoreOpenAI)\": \"non-commercial\", \"License Attribution (DataProvenance IgnoreOpenAI)\": 1, \"License Share Alike (DataProvenance IgnoreOpenAI)\": 0, \"License Type\": \"Non-Commercial/Academic\"}, {\"Unique Dataset Identifier\": \"youcook-2\", \"Collection\": \"youcook-2\", \"Collection URL\": \"https://arxiv.org/abs/1703.09788\", \"Dataset Name\": \"YouCook2\", \"Paper Title\": \"YouCook2\", \"Paper URL\": \"https://arxiv.org/abs/1703.09788\", \"GitHub URL\": \"\", \"Hugging Face URL\": \"\", \"Papers with Code URL\": \"https://paperswithcode.com/dataset/youcook2\", \"ArXiv URL\": \"https://arxiv.org/abs/1703.09788\", \"Semantic Scholar Corpus ID\": 19713015, \"Year Released\": \"2017\", \"Text Sources\": [\"youtube\"], \"Licenses\": [{\"License\": \"MIT License\", \"License URL\": \"http://youcook2.eecs.umich.edu/static/YouCookII/LICENSE_YOUCOOK2.txt\"}], \"Creators\": [\"University of Rochester\", \"University of Michigan\"], \"Countries\": [\"United States of America\"], \"License Verified By\": \"Vivek Sharma\", \"Video Hours\": 175.6, \"Taken Down\": \"False\", \"Video Sources\": \"youtube\", \"Task Categories\": [\"Video Captioning\"], \"Inferred Metadata\": {\"GitHub License\": \"\", \"GitHub Stars (June 2024)\": \"\", \"GitHub Topics\": \"\", \"Github Date\": \"\", \"HF Config\": \"\", \"HF Config License\": \"\", \"HF Dataset\": \"\", \"HF Date\": \"\", \"HF Downloads (June 2024)\": \"\", \"HF Likes (June 2024)\": \"\", \"HF Yaml License\": \"\", \"PwC Date\": \"2018-01-01\", \"PwC Description\": \"YouCook2 is the largest task-oriented, instructional video dataset in the vision community. It contains 2000 long untrimmed videos from 89 cooking recipes; on average, each distinct recipe has 22 videos. The procedure steps for each video are annotated with temporal boundaries and described by imperative English sentences (see the example below). The videos were downloaded from YouTube and are all in the third-person viewpoint. All the videos are unconstrained and can be performed by individual persons at their houses with unfixed cameras. YouCook2 contains rich recipe types and various cooking styles from all over the world.\", \"PwC License Name\": \"Various\", \"PwC License URL\": \"http://youcook2.eecs.umich.edu/download\", \"S2 Citation Count (June 2024)\": \"\", \"S2 Date\": \"\"}, \"Bibtex\": \"@Article{Zhou2017TowardsAL,\\n author = {Luowei Zhou and Chenliang Xu and Jason J. Corso},\\n booktitle = {AAAI Conference on Artificial Intelligence},\\n pages = {7590-7598},\\n title = {Towards Automatic Learning of Procedures From Web Instructional Videos},\\n year = {2017}\\n}\\n\", \"License Use (DataProvenance)\": \"commercial\", \"License Attribution (DataProvenance)\": 1, \"License Share Alike (DataProvenance)\": 0, \"License Use (DataProvenance IgnoreOpenAI)\": \"commercial\", \"License Attribution (DataProvenance IgnoreOpenAI)\": 1, \"License Share Alike (DataProvenance IgnoreOpenAI)\": 0, \"License Type\": \"Commercial\"}, {\"Unique Dataset Identifier\": \"lsmdc\", \"Collection\": \"lsmdc\", \"Collection URL\": \"https://arxiv.org/pdf/1605.03705\", \"Dataset Name\": \"LSMDC\", \"Paper Title\": \"LSMDC\", \"Paper URL\": \"https://arxiv.org/pdf/1605.03705\", \"GitHub URL\": \"\", \"Hugging Face URL\": \"\", \"Papers with Code URL\": \"https://paperswithcode.com/paper/movie-description\", \"ArXiv URL\": \"https://arxiv.org/pdf/1605.03705\", \"Semantic Scholar Corpus ID\": 18217052, \"Year Released\": \"2017\", \"Text Sources\": [\"movies\"], \"Licenses\": [{\"License\": \"Custom\", \"License URL\": \"https://datasets.d2.mpi-inf.mpg.de/movieDescription/protected/lsmdc2016/README.txt\"}], \"Creators\": [\"UC Berkeley\", \"Disney Research\", \"Universite de Montreal\", \"Polytechnique Montr\\u00e9al\", \"Universit\\u00e9 de Sherbrooke\", \"Twitter\"], \"Countries\": [\"United States of America\", \"Canada\"], \"License Verified By\": \"Vivek Sharma\", \"Video Hours\": 158.0, \"Taken Down\": \"False\", \"Video Sources\": \"movies\", \"Task Categories\": [\"Video Summarization\", \"Misc\", \"Video Classification\"], \"Inferred Metadata\": {\"GitHub License\": \"\", \"GitHub Stars (June 2024)\": \"\", \"GitHub Topics\": \"\", \"Github Date\": \"\", \"HF Config\": \"\", \"HF Config License\": \"\", \"HF Dataset\": \"\", \"HF Date\": \"\", \"HF Downloads (June 2024)\": \"\", \"HF Likes (June 2024)\": \"\", \"HF Yaml License\": \"\", \"PwC Date\": \"\", \"PwC Description\": \"No code available yet.\", \"PwC License Name\": \"\", \"PwC License URL\": \"\", \"S2 Citation Count (June 2024)\": \"\", \"S2 Date\": \"\"}, \"Bibtex\": \"@Article{Rohrbach2016MovieD,\\n author = {Anna Rohrbach and Atousa Torabi and Marcus Rohrbach and Niket Tandon and C. Pal and H. Larochelle and Aaron C. Courville and B. Schiele},\\n booktitle = {International Journal of Computer Vision},\\n journal = {International Journal of Computer Vision},\\n pages = {94 - 120},\\n title = {Movie Description},\\n volume = {123},\\n year = {2016}\\n}\\n\", \"License Use (DataProvenance)\": \"academic-only\", \"License Attribution (DataProvenance)\": 1, \"License Share Alike (DataProvenance)\": 0, \"License Use (DataProvenance IgnoreOpenAI)\": \"academic-only\", \"License Attribution (DataProvenance IgnoreOpenAI)\": 1, \"License Share Alike (DataProvenance IgnoreOpenAI)\": 0, \"License Type\": \"Non-Commercial/Academic\"}, {\"Unique Dataset Identifier\": \"voxceleb\", \"Collection\": \"voxceleb\", \"Collection URL\": \"https://arxiv.org/abs/1706.08612\", \"Dataset Name\": \"VoxCeleb\", \"Paper Title\": \"VoxCeleb\", \"Paper URL\": \"https://arxiv.org/abs/1706.08612\", \"GitHub URL\": \"https://github.com/a-nagrani/VGGVox\", \"Hugging Face URL\": \"\", \"Papers with Code URL\": \"https://cs.paperswithcode.com/paper/voxceleb-a-large-scale-speaker-identification\", \"ArXiv URL\": \"https://arxiv.org/abs/1706.08612\", \"Semantic Scholar Corpus ID\": 10475843, \"Year Released\": \"2017\", \"Text Sources\": [\"youtube\"], \"Licenses\": [{\"License\": \"Custom\", \"License URL\": \"https://www.robots.ox.ac.uk/~vgg/data/voxceleb/\"}], \"Creators\": [\"University of Oxford\"], \"Countries\": [\"United Kingdom\", \"South Korea\"], \"License Verified By\": \"Vivek Sharma\", \"Video Hours\": 2000.0, \"Taken Down\": \"False\", \"Video Sources\": \"youtube\", \"Task Categories\": [\"Video Classification\"], \"Inferred Metadata\": {\"GitHub License\": \"\", \"GitHub Stars (June 2024)\": 366, \"GitHub Topics\": [], \"Github Date\": \"\", \"HF Config\": \"\", \"HF Config License\": \"\", \"HF Dataset\": \"\", \"HF Date\": \"\", \"HF Downloads (June 2024)\": \"\", \"HF Likes (June 2024)\": \"\", \"HF Yaml License\": \"\", \"PwC Date\": \"\", \"PwC Description\": \"Implemented in 8 code libraries.\", \"PwC License Name\": \"\", \"PwC License URL\": \"\", \"S2 Citation Count (June 2024)\": \"\", \"S2 Date\": \"\"}, \"Bibtex\": \"@Article{Nagrani2017VoxCelebAL,\\n author = {Arsha Nagrani and Joon Son Chung and Andrew Zisserman},\\n booktitle = {Interspeech},\\n pages = {2616-2620},\\n title = {VoxCeleb: A Large-Scale Speaker Identification Dataset},\\n year = {2017}\\n}\\n\", \"License Use (DataProvenance)\": \"academic-only\", \"License Attribution (DataProvenance)\": 1, \"License Share Alike (DataProvenance)\": 0, \"License Use (DataProvenance IgnoreOpenAI)\": \"academic-only\", \"License Attribution (DataProvenance IgnoreOpenAI)\": 1, \"License Share Alike (DataProvenance IgnoreOpenAI)\": 0, \"License Type\": \"Non-Commercial/Academic\"}, {\"Unique Dataset Identifier\": \"epic-kitchenes\", \"Collection\": \"epic-kitchenes\", \"Collection URL\": \"https://arxiv.org/abs/1804.02748\", \"Dataset Name\": \"EPIC-KITCHENS\", \"Paper Title\": \"EPIC-KITCHENS\", \"Paper URL\": \"https://arxiv.org/abs/1804.02748\", \"GitHub URL\": \"https://github.com/epic-kitchens\", \"Hugging Face URL\": \"\", \"Papers with Code URL\": \"https://paperswithcode.com/dataset/epic-kitchens-100\", \"ArXiv URL\": \"https://arxiv.org/abs/1804.02748\", \"Semantic Scholar Corpus ID\": 4710439, \"Year Released\": \"2018\", \"Text Sources\": [\"human\"], \"Licenses\": [{\"License\": \"CC BY-NC 4.0\", \"License URL\": \"https://epic-kitchens.github.io/2024\"}], \"Creators\": [\"University of Toronto\", \"University of Bristol\", \"University of Catania\"], \"Countries\": [\"United Kingdom\", \"Canada\", \"Spain\"], \"License Verified By\": \"Vivek Sharma\", \"Video Hours\": 100.0, \"Taken Down\": \"False\", \"Video Sources\": \"human\", \"Task Categories\": [\"Video Captioning\"], \"Inferred Metadata\": {\"GitHub License\": \"\", \"GitHub Stars (June 2024)\": 0, \"GitHub Topics\": [\"\"], \"Github Date\": \"\", \"HF Config\": \"\", \"HF Config License\": \"\", \"HF Dataset\": \"\", \"HF Date\": \"\", \"HF Downloads (June 2024)\": \"\", \"HF Likes (June 2024)\": \"\", \"HF Yaml License\": \"\", \"PwC Date\": \"2020-06-23\", \"PwC Description\": \"This paper introduces the pipeline to scale the largest dataset in egocentric vision EPIC-KITCHENS. The effort culminates in EPIC-KITCHENS-100, a collection of 100 hours, 20M frames, 90K actions in 700 variable-length videos, capturing long-term unscripted activities in 45 environments, using head-mounted cameras. Compared to its previous version (EPIC-KITCHENS-55), EPIC-KITCHENS-100 has been annotated using a novel pipeline that allows denser (54% more actions per minute) and more complete annotations of fine-grained actions (+128% more action segments). This collection also enables evaluating the \\\"test of time\\\" - i.e. whether models trained on data collected in 2018 can generalise to new footage collected under the same hypotheses albeit \\\"two years on\\\".\\nThe dataset is aligned with 6 challenges: action recognition (full and weak supervision), action detection, action anticipation, cross-modal retrieval (from captions), as well as unsupervised domain adaptation for action recognition. For each challenge, we define the task, provide baselines and evaluation metrics.\", \"PwC License Name\": \"CC BY-NC 4.0\", \"PwC License URL\": \"https://epic-kitchens.github.io/2021\", \"S2 Citation Count (June 2024)\": \"\", \"S2 Date\": \"\"}, \"Bibtex\": \"@Article{Damen2018ScalingEV,\\n author = {D. Damen and Hazel Doughty and G. Farinella and S. Fidler and Antonino Furnari and E. Kazakos and D. Moltisanti and Jonathan Munro and Toby Perrett and Will Price and Michael Wray},\\n booktitle = {arXiv.org},\\n journal = {ArXiv},\\n title = {Scaling Egocentric Vision: The EPIC-KITCHENS Dataset},\\n volume = {abs/1804.02748},\\n year = {2018}\\n}\\n\", \"License Use (DataProvenance)\": \"non-commercial\", \"License Attribution (DataProvenance)\": 1, \"License Share Alike (DataProvenance)\": 0, \"License Use (DataProvenance IgnoreOpenAI)\": \"non-commercial\", \"License Attribution (DataProvenance IgnoreOpenAI)\": 1, \"License Share Alike (DataProvenance IgnoreOpenAI)\": 0, \"License Type\": \"Non-Commercial/Academic\"}, {\"Unique Dataset Identifier\": \"didemo\", \"Collection\": \"didemo\", \"Collection URL\": \"https://paperswithcode.com/dataset/didemo\", \"Dataset Name\": \"DiDeMo: Localizing Moments in Video with Temporal Language (EMNLP 2018)\", \"Paper Title\": \"DiDeMo: Localizing Moments in Video with Temporal Language (EMNLP 2018)\", \"Paper URL\": \"https://paperswithcode.com/dataset/didemo\", \"GitHub URL\": \"https://github.com/LisaAnne/TemporalLanguageRelease\", \"Hugging Face URL\": \"\", \"Papers with Code URL\": \"\", \"ArXiv URL\": \"https://paperswithcode.com/dataset/didemo\", \"Semantic Scholar Corpus ID\": 52164739, \"Year Released\": \"2018\", \"Text Sources\": [\"flickr\"], \"Licenses\": [{\"License\": \"BSD 2-Clause License\", \"License URL\": \"https://github.com/LisaAnne/TemporalLanguageRelease\"}], \"Creators\": [\"UC Berkeley\"], \"Countries\": [\"United States of America\"], \"License Verified By\": \"Vivek Sharma\", \"Video Hours\": 275.0, \"Taken Down\": \"False\", \"Video Sources\": \"flickr\", \"Task Categories\": [\"Temporal Action Localization\"], \"Inferred Metadata\": {\"GitHub License\": \"\", \"GitHub Stars (June 2024)\": 40, \"GitHub Topics\": [], \"Github Date\": \"\", \"HF Config\": \"\", \"HF Config License\": \"\", \"HF Dataset\": \"\", \"HF Date\": \"\", \"HF Downloads (June 2024)\": \"\", \"HF Likes (June 2024)\": \"\", \"HF Yaml License\": \"\", \"PwC Date\": \"\", \"PwC Description\": \"\", \"PwC License Name\": \"\", \"PwC License URL\": \"\", \"S2 Citation Count (June 2024)\": \"\", \"S2 Date\": \"\"}, \"Bibtex\": \"@Article{Hendricks2018LocalizingMI,\\n author = {Lisa Anne Hendricks and Oliver Wang and Eli Shechtman and Josef Sivic and Trevor Darrell and Bryan C. Russell},\\n booktitle = {Conference on Empirical Methods in Natural Language Processing},\\n journal = {ArXiv},\\n title = {Localizing Moments in Video with Temporal Language},\\n volume = {abs/1809.01337},\\n year = {2018}\\n}\\n\", \"License Use (DataProvenance)\": \"commercial\", \"License Attribution (DataProvenance)\": 1, \"License Share Alike (DataProvenance)\": 0, \"License Use (DataProvenance IgnoreOpenAI)\": \"commercial\", \"License Attribution (DataProvenance IgnoreOpenAI)\": 1, \"License Share Alike (DataProvenance IgnoreOpenAI)\": 0, \"License Type\": \"Commercial\"}, {\"Unique Dataset Identifier\": \"how2\", \"Collection\": \"how2\", \"Collection URL\": \"https://arxiv.org/abs/1811.00347\", \"Dataset Name\": \"How2: A Large-scale Dataset for Multimodal Language Understanding (NeurIPS 2018)\", \"Paper Title\": \"How2: A Large-scale Dataset for Multimodal Language Understanding (NeurIPS 2018)\", \"Paper URL\": \"https://arxiv.org/abs/1811.00347\", \"GitHub URL\": \"https://github.com/srvk/how2-dataset\", \"Hugging Face URL\": \"\", \"Papers with Code URL\": \"\", \"ArXiv URL\": \"https://arxiv.org/abs/1811.00347\", \"Semantic Scholar Corpus ID\": 53186236, \"Year Released\": \"2018\", \"Text Sources\": [\"youtube\"], \"Licenses\": [{\"License\": \"Various\", \"License URL\": \"https://github.com/srvk/how2-dataset?tab=readme-ov-file#how2-license\"}], \"Creators\": [\"Carnegie Mellon University\"], \"Countries\": [\"United States of America\"], \"License Verified By\": \"Vivek Sharma\", \"Video Hours\": 2300.0, \"Taken Down\": \"False\", \"Video Sources\": \"youtube\", \"Task Categories\": [\"Video Captioning\"], \"Inferred Metadata\": {\"GitHub License\": \"\", \"GitHub Stars (June 2024)\": 150, \"GitHub Topics\": [\"corpus\", \"dataset\", \"how2-dataset\", \"language\", \"machine-translation\", \"multimodality\", \"speech-recognition\", \"video\"], \"Github Date\": \"\", \"HF Config\": \"\", \"HF Config License\": \"\", \"HF Dataset\": \"\", \"HF Date\": \"\", \"HF Downloads (June 2024)\": \"\", \"HF Likes (June 2024)\": \"\", \"HF Yaml License\": \"\", \"PwC Date\": \"\", \"PwC Description\": \"\", \"PwC License Name\": \"\", \"PwC License URL\": \"\", \"S2 Citation Count (June 2024)\": \"\", \"S2 Date\": \"\"}, \"Bibtex\": \"@Article{Sanabria2018How2AL,\\n author = {Ramon Sanabria and Ozan Caglayan and Shruti Palaskar and Desmond Elliott and Lo\\u00efc Barrault and Lucia Specia and Florian Metze},\\n booktitle = {Neural Information Processing Systems},\\n journal = {ArXiv},\\n title = {How2: A Large-scale Dataset for Multimodal Language Understanding},\\n volume = {abs/1811.00347},\\n year = {2018}\\n}\\n\", \"License Use (DataProvenance)\": \"academic-only\", \"License Attribution (DataProvenance)\": 1, \"License Share Alike (DataProvenance)\": 1, \"License Use (DataProvenance IgnoreOpenAI)\": \"academic-only\", \"License Attribution (DataProvenance IgnoreOpenAI)\": 1, \"License Share Alike (DataProvenance IgnoreOpenAI)\": 1, \"License Type\": \"Non-Commercial/Academic\"}, {\"Unique Dataset Identifier\": \"charades-ego\", \"Collection\": \"charades-ego\", \"Collection URL\": \"https://arxiv.org/abs/1804.09627\", \"Dataset Name\": \"Charades-Ego: Actor and Observer: Joint Modeling of First and Third-Person Videos (CVPR 2018)\", \"Paper Title\": \"Charades-Ego: Actor and Observer: Joint Modeling of First and Third-Person Videos (CVPR 2018)\", \"Paper URL\": \"https://arxiv.org/abs/1804.09627\", \"GitHub URL\": \"https://github.com/gsig/actor-observer\", \"Hugging Face URL\": \"\", \"Papers with Code URL\": \"https://paperswithcode.com/search?q_meta=&q_type=&q=Actor+and+Observer%3A+Joint+Modeling+of+First+and+Third-Person+Videos\", \"ArXiv URL\": \"https://arxiv.org/abs/1804.09627\", \"Semantic Scholar Corpus ID\": 4562167, \"Year Released\": \"2018\", \"Text Sources\": [\"human\"], \"Licenses\": [{\"License\": \"Custom\", \"License URL\": \"https://prior.allenai.org/projects/data/charades-ego/license.txt\"}], \"Creators\": [\"Allen Institute for AI\"], \"Countries\": [\"United States of America\"], \"License Verified By\": \"Vivek Sharma\", \"Video Hours\": 69.33, \"Taken Down\": \"False\", \"Video Sources\": \"human\", \"Task Categories\": [\"Video Classification\"], \"Inferred Metadata\": {\"GitHub License\": \"GNU General Public License v3.0\", \"GitHub Stars (June 2024)\": 75, \"GitHub Topics\": [], \"Github Date\": \"\", \"HF Config\": \"\", \"HF Config License\": \"\", \"HF Dataset\": \"\", \"HF Date\": \"\", \"HF Downloads (June 2024)\": \"\", \"HF Likes (June 2024)\": \"\", \"HF Yaml License\": \"\", \"PwC Date\": \"\", \"PwC Description\": \"\", \"PwC License Name\": \"\", \"PwC License URL\": \"\", \"S2 Citation Count (June 2024)\": \"\", \"S2 Date\": \"\"}, \"Bibtex\": \"@Book{Sigurdsson2018ActorAO,\\n author = {Gunnar A. Sigurdsson and A. Gupta and C. Schmid and Ali Farhadi and Alahari Karteek},\\n booktitle = {2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition},\\n journal = {2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition},\\n pages = {7396-7404},\\n title = {Actor and Observer: Joint Modeling of First and Third-Person Videos},\\n year = {2018}\\n}\\n\", \"License Use (DataProvenance)\": \"academic-only\", \"License Attribution (DataProvenance)\": 1, \"License Share Alike (DataProvenance)\": 0, \"License Use (DataProvenance IgnoreOpenAI)\": \"academic-only\", \"License Attribution (DataProvenance IgnoreOpenAI)\": 1, \"License Share Alike (DataProvenance IgnoreOpenAI)\": 0, \"License Type\": \"Non-Commercial/Academic\"}, {\"Unique Dataset Identifier\": \"soa-dataset\", \"Collection\": \"soa-dataset\", \"Collection URL\": \"https://arxiv.org/pdf/1904.11451\", \"Dataset Name\": \"Scenes-objects-actions: A multi-task, multi-label video dataset\", \"Paper Title\": \"Scenes-objects-actions: A multi-task, multi-label video dataset\", \"Paper URL\": \"https://arxiv.org/pdf/1904.11451\", \"GitHub URL\": \"\", \"Hugging Face URL\": \"\", \"Papers with Code URL\": \"https://paperswithcode.com/paper/scenes-objects-actions-a-multi-task-multi\", \"ArXiv URL\": \"https://arxiv.org/pdf/1904.11451\", \"Semantic Scholar Corpus ID\": 52968009, \"Year Released\": \"2018\", \"Text Sources\": [\"facebook\"], \"Licenses\": [{\"License\": \"Unspecified\", \"License URL\": \"\"}], \"Creators\": [\"Meta\"], \"Countries\": [\"United States of America\"], \"License Verified By\": \"Vivek Sharma\", \"Video Hours\": 1561.1, \"Taken Down\": \"False\", \"Video Sources\": \"Other\", \"Task Categories\": [\"Video Classification\"], \"Inferred Metadata\": {\"GitHub License\": \"\", \"GitHub Stars (June 2024)\": \"\", \"GitHub Topics\": \"\", \"Github Date\": \"\", \"HF Config\": \"\", \"HF Config License\": \"\", \"HF Dataset\": \"\", \"HF Date\": \"\", \"HF Downloads (June 2024)\": \"\", \"HF Likes (June 2024)\": \"\", \"HF Yaml License\": \"\", \"PwC Date\": \"\", \"PwC Description\": \"No code available yet.\", \"PwC License Name\": \"\", \"PwC License URL\": \"\", \"S2 Citation Count (June 2024)\": \"\", \"S2 Date\": \"\"}, \"Bibtex\": \"@Article{Ray2018ScenesObjectsActionsAM,\\n author = {Jamie Ray and Heng Wang and Du Tran and Yufei Wang and Matt Feiszli and L. Torresani and Manohar Paluri},\\n booktitle = {European Conference on Computer Vision},\\n pages = {660-676},\\n title = {Scenes-Objects-Actions: A Multi-task, Multi-label Video Dataset},\\n year = {2018}\\n}\\n\", \"License Use (DataProvenance)\": \"unspecified\", \"License Attribution (DataProvenance)\": 0, \"License Share Alike (DataProvenance)\": 0, \"License Use (DataProvenance IgnoreOpenAI)\": \"unspecified\", \"License Attribution (DataProvenance IgnoreOpenAI)\": 0, \"License Share Alike (DataProvenance IgnoreOpenAI)\": 0, \"License Type\": \"Unspecified\"}, {\"Unique Dataset Identifier\": \"vlog-vids\", \"Collection\": \"vlog-vids\", \"Collection URL\": \"https://arxiv.org/abs/1712.02310\", \"Dataset Name\": \"VLOG: From Lifestyle Vlogs to Everyday Interactions (CVPR 2018)\", \"Paper Title\": \"VLOG: From Lifestyle Vlogs to Everyday Interactions (CVPR 2018)\", \"Paper URL\": \"https://arxiv.org/abs/1712.02310\", \"GitHub URL\": \"\", \"Hugging Face URL\": \"\", \"Papers with Code URL\": \"https://paperswithcode.com/dataset/vlog-dataset\", \"ArXiv URL\": \"https://arxiv.org/abs/1712.02310\", \"Semantic Scholar Corpus ID\": 22264672, \"Year Released\": \"2018\", \"Text Sources\": [\"youtube\"], \"Licenses\": [{\"License\": \"Custom\", \"License URL\": \"https://web.eecs.umich.edu/~fouhey/2017/VLOG/index.html\"}], \"Creators\": [\"UC Berkeley\"], \"Countries\": [\"United States of America\"], \"License Verified By\": \"Vivek Sharma\", \"Video Hours\": 336.0, \"Taken Down\": \"False\", \"Video Sources\": \"youtube\", \"Task Categories\": [\"Video Classification\"], \"Inferred Metadata\": {\"GitHub License\": \"\", \"GitHub Stars (June 2024)\": \"\", \"GitHub Topics\": \"\", \"Github Date\": \"\", \"HF Config\": \"\", \"HF Config License\": \"\", \"HF Dataset\": \"\", \"HF Date\": \"\", \"HF Downloads (June 2024)\": \"\", \"HF Likes (June 2024)\": \"\", \"HF Yaml License\": \"\", \"PwC Date\": \"\", \"PwC Description\": \"A large collection of interaction-rich video data which are annotated and analyzed.\", \"PwC License Name\": \"Unspecified\", \"PwC License URL\": null, \"S2 Citation Count (June 2024)\": \"\", \"S2 Date\": \"\"}, \"Bibtex\": \"@Article{Fouhey2017FromLV,\\n author = {D. Fouhey and Weicheng Kuo and Alexei A. Efros and Jitendra Malik},\\n booktitle = {2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition},\\n journal = {2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition},\\n pages = {4991-5000},\\n title = {From Lifestyle Vlogs to Everyday Interactions},\\n year = {2017}\\n}\\n\", \"License Use (DataProvenance)\": \"academic-only\", \"License Attribution (DataProvenance)\": 1, \"License Share Alike (DataProvenance)\": 0, \"License Use (DataProvenance IgnoreOpenAI)\": \"academic-only\", \"License Attribution (DataProvenance IgnoreOpenAI)\": 1, \"License Share Alike (DataProvenance IgnoreOpenAI)\": 0, \"License Type\": \"Non-Commercial/Academic\"}, {\"Unique Dataset Identifier\": \"moviegraphs\", \"Collection\": \"moviegraphs\", \"Collection URL\": \"https://arxiv.org/pdf/1712.06761\", \"Dataset Name\": \"MovieGraphs\", \"Paper Title\": \"MovieGraphs\", \"Paper URL\": \"https://arxiv.org/pdf/1712.06761\", \"GitHub URL\": \"\", \"Hugging Face URL\": \"\", \"Papers with Code URL\": \"https://paperswithcode.com/dataset/moviegraphs\", \"ArXiv URL\": \"https://arxiv.org/pdf/1712.06761\", \"Semantic Scholar Corpus ID\": 4856028, \"Year Released\": \"2018\", \"Text Sources\": [\"movies\"], \"Licenses\": [{\"License\": \"Custom\", \"License URL\": \"http://moviegraphs.cs.toronto.edu/download.html\"}], \"Creators\": [\"Vector Institute for Artificial Intelligence\", \"Montreal Institute of Learning Algorithms (Mila)\", \"University of Toronto\"], \"Countries\": [\"Canada\"], \"License Verified By\": \"Vivek Sharma\", \"Video Hours\": 93.9, \"Taken Down\": \"True\", \"Video Sources\": \"movies\", \"Task Categories\": [\"Misc (video retrieval\", \"interaction understanding via ordering\", \"reason prediction)\"], \"Inferred Metadata\": {\"GitHub License\": \"\", \"GitHub Stars (June 2024)\": \"\", \"GitHub Topics\": \"\", \"Github Date\": \"\", \"HF Config\": \"\", \"HF Config License\": \"\", \"HF Dataset\": \"\", \"HF Date\": \"\", \"HF Downloads (June 2024)\": \"\", \"HF Likes (June 2024)\": \"\", \"HF Yaml License\": \"\", \"PwC Date\": \"\", \"PwC Description\": \"Provides detailed, graph-based annotations of social situations depicted in movie clips. Each graph consists of several types of nodes, to capture who is present in the clip, their emotional and physical attributes, their relationships (i.e., parent/child), and the interactions between them. Most interactions are associated with topics that provide additional details, and reasons that give motivations for actions.\", \"PwC License Name\": \"Academic Research Purposes Only\", \"PwC License URL\": \"https://docs.google.com/forms/d/e/1FAIpQLScytuCn4kRBKFPPei0t01Sfadpu8Qh5i9fFvfODWAAJGyEs7g/viewform\", \"S2 Citation Count (June 2024)\": \"\", \"S2 Date\": \"\"}, \"Bibtex\": \"@Article{Vicol2017MovieGraphsTU,\\n author = {Paul Vicol and Makarand Tapaswi and Llu\\u00eds Castrej\\u00f3n and S. Fidler},\\n booktitle = {2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition},\\n journal = {2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition},\\n pages = {8581-8590},\\n title = {MovieGraphs: Towards Understanding Human-Centric Situations from Videos},\\n year = {2017}\\n}\\n\", \"License Use (DataProvenance)\": \"academic-only\", \"License Attribution (DataProvenance)\": 1, \"License Share Alike (DataProvenance)\": 0, \"License Use (DataProvenance IgnoreOpenAI)\": \"academic-only\", \"License Attribution (DataProvenance IgnoreOpenAI)\": 1, \"License Share Alike (DataProvenance IgnoreOpenAI)\": 0, \"License Type\": \"Non-Commercial/Academic\"}, {\"Unique Dataset Identifier\": \"kinetics-600\", \"Collection\": \"kinetics-600\", \"Collection URL\": \"https://arxiv.org/abs/1808.01340\", \"Dataset Name\": \"Kinetics 600\", \"Paper Title\": \"Kinetics 600\", \"Paper URL\": \"https://arxiv.org/abs/1808.01340\", \"GitHub URL\": \"https://github.com/cvdfoundation/kinetics-dataset\", \"Hugging Face URL\": \"\", \"Papers with Code URL\": \"https://paperswithcode.com/dataset/kinetics-600\", \"ArXiv URL\": \"https://arxiv.org/abs/1808.01340\", \"Semantic Scholar Corpus ID\": 51927456, \"Year Released\": \"2018\", \"Text Sources\": [\"youtube\"], \"Licenses\": [{\"License\": \"Unspecified\", \"License URL\": \"\"}], \"Creators\": [\"DeepMind\"], \"Countries\": [\"United Kingdom\"], \"License Verified By\": \"Vivek Sharma\", \"Video Hours\": 1376.52, \"Taken Down\": \"False\", \"Video Sources\": \"youtube\", \"Task Categories\": [\"Video Classification\"], \"Inferred Metadata\": {\"GitHub License\": \"\", \"GitHub Stars (June 2024)\": 704, \"GitHub Topics\": [], \"Github Date\": \"\", \"HF Config\": \"\", \"HF Config License\": \"\", \"HF Dataset\": \"\", \"HF Date\": \"\", \"HF Downloads (June 2024)\": \"\", \"HF Likes (June 2024)\": \"\", \"HF Yaml License\": \"\", \"PwC Date\": \"2018-01-01\", \"PwC Description\": \"The Kinetics-600 is a large-scale action recognition dataset which consists of around 480K videos from 600 action categories. The 480K videos are divided into 390K, 30K, 60K for training, validation and test sets, respectively. Each video in the dataset is a 10-second clip of action moment annotated from raw YouTube video. It is an extensions of the Kinetics-400 dataset.\", \"PwC License Name\": \"CC BY 4.0\", \"PwC License URL\": \"https://creativecommons.org/licenses/by/4.0/\", \"S2 Citation Count (June 2024)\": \"\", \"S2 Date\": \"\"}, \"Bibtex\": \"@Article{Carreira2018ASN,\\n author = {Jo\\u00e3o Carreira and Eric Noland and Andras Banki-Horvath and Chloe Hillier and Andrew Zisserman},\\n booktitle = {arXiv.org},\\n journal = {ArXiv},\\n title = {A Short Note about Kinetics-600},\\n volume = {abs/1808.01340},\\n year = {2018}\\n}\\n\", \"License Use (DataProvenance)\": \"unspecified\", \"License Attribution (DataProvenance)\": 0, \"License Share Alike (DataProvenance)\": 0, \"License Use (DataProvenance IgnoreOpenAI)\": \"unspecified\", \"License Attribution (DataProvenance IgnoreOpenAI)\": 0, \"License Share Alike (DataProvenance IgnoreOpenAI)\": 0, \"License Type\": \"Unspecified\"}, {\"Unique Dataset Identifier\": \"howto100m\", \"Collection\": \"howto100m\", \"Collection URL\": \"https://arxiv.org/abs/1906.03327\", \"Dataset Name\": \"HowTo100M\", \"Paper Title\": \"HowTo100M\", \"Paper URL\": \"https://arxiv.org/abs/1906.03327\", \"GitHub URL\": \"https://github.com/antoine77340/howto100m\", \"Hugging Face URL\": \"\", \"Papers with Code URL\": \"https://paperswithcode.com/dataset/howto100m\", \"ArXiv URL\": \"https://arxiv.org/abs/1906.03327\", \"Semantic Scholar Corpus ID\": 182952863, \"Year Released\": \"2019\", \"Text Sources\": [\"youtube\"], \"Licenses\": [{\"License\": \"Unspecified\", \"License URL\": \"\"}], \"Creators\": [\"Ecole Normale Sup\\u00e9rieure\", \"Inria\", \"CIIRC\", \"Czech Technical University\"], \"Countries\": [\"France\", \"Czech Republic\"], \"License Verified By\": \"Vivek Sharma\", \"Video Hours\": 134472.0, \"Taken Down\": \"False\", \"Video Sources\": \"youtube\", \"Task Categories\": [\"Video Captioning\"], \"Inferred Metadata\": {\"GitHub License\": \"Apache License 2.0\", \"GitHub Stars (June 2024)\": 242, \"GitHub Topics\": [], \"Github Date\": \"\", \"HF Config\": \"\", \"HF Config License\": \"\", \"HF Dataset\": \"\", \"HF Date\": \"\", \"HF Downloads (June 2024)\": \"\", \"HF Likes (June 2024)\": \"\", \"HF Yaml License\": \"\", \"PwC Date\": \"2019-06-07\", \"PwC Description\": \"HowTo100M is a large-scale dataset of narrated videos with an emphasis on instructional videos where content creators teach complex tasks with an explicit intention of explaining the visual content on screen. HowTo100M features a total of:\\n\\n\\n136M video clips with captions sourced from 1.2M Youtube videos (15 years of video)\\n23k activities from domains such as cooking, hand crafting, personal care, gardening or fitness\\n\\nEach video is associated with a narration available as subtitles automatically downloaded from Youtube.\", \"PwC License Name\": \"Various\", \"PwC License URL\": \"https://www.di.ens.fr/willow/research/howto100m/\", \"S2 Citation Count (June 2024)\": \"\", \"S2 Date\": \"\"}, \"Bibtex\": \"@Article{Miech2019HowTo100MLA,\\n author = {Antoine Miech and Dimitri Zhukov and Jean-Baptiste Alayrac and Makarand Tapaswi and I. Laptev and Josef Sivic},\\n booktitle = {IEEE International Conference on Computer Vision},\\n journal = {2019 IEEE/CVF International Conference on Computer Vision (ICCV)},\\n pages = {2630-2640},\\n title = {HowTo100M: Learning a Text-Video Embedding by Watching Hundred Million Narrated Video Clips},\\n year = {2019}\\n}\\n\", \"License Use (DataProvenance)\": \"unspecified\", \"License Attribution (DataProvenance)\": 0, \"License Share Alike (DataProvenance)\": 0, \"License Use (DataProvenance IgnoreOpenAI)\": \"unspecified\", \"License Attribution (DataProvenance IgnoreOpenAI)\": 0, \"License Share Alike (DataProvenance IgnoreOpenAI)\": 0, \"License Type\": \"Unspecified\"}, {\"Unique Dataset Identifier\": \"coin-dataset\", \"Collection\": \"coin-dataset\", \"Collection URL\": \"https://arxiv.org/abs/1903.02874\", \"Dataset Name\": \"COIN: A Large-scale Dataset for Comprehensive Instructional Video Analysis (CVPR 2019)\", \"Paper Title\": \"COIN: A Large-scale Dataset for Comprehensive Instructional Video Analysis (CVPR 2019)\", \"Paper URL\": \"https://arxiv.org/abs/1903.02874\", \"GitHub URL\": \"https://github.com/coin-dataset/annotations\", \"Hugging Face URL\": \"\", \"Papers with Code URL\": \"https://paperswithcode.com/dataset/coin#:~:text=The%20COIN%20dataset%20(a%20large,are%20all%20collected%20from%20YouTube.\", \"ArXiv URL\": \"https://arxiv.org/abs/1903.02874\", \"Semantic Scholar Corpus ID\": 71147568, \"Year Released\": \"2019\", \"Text Sources\": [\"youtube\"], \"Licenses\": [{\"License\": \"Custom\", \"License URL\": \"https://github.com/coin-dataset/annotations?tab=readme-ov-file#license\"}], \"Creators\": [\"Tsinghua University\", \"Meitu Inc.\"], \"Countries\": [\"China\"], \"License Verified By\": \"Vivek Sharma\", \"Video Hours\": 476.0, \"Taken Down\": \"False\", \"Video Sources\": \"youtube\", \"Task Categories\": [\"Video Classification\"], \"Inferred Metadata\": {\"GitHub License\": \"\", \"GitHub Stars (June 2024)\": 100, \"GitHub Topics\": [], \"Github Date\": \"\", \"HF Config\": \"\", \"HF Config License\": \"\", \"HF Dataset\": \"\", \"HF Date\": \"\", \"HF Downloads (June 2024)\": \"\", \"HF Likes (June 2024)\": \"\", \"HF Yaml License\": \"\", \"PwC Date\": \"\", \"PwC Description\": \"The COIN dataset (a large-scale dataset for COmprehensive INstructional video analysis) consists of 11,827 videos related to 180 different tasks in 12 domains (e.g., vehicles, gadgets, etc.) related to our daily life. The videos are all collected from YouTube. The average length of a video is 2.36 minutes. Each video is labelled with 3.91 step segments, where each segment lasts 14.91 seconds on average. In total, the dataset contains videos of 476 hours, with 46,354 annotated segments.\", \"PwC License Name\": \"Various\", \"PwC License URL\": \"https://coin-dataset.github.io/\", \"S2 Citation Count (June 2024)\": \"\", \"S2 Date\": \"\"}, \"Bibtex\": \"@Article{Tang2019COINAL,\\n author = {Yansong Tang and Dajun Ding and Yongming Rao and Yu Zheng and Danyang Zhang and Lili Zhao and Jiwen Lu and Jie Zhou},\\n booktitle = {Computer Vision and Pattern Recognition},\\n journal = {2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},\\n pages = {1207-1216},\\n title = {COIN: A Large-Scale Dataset for Comprehensive Instructional Video Analysis},\\n year = {2019}\\n}\\n\", \"License Use (DataProvenance)\": \"academic-only\", \"License Attribution (DataProvenance)\": 1, \"License Share Alike (DataProvenance)\": 0, \"License Use (DataProvenance IgnoreOpenAI)\": \"academic-only\", \"License Attribution (DataProvenance IgnoreOpenAI)\": 1, \"License Share Alike (DataProvenance IgnoreOpenAI)\": 0, \"License Type\": \"Non-Commercial/Academic\"}, {\"Unique Dataset Identifier\": \"20BN-jester\", \"Collection\": \"20BN-jester\", \"Collection URL\": \"https://openaccess.thecvf.com/content_ICCVW_2019/papers/HANDS/Materzynska_The_Jester_Dataset_A_Large-Scale_Video_Dataset_of_Human_Gestures_ICCVW_2019_paper.pdf\", \"Dataset Name\": \"20BN-jester: The Jester Dataset: A Large-Scale Video Dataset of Human Gestures (ICCVW 2019)\", \"Paper Title\": \"20BN-jester: The Jester Dataset: A Large-Scale Video Dataset of Human Gestures (ICCVW 2019)\", \"Paper URL\": \"https://openaccess.thecvf.com/content_ICCVW_2019/papers/HANDS/Materzynska_The_Jester_Dataset_A_Large-Scale_Video_Dataset_of_Human_Gestures_ICCVW_2019_paper.pdf\", \"GitHub URL\": \"\", \"Hugging Face URL\": \"\", \"Papers with Code URL\": \"\", \"ArXiv URL\": \"https://openaccess.thecvf.com/content_ICCVW_2019/papers/HANDS/Materzynska_The_Jester_Dataset_A_Large-Scale_Video_Dataset_of_Human_Gestures_ICCVW_2019_paper.pdf\", \"Semantic Scholar Corpus ID\": 208010438, \"Year Released\": \"2019\", \"Text Sources\": [\"crowdsourced\"], \"Licenses\": [{\"License\": \"Custom\", \"License URL\": \"https://developer.qualcomm.com/software/ai-datasets/jester\"}], \"Creators\": [\"Twenty Billion Neurons GmbH\"], \"Countries\": [\"Germany\"], \"License Verified By\": \"Vivek Sharma\", \"Video Hours\": 13.0, \"Taken Down\": \"False\", \"Video Sources\": \"crowdsourced\", \"Task Categories\": [\"Video Classification\"], \"Inferred Metadata\": {\"GitHub License\": \"\", \"GitHub Stars (June 2024)\": \"\", \"GitHub Topics\": \"\", \"Github Date\": \"\", \"HF Config\": \"\", \"HF Config License\": \"\", \"HF Dataset\": \"\", \"HF Date\": \"\", \"HF Downloads (June 2024)\": \"\", \"HF Likes (June 2024)\": \"\", \"HF Yaml License\": \"\", \"PwC Date\": \"\", \"PwC Description\": \"\", \"PwC License Name\": \"\", \"PwC License URL\": \"\", \"S2 Citation Count (June 2024)\": \"\", \"S2 Date\": \"\"}, \"Bibtex\": \"@Article{Materzynska2019TheJD,\\n author = {Joanna Materzynska and Guillaume Berger and Ingo Bax and R. Memisevic},\\n booktitle = {2019 IEEE/CVF International Conference on Computer Vision Workshop (ICCVW)},\\n journal = {2019 IEEE/CVF International Conference on Computer Vision Workshop (ICCVW)},\\n pages = {2874-2882},\\n title = {The Jester Dataset: A Large-Scale Video Dataset of Human Gestures},\\n year = {2019}\\n}\\n\", \"License Use (DataProvenance)\": \"commercial\", \"License Attribution (DataProvenance)\": 1, \"License Share Alike (DataProvenance)\": 1, \"License Use (DataProvenance IgnoreOpenAI)\": \"commercial\", \"License Attribution (DataProvenance IgnoreOpenAI)\": 1, \"License Share Alike (DataProvenance IgnoreOpenAI)\": 1, \"License Type\": \"Commercial\"}, {\"Unique Dataset Identifier\": \"moments-in-time-dataset\", \"Collection\": \"moments-in-time-dataset\", \"Collection URL\": \"https://arxiv.org/pdf/1801.03150.pdf\", \"Dataset Name\": \"Moments in Time Dataset: one million videos for event understanding (TPAMI 2019)\", \"Paper Title\": \"Moments in Time Dataset: one million videos for event understanding (TPAMI 2019)\", \"Paper URL\": \"https://arxiv.org/pdf/1801.03150.pdf\", \"GitHub URL\": \"\", \"Hugging Face URL\": \"\", \"Papers with Code URL\": \"\", \"ArXiv URL\": \"https://arxiv.org/pdf/1801.03150.pdf\", \"Semantic Scholar Corpus ID\": 11868155, \"Year Released\": \"2019\", \"Text Sources\": [\"youtube\", \"flickr\", \"vine\", \"metacafe\", \"peeks\", \"vimeo\", \"videoblocks\", \"bing videos\", \"giphy\", \"weather channel\", \"getty-images\"], \"Licenses\": [{\"License\": \"Custom\", \"License URL\": \"http://moments.csail.mit.edu/\"}], \"Creators\": [\"Massachusetts Institute of Technology\"], \"Countries\": [\"United States of America\"], \"License Verified By\": \"Vivek Sharma\", \"Video Hours\": 833.0, \"Taken Down\": \"False\", \"Video Sources\": \"Other\", \"Task Categories\": [\"Video Classification\"], \"Inferred Metadata\": {\"GitHub License\": \"\", \"GitHub Stars (June 2024)\": \"\", \"GitHub Topics\": \"\", \"Github Date\": \"\", \"HF Config\": \"\", \"HF Config License\": \"\", \"HF Dataset\": \"\", \"HF Date\": \"\", \"HF Downloads (June 2024)\": \"\", \"HF Likes (June 2024)\": \"\", \"HF Yaml License\": \"\", \"PwC Date\": \"\", \"PwC Description\": \"\", \"PwC License Name\": \"\", \"PwC License URL\": \"\", \"S2 Citation Count (June 2024)\": \"\", \"S2 Date\": \"\"}, \"Bibtex\": \"@Article{Monfort2018MomentsIT,\\n author = {Mathew Monfort and Bolei Zhou and Sarah Adel Bargal and A. Andonian and Tom Yan and K. Ramakrishnan and L. Brown and Quanfu Fan and Dan Gutfreund and Carl Vondrick and A. Oliva},\\n booktitle = {IEEE Transactions on Pattern Analysis and Machine Intelligence},\\n journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},\\n pages = {502-508},\\n title = {Moments in Time Dataset: One Million Videos for Event Understanding},\\n volume = {42},\\n year = {2018}\\n}\\n\", \"License Use (DataProvenance)\": \"academic-only\", \"License Attribution (DataProvenance)\": 1, \"License Share Alike (DataProvenance)\": 0, \"License Use (DataProvenance IgnoreOpenAI)\": \"academic-only\", \"License Attribution (DataProvenance IgnoreOpenAI)\": 1, \"License Share Alike (DataProvenance IgnoreOpenAI)\": 0, \"License Type\": \"Non-Commercial/Academic\"}, {\"Unique Dataset Identifier\": \"mmact\", \"Collection\": \"mmact\", \"Collection URL\": \"https://ieeexplore.ieee.org/document/9009579\", \"Dataset Name\": \"MMAct\", \"Paper Title\": \"MMAct\", \"Paper URL\": \"https://ieeexplore.ieee.org/document/9009579\", \"GitHub URL\": \"\", \"Hugging Face URL\": \"\", \"Papers with Code URL\": \"https://paperswithcode.com/dataset/mmact\", \"ArXiv URL\": \"https://ieeexplore.ieee.org/document/9009579\", \"Semantic Scholar Corpus ID\": 207980205, \"Year Released\": \"2019\", \"Text Sources\": [\"human\"], \"Licenses\": [{\"License\": \"Custom\", \"License URL\": \"https://mmact19.github.io/2019/\"}], \"Creators\": [\"The Hong Kong University of Science and Technology\", \"Alibaba Group\"], \"Countries\": [\"Hong Kong\", \"China\"], \"License Verified By\": \"Vivek Sharma\", \"Video Hours\": 100.0, \"Taken Down\": \"False\", \"Video Sources\": \"human\", \"Task Categories\": [\"Temporal Localization\", \"Action Recognition\", \"Spatial-Temporal Action Localization\"], \"Inferred Metadata\": {\"GitHub License\": \"\", \"GitHub Stars (June 2024)\": \"\", \"GitHub Topics\": \"\", \"Github Date\": \"\", \"HF Config\": \"\", \"HF Config License\": \"\", \"HF Dataset\": \"\", \"HF Date\": \"\", \"HF Downloads (June 2024)\": \"\", \"HF Likes (June 2024)\": \"\", \"HF Yaml License\": \"\", \"PwC Date\": \"\", \"PwC Description\": \"MMAct is a large-scale dataset for multi/cross modal action understanding. This dataset has been recorded from 20 distinct subjects with seven different types of modalities: RGB videos, keypoints, acceleration, gyroscope, orientation, Wi-Fi and pressure signal. The dataset consists of more than 36k video clips for 37 action classes covering a wide range of daily life activities such as desktop-related and check-in-based ones in four different distinct scenarios.\", \"PwC License Name\": \"Unspecified\", \"PwC License URL\": null, \"S2 Citation Count (June 2024)\": \"\", \"S2 Date\": \"\"}, \"Bibtex\": \"@Article{Kong2019MMActAL,\\n author = {Quan Kong and Ziming Wu and Ziwei Deng and Martin Klinkigt and Bin Tong and Tomokazu Murakami},\\n booktitle = {IEEE International Conference on Computer Vision},\\n journal = {2019 IEEE/CVF International Conference on Computer Vision (ICCV)},\\n pages = {8657-8666},\\n title = {MMAct: A Large-Scale Dataset for Cross Modal Human Action Understanding},\\n year = {2019}\\n}\\n\", \"License Use (DataProvenance)\": \"academic-only\", \"License Attribution (DataProvenance)\": 1, \"License Share Alike (DataProvenance)\": 0, \"License Use (DataProvenance IgnoreOpenAI)\": \"academic-only\", \"License Attribution (DataProvenance IgnoreOpenAI)\": 1, \"License Share Alike (DataProvenance IgnoreOpenAI)\": 0, \"License Type\": \"Non-Commercial/Academic\"}, {\"Unique Dataset Identifier\": \"vatex\", \"Collection\": \"vatex\", \"Collection URL\": \"https://arxiv.org/abs/1904.03493\", \"Dataset Name\": \"VaTeX\", \"Paper Title\": \"VaTeX\", \"Paper URL\": \"https://arxiv.org/abs/1904.03493\", \"GitHub URL\": \"\", \"Hugging Face URL\": \"https://huggingface.co/datasets/HuggingFaceM4/vatex\", \"Papers with Code URL\": \"https://paperswithcode.com/dataset/vatex\", \"ArXiv URL\": \"https://arxiv.org/abs/1904.03493\", \"Semantic Scholar Corpus ID\": 102352148, \"Year Released\": \"2019\", \"Text Sources\": [\"youtube\"], \"Licenses\": [{\"License\": \"CC BY 4.0\", \"License URL\": \"https://eric-xw.github.io/vatex-website/index.html\"}], \"Creators\": [\"ByteDance AI Lab\", \"UC Santa Barbara\"], \"Countries\": [\"China\", \"United States of America\"], \"License Verified By\": \"Vivek Sharma\", \"Video Hours\": 114.58, \"Taken Down\": \"False\", \"Video Sources\": \"youtube\", \"Task Categories\": [\"Video Captioning\"], \"Inferred Metadata\": {\"GitHub License\": \"\", \"GitHub Stars (June 2024)\": \"\", \"GitHub Topics\": \"\", \"Github Date\": \"\", \"HF Config\": \"v1.1\", \"HF Config License\": \"\", \"HF Dataset\": \"HuggingFaceM4/vatex\", \"HF Date\": \"2022-05-13\", \"HF Downloads (June 2024)\": 112, \"HF Likes (June 2024)\": 4, \"HF Yaml License\": \"\", \"PwC Date\": \"2019-01-01\", \"PwC Description\": \"VATEX is multilingual, large, linguistically complex, and diverse dataset in terms of both video and natural language descriptions. It has two tasks for video-and-language research: (1) Multilingual Video Captioning, aimed at describing a video in various languages with a compact unified captioning model, and (2) Video-guided Machine Translation, to translate a source language description into the target language using the video information as additional spatiotemporal context.\", \"PwC License Name\": \"CC BY 4.0\", \"PwC License URL\": \"https://creativecommons.org/licenses/by/4.0/\", \"S2 Citation Count (June 2024)\": \"\", \"S2 Date\": \"\"}, \"Bibtex\": \"@Article{Wang2019VaTeXAL,\\n author = {Xin Eric Wang and Jiawei Wu and Junkun Chen and Lei Li and Yuan-fang Wang and William Yang Wang},\\n booktitle = {IEEE International Conference on Computer Vision},\\n journal = {2019 IEEE/CVF International Conference on Computer Vision (ICCV)},\\n pages = {4580-4590},\\n title = {VaTeX: A Large-Scale, High-Quality Multilingual Dataset for Video-and-Language Research},\\n year = {2019}\\n}\\n\", \"License Use (DataProvenance)\": \"commercial\", \"License Attribution (DataProvenance)\": 1, \"License Share Alike (DataProvenance)\": 0, \"License Use (DataProvenance IgnoreOpenAI)\": \"commercial\", \"License Attribution (DataProvenance IgnoreOpenAI)\": 1, \"License Share Alike (DataProvenance IgnoreOpenAI)\": 0, \"License Type\": \"Commercial\"}, {\"Unique Dataset Identifier\": \"moments-in-time-dataset\", \"Collection\": \"moments-in-time-dataset\", \"Collection URL\": \"https://arxiv.org/pdf/1801.03150.pdf\", \"Dataset Name\": \"Moments in Time Dataset: one million videos for event understanding (TPAMI 2019)\", \"Paper Title\": \"Moments in Time Dataset: one million videos for event understanding (TPAMI 2019)\", \"Paper URL\": \"https://arxiv.org/pdf/1801.03150.pdf\", \"GitHub URL\": \"\", \"Hugging Face URL\": \"\", \"Papers with Code URL\": \"\", \"ArXiv URL\": \"https://arxiv.org/pdf/1801.03150.pdf\", \"Semantic Scholar Corpus ID\": 11868155, \"Year Released\": \"2019\", \"Text Sources\": [\"youtube\", \"flickr\", \"vine\", \"metacafe\", \"peeks\", \"vimeo\", \"videoblocks\", \"bing videos\", \"giphy\", \"weather channel\", \"getty-images\"], \"Licenses\": [{\"License\": \"Custom\", \"License URL\": \"http://moments.csail.mit.edu/\"}], \"Creators\": [\"Massachusetts Institute of Technology\"], \"Countries\": [\"United States of America\"], \"License Verified By\": \"Vivek Sharma\", \"Video Hours\": 833.0, \"Taken Down\": \"False\", \"Video Sources\": \"Other\", \"Task Categories\": [\"Video Classification\"], \"Inferred Metadata\": {\"GitHub License\": \"\", \"GitHub Stars (June 2024)\": \"\", \"GitHub Topics\": \"\", \"Github Date\": \"\", \"HF Config\": \"\", \"HF Config License\": \"\", \"HF Dataset\": \"\", \"HF Date\": \"\", \"HF Downloads (June 2024)\": \"\", \"HF Likes (June 2024)\": \"\", \"HF Yaml License\": \"\", \"PwC Date\": \"\", \"PwC Description\": \"\", \"PwC License Name\": \"\", \"PwC License URL\": \"\", \"S2 Citation Count (June 2024)\": \"\", \"S2 Date\": \"\"}, \"Bibtex\": \"@Article{Monfort2018MomentsIT,\\n author = {Mathew Monfort and Bolei Zhou and Sarah Adel Bargal and A. Andonian and Tom Yan and K. Ramakrishnan and L. Brown and Quanfu Fan and Dan Gutfreund and Carl Vondrick and A. Oliva},\\n booktitle = {IEEE Transactions on Pattern Analysis and Machine Intelligence},\\n journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},\\n pages = {502-508},\\n title = {Moments in Time Dataset: One Million Videos for Event Understanding},\\n volume = {42},\\n year = {2018}\\n}\\n\", \"License Use (DataProvenance)\": \"academic-only\", \"License Attribution (DataProvenance)\": 1, \"License Share Alike (DataProvenance)\": 0, \"License Use (DataProvenance IgnoreOpenAI)\": \"academic-only\", \"License Attribution (DataProvenance IgnoreOpenAI)\": 1, \"License Share Alike (DataProvenance IgnoreOpenAI)\": 0, \"License Type\": \"Non-Commercial/Academic\"}, {\"Unique Dataset Identifier\": \"toyota-smarthome\", \"Collection\": \"toyota-smarthome\", \"Collection URL\": \"https://openaccess.thecvf.com/content_ICCV_2019/papers/Das_Toyota_Smarthome_Real-World_Activities_of_Daily_Living_ICCV_2019_paper.pdf\", \"Dataset Name\": \"Toyota Smarthome: Real-World Activities of Daily Living (ICCV 2019)\", \"Paper Title\": \"Toyota Smarthome: Real-World Activities of Daily Living (ICCV 2019)\", \"Paper URL\": \"https://openaccess.thecvf.com/content_ICCV_2019/papers/Das_Toyota_Smarthome_Real-World_Activities_of_Daily_Living_ICCV_2019_paper.pdf\", \"GitHub URL\": \"\", \"Hugging Face URL\": \"\", \"Papers with Code URL\": \"\", \"ArXiv URL\": \"https://openaccess.thecvf.com/content_ICCV_2019/papers/Das_Toyota_Smarthome_Real-World_Activities_of_Daily_Living_ICCV_2019_paper.pdf\", \"Semantic Scholar Corpus ID\": 207971208, \"Year Released\": \"2019\", \"Text Sources\": [\"human\"], \"Licenses\": [{\"License\": \"Custom\", \"License URL\": \"https://project.inria.fr/toyotasmarthome/files/2020/12/License_v2.pdf\"}], \"Creators\": [\"Toyota\"], \"Countries\": [\"France\"], \"License Verified By\": \"Vivek Sharma\", \"Video Hours\": 268.58, \"Taken Down\": \"False\", \"Video Sources\": \"human\", \"Task Categories\": [\"Video Classification\"], \"Inferred Metadata\": {\"GitHub License\": \"\", \"GitHub Stars (June 2024)\": \"\", \"GitHub Topics\": \"\", \"Github Date\": \"\", \"HF Config\": \"\", \"HF Config License\": \"\", \"HF Dataset\": \"\", \"HF Date\": \"\", \"HF Downloads (June 2024)\": \"\", \"HF Likes (June 2024)\": \"\", \"HF Yaml License\": \"\", \"PwC Date\": \"\", \"PwC Description\": \"\", \"PwC License Name\": \"\", \"PwC License URL\": \"\", \"S2 Citation Count (June 2024)\": \"\", \"S2 Date\": \"\"}, \"Bibtex\": \"@Article{Das2019ToyotaSR,\\n author = {Srijan Das and Rui Dai and Michal Koperski and Luca Minciullo and L. Garattoni and F. Br\\u00e9mond and G. Francesca},\\n booktitle = {IEEE International Conference on Computer Vision},\\n journal = {2019 IEEE/CVF International Conference on Computer Vision (ICCV)},\\n pages = {833-842},\\n title = {Toyota Smarthome: Real-World Activities of Daily Living},\\n year = {2019}\\n}\\n\", \"License Use (DataProvenance)\": \"academic-only\", \"License Attribution (DataProvenance)\": 1, \"License Share Alike (DataProvenance)\": 0, \"License Use (DataProvenance IgnoreOpenAI)\": \"academic-only\", \"License Attribution (DataProvenance IgnoreOpenAI)\": 1, \"License Share Alike (DataProvenance IgnoreOpenAI)\": 0, \"License Type\": \"Non-Commercial/Academic\"}, {\"Unique Dataset Identifier\": \"moments-in-time-dataset\", \"Collection\": \"moments-in-time-dataset\", \"Collection URL\": \"https://arxiv.org/pdf/1801.03150.pdf\", \"Dataset Name\": \"Moments in Time Dataset: one million videos for event understanding (TPAMI 2019)\", \"Paper Title\": \"Moments in Time Dataset: one million videos for event understanding (TPAMI 2019)\", \"Paper URL\": \"https://arxiv.org/pdf/1801.03150.pdf\", \"GitHub URL\": \"\", \"Hugging Face URL\": \"\", \"Papers with Code URL\": \"\", \"ArXiv URL\": \"https://arxiv.org/pdf/1801.03150.pdf\", \"Semantic Scholar Corpus ID\": 11868155, \"Year Released\": \"2019\", \"Text Sources\": [\"youtube\", \"flickr\", \"vine\", \"metacafe\", \"peeks\", \"vimeo\", \"videoblocks\", \"bing videos\", \"giphy\", \"weather channel\", \"getty-images\"], \"Licenses\": [{\"License\": \"Custom\", \"License URL\": \"http://moments.csail.mit.edu/\"}], \"Creators\": [\"Massachusetts Institute of Technology\"], \"Countries\": [\"United States of America\"], \"License Verified By\": \"Vivek Sharma\", \"Video Hours\": 833.0, \"Taken Down\": \"False\", \"Video Sources\": \"Other\", \"Task Categories\": [\"Video Classification\"], \"Inferred Metadata\": {\"GitHub License\": \"\", \"GitHub Stars (June 2024)\": \"\", \"GitHub Topics\": \"\", \"Github Date\": \"\", \"HF Config\": \"\", \"HF Config License\": \"\", \"HF Dataset\": \"\", \"HF Date\": \"\", \"HF Downloads (June 2024)\": \"\", \"HF Likes (June 2024)\": \"\", \"HF Yaml License\": \"\", \"PwC Date\": \"\", \"PwC Description\": \"\", \"PwC License Name\": \"\", \"PwC License URL\": \"\", \"S2 Citation Count (June 2024)\": \"\", \"S2 Date\": \"\"}, \"Bibtex\": \"@Article{Monfort2018MomentsIT,\\n author = {Mathew Monfort and Bolei Zhou and Sarah Adel Bargal and A. Andonian and Tom Yan and K. Ramakrishnan and L. Brown and Quanfu Fan and Dan Gutfreund and Carl Vondrick and A. Oliva},\\n booktitle = {IEEE Transactions on Pattern Analysis and Machine Intelligence},\\n journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},\\n pages = {502-508},\\n title = {Moments in Time Dataset: One Million Videos for Event Understanding},\\n volume = {42},\\n year = {2018}\\n}\\n\", \"License Use (DataProvenance)\": \"academic-only\", \"License Attribution (DataProvenance)\": 1, \"License Share Alike (DataProvenance)\": 0, \"License Use (DataProvenance IgnoreOpenAI)\": \"academic-only\", \"License Attribution (DataProvenance IgnoreOpenAI)\": 1, \"License Share Alike (DataProvenance IgnoreOpenAI)\": 0, \"License Type\": \"Non-Commercial/Academic\"}, {\"Unique Dataset Identifier\": \"trecvid\", \"Collection\": \"trecvid\", \"Collection URL\": \"https://arxiv.org/abs/2009.09984\", \"Dataset Name\": \"TRECVID\", \"Paper Title\": \"TRECVID\", \"Paper URL\": \"https://arxiv.org/abs/2009.09984\", \"GitHub URL\": \"\", \"Hugging Face URL\": \"\", \"Papers with Code URL\": \"https://paperswithcode.com/paper/trecvid-2019-an-evaluation-campaign-to/review/\", \"ArXiv URL\": \"https://arxiv.org/abs/2009.09984\", \"Semantic Scholar Corpus ID\": 212694843, \"Year Released\": \"2019\", \"Text Sources\": [\"undisclosed web\", \"bbc\"], \"Licenses\": [{\"License\": \"CC BY-NC-SA 4.0\", \"License URL\": \"https://trecvid.nist.gov/\"}], \"Creators\": [\"National Institute of Standards and Technology\"], \"Countries\": [\"United States of America\"], \"License Verified By\": \"Vivek Sharma\", \"Video Hours\": 1000.0, \"Taken Down\": \"False\", \"Video Sources\": \"undisclosed web\", \"Task Categories\": [\"Video Captioning\", \"Video Classification\"], \"Inferred Metadata\": {\"GitHub License\": \"\", \"GitHub Stars (June 2024)\": \"\", \"GitHub Topics\": \"\", \"Github Date\": \"\", \"HF Config\": \"\", \"HF Config License\": \"\", \"HF Dataset\": \"\", \"HF Date\": \"\", \"HF Downloads (June 2024)\": \"\", \"HF Likes (June 2024)\": \"\", \"HF Yaml License\": \"\", \"PwC Date\": \"\", \"PwC Description\": \"Paper tables with annotated results for TRECVID 2019: An Evaluation Campaign to Benchmark Video Activity Detection, Video Captioning and Matching, and Video Search & Retrieval\", \"PwC License Name\": \"\", \"PwC License URL\": \"\", \"S2 Citation Count (June 2024)\": \"\", \"S2 Date\": \"\"}, \"Bibtex\": \"@Article{Awad2019TRECVID2A,\\n author = {G. Awad and A. Butt and Keith Curtis and Yooyoung Lee and Jonathan G. Fiscus and A. Godil and Andrew Delgado and Jesse Zhang and Eliot Godard and Lukas L. Diduch and A. Smeaton and Yyette Graham and Wessel Kraaij and G. Qu\\u00e9not},\\n booktitle = {TREC Video Retrieval Evaluation},\\n journal = {ArXiv},\\n title = {TRECVID 2019: An evaluation campaign to benchmark Video Activity Detection, Video Captioning and Matching, and Video Search & retrieval},\\n volume = {abs/2009.09984},\\n year = {2019}\\n}\\n\", \"License Use (DataProvenance)\": \"non-commercial\", \"License Attribution (DataProvenance)\": 1, \"License Share Alike (DataProvenance)\": 1, \"License Use (DataProvenance IgnoreOpenAI)\": \"non-commercial\", \"License Attribution (DataProvenance IgnoreOpenAI)\": 1, \"License Share Alike (DataProvenance IgnoreOpenAI)\": 1, \"License Type\": \"Non-Commercial/Academic\"}, {\"Unique Dataset Identifier\": \"moments-in-time-dataset\", \"Collection\": \"moments-in-time-dataset\", \"Collection URL\": \"https://arxiv.org/pdf/1801.03150.pdf\", \"Dataset Name\": \"Moments in Time Dataset: one million videos for event understanding (TPAMI 2019)\", \"Paper Title\": \"Moments in Time Dataset: one million videos for event understanding (TPAMI 2019)\", \"Paper URL\": \"https://arxiv.org/pdf/1801.03150.pdf\", \"GitHub URL\": \"\", \"Hugging Face URL\": \"\", \"Papers with Code URL\": \"\", \"ArXiv URL\": \"https://arxiv.org/pdf/1801.03150.pdf\", \"Semantic Scholar Corpus ID\": 11868155, \"Year Released\": \"2019\", \"Text Sources\": [\"youtube\", \"flickr\", \"vine\", \"metacafe\", \"peeks\", \"vimeo\", \"videoblocks\", \"bing videos\", \"giphy\", \"weather channel\", \"getty-images\"], \"Licenses\": [{\"License\": \"Custom\", \"License URL\": \"http://moments.csail.mit.edu/\"}], \"Creators\": [\"Massachusetts Institute of Technology\"], \"Countries\": [\"United States of America\"], \"License Verified By\": \"Vivek Sharma\", \"Video Hours\": 833.0, \"Taken Down\": \"False\", \"Video Sources\": \"youtube\", \"Task Categories\": [\"Video Classification\"], \"Inferred Metadata\": {\"GitHub License\": \"\", \"GitHub Stars (June 2024)\": \"\", \"GitHub Topics\": \"\", \"Github Date\": \"\", \"HF Config\": \"\", \"HF Config License\": \"\", \"HF Dataset\": \"\", \"HF Date\": \"\", \"HF Downloads (June 2024)\": \"\", \"HF Likes (June 2024)\": \"\", \"HF Yaml License\": \"\", \"PwC Date\": \"\", \"PwC Description\": \"\", \"PwC License Name\": \"\", \"PwC License URL\": \"\", \"S2 Citation Count (June 2024)\": \"\", \"S2 Date\": \"\"}, \"Bibtex\": \"@Article{Monfort2018MomentsIT,\\n author = {Mathew Monfort and Bolei Zhou and Sarah Adel Bargal and A. Andonian and Tom Yan and K. Ramakrishnan and L. Brown and Quanfu Fan and Dan Gutfreund and Carl Vondrick and A. Oliva},\\n booktitle = {IEEE Transactions on Pattern Analysis and Machine Intelligence},\\n journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},\\n pages = {502-508},\\n title = {Moments in Time Dataset: One Million Videos for Event Understanding},\\n volume = {42},\\n year = {2018}\\n}\\n\", \"License Use (DataProvenance)\": \"academic-only\", \"License Attribution (DataProvenance)\": 1, \"License Share Alike (DataProvenance)\": 0, \"License Use (DataProvenance IgnoreOpenAI)\": \"academic-only\", \"License Attribution (DataProvenance IgnoreOpenAI)\": 1, \"License Share Alike (DataProvenance IgnoreOpenAI)\": 0, \"License Type\": \"Non-Commercial/Academic\"}, {\"Unique Dataset Identifier\": \"moments-in-time-dataset\", \"Collection\": \"moments-in-time-dataset\", \"Collection URL\": \"https://arxiv.org/pdf/1801.03150.pdf\", \"Dataset Name\": \"Moments in Time Dataset: one million videos for event understanding (TPAMI 2019)\", \"Paper Title\": \"Moments in Time Dataset: one million videos for event understanding (TPAMI 2019)\", \"Paper URL\": \"https://arxiv.org/pdf/1801.03150.pdf\", \"GitHub URL\": \"\", \"Hugging Face URL\": \"\", \"Papers with Code URL\": \"\", \"ArXiv URL\": \"https://arxiv.org/pdf/1801.03150.pdf\", \"Semantic Scholar Corpus ID\": 11868155, \"Year Released\": \"2019\", \"Text Sources\": [\"youtube\", \"flickr\", \"vine\", \"metacafe\", \"peeks\", \"vimeo\", \"videoblocks\", \"bing videos\", \"giphy\", \"weather channel\", \"getty-images\"], \"Licenses\": [{\"License\": \"Custom\", \"License URL\": \"http://moments.csail.mit.edu/\"}], \"Creators\": [\"Massachusetts Institute of Technology\"], \"Countries\": [\"United States of America\"], \"License Verified By\": \"Vivek Sharma\", \"Video Hours\": 833.0, \"Taken Down\": \"False\", \"Video Sources\": \"flickr\", \"Task Categories\": [\"Video Classification\"], \"Inferred Metadata\": {\"GitHub License\": \"\", \"GitHub Stars (June 2024)\": \"\", \"GitHub Topics\": \"\", \"Github Date\": \"\", \"HF Config\": \"\", \"HF Config License\": \"\", \"HF Dataset\": \"\", \"HF Date\": \"\", \"HF Downloads (June 2024)\": \"\", \"HF Likes (June 2024)\": \"\", \"HF Yaml License\": \"\", \"PwC Date\": \"\", \"PwC Description\": \"\", \"PwC License Name\": \"\", \"PwC License URL\": \"\", \"S2 Citation Count (June 2024)\": \"\", \"S2 Date\": \"\"}, \"Bibtex\": \"@Article{Monfort2018MomentsIT,\\n author = {Mathew Monfort and Bolei Zhou and Sarah Adel Bargal and A. Andonian and Tom Yan and K. Ramakrishnan and L. Brown and Quanfu Fan and Dan Gutfreund and Carl Vondrick and A. Oliva},\\n booktitle = {IEEE Transactions on Pattern Analysis and Machine Intelligence},\\n journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},\\n pages = {502-508},\\n title = {Moments in Time Dataset: One Million Videos for Event Understanding},\\n volume = {42},\\n year = {2018}\\n}\\n\", \"License Use (DataProvenance)\": \"academic-only\", \"License Attribution (DataProvenance)\": 1, \"License Share Alike (DataProvenance)\": 0, \"License Use (DataProvenance IgnoreOpenAI)\": \"academic-only\", \"License Attribution (DataProvenance IgnoreOpenAI)\": 1, \"License Share Alike (DataProvenance IgnoreOpenAI)\": 0, \"License Type\": \"Non-Commercial/Academic\"}, {\"Unique Dataset Identifier\": \"moments-in-time-dataset\", \"Collection\": \"moments-in-time-dataset\", \"Collection URL\": \"https://arxiv.org/pdf/1801.03150.pdf\", \"Dataset Name\": \"Moments in Time Dataset: one million videos for event understanding (TPAMI 2019)\", \"Paper Title\": \"Moments in Time Dataset: one million videos for event understanding (TPAMI 2019)\", \"Paper URL\": \"https://arxiv.org/pdf/1801.03150.pdf\", \"GitHub URL\": \"\", \"Hugging Face URL\": \"\", \"Papers with Code URL\": \"\", \"ArXiv URL\": \"https://arxiv.org/pdf/1801.03150.pdf\", \"Semantic Scholar Corpus ID\": 11868155, \"Year Released\": \"2019\", \"Text Sources\": [\"youtube\", \"flickr\", \"vine\", \"metacafe\", \"peeks\", \"vimeo\", \"videoblocks\", \"bing videos\", \"giphy\", \"weather channel\", \"getty-images\"], \"Licenses\": [{\"License\": \"Custom\", \"License URL\": \"http://moments.csail.mit.edu/\"}], \"Creators\": [\"Massachusetts Institute of Technology\"], \"Countries\": [\"United States of America\"], \"License Verified By\": \"Vivek Sharma\", \"Video Hours\": 833.0, \"Taken Down\": \"False\", \"Video Sources\": \"Other\", \"Task Categories\": [\"Video Classification\"], \"Inferred Metadata\": {\"GitHub License\": \"\", \"GitHub Stars (June 2024)\": \"\", \"GitHub Topics\": \"\", \"Github Date\": \"\", \"HF Config\": \"\", \"HF Config License\": \"\", \"HF Dataset\": \"\", \"HF Date\": \"\", \"HF Downloads (June 2024)\": \"\", \"HF Likes (June 2024)\": \"\", \"HF Yaml License\": \"\", \"PwC Date\": \"\", \"PwC Description\": \"\", \"PwC License Name\": \"\", \"PwC License URL\": \"\", \"S2 Citation Count (June 2024)\": \"\", \"S2 Date\": \"\"}, \"Bibtex\": \"@Article{Monfort2018MomentsIT,\\n author = {Mathew Monfort and Bolei Zhou and Sarah Adel Bargal and A. Andonian and Tom Yan and K. Ramakrishnan and L. Brown and Quanfu Fan and Dan Gutfreund and Carl Vondrick and A. Oliva},\\n booktitle = {IEEE Transactions on Pattern Analysis and Machine Intelligence},\\n journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},\\n pages = {502-508},\\n title = {Moments in Time Dataset: One Million Videos for Event Understanding},\\n volume = {42},\\n year = {2018}\\n}\\n\", \"License Use (DataProvenance)\": \"academic-only\", \"License Attribution (DataProvenance)\": 1, \"License Share Alike (DataProvenance)\": 0, \"License Use (DataProvenance IgnoreOpenAI)\": \"academic-only\", \"License Attribution (DataProvenance IgnoreOpenAI)\": 1, \"License Share Alike (DataProvenance IgnoreOpenAI)\": 0, \"License Type\": \"Non-Commercial/Academic\"}, {\"Unique Dataset Identifier\": \"crosstask\", \"Collection\": \"crosstask\", \"Collection URL\": \"https://arxiv.org/abs/1903.08225\", \"Dataset Name\": \"CrossTask: weakly supervised learning from instructional videos (CVPR 2019)\", \"Paper Title\": \"CrossTask: weakly supervised learning from instructional videos (CVPR 2019)\", \"Paper URL\": \"https://arxiv.org/abs/1903.08225\", \"GitHub URL\": \"https://github.com/DmZhukov/CrossTask\", \"Hugging Face URL\": \"\", \"Papers with Code URL\": \"https://paperswithcode.com/dataset/crosstask\", \"ArXiv URL\": \"https://arxiv.org/abs/1903.08225\", \"Semantic Scholar Corpus ID\": 84187266, \"Year Released\": \"2019\", \"Text Sources\": [\"youtube\"], \"Licenses\": [{\"License\": \"Unspecified\", \"License URL\": \"\"}], \"Creators\": [\"Ecole Normale Sup\\u00e9rieure\", \"Inria\", \"CIIRC\", \"Czech Technical University\", \"University of Michigan\"], \"Countries\": [\"France\", \"Turkey\", \"United States of America\", \"Czech Republic\"], \"License Verified By\": \"Vivek Sharma\", \"Video Hours\": 376.0, \"Taken Down\": \"False\", \"Video Sources\": \"youtube\", \"Task Categories\": [\"Temporal Action Localization\"], \"Inferred Metadata\": {\"GitHub License\": \"BSD 3-Clause License\", \"GitHub Stars (June 2024)\": 83, \"GitHub Topics\": [], \"Github Date\": \"\", \"HF Config\": \"\", \"HF Config License\": \"\", \"HF Dataset\": \"\", \"HF Date\": \"\", \"HF Downloads (June 2024)\": \"\", \"HF Likes (June 2024)\": \"\", \"HF Yaml License\": \"\", \"PwC Date\": \"2019-01-01\", \"PwC Description\": \"CrossTask dataset contains instructional videos, collected for 83 different tasks. For each task an ordered list of steps with manual descriptions is provided. The dataset is divided in two parts: 18 primary and 65 related tasks. Videos for the primary tasks are collected manually and provided with annotations for temporal step boundaries. Videos for the related tasks are collected automatically and don't have annotations.\", \"PwC License Name\": \"Unspecified\", \"PwC License URL\": null, \"S2 Citation Count (June 2024)\": \"\", \"S2 Date\": \"\"}, \"Bibtex\": \"@Article{Zhukov2019CrossTaskWS,\\n author = {Dimitri Zhukov and Jean-Baptiste Alayrac and R. G. Cinbis and D. Fouhey and I. Laptev and Josef Sivic},\\n booktitle = {Computer Vision and Pattern Recognition},\\n journal = {2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},\\n pages = {3532-3540},\\n title = {Cross-Task Weakly Supervised Learning From Instructional Videos},\\n year = {2019}\\n}\\n\", \"License Use (DataProvenance)\": \"unspecified\", \"License Attribution (DataProvenance)\": 0, \"License Share Alike (DataProvenance)\": 0, \"License Use (DataProvenance IgnoreOpenAI)\": \"unspecified\", \"License Attribution (DataProvenance IgnoreOpenAI)\": 0, \"License Share Alike (DataProvenance IgnoreOpenAI)\": 0, \"License Type\": \"Unspecified\"}, {\"Unique Dataset Identifier\": \"moments-in-time-dataset\", \"Collection\": \"moments-in-time-dataset\", \"Collection URL\": \"https://arxiv.org/pdf/1801.03150.pdf\", \"Dataset Name\": \"Moments in Time Dataset: one million videos for event understanding (TPAMI 2019)\", \"Paper Title\": \"Moments in Time Dataset: one million videos for event understanding (TPAMI 2019)\", \"Paper URL\": \"https://arxiv.org/pdf/1801.03150.pdf\", \"GitHub URL\": \"\", \"Hugging Face URL\": \"\", \"Papers with Code URL\": \"\", \"ArXiv URL\": \"https://arxiv.org/pdf/1801.03150.pdf\", \"Semantic Scholar Corpus ID\": 11868155, \"Year Released\": \"2019\", \"Text Sources\": [\"youtube\", \"flickr\", \"vine\", \"metacafe\", \"peeks\", \"vimeo\", \"videoblocks\", \"bing videos\", \"giphy\", \"weather channel\", \"getty-images\"], \"Licenses\": [{\"License\": \"Custom\", \"License URL\": \"http://moments.csail.mit.edu/\"}], \"Creators\": [\"Massachusetts Institute of Technology\"], \"Countries\": [\"United States of America\"], \"License Verified By\": \"Vivek Sharma\", \"Video Hours\": 833.0, \"Taken Down\": \"False\", \"Video Sources\": \"Other\", \"Task Categories\": [\"Video Classification\"], \"Inferred Metadata\": {\"GitHub License\": \"\", \"GitHub Stars (June 2024)\": \"\", \"GitHub Topics\": \"\", \"Github Date\": \"\", \"HF Config\": \"\", \"HF Config License\": \"\", \"HF Dataset\": \"\", \"HF Date\": \"\", \"HF Downloads (June 2024)\": \"\", \"HF Likes (June 2024)\": \"\", \"HF Yaml License\": \"\", \"PwC Date\": \"\", \"PwC Description\": \"\", \"PwC License Name\": \"\", \"PwC License URL\": \"\", \"S2 Citation Count (June 2024)\": \"\", \"S2 Date\": \"\"}, \"Bibtex\": \"@Article{Monfort2018MomentsIT,\\n author = {Mathew Monfort and Bolei Zhou and Sarah Adel Bargal and A. Andonian and Tom Yan and K. Ramakrishnan and L. Brown and Quanfu Fan and Dan Gutfreund and Carl Vondrick and A. Oliva},\\n booktitle = {IEEE Transactions on Pattern Analysis and Machine Intelligence},\\n journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},\\n pages = {502-508},\\n title = {Moments in Time Dataset: One Million Videos for Event Understanding},\\n volume = {42},\\n year = {2018}\\n}\\n\", \"License Use (DataProvenance)\": \"academic-only\", \"License Attribution (DataProvenance)\": 1, \"License Share Alike (DataProvenance)\": 0, \"License Use (DataProvenance IgnoreOpenAI)\": \"academic-only\", \"License Attribution (DataProvenance IgnoreOpenAI)\": 1, \"License Share Alike (DataProvenance IgnoreOpenAI)\": 0, \"License Type\": \"Non-Commercial/Academic\"}, {\"Unique Dataset Identifier\": \"moments-in-time-dataset\", \"Collection\": \"moments-in-time-dataset\", \"Collection URL\": \"https://arxiv.org/pdf/1801.03150.pdf\", \"Dataset Name\": \"Moments in Time Dataset: one million videos for event understanding (TPAMI 2019)\", \"Paper Title\": \"Moments in Time Dataset: one million videos for event understanding (TPAMI 2019)\", \"Paper URL\": \"https://arxiv.org/pdf/1801.03150.pdf\", \"GitHub URL\": \"\", \"Hugging Face URL\": \"\", \"Papers with Code URL\": \"\", \"ArXiv URL\": \"https://arxiv.org/pdf/1801.03150.pdf\", \"Semantic Scholar Corpus ID\": 11868155, \"Year Released\": \"2019\", \"Text Sources\": [\"youtube\", \"flickr\", \"vine\", \"metacafe\", \"peeks\", \"vimeo\", \"videoblocks\", \"bing videos\", \"giphy\", \"weather channel\", \"getty-images\"], \"Licenses\": [{\"License\": \"Custom\", \"License URL\": \"http://moments.csail.mit.edu/\"}], \"Creators\": [\"Massachusetts Institute of Technology\"], \"Countries\": [\"United States of America\"], \"License Verified By\": \"Vivek Sharma\", \"Video Hours\": 833.0, \"Taken Down\": \"False\", \"Video Sources\": \"Other\", \"Task Categories\": [\"Video Classification\"], \"Inferred Metadata\": {\"GitHub License\": \"\", \"GitHub Stars (June 2024)\": \"\", \"GitHub Topics\": \"\", \"Github Date\": \"\", \"HF Config\": \"\", \"HF Config License\": \"\", \"HF Dataset\": \"\", \"HF Date\": \"\", \"HF Downloads (June 2024)\": \"\", \"HF Likes (June 2024)\": \"\", \"HF Yaml License\": \"\", \"PwC Date\": \"\", \"PwC Description\": \"\", \"PwC License Name\": \"\", \"PwC License URL\": \"\", \"S2 Citation Count (June 2024)\": \"\", \"S2 Date\": \"\"}, \"Bibtex\": \"@Article{Monfort2018MomentsIT,\\n author = {Mathew Monfort and Bolei Zhou and Sarah Adel Bargal and A. Andonian and Tom Yan and K. Ramakrishnan and L. Brown and Quanfu Fan and Dan Gutfreund and Carl Vondrick and A. Oliva},\\n booktitle = {IEEE Transactions on Pattern Analysis and Machine Intelligence},\\n journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},\\n pages = {502-508},\\n title = {Moments in Time Dataset: One Million Videos for Event Understanding},\\n volume = {42},\\n year = {2018}\\n}\\n\", \"License Use (DataProvenance)\": \"academic-only\", \"License Attribution (DataProvenance)\": 1, \"License Share Alike (DataProvenance)\": 0, \"License Use (DataProvenance IgnoreOpenAI)\": \"academic-only\", \"License Attribution (DataProvenance IgnoreOpenAI)\": 1, \"License Share Alike (DataProvenance IgnoreOpenAI)\": 0, \"License Type\": \"Non-Commercial/Academic\"}, {\"Unique Dataset Identifier\": \"moments-in-time-dataset\", \"Collection\": \"moments-in-time-dataset\", \"Collection URL\": \"https://arxiv.org/pdf/1801.03150.pdf\", \"Dataset Name\": \"Moments in Time Dataset: one million videos for event understanding (TPAMI 2019)\", \"Paper Title\": \"Moments in Time Dataset: one million videos for event understanding (TPAMI 2019)\", \"Paper URL\": \"https://arxiv.org/pdf/1801.03150.pdf\", \"GitHub URL\": \"\", \"Hugging Face URL\": \"\", \"Papers with Code URL\": \"\", \"ArXiv URL\": \"https://arxiv.org/pdf/1801.03150.pdf\", \"Semantic Scholar Corpus ID\": 11868155, \"Year Released\": \"2019\", \"Text Sources\": [\"youtube\", \"flickr\", \"vine\", \"metacafe\", \"peeks\", \"vimeo\", \"videoblocks\", \"bing videos\", \"giphy\", \"weather channel\", \"getty-images\"], \"Licenses\": [{\"License\": \"Custom\", \"License URL\": \"http://moments.csail.mit.edu/\"}], \"Creators\": [\"Massachusetts Institute of Technology\"], \"Countries\": [\"United States of America\"], \"License Verified By\": \"Vivek Sharma\", \"Video Hours\": 833.0, \"Taken Down\": \"False\", \"Video Sources\": \"Other\", \"Task Categories\": [\"Video Classification\"], \"Inferred Metadata\": {\"GitHub License\": \"\", \"GitHub Stars (June 2024)\": \"\", \"GitHub Topics\": \"\", \"Github Date\": \"\", \"HF Config\": \"\", \"HF Config License\": \"\", \"HF Dataset\": \"\", \"HF Date\": \"\", \"HF Downloads (June 2024)\": \"\", \"HF Likes (June 2024)\": \"\", \"HF Yaml License\": \"\", \"PwC Date\": \"\", \"PwC Description\": \"\", \"PwC License Name\": \"\", \"PwC License URL\": \"\", \"S2 Citation Count (June 2024)\": \"\", \"S2 Date\": \"\"}, \"Bibtex\": \"@Article{Monfort2018MomentsIT,\\n author = {Mathew Monfort and Bolei Zhou and Sarah Adel Bargal and A. Andonian and Tom Yan and K. Ramakrishnan and L. Brown and Quanfu Fan and Dan Gutfreund and Carl Vondrick and A. Oliva},\\n booktitle = {IEEE Transactions on Pattern Analysis and Machine Intelligence},\\n journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},\\n pages = {502-508},\\n title = {Moments in Time Dataset: One Million Videos for Event Understanding},\\n volume = {42},\\n year = {2018}\\n}\\n\", \"License Use (DataProvenance)\": \"academic-only\", \"License Attribution (DataProvenance)\": 1, \"License Share Alike (DataProvenance)\": 0, \"License Use (DataProvenance IgnoreOpenAI)\": \"academic-only\", \"License Attribution (DataProvenance IgnoreOpenAI)\": 1, \"License Share Alike (DataProvenance IgnoreOpenAI)\": 0, \"License Type\": \"Non-Commercial/Academic\"}, {\"Unique Dataset Identifier\": \"moments-in-time-dataset\", \"Collection\": \"moments-in-time-dataset\", \"Collection URL\": \"https://arxiv.org/pdf/1801.03150.pdf\", \"Dataset Name\": \"Moments in Time Dataset: one million videos for event understanding (TPAMI 2019)\", \"Paper Title\": \"Moments in Time Dataset: one million videos for event understanding (TPAMI 2019)\", \"Paper URL\": \"https://arxiv.org/pdf/1801.03150.pdf\", \"GitHub URL\": \"\", \"Hugging Face URL\": \"\", \"Papers with Code URL\": \"\", \"ArXiv URL\": \"https://arxiv.org/pdf/1801.03150.pdf\", \"Semantic Scholar Corpus ID\": 11868155, \"Year Released\": \"2019\", \"Text Sources\": [\"youtube\", \"flickr\", \"vine\", \"metacafe\", \"peeks\", \"vimeo\", \"videoblocks\", \"bing videos\", \"giphy\", \"weather channel\", \"getty-images\"], \"Licenses\": [{\"License\": \"Custom\", \"License URL\": \"http://moments.csail.mit.edu/\"}], \"Creators\": [\"Massachusetts Institute of Technology\"], \"Countries\": [\"United States of America\"], \"License Verified By\": \"Vivek Sharma\", \"Video Hours\": 833.0, \"Taken Down\": \"False\", \"Video Sources\": \"Other\", \"Task Categories\": [\"Video Classification\"], \"Inferred Metadata\": {\"GitHub License\": \"\", \"GitHub Stars (June 2024)\": \"\", \"GitHub Topics\": \"\", \"Github Date\": \"\", \"HF Config\": \"\", \"HF Config License\": \"\", \"HF Dataset\": \"\", \"HF Date\": \"\", \"HF Downloads (June 2024)\": \"\", \"HF Likes (June 2024)\": \"\", \"HF Yaml License\": \"\", \"PwC Date\": \"\", \"PwC Description\": \"\", \"PwC License Name\": \"\", \"PwC License URL\": \"\", \"S2 Citation Count (June 2024)\": \"\", \"S2 Date\": \"\"}, \"Bibtex\": \"@Article{Monfort2018MomentsIT,\\n author = {Mathew Monfort and Bolei Zhou and Sarah Adel Bargal and A. Andonian and Tom Yan and K. Ramakrishnan and L. Brown and Quanfu Fan and Dan Gutfreund and Carl Vondrick and A. Oliva},\\n booktitle = {IEEE Transactions on Pattern Analysis and Machine Intelligence},\\n journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},\\n pages = {502-508},\\n title = {Moments in Time Dataset: One Million Videos for Event Understanding},\\n volume = {42},\\n year = {2018}\\n}\\n\", \"License Use (DataProvenance)\": \"academic-only\", \"License Attribution (DataProvenance)\": 1, \"License Share Alike (DataProvenance)\": 0, \"License Use (DataProvenance IgnoreOpenAI)\": \"academic-only\", \"License Attribution (DataProvenance IgnoreOpenAI)\": 1, \"License Share Alike (DataProvenance IgnoreOpenAI)\": 0, \"License Type\": \"Non-Commercial/Academic\"}, {\"Unique Dataset Identifier\": \"msa\", \"Collection\": \"msa\", \"Collection URL\": \"https://arxiv.org/abs/1910.11009\", \"Dataset Name\": \"MSA\", \"Paper Title\": \"MSA\", \"Paper URL\": \"https://arxiv.org/abs/1910.11009\", \"GitHub URL\": \"https://github.com/ycxioooong/MovieSynopsisAssociation\", \"Hugging Face URL\": \"\", \"Papers with Code URL\": \"https://paperswithcode.com/paper/a-graph-based-framework-to-bridge-movies-and-1\", \"ArXiv URL\": \"https://arxiv.org/abs/1910.11009\", \"Semantic Scholar Corpus ID\": 204852218, \"Year Released\": \"2019\", \"Text Sources\": [\"movies\"], \"Licenses\": [{\"License\": \"Unspecified\", \"License URL\": \"\"}], \"Creators\": [\"The Chinese University of Hong Kong\", \"UC Berkeley\"], \"Countries\": [\"China\", \"United States of America\"], \"License Verified By\": \"Vivek Sharma\", \"Video Hours\": 516.0, \"Taken Down\": \"False\", \"Video Sources\": \"movies\", \"Task Categories\": [\"Video Summarization\"], \"Inferred Metadata\": {\"GitHub License\": \"\", \"GitHub Stars (June 2024)\": 52, \"GitHub Topics\": [], \"Github Date\": \"\", \"HF Config\": \"\", \"HF Config License\": \"\", \"HF Dataset\": \"\", \"HF Date\": \"\", \"HF Downloads (June 2024)\": \"\", \"HF Likes (June 2024)\": \"\", \"HF Yaml License\": \"\", \"PwC Date\": \"\", \"PwC Description\": \"No code available yet.\", \"PwC License Name\": \"\", \"PwC License URL\": \"\", \"S2 Citation Count (June 2024)\": \"\", \"S2 Date\": \"\"}, \"Bibtex\": \"@Article{Xiong2019AGF,\\n author = {Yu Xiong and Qingqiu Huang and Lingfeng Guo and Hang Zhou and Bolei Zhou and Dahua Lin},\\n booktitle = {IEEE International Conference on Computer Vision},\\n journal = {2019 IEEE/CVF International Conference on Computer Vision (ICCV)},\\n pages = {4591-4600},\\n title = {A Graph-Based Framework to Bridge Movies and Synopses},\\n year = {2019}\\n}\\n\", \"License Use (DataProvenance)\": \"unspecified\", \"License Attribution (DataProvenance)\": 0, \"License Share Alike (DataProvenance)\": 0, \"License Use (DataProvenance IgnoreOpenAI)\": \"unspecified\", \"License Attribution (DataProvenance IgnoreOpenAI)\": 0, \"License Share Alike (DataProvenance IgnoreOpenAI)\": 0, \"License Type\": \"Unspecified\"}, {\"Unique Dataset Identifier\": \"moments-in-time-dataset\", \"Collection\": \"moments-in-time-dataset\", \"Collection URL\": \"https://arxiv.org/pdf/1801.03150.pdf\", \"Dataset Name\": \"Moments in Time Dataset: one million videos for event understanding (TPAMI 2019)\", \"Paper Title\": \"Moments in Time Dataset: one million videos for event understanding (TPAMI 2019)\", \"Paper URL\": \"https://arxiv.org/pdf/1801.03150.pdf\", \"GitHub URL\": \"\", \"Hugging Face URL\": \"\", \"Papers with Code URL\": \"\", \"ArXiv URL\": \"https://arxiv.org/pdf/1801.03150.pdf\", \"Semantic Scholar Corpus ID\": 11868155, \"Year Released\": \"2019\", \"Text Sources\": [\"youtube\", \"flickr\", \"vine\", \"metacafe\", \"peeks\", \"vimeo\", \"videoblocks\", \"bing videos\", \"giphy\", \"weather channel\", \"getty-images\"], \"Licenses\": [{\"License\": \"Custom\", \"License URL\": \"http://moments.csail.mit.edu/\"}], \"Creators\": [\"Massachusetts Institute of Technology\"], \"Countries\": [\"United States of America\"], \"License Verified By\": \"Vivek Sharma\", \"Video Hours\": 833.0, \"Taken Down\": \"False\", \"Video Sources\": \"Other\", \"Task Categories\": [\"Video Classification\"], \"Inferred Metadata\": {\"GitHub License\": \"\", \"GitHub Stars (June 2024)\": \"\", \"GitHub Topics\": \"\", \"Github Date\": \"\", \"HF Config\": \"\", \"HF Config License\": \"\", \"HF Dataset\": \"\", \"HF Date\": \"\", \"HF Downloads (June 2024)\": \"\", \"HF Likes (June 2024)\": \"\", \"HF Yaml License\": \"\", \"PwC Date\": \"\", \"PwC Description\": \"\", \"PwC License Name\": \"\", \"PwC License URL\": \"\", \"S2 Citation Count (June 2024)\": \"\", \"S2 Date\": \"\"}, \"Bibtex\": \"@Article{Monfort2018MomentsIT,\\n author = {Mathew Monfort and Bolei Zhou and Sarah Adel Bargal and A. Andonian and Tom Yan and K. Ramakrishnan and L. Brown and Quanfu Fan and Dan Gutfreund and Carl Vondrick and A. Oliva},\\n booktitle = {IEEE Transactions on Pattern Analysis and Machine Intelligence},\\n journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},\\n pages = {502-508},\\n title = {Moments in Time Dataset: One Million Videos for Event Understanding},\\n volume = {42},\\n year = {2018}\\n}\\n\", \"License Use (DataProvenance)\": \"academic-only\", \"License Attribution (DataProvenance)\": 1, \"License Share Alike (DataProvenance)\": 0, \"License Use (DataProvenance IgnoreOpenAI)\": \"academic-only\", \"License Attribution (DataProvenance IgnoreOpenAI)\": 1, \"License Share Alike (DataProvenance IgnoreOpenAI)\": 0, \"License Type\": \"Non-Commercial/Academic\"}, {\"Unique Dataset Identifier\": \"trecvid\", \"Collection\": \"trecvid\", \"Collection URL\": \"https://arxiv.org/abs/2009.09984\", \"Dataset Name\": \"TRECVID\", \"Paper Title\": \"TRECVID\", \"Paper URL\": \"https://arxiv.org/abs/2009.09984\", \"GitHub URL\": \"\", \"Hugging Face URL\": \"\", \"Papers with Code URL\": \"https://paperswithcode.com/paper/trecvid-2019-an-evaluation-campaign-to/review/\", \"ArXiv URL\": \"https://arxiv.org/abs/2009.09984\", \"Semantic Scholar Corpus ID\": 212694843, \"Year Released\": \"2019\", \"Text Sources\": [\"undisclosed web\", \"bbc\"], \"Licenses\": [{\"License\": \"CC BY-NC-SA 4.0\", \"License URL\": \"https://trecvid.nist.gov/\"}], \"Creators\": [\"National Institute of Standards and Technology\"], \"Countries\": [\"United States of America\"], \"License Verified By\": \"Vivek Sharma\", \"Video Hours\": 1000.0, \"Taken Down\": \"False\", \"Video Sources\": \"Other\", \"Task Categories\": [\"Video Captioning\", \"Video Classification\"], \"Inferred Metadata\": {\"GitHub License\": \"\", \"GitHub Stars (June 2024)\": \"\", \"GitHub Topics\": \"\", \"Github Date\": \"\", \"HF Config\": \"\", \"HF Config License\": \"\", \"HF Dataset\": \"\", \"HF Date\": \"\", \"HF Downloads (June 2024)\": \"\", \"HF Likes (June 2024)\": \"\", \"HF Yaml License\": \"\", \"PwC Date\": \"\", \"PwC Description\": \"Paper tables with annotated results for TRECVID 2019: An Evaluation Campaign to Benchmark Video Activity Detection, Video Captioning and Matching, and Video Search & Retrieval\", \"PwC License Name\": \"\", \"PwC License URL\": \"\", \"S2 Citation Count (June 2024)\": \"\", \"S2 Date\": \"\"}, \"Bibtex\": \"@Article{Awad2019TRECVID2A,\\n author = {G. Awad and A. Butt and Keith Curtis and Yooyoung Lee and Jonathan G. Fiscus and A. Godil and Andrew Delgado and Jesse Zhang and Eliot Godard and Lukas L. Diduch and A. Smeaton and Yyette Graham and Wessel Kraaij and G. Qu\\u00e9not},\\n booktitle = {TREC Video Retrieval Evaluation},\\n journal = {ArXiv},\\n title = {TRECVID 2019: An evaluation campaign to benchmark Video Activity Detection, Video Captioning and Matching, and Video Search & retrieval},\\n volume = {abs/2009.09984},\\n year = {2019}\\n}\\n\", \"License Use (DataProvenance)\": \"non-commercial\", \"License Attribution (DataProvenance)\": 1, \"License Share Alike (DataProvenance)\": 1, \"License Use (DataProvenance IgnoreOpenAI)\": \"non-commercial\", \"License Attribution (DataProvenance IgnoreOpenAI)\": 1, \"License Share Alike (DataProvenance IgnoreOpenAI)\": 1, \"License Type\": \"Non-Commercial/Academic\"}, {\"Unique Dataset Identifier\": \"ava-dataset\", \"Collection\": \"ava-dataset\", \"Collection URL\": \"https://arxiv.org/abs/1901.01342\", \"Dataset Name\": \"AVA Active Speaker\", \"Paper Title\": \"AVA Active Speaker\", \"Paper URL\": \"https://arxiv.org/abs/1901.01342\", \"GitHub URL\": \"https://github.com/cvdfoundation/ava-dataset\", \"Hugging Face URL\": \"\", \"Papers with Code URL\": \"https://paperswithcode.com/dataset/ava-activespeaker\", \"ArXiv URL\": \"https://arxiv.org/abs/1901.01342\", \"Semantic Scholar Corpus ID\": 216211909, \"Year Released\": \"2019\", \"Text Sources\": [\"Not Prohibited\"], \"Licenses\": [{\"License\": \"CC BY 4.0\", \"License URL\": \"https://research.google.com/ava/download.html#ava_active_speaker_download\"}], \"Creators\": [\"Google Research\"], \"Countries\": [\"United States of America\"], \"License Verified By\": \"Vivek Sharma\", \"Video Hours\": 38.5, \"Taken Down\": \"False\", \"Video Sources\": \"Other\", \"Task Categories\": [\"Video Classification\"], \"Inferred Metadata\": {\"GitHub License\": \"\", \"GitHub Stars (June 2024)\": 305, \"GitHub Topics\": [], \"Github Date\": \"\", \"HF Config\": \"\", \"HF Config License\": \"\", \"HF Dataset\": \"\", \"HF Date\": \"\", \"HF Downloads (June 2024)\": \"\", \"HF Likes (June 2024)\": \"\", \"HF Yaml License\": \"\", \"PwC Date\": \"\", \"PwC Description\": \"Contains temporally labeled face tracks in video, where each face instance is labeled as speaking or not, and whether the speech is audible. This dataset contains about 3.65 million human labeled frames or about 38.5 hours of face tracks, and the corresponding audio.\", \"PwC License Name\": \"Unspecified\", \"PwC License URL\": null, \"S2 Citation Count (June 2024)\": \"\", \"S2 Date\": \"\"}, \"Bibtex\": \"@Article{Roth2019AvaAS,\\n author = {Joseph Roth and Sourish Chaudhuri and Ondrej Klejch and Radhika Marvin and Andrew C. Gallagher and Liat Kaver and S. Ramaswamy and Arkadiusz Stopczynski and C. Schmid and Zhonghua Xi and C. Pantofaru},\\n booktitle = {IEEE International Conference on Acoustics, Speech, and Signal Processing},\\n journal = {ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},\\n pages = {4492-4496},\\n title = {Ava Active Speaker: An Audio-Visual Dataset for Active Speaker Detection},\\n year = {2019}\\n}\\n\", \"License Use (DataProvenance)\": \"commercial\", \"License Attribution (DataProvenance)\": 1, \"License Share Alike (DataProvenance)\": 0, \"License Use (DataProvenance IgnoreOpenAI)\": \"commercial\", \"License Attribution (DataProvenance IgnoreOpenAI)\": 1, \"License Share Alike (DataProvenance IgnoreOpenAI)\": 0, \"License Type\": \"Commercial\"}, {\"Unique Dataset Identifier\": \"hacs-dataset\", \"Collection\": \"hacs-dataset\", \"Collection URL\": \"https://arxiv.org/abs/1712.09374\", \"Dataset Name\": \"HACS: Human Action Clips and Segments Dataset for Recognition and Temporal Localization\", \"Paper Title\": \"HACS: Human Action Clips and Segments Dataset for Recognition and Temporal Localization\", \"Paper URL\": \"https://arxiv.org/abs/1712.09374\", \"GitHub URL\": \"https://github.com/hangzhaomit/HACS-dataset\", \"Hugging Face URL\": \"\", \"Papers with Code URL\": \"https://paperswithcode.com/dataset/hacs\", \"ArXiv URL\": \"https://arxiv.org/abs/1712.09374\", \"Semantic Scholar Corpus ID\": 68049510, \"Year Released\": \"2019\", \"Text Sources\": [\"youtube\"], \"Licenses\": [{\"License\": \"Custom\", \"License URL\": \"https://github.com/hangzhaomit/HACS-dataset?tab=readme-ov-file#request-testing-videos-and-missing-videos-new\"}], \"Creators\": [\"Massachusetts Institute of Technology\", \"Dartmouth University\", \"UIUC\"], \"Countries\": [\"United States of America\"], \"License Verified By\": \"Vivek Sharma\", \"Video Hours\": 833.0, \"Taken Down\": \"False\", \"Video Sources\": \"youtube\", \"Task Categories\": [\"Video Classification\"], \"Inferred Metadata\": {\"GitHub License\": \"BSD 3-Clause License\", \"GitHub Stars (June 2024)\": 184, \"GitHub Topics\": [], \"Github Date\": \"\", \"HF Config\": \"\", \"HF Config License\": \"\", \"HF Dataset\": \"\", \"HF Date\": \"\", \"HF Downloads (June 2024)\": \"\", \"HF Likes (June 2024)\": \"\", \"HF Yaml License\": \"\", \"PwC Date\": \"\", \"PwC Description\": \"HACS is a dataset for human action recognition. It uses a taxonomy of 200 action classes, which is identical to that of the ActivityNet-v1.3 dataset. It has 504K videos retrieved from YouTube. Each one is strictly shorter than 4 minutes, and the average length is 2.6 minutes. A total of 1.5M clips of 2-second duration are sparsely sampled by methods based on both uniform randomness and consensus/disagreement of image classifiers. 0.6M and 0.9M clips are annotated as positive and negative samples, respectively.\\n\\nAuthors split the collection into training, validation and testing sets of size 1.4M, 50K and 50K clips, which are sampled\\nfrom 492K, 6K and 6K videos, respectively.\", \"PwC License Name\": \"BSD 3-Clause License\", \"PwC License URL\": \"https://github.com/hangzhaomit/HACS-dataset/blob/master/LICENSE\", \"S2 Citation Count (June 2024)\": \"\", \"S2 Date\": \"\"}, \"Bibtex\": \"@Article{Zhao2017HACSHA,\\n author = {Hang Zhao and A. Torralba and L. Torresani and Zhicheng Yan},\\n booktitle = {IEEE International Conference on Computer Vision},\\n journal = {2019 IEEE/CVF International Conference on Computer Vision (ICCV)},\\n pages = {8667-8677},\\n title = {HACS: Human Action Clips and Segments Dataset for Recognition and Temporal Localization},\\n year = {2017}\\n}\\n\", \"License Use (DataProvenance)\": \"academic-only\", \"License Attribution (DataProvenance)\": 1, \"License Share Alike (DataProvenance)\": 0, \"License Use (DataProvenance IgnoreOpenAI)\": \"academic-only\", \"License Attribution (DataProvenance IgnoreOpenAI)\": 1, \"License Share Alike (DataProvenance IgnoreOpenAI)\": 0, \"License Type\": \"Non-Commercial/Academic\"}, {\"Unique Dataset Identifier\": \"lsmdc-ordering\", \"Collection\": \"lsmdc-ordering\", \"Collection URL\": \"https://arxiv.org/pdf/2004.02205\", \"Dataset Name\": \"LSMDC Ordering\", \"Paper Title\": \"LSMDC Ordering\", \"Paper URL\": \"https://arxiv.org/pdf/2004.02205\", \"GitHub URL\": \"https://github.com/vivoutlaw/TCBP\", \"Hugging Face URL\": \"\", \"Papers with Code URL\": \"https://paperswithcode.com/paper/deep-multimodal-feature-encoding-for-video\", \"ArXiv URL\": \"https://arxiv.org/pdf/2004.02205\", \"Semantic Scholar Corpus ID\": 214802821, \"Year Released\": \"2020\", \"Text Sources\": [\"movies\"], \"Licenses\": [{\"License\": \"MIT License\", \"License URL\": \"https://github.com/vivoutlaw/tcbp/blob/master/LICENSE\"}], \"Creators\": [\"University of Toronto\", \"Karlsruhe Institute of Technology\", \"Inria\", \"Massachusetts Institute of Technology\"], \"Countries\": [\"Germany\", \"Canada\", \"United States of America\", \"France\"], \"License Verified By\": \"Vivek Sharma\", \"Video Hours\": 158.0, \"Taken Down\": \"False\", \"Video Sources\": \"movies\", \"Task Categories\": [\"Video Classification\"], \"Inferred Metadata\": {\"GitHub License\": \"MIT License\", \"GitHub Stars (June 2024)\": 11, \"GitHub Topics\": [], \"Github Date\": \"\", \"HF Config\": \"\", \"HF Config License\": \"\", \"HF Dataset\": \"\", \"HF Date\": \"\", \"HF Downloads (June 2024)\": \"\", \"HF Likes (June 2024)\": \"\", \"HF Yaml License\": \"\", \"PwC Date\": \"\", \"PwC Description\": \"1 code implementation in PyTorch. True understanding of videos comes from a joint analysis of all its modalities: the video frames, the audio track, and any accompanying text such as closed captions. We present a way to learn a compact multimodal feature representation that encodes all these modalities. Our model parameters are learned through a proxy task of inferring the temporal ordering of a set of unordered videos in a timeline. To this end, we create a new multimodal dataset for temporal ordering that consists of approximately 30K scenes (2-6 clips per scene) based on the \\\"Large Scale Movie Description Challenge\\\". We analyze and evaluate the individual and joint modalities on three challenging tasks: (i) inferring the temporal ordering of a set of videos; and (ii) action recognition. We demonstrate empirically that multimodal representations are indeed complementary, and can play a key role in improving the performance of many applications.\", \"PwC License Name\": \"\", \"PwC License URL\": \"\", \"S2 Citation Count (June 2024)\": \"\", \"S2 Date\": \"\"}, \"Bibtex\": \"@Article{Sharma2020DeepMF,\\n author = {Vivek Sharma and Makarand Tapaswi and R. Stiefelhagen},\\n booktitle = {arXiv.org},\\n journal = {ArXiv},\\n title = {Deep Multimodal Feature Encoding for Video Ordering},\\n volume = {abs/2004.02205},\\n year = {2020}\\n}\\n\", \"License Use (DataProvenance)\": \"commercial\", \"License Attribution (DataProvenance)\": 1, \"License Share Alike (DataProvenance)\": 0, \"License Use (DataProvenance IgnoreOpenAI)\": \"commercial\", \"License Attribution (DataProvenance IgnoreOpenAI)\": 1, \"License Share Alike (DataProvenance IgnoreOpenAI)\": 0, \"License Type\": \"Commercial\"}, {\"Unique Dataset Identifier\": \"oops-dataset\", \"Collection\": \"oops-dataset\", \"Collection URL\": \"https://openaccess.thecvf.com/content_CVPR_2020/papers/Epstein_Oops_Predicting_Unintentional_Action_in_Video_CVPR_2020_paper.pdf\", \"Dataset Name\": \"Oops!: Predicting Unintentional Action in Video (CVPR 2020)\", \"Paper Title\": \"Oops!: Predicting Unintentional Action in Video (CVPR 2020)\", \"Paper URL\": \"https://openaccess.thecvf.com/content_CVPR_2020/papers/Epstein_Oops_Predicting_Unintentional_Action_in_Video_CVPR_2020_paper.pdf\", \"GitHub URL\": \"https://github.com/cvlab-columbia/oops\", \"Hugging Face URL\": \"\", \"Papers with Code URL\": \"https://paperswithcode.com/paper/oops-predicting-unintentional-action-in-video\", \"ArXiv URL\": \"https://openaccess.thecvf.com/content_CVPR_2020/papers/Epstein_Oops_Predicting_Unintentional_Action_in_Video_CVPR_2020_paper.pdf\", \"Semantic Scholar Corpus ID\": 208291335, \"Year Released\": \"2020\", \"Text Sources\": [\"youtube\"], \"Licenses\": [{\"License\": \"CC BY-NC-SA 4.0\", \"License URL\": \"https://oops.cs.columbia.edu/data/\"}], \"Creators\": [\"Columbia University\"], \"Countries\": [\"United States of America\"], \"License Verified By\": \"Vivek Sharma\", \"Video Hours\": 50.0, \"Taken Down\": \"False\", \"Video Sources\": \"youtube\", \"Task Categories\": [\"Video Classification\"], \"Inferred Metadata\": {\"GitHub License\": \"MIT License\", \"GitHub Stars (June 2024)\": 77, \"GitHub Topics\": [], \"Github Date\": \"\", \"HF Config\": \"\", \"HF Config License\": \"\", \"HF Dataset\": \"\", \"HF Date\": \"\", \"HF Downloads (June 2024)\": \"\", \"HF Likes (June 2024)\": \"\", \"HF Yaml License\": \"\", \"PwC Date\": \"\", \"PwC Description\": \"Implemented in one code library.\", \"PwC License Name\": \"\", \"PwC License URL\": \"\", \"S2 Citation Count (June 2024)\": \"\", \"S2 Date\": \"\"}, \"Bibtex\": \"@Article{Epstein2019OopsPU,\\n author = {Dave Epstein and Boyuan Chen and Carl Vondrick},\\n booktitle = {Computer Vision and Pattern Recognition},\\n journal = {2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},\\n pages = {916-926},\\n title = {Oops! Predicting Unintentional Action in Video},\\n year = {2019}\\n}\\n\", \"License Use (DataProvenance)\": \"non-commercial\", \"License Attribution (DataProvenance)\": 1, \"License Share Alike (DataProvenance)\": 1, \"License Use (DataProvenance IgnoreOpenAI)\": \"non-commercial\", \"License Attribution (DataProvenance IgnoreOpenAI)\": 1, \"License Share Alike (DataProvenance IgnoreOpenAI)\": 1, \"License Type\": \"Non-Commercial/Academic\"}, {\"Unique Dataset Identifier\": \"eev-dataset\", \"Collection\": \"eev-dataset\", \"Collection URL\": \"https://arxiv.org/abs/2001.05488\", \"Dataset Name\": \"EEV: A Large-Scale Dataset for Studying Evoked Expressions from Video\", \"Paper Title\": \"EEV: A Large-Scale Dataset for Studying Evoked Expressions from Video\", \"Paper URL\": \"https://arxiv.org/abs/2001.05488\", \"GitHub URL\": \"https://github.com/google-research-datasets/eev\", \"Hugging Face URL\": \"\", \"Papers with Code URL\": \"\", \"ArXiv URL\": \"https://arxiv.org/abs/2001.05488\", \"Semantic Scholar Corpus ID\": 210701992, \"Year Released\": \"2020\", \"Text Sources\": [\"undisclosed web\"], \"Licenses\": [{\"License\": \"CC BY 4.0\", \"License URL\": \"https://github.com/google-research-datasets/eev?tab=readme-ov-file#license\"}], \"Creators\": [\"Google Research\", \"California Institute of Technology\"], \"Countries\": [\"United States of America\"], \"License Verified By\": \"Vivek Sharma\", \"Video Hours\": 370.0, \"Taken Down\": \"False\", \"Video Sources\": \"undisclosed web\", \"Task Categories\": [\"Video Classification\"], \"Inferred Metadata\": {\"GitHub License\": \"\", \"GitHub Stars (June 2024)\": 34, \"GitHub Topics\": [], \"Github Date\": \"\", \"HF Config\": \"\", \"HF Config License\": \"\", \"HF Dataset\": \"\", \"HF Date\": \"\", \"HF Downloads (June 2024)\": \"\", \"HF Likes (June 2024)\": \"\", \"HF Yaml License\": \"\", \"PwC Date\": \"\", \"PwC Description\": \"\", \"PwC License Name\": \"\", \"PwC License URL\": \"\", \"S2 Citation Count (June 2024)\": \"\", \"S2 Date\": \"\"}, \"Bibtex\": \"@Article{Sun2020EEVDP,\\n author = {Jennifer J. Sun and Ting Liu and Alan S. Cowen and Florian Schroff and Hartwig Adam and Gautam Prasad},\\n booktitle = {arXiv.org},\\n journal = {ArXiv},\\n title = {EEV Dataset: Predicting Expressions Evoked by Diverse Videos},\\n volume = {abs/2001.05488},\\n year = {2020}\\n}\\n\", \"License Use (DataProvenance)\": \"commercial\", \"License Attribution (DataProvenance)\": 1, \"License Share Alike (DataProvenance)\": 0, \"License Use (DataProvenance IgnoreOpenAI)\": \"commercial\", \"License Attribution (DataProvenance IgnoreOpenAI)\": 1, \"License Share Alike (DataProvenance IgnoreOpenAI)\": 0, \"License Type\": \"Commercial\"}, {\"Unique Dataset Identifier\": \"omnisource-web-dataset\", \"Collection\": \"omnisource-web-dataset\", \"Collection URL\": \"https://arxiv.org/abs/2003.13042\", \"Dataset Name\": \"OmniSource Web Dataset\", \"Paper Title\": \"OmniSource Web Dataset\", \"Paper URL\": \"https://arxiv.org/abs/2003.13042\", \"GitHub URL\": \"https://github.com/open-mmlab/mmaction\", \"Hugging Face URL\": \"\", \"Papers with Code URL\": \"https://paperswithcode.com/paper/omni-sourced-webly-supervised-learning-for#code\", \"ArXiv URL\": \"https://arxiv.org/abs/2003.13042\", \"Semantic Scholar Corpus ID\": 214714240, \"Year Released\": \"2020\", \"Text Sources\": [\"google videos\", \"instagram\", \"youtube\"], \"Licenses\": [{\"License\": \"Apache License 2.0\", \"License URL\": \"https://github.com/open-mmlab/mmaction?tab=readme-ov-file#license\"}], \"Creators\": [\"The Chinese University of Hong Kong\"], \"Countries\": [\"China\"], \"License Verified By\": \"Vivek Sharma\", \"Video Hours\": 13333.0, \"Taken Down\": \"False\", \"Video Sources\": \"youtube\", \"Task Categories\": [\"Video Classification\"], \"Inferred Metadata\": {\"GitHub License\": \"Apache License 2.0\", \"GitHub Stars (June 2024)\": 1856, \"GitHub Topics\": [\"action-detection\", \"action-recognition\", \"pytorch\", \"spatial-temporal-action-detection\", \"temporal-action-detection\", \"temporal-action-localization\", \"video-understanding\"], \"Github Date\": \"\", \"HF Config\": \"\", \"HF Config License\": \"\", \"HF Dataset\": \"\", \"HF Date\": \"\", \"HF Downloads (June 2024)\": \"\", \"HF Likes (June 2024)\": \"\", \"HF Yaml License\": \"\", \"PwC Date\": \"\", \"PwC Description\": \"3 code implementations in PyTorch. We introduce OmniSource, a novel framework for leveraging web data to train video recognition models. OmniSource overcomes the barriers between data formats, such as images, short videos, and long untrimmed videos for webly-supervised learning. First, data samples with multiple formats, curated by task-specific data collection and automatically filtered by a teacher model, are transformed into a unified form. Then a joint-training strategy is proposed to deal with the domain gaps between multiple data sources and formats in webly-supervised learning. Several good practices, including data balancing, resampling, and cross-dataset mixup are adopted in joint training. Experiments show that by utilizing data from multiple sources and formats, OmniSource is more data-efficient in training. With only 3.5M images and 800K minutes videos crawled from the internet without human labeling (less than 2% of prior works), our models learned with OmniSource improve Top-1 accuracy of 2D- and 3D-ConvNet baseline models by 3.0% and 3.9%, respectively, on the Kinetics-400 benchmark. With OmniSource, we establish new records with different pretraining strategies for video recognition. Our best models achieve 80.4%, 80.5%, and 83.6 Top-1 accuracies on the Kinetics-400 benchmark respectively for training-from-scratch, ImageNet pre-training and IG-65M pre-training.\", \"PwC License Name\": \"\", \"PwC License URL\": \"\", \"S2 Citation Count (June 2024)\": \"\", \"S2 Date\": \"\"}, \"Bibtex\": \"@Article{Duan2020OmnisourcedWL,\\n author = {Haodong Duan and Yue Zhao and Yuanjun Xiong and Wentao Liu and Dahua Lin},\\n booktitle = {European Conference on Computer Vision},\\n journal = {ArXiv},\\n title = {Omni-sourced Webly-supervised Learning for Video Recognition},\\n volume = {abs/2003.13042},\\n year = {2020}\\n}\\n\", \"License Use (DataProvenance)\": \"commercial\", \"License Attribution (DataProvenance)\": 1, \"License Share Alike (DataProvenance)\": 0, \"License Use (DataProvenance IgnoreOpenAI)\": \"commercial\", \"License Attribution (DataProvenance IgnoreOpenAI)\": 1, \"License Share Alike (DataProvenance IgnoreOpenAI)\": 0, \"License Type\": \"Commercial\"}, {\"Unique Dataset Identifier\": \"omnisource-web-dataset\", \"Collection\": \"omnisource-web-dataset\", \"Collection URL\": \"https://arxiv.org/abs/2003.13042\", \"Dataset Name\": \"OmniSource Web Dataset\", \"Paper Title\": \"OmniSource Web Dataset\", \"Paper URL\": \"https://arxiv.org/abs/2003.13042\", \"GitHub URL\": \"https://github.com/open-mmlab/mmaction\", \"Hugging Face URL\": \"\", \"Papers with Code URL\": \"https://paperswithcode.com/paper/omni-sourced-webly-supervised-learning-for#code\", \"ArXiv URL\": \"https://arxiv.org/abs/2003.13042\", \"Semantic Scholar Corpus ID\": 214714240, \"Year Released\": \"2020\", \"Text Sources\": [\"google videos\", \"instagram\", \"youtube\"], \"Licenses\": [{\"License\": \"Apache License 2.0\", \"License URL\": \"https://github.com/open-mmlab/mmaction?tab=readme-ov-file#license\"}], \"Creators\": [\"The Chinese University of Hong Kong\"], \"Countries\": [\"China\"], \"License Verified By\": \"Vivek Sharma\", \"Video Hours\": 13333.0, \"Taken Down\": \"False\", \"Video Sources\": \"Other\", \"Task Categories\": [\"Video Classification\"], \"Inferred Metadata\": {\"GitHub License\": \"Apache License 2.0\", \"GitHub Stars (June 2024)\": 1856, \"GitHub Topics\": [\"action-detection\", \"action-recognition\", \"pytorch\", \"spatial-temporal-action-detection\", \"temporal-action-detection\", \"temporal-action-localization\", \"video-understanding\"], \"Github Date\": \"\", \"HF Config\": \"\", \"HF Config License\": \"\", \"HF Dataset\": \"\", \"HF Date\": \"\", \"HF Downloads (June 2024)\": \"\", \"HF Likes (June 2024)\": \"\", \"HF Yaml License\": \"\", \"PwC Date\": \"\", \"PwC Description\": \"3 code implementations in PyTorch. We introduce OmniSource, a novel framework for leveraging web data to train video recognition models. OmniSource overcomes the barriers between data formats, such as images, short videos, and long untrimmed videos for webly-supervised learning. First, data samples with multiple formats, curated by task-specific data collection and automatically filtered by a teacher model, are transformed into a unified form. Then a joint-training strategy is proposed to deal with the domain gaps between multiple data sources and formats in webly-supervised learning. Several good practices, including data balancing, resampling, and cross-dataset mixup are adopted in joint training. Experiments show that by utilizing data from multiple sources and formats, OmniSource is more data-efficient in training. With only 3.5M images and 800K minutes videos crawled from the internet without human labeling (less than 2% of prior works), our models learned with OmniSource improve Top-1 accuracy of 2D- and 3D-ConvNet baseline models by 3.0% and 3.9%, respectively, on the Kinetics-400 benchmark. With OmniSource, we establish new records with different pretraining strategies for video recognition. Our best models achieve 80.4%, 80.5%, and 83.6 Top-1 accuracies on the Kinetics-400 benchmark respectively for training-from-scratch, ImageNet pre-training and IG-65M pre-training.\", \"PwC License Name\": \"\", \"PwC License URL\": \"\", \"S2 Citation Count (June 2024)\": \"\", \"S2 Date\": \"\"}, \"Bibtex\": \"@Article{Duan2020OmnisourcedWL,\\n author = {Haodong Duan and Yue Zhao and Yuanjun Xiong and Wentao Liu and Dahua Lin},\\n booktitle = {European Conference on Computer Vision},\\n journal = {ArXiv},\\n title = {Omni-sourced Webly-supervised Learning for Video Recognition},\\n volume = {abs/2003.13042},\\n year = {2020}\\n}\\n\", \"License Use (DataProvenance)\": \"commercial\", \"License Attribution (DataProvenance)\": 1, \"License Share Alike (DataProvenance)\": 0, \"License Use (DataProvenance IgnoreOpenAI)\": \"commercial\", \"License Attribution (DataProvenance IgnoreOpenAI)\": 1, \"License Share Alike (DataProvenance IgnoreOpenAI)\": 0, \"License Type\": \"Commercial\"}, {\"Unique Dataset Identifier\": \"omnisource-web-dataset\", \"Collection\": \"omnisource-web-dataset\", \"Collection URL\": \"https://arxiv.org/abs/2003.13042\", \"Dataset Name\": \"OmniSource Web Dataset\", \"Paper Title\": \"OmniSource Web Dataset\", \"Paper URL\": \"https://arxiv.org/abs/2003.13042\", \"GitHub URL\": \"https://github.com/open-mmlab/mmaction\", \"Hugging Face URL\": \"\", \"Papers with Code URL\": \"https://paperswithcode.com/paper/omni-sourced-webly-supervised-learning-for#code\", \"ArXiv URL\": \"https://arxiv.org/abs/2003.13042\", \"Semantic Scholar Corpus ID\": 214714240, \"Year Released\": \"2020\", \"Text Sources\": [\"google videos\", \"instagram\", \"youtube\"], \"Licenses\": [{\"License\": \"Apache License 2.0\", \"License URL\": \"https://github.com/open-mmlab/mmaction?tab=readme-ov-file#license\"}], \"Creators\": [\"The Chinese University of Hong Kong\"], \"Countries\": [\"China\"], \"License Verified By\": \"Vivek Sharma\", \"Video Hours\": 13333.0, \"Taken Down\": \"False\", \"Video Sources\": \"Other\", \"Task Categories\": [\"Video Classification\"], \"Inferred Metadata\": {\"GitHub License\": \"Apache License 2.0\", \"GitHub Stars (June 2024)\": 1856, \"GitHub Topics\": [\"action-detection\", \"action-recognition\", \"pytorch\", \"spatial-temporal-action-detection\", \"temporal-action-detection\", \"temporal-action-localization\", \"video-understanding\"], \"Github Date\": \"\", \"HF Config\": \"\", \"HF Config License\": \"\", \"HF Dataset\": \"\", \"HF Date\": \"\", \"HF Downloads (June 2024)\": \"\", \"HF Likes (June 2024)\": \"\", \"HF Yaml License\": \"\", \"PwC Date\": \"\", \"PwC Description\": \"3 code implementations in PyTorch. We introduce OmniSource, a novel framework for leveraging web data to train video recognition models. OmniSource overcomes the barriers between data formats, such as images, short videos, and long untrimmed videos for webly-supervised learning. First, data samples with multiple formats, curated by task-specific data collection and automatically filtered by a teacher model, are transformed into a unified form. Then a joint-training strategy is proposed to deal with the domain gaps between multiple data sources and formats in webly-supervised learning. Several good practices, including data balancing, resampling, and cross-dataset mixup are adopted in joint training. Experiments show that by utilizing data from multiple sources and formats, OmniSource is more data-efficient in training. With only 3.5M images and 800K minutes videos crawled from the internet without human labeling (less than 2% of prior works), our models learned with OmniSource improve Top-1 accuracy of 2D- and 3D-ConvNet baseline models by 3.0% and 3.9%, respectively, on the Kinetics-400 benchmark. With OmniSource, we establish new records with different pretraining strategies for video recognition. Our best models achieve 80.4%, 80.5%, and 83.6 Top-1 accuracies on the Kinetics-400 benchmark respectively for training-from-scratch, ImageNet pre-training and IG-65M pre-training.\", \"PwC License Name\": \"\", \"PwC License URL\": \"\", \"S2 Citation Count (June 2024)\": \"\", \"S2 Date\": \"\"}, \"Bibtex\": \"@Article{Duan2020OmnisourcedWL,\\n author = {Haodong Duan and Yue Zhao and Yuanjun Xiong and Wentao Liu and Dahua Lin},\\n booktitle = {European Conference on Computer Vision},\\n journal = {ArXiv},\\n title = {Omni-sourced Webly-supervised Learning for Video Recognition},\\n volume = {abs/2003.13042},\\n year = {2020}\\n}\\n\", \"License Use (DataProvenance)\": \"commercial\", \"License Attribution (DataProvenance)\": 1, \"License Share Alike (DataProvenance)\": 0, \"License Use (DataProvenance IgnoreOpenAI)\": \"commercial\", \"License Attribution (DataProvenance IgnoreOpenAI)\": 1, \"License Share Alike (DataProvenance IgnoreOpenAI)\": 0, \"License Type\": \"Commercial\"}, {\"Unique Dataset Identifier\": \"finegym-dataset\", \"Collection\": \"finegym-dataset\", \"Collection URL\": \"https://arxiv.org/abs/2004.06704\", \"Dataset Name\": \"FineGym: A Hierarchical Video Dataset for Fine-grained Action Understanding (CVPR 2020)\", \"Paper Title\": \"FineGym: A Hierarchical Video Dataset for Fine-grained Action Understanding (CVPR 2020)\", \"Paper URL\": \"https://arxiv.org/abs/2004.06704\", \"GitHub URL\": \"https://github.com/SDOlivia/FineGym/\", \"Hugging Face URL\": \"\", \"Papers with Code URL\": \"https://paperswithcode.com/dataset/finegym\", \"ArXiv URL\": \"https://arxiv.org/abs/2004.06704\", \"Semantic Scholar Corpus ID\": 215754360, \"Year Released\": \"2020\", \"Text Sources\": [\"undisclosed web\"], \"Licenses\": [{\"License\": \"CC BY-NC 4.0\", \"License URL\": \"https://sdolivia.github.io/FineGym/\"}], \"Creators\": [\"The Chinese University of Hong Kong\"], \"Countries\": [\"China\"], \"License Verified By\": \"Vivek Sharma\", \"Video Hours\": 708.0, \"Taken Down\": \"False\", \"Video Sources\": \"undisclosed web\", \"Task Categories\": [\"Video Classification\"], \"Inferred Metadata\": {\"GitHub License\": \"\", \"GitHub Stars (June 2024)\": 124, \"GitHub Topics\": [], \"Github Date\": \"\", \"HF Config\": \"\", \"HF Config License\": \"\", \"HF Dataset\": \"\", \"HF Date\": \"\", \"HF Downloads (June 2024)\": \"\", \"HF Likes (June 2024)\": \"\", \"HF Yaml License\": \"\", \"PwC Date\": \"2020-04-14\", \"PwC Description\": \"FineGym is an action recognition dataset build on top of gymnasium videos. Compared to existing action recognition datasets, FineGym is distinguished in richness, quality, and diversity. In particular, it provides temporal annotations at both action and sub-action levels with a three-level semantic hierarchy. For example, a \\\"balance beam\\\" event will be annotated as a sequence of elementary sub-actions derived from five sets: \\\"leap-jumphop\\\", \\\"beam-turns\\\", \\\"flight-salto\\\", \\\"flight-handspring\\\", and \\\"dismount\\\", where the sub-action in each set will be further annotated with finely defined class labels. This new level of granularity presents significant challenges for action recognition, e.g. how to parse the temporal structures from a coherent action, and how to distinguish between subtly different action classes.\", \"PwC License Name\": \"CC BY-NC 4.0\", \"PwC License URL\": \"https://creativecommons.org/licenses/by-nc/4.0/\", \"S2 Citation Count (June 2024)\": \"\", \"S2 Date\": \"\"}, \"Bibtex\": \"@Article{Shao2020FineGymAH,\\n author = {Dian Shao and Yue Zhao and Bo Dai and Dahua Lin},\\n booktitle = {Computer Vision and Pattern Recognition},\\n journal = {2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},\\n pages = {2613-2622},\\n title = {FineGym: A Hierarchical Video Dataset for Fine-Grained Action Understanding},\\n year = {2020}\\n}\\n\", \"License Use (DataProvenance)\": \"non-commercial\", \"License Attribution (DataProvenance)\": 1, \"License Share Alike (DataProvenance)\": 0, \"License Use (DataProvenance IgnoreOpenAI)\": \"non-commercial\", \"License Attribution (DataProvenance IgnoreOpenAI)\": 1, \"License Share Alike (DataProvenance IgnoreOpenAI)\": 0, \"License Type\": \"Non-Commercial/Academic\"}, {\"Unique Dataset Identifier\": \"condensed-movies\", \"Collection\": \"condensed-movies\", \"Collection URL\": \"https://arxiv.org/pdf/2005.04208\", \"Dataset Name\": \"Condensed Movies\", \"Paper Title\": \"Condensed Movies\", \"Paper URL\": \"https://arxiv.org/pdf/2005.04208\", \"GitHub URL\": \"https://github.com/m-bain/CondensedMovies\", \"Hugging Face URL\": \"\", \"Papers with Code URL\": \"https://paperswithcode.com/dataset/condensed-movies\", \"ArXiv URL\": \"https://arxiv.org/pdf/2005.04208\", \"Semantic Scholar Corpus ID\": 218571391, \"Year Released\": \"2020\", \"Text Sources\": [\"movies\"], \"Licenses\": [{\"License\": \"CC BY 4.0\", \"License URL\": \"https://www.robots.ox.ac.uk/~vgg/data/condensed-movies/#download\"}], \"Creators\": [\"University of Oxford\"], \"Countries\": [\"United Kingdom\"], \"License Verified By\": \"Vivek Sharma\", \"Video Hours\": 1270.0, \"Taken Down\": \"False\", \"Video Sources\": \"movies\", \"Task Categories\": [\"Video Summarization\"], \"Inferred Metadata\": {\"GitHub License\": \"\", \"GitHub Stars (June 2024)\": 153, \"GitHub Topics\": [\"dataset\", \"precomputed-features\", \"retrieval\", \"source-videos\", \"video-text-retrieval\"], \"Github Date\": \"\", \"HF Config\": \"\", \"HF Config License\": \"\", \"HF Dataset\": \"\", \"HF Date\": \"\", \"HF Downloads (June 2024)\": \"\", \"HF Likes (June 2024)\": \"\", \"HF Yaml License\": \"\", \"PwC Date\": \"2020-05-08\", \"PwC Description\": \"A large-scale video dataset, featuring clips from movies with detailed captions.\", \"PwC License Name\": \"Unspecified\", \"PwC License URL\": null, \"S2 Citation Count (June 2024)\": \"\", \"S2 Date\": \"\"}, \"Bibtex\": \"@Article{Bain2020CondensedMS,\\n author = {Max Bain and Arsha Nagrani and A. Brown and Andrew Zisserman},\\n booktitle = {Asian Conference on Computer Vision},\\n journal = {ArXiv},\\n title = {Condensed Movies: Story Based Retrieval with Contextual Embeddings},\\n volume = {abs/2005.04208},\\n year = {2020}\\n}\\n\", \"License Use (DataProvenance)\": \"commercial\", \"License Attribution (DataProvenance)\": 1, \"License Share Alike (DataProvenance)\": 0, \"License Use (DataProvenance IgnoreOpenAI)\": \"commercial\", \"License Attribution (DataProvenance IgnoreOpenAI)\": 1, \"License Share Alike (DataProvenance IgnoreOpenAI)\": 0, \"License Type\": \"Commercial\"}, {\"Unique Dataset Identifier\": \"haa500-dataset\", \"Collection\": \"haa500-dataset\", \"Collection URL\": \"https://arxiv.org/abs/2009.05224\", \"Dataset Name\": \"HAA500: Human-Centric Atomic Action Dataset with Curated Videos (ICCV 2021)\", \"Paper Title\": \"HAA500: Human-Centric Atomic Action Dataset with Curated Videos (ICCV 2021)\", \"Paper URL\": \"https://arxiv.org/abs/2009.05224\", \"GitHub URL\": \"\", \"Hugging Face URL\": \"\", \"Papers with Code URL\": \"https://paperswithcode.com/paper/haa500-human-centric-atomic-action-dataset\", \"ArXiv URL\": \"https://arxiv.org/abs/2009.05224\", \"Semantic Scholar Corpus ID\": 221640805, \"Year Released\": \"2020\", \"Text Sources\": [\"human\"], \"Licenses\": [{\"License\": \"Unspecified\", \"License URL\": \"\"}], \"Creators\": [\"Carnegie Mellon University\", \"The Hong Kong University of Science and Technology\", \"Princeton University\", \"Kuaishou Technology\"], \"Countries\": [\"China\", \"United States of America\"], \"License Verified By\": \"Vivek Sharma\", \"Video Hours\": 5.48, \"Taken Down\": \"False\", \"Video Sources\": \"human\", \"Task Categories\": [\"Video Classification\"], \"Inferred Metadata\": {\"GitHub License\": \"\", \"GitHub Stars (June 2024)\": \"\", \"GitHub Topics\": \"\", \"Github Date\": \"\", \"HF Config\": \"\", \"HF Config License\": \"\", \"HF Dataset\": \"\", \"HF Date\": \"\", \"HF Downloads (June 2024)\": \"\", \"HF Likes (June 2024)\": \"\", \"HF Yaml License\": \"\", \"PwC Date\": \"\", \"PwC Description\": \"\\ud83c\\udfc6 SOTA for Action Recognition on HAA500 (Top-1 (%) metric)\", \"PwC License Name\": \"\", \"PwC License URL\": \"\", \"S2 Citation Count (June 2024)\": \"\", \"S2 Date\": \"\"}, \"Bibtex\": \"@Article{Chung2020HAA500HA,\\n author = {Jihoon Chung and Cheng-hsin Wuu and Hsuan-ru Yang and Yu-Wing Tai and Chi-Keung Tang},\\n booktitle = {IEEE International Conference on Computer Vision},\\n journal = {2021 IEEE/CVF International Conference on Computer Vision (ICCV)},\\n pages = {13445-13454},\\n title = {HAA500: Human-Centric Atomic Action Dataset with Curated Videos},\\n year = {2020}\\n}\\n\", \"License Use (DataProvenance)\": \"unspecified\", \"License Attribution (DataProvenance)\": 0, \"License Share Alike (DataProvenance)\": 0, \"License Use (DataProvenance IgnoreOpenAI)\": \"unspecified\", \"License Attribution (DataProvenance IgnoreOpenAI)\": 0, \"License Share Alike (DataProvenance IgnoreOpenAI)\": 0, \"License Type\": \"Unspecified\"}, {\"Unique Dataset Identifier\": \"movie-net\", \"Collection\": \"movie-net\", \"Collection URL\": \"https://arxiv.org/abs/2007.10937\", \"Dataset Name\": \"MovieNet\", \"Paper Title\": \"MovieNet\", \"Paper URL\": \"https://arxiv.org/abs/2007.10937\", \"GitHub URL\": \"https://github.com/movienet/movienet-tools\", \"Hugging Face URL\": \"\", \"Papers with Code URL\": \"https://paperswithcode.com/dataset/movienet\", \"ArXiv URL\": \"https://arxiv.org/abs/2007.10937\", \"Semantic Scholar Corpus ID\": 220665753, \"Year Released\": \"2020\", \"Text Sources\": [\"movies\"], \"Licenses\": [{\"License\": \"Unspecified\", \"License URL\": \"\"}], \"Creators\": [\"The Chinese University of Hong Kong\"], \"Countries\": [\"China\"], \"License Verified By\": \"Vivek Sharma\", \"Video Hours\": 3000.0, \"Taken Down\": \"False\", \"Video Sources\": \"movies\", \"Task Categories\": [\"Video Summarization\", \"Misc\"], \"Inferred Metadata\": {\"GitHub License\": \"\", \"GitHub Stars (June 2024)\": 256, \"GitHub Topics\": [\"action-recognition\", \"computer-vision\", \"cross-modality\", \"deep-learning\", \"movie\", \"person-analysis\", \"shot-detection\", \"video-understanding\", \"vision-language\"], \"Github Date\": \"\", \"HF Config\": \"\", \"HF Config License\": \"\", \"HF Dataset\": \"\", \"HF Date\": \"\", \"HF Downloads (June 2024)\": \"\", \"HF Likes (June 2024)\": \"\", \"HF Yaml License\": \"\", \"PwC Date\": \"2020-07-21\", \"PwC Description\": \"MovieNet is a holistic dataset for movie understanding. MovieNet contains 1,100 movies with a large amount of multi-modal data, e.g. trailers, photos, plot descriptions, etc.. Besides, different aspects of manual annotations are provided in MovieNet, including 1.1M characters with bounding boxes and identities, 42K scene boundaries, 2.5K aligned description sentences, 65K tags of place and action, and 92 K tags of cinematic style.\", \"PwC License Name\": \"Unspecified\", \"PwC License URL\": null, \"S2 Citation Count (June 2024)\": \"\", \"S2 Date\": \"\"}, \"Bibtex\": \"@Article{Huang2020MovieNetAH,\\n author = {Qingqiu Huang and Yu Xiong and Anyi Rao and Jiaze Wang and Dahua Lin},\\n booktitle = {European Conference on Computer Vision},\\n pages = {709-727},\\n title = {MovieNet: A Holistic Dataset for Movie Understanding},\\n year = {2020}\\n}\\n\", \"License Use (DataProvenance)\": \"unspecified\", \"License Attribution (DataProvenance)\": 0, \"License Share Alike (DataProvenance)\": 0, \"License Use (DataProvenance IgnoreOpenAI)\": \"unspecified\", \"License Attribution (DataProvenance IgnoreOpenAI)\": 0, \"License Share Alike (DataProvenance IgnoreOpenAI)\": 0, \"License Type\": \"Unspecified\"}, {\"Unique Dataset Identifier\": \"titan\", \"Collection\": \"titan\", \"Collection URL\": \"https://openaccess.thecvf.com/content_CVPR_2020/papers/Malla_TITAN_Future_Forecast_Using_Action_Priors_CVPR_2020_paper.pdf\", \"Dataset Name\": \"TITAN: Future Forecast using Action Priors (CVPR 2020)\", \"Paper Title\": \"TITAN: Future Forecast using Action Priors (CVPR 2020)\", \"Paper URL\": \"https://openaccess.thecvf.com/content_CVPR_2020/papers/Malla_TITAN_Future_Forecast_Using_Action_Priors_CVPR_2020_paper.pdf\", \"GitHub URL\": \"\", \"Hugging Face URL\": \"\", \"Papers with Code URL\": \"https://paperswithcode.com/paper/titan-future-forecast-using-action-priors\", \"ArXiv URL\": \"https://openaccess.thecvf.com/content_CVPR_2020/papers/Malla_TITAN_Future_Forecast_Using_Action_Priors_CVPR_2020_paper.pdf\", \"Semantic Scholar Corpus ID\": 214727763, \"Year Released\": \"2020\", \"Text Sources\": [\"crowdsourced\"], \"Licenses\": [{\"License\": \"Non Commercial\", \"License URL\": \"https://usa.honda-ri.com/titan\"}], \"Creators\": [\"Honda Research Institute\"], \"Countries\": [\"United States of America\"], \"License Verified By\": \"Vivek Sharma\", \"Video Hours\": 2.91, \"Taken Down\": \"False\", \"Video Sources\": \"crowdsourced\", \"Task Categories\": [\"Video Classification\"], \"Inferred Metadata\": {\"GitHub License\": \"\", \"GitHub Stars (June 2024)\": \"\", \"GitHub Topics\": \"\", \"Github Date\": \"\", \"HF Config\": \"\", \"HF Config License\": \"\", \"HF Dataset\": \"\", \"HF Date\": \"\", \"HF Downloads (June 2024)\": \"\", \"HF Likes (June 2024)\": \"\", \"HF Yaml License\": \"\", \"PwC Date\": \"\", \"PwC Description\": \"We consider the problem of predicting the future trajectory of scene agents from egocentric views obtained from a moving platform. This problem is important in a variety of domains, particularly for autonomous systems making reactive or strategic decisions in navigation. In an attempt to address this problem, we introduce TITAN (Trajectory Inference using Targeted Action priors Network), a new model that incorporates prior positions, actions, and context to forecast future trajectory of agents and future ego-motion. In the absence of an appropriate dataset for this task, we created the TITAN dataset that consists of 700 labeled video-clips (with odometry) captured from a moving vehicle on highly interactive urban traffic scenes in Tokyo. Our dataset includes 50 labels including vehicle states and actions, pedestrian age groups, and targeted pedestrian action attributes that are organized hierarchically corresponding to atomic, simple/complex-contextual, transportive, and communicative actions. To evaluate our model, we conducted extensive experiments on the TITAN dataset, revealing significant performance improvement against baselines and state-of-the-art algorithms. We also report promising results from our Agent Importance Mechanism (AIM), a module which provides insight into assessment of perceived risk by calculating the relative influence of each agent on the future ego-trajectory. The dataset is available at https://usa.honda-ri.com/titan\", \"PwC License Name\": \"\", \"PwC License URL\": \"\", \"S2 Citation Count (June 2024)\": \"\", \"S2 Date\": \"\"}, \"Bibtex\": \"@Article{Malla2020TITANFF,\\n author = {Srikanth Malla and B. Dariush and Chiho Choi},\\n booktitle = {Computer Vision and Pattern Recognition},\\n journal = {2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},\\n pages = {11183-11193},\\n title = {TITAN: Future Forecast Using Action Priors},\\n year = {2020}\\n}\\n\", \"License Use (DataProvenance)\": \"non-commercial\", \"License Attribution (DataProvenance)\": 0, \"License Share Alike (DataProvenance)\": 0, \"License Use (DataProvenance IgnoreOpenAI)\": \"non-commercial\", \"License Attribution (DataProvenance IgnoreOpenAI)\": 0, \"License Share Alike (DataProvenance IgnoreOpenAI)\": 0, \"License Type\": \"Non-Commercial/Academic\"}, {\"Unique Dataset Identifier\": \"100doh\", \"Collection\": \"100doh\", \"Collection URL\": \"https://arxiv.org/abs/2006.06669\", \"Dataset Name\": \"100DOH: Understanding Human Hands in Contact at Internet Scale (CVPR 2020)\", \"Paper Title\": \"100DOH: Understanding Human Hands in Contact at Internet Scale (CVPR 2020)\", \"Paper URL\": \"https://arxiv.org/abs/2006.06669\", \"GitHub URL\": \"https://github.com/ddshan/hand_object_detector\", \"Hugging Face URL\": \"\", \"Papers with Code URL\": \"https://paperswithcode.com/paper/understanding-human-hands-in-contact-at-1\", \"ArXiv URL\": \"https://arxiv.org/abs/2006.06669\", \"Semantic Scholar Corpus ID\": 215413188, \"Year Released\": \"2020\", \"Text Sources\": [\"youtube\"], \"Licenses\": [{\"License\": \"Custom\", \"License URL\": \"https://fouheylab.eecs.umich.edu/~dandans/projects/100DOH/download.html\"}], \"Creators\": [\"University of Michigan\", \"Johns Hopkins University\"], \"Countries\": [\"United States of America\"], \"License Verified By\": \"Vivek Sharma\", \"Video Hours\": 4577.3, \"Taken Down\": \"False\", \"Video Sources\": \"youtube\", \"Task Categories\": [\"Misc (Hand/Object Detection)\"], \"Inferred Metadata\": {\"GitHub License\": \"MIT License\", \"GitHub Stars (June 2024)\": 213, \"GitHub Topics\": [\"cvpr2020\", \"dataset\", \"handobjectdetection\", \"interactiondetection\"], \"Github Date\": \"\", \"HF Config\": \"\", \"HF Config License\": \"\", \"HF Dataset\": \"\", \"HF Date\": \"\", \"HF Downloads (June 2024)\": \"\", \"HF Likes (June 2024)\": \"\", \"HF Yaml License\": \"\", \"PwC Date\": \"\", \"PwC Description\": \"Implemented in one code library.\", \"PwC License Name\": \"\", \"PwC License URL\": \"\", \"S2 Citation Count (June 2024)\": \"\", \"S2 Date\": \"\"}, \"Bibtex\": \"@Article{Shan2020UnderstandingHH,\\n author = {Dandan Shan and Jiaqi Geng and Michelle Shu and D. Fouhey},\\n booktitle = {Computer Vision and Pattern Recognition},\\n journal = {2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},\\n pages = {9866-9875},\\n title = {Understanding Human Hands in Contact at Internet Scale},\\n year = {2020}\\n}\\n\", \"License Use (DataProvenance)\": \"academic-only\", \"License Attribution (DataProvenance)\": 1, \"License Share Alike (DataProvenance)\": 0, \"License Use (DataProvenance IgnoreOpenAI)\": \"academic-only\", \"License Attribution (DataProvenance IgnoreOpenAI)\": 1, \"License Share Alike (DataProvenance IgnoreOpenAI)\": 0, \"License Type\": \"Non-Commercial/Academic\"}, {\"Unique Dataset Identifier\": \"kinetics-700\", \"Collection\": \"kinetics-700\", \"Collection URL\": \"https://arxiv.org/pdf/2010.10864.pdf\", \"Dataset Name\": \"Kinetics-700\", \"Paper Title\": \"Kinetics-700\", \"Paper URL\": \"https://arxiv.org/pdf/2010.10864.pdf\", \"GitHub URL\": \"\", \"Hugging Face URL\": \"\", \"Papers with Code URL\": \"https://paperswithcode.com/dataset/kinetics-700\", \"ArXiv URL\": \"https://arxiv.org/pdf/2010.10864.pdf\", \"Semantic Scholar Corpus ID\": 196831809, \"Year Released\": \"2020\", \"Text Sources\": [\"youtube\"], \"Licenses\": [{\"License\": \"Unspecified\", \"License URL\": \"\"}], \"Creators\": [\"DeepMind\"], \"Countries\": [\"United Kingdom\"], \"License Verified By\": \"Vivek Sharma\", \"Video Hours\": 1805.56, \"Taken Down\": \"False\", \"Video Sources\": \"youtube\", \"Task Categories\": [\"Video Classification\"], \"Inferred Metadata\": {\"GitHub License\": \"\", \"GitHub Stars (June 2024)\": \"\", \"GitHub Topics\": \"\", \"Github Date\": \"\", \"HF Config\": \"\", \"HF Config License\": \"\", \"HF Dataset\": \"\", \"HF Date\": \"\", \"HF Downloads (June 2024)\": \"\", \"HF Likes (June 2024)\": \"\", \"HF Yaml License\": \"\", \"PwC Date\": \"2019-07-15\", \"PwC Description\": \"Kinetics-700 is a video dataset of 650,000 clips that covers 700 human action classes. The videos include human-object interactions such as playing instruments, as well as human-human interactions such as shaking hands and hugging. Each action class has at least 700 video clips. Each clip is annotated with an action class and lasts approximately 10 seconds.\", \"PwC License Name\": \"CC BY 4.0\", \"PwC License URL\": \"https://creativecommons.org/licenses/by/4.0/\", \"S2 Citation Count (June 2024)\": \"\", \"S2 Date\": \"\"}, \"Bibtex\": \"@Article{Carreira2019ASN,\\n author = {Jo\\u00e3o Carreira and Eric Noland and Chloe Hillier and Andrew Zisserman},\\n booktitle = {arXiv.org},\\n journal = {ArXiv},\\n title = {A Short Note on the Kinetics-700 Human Action Dataset},\\n volume = {abs/1907.06987},\\n year = {2019}\\n}\\n\", \"License Use (DataProvenance)\": \"unspecified\", \"License Attribution (DataProvenance)\": 0, \"License Share Alike (DataProvenance)\": 0, \"License Use (DataProvenance IgnoreOpenAI)\": \"unspecified\", \"License Attribution (DataProvenance IgnoreOpenAI)\": 0, \"License Share Alike (DataProvenance IgnoreOpenAI)\": 0, \"License Type\": \"Unspecified\"}, {\"Unique Dataset Identifier\": \"rare-act-dataset\", \"Collection\": \"rare-act-dataset\", \"Collection URL\": \"https://arxiv.org/pdf/2008.01018.pdf\", \"Dataset Name\": \"RareAct: A video dataset of unusual interactions\", \"Paper Title\": \"RareAct: A video dataset of unusual interactions\", \"Paper URL\": \"https://arxiv.org/pdf/2008.01018.pdf\", \"GitHub URL\": \"https://github.com/antoine77340/RareAct\", \"Hugging Face URL\": \"\", \"Papers with Code URL\": \"https://paperswithcode.com/dataset/rareact\", \"ArXiv URL\": \"https://arxiv.org/pdf/2008.01018.pdf\", \"Semantic Scholar Corpus ID\": 220936243, \"Year Released\": \"2020\", \"Text Sources\": [\"undisclosed web\"], \"Licenses\": [{\"License\": \"Unspecified\", \"License URL\": \"\"}], \"Creators\": [\"University of Oxford\", \"Ecole Normale Sup\\u00e9rieure\", \"Inria\", \"CIIRC\", \"Czech Technical University\"], \"Countries\": [\"United Kingdom\", \"France\", \"Czech Republic\"], \"License Verified By\": \"Vivek Sharma\", \"Video Hours\": 21.13, \"Taken Down\": \"False\", \"Video Sources\": \"undisclosed web\", \"Task Categories\": [\"Video Classification\"], \"Inferred Metadata\": {\"GitHub License\": \"Apache License 2.0\", \"GitHub Stars (June 2024)\": 32, \"GitHub Topics\": [], \"Github Date\": \"\", \"HF Config\": \"\", \"HF Config License\": \"\", \"HF Dataset\": \"\", \"HF Date\": \"\", \"HF Downloads (June 2024)\": \"\", \"HF Likes (June 2024)\": \"\", \"HF Yaml License\": \"\", \"PwC Date\": \"\", \"PwC Description\": \"RareAct is a video dataset of unusual actions, including actions like \\u201cblend phone\\u201d, \\u201ccut keyboard\\u201d and \\u201cmicrowave shoes\\u201d. It aims at evaluating the zero-shot and few-shot compositionality of action recognition models for unlikely compositions of common action verbs and object nouns. It contains 122 different actions which were obtained by combining verbs and nouns rarely co-occurring together in the large-scale textual corpus from HowTo100M, but that frequently appear separately.\", \"PwC License Name\": \"Unspecified\", \"PwC License URL\": null, \"S2 Citation Count (June 2024)\": \"\", \"S2 Date\": \"\"}, \"Bibtex\": \"@Article{Miech2020RareActAV,\\n author = {Antoine Miech and Jean-Baptiste Alayrac and I. Laptev and Josef Sivic and Andrew Zisserman},\\n booktitle = {arXiv.org},\\n journal = {ArXiv},\\n title = {RareAct: A video dataset of unusual interactions},\\n volume = {abs/2008.01018},\\n year = {2020}\\n}\\n\", \"License Use (DataProvenance)\": \"unspecified\", \"License Attribution (DataProvenance)\": 0, \"License Share Alike (DataProvenance)\": 0, \"License Use (DataProvenance IgnoreOpenAI)\": \"unspecified\", \"License Attribution (DataProvenance IgnoreOpenAI)\": 0, \"License Share Alike (DataProvenance IgnoreOpenAI)\": 0, \"License Type\": \"Unspecified\"}, {\"Unique Dataset Identifier\": \"violin\", \"Collection\": \"violin\", \"Collection URL\": \"https://arxiv.org/abs/2003.11618\", \"Dataset Name\": \"VIOLIN\", \"Paper Title\": \"VIOLIN\", \"Paper URL\": \"https://arxiv.org/abs/2003.11618\", \"GitHub URL\": \"https://github.com/jimmy646/violin\", \"Hugging Face URL\": \"\", \"Papers with Code URL\": \"https://paperswithcode.com/dataset/violin\", \"ArXiv URL\": \"https://arxiv.org/abs/2003.11618\", \"Semantic Scholar Corpus ID\": 214668012, \"Year Released\": \"2020\", \"Text Sources\": [\"youtube\"], \"Licenses\": [{\"License\": \"Unspecified\", \"License URL\": \"\"}], \"Creators\": [\"Microsoft\"], \"Countries\": [\"United States of America\"], \"License Verified By\": \"Vivek Sharma\", \"Video Hours\": 582.0, \"Taken Down\": \"False\", \"Video Sources\": \"youtube\", \"Task Categories\": [\"Video Captioning\"], \"Inferred Metadata\": {\"GitHub License\": \"MIT License\", \"GitHub Stars (June 2024)\": 156, \"GitHub Topics\": [], \"Github Date\": \"\", \"HF Config\": \"\", \"HF Config License\": \"\", \"HF Dataset\": \"\", \"HF Date\": \"\", \"HF Downloads (June 2024)\": \"\", \"HF Likes (June 2024)\": \"\", \"HF Yaml License\": \"\", \"PwC Date\": \"\", \"PwC Description\": \"Video-and-Language Inference is the task of joint multimodal understanding of video and text. Given a video clip with aligned subtitles as premise, paired with a natural language hypothesis based on the video content, a model needs to infer whether the hypothesis is entailed or contradicted by the given video clip. The Violin dataset is a dataset for this task which consists of 95,322 video-hypothesis pairs from 15,887 video clips, spanning over 582 hours of video. These video clips contain rich content with diverse temporal dynamics, event shifts, and people interactions, collected from two sources: (i) popular TV shows, and (ii) movie clips from YouTube channels.\", \"PwC License Name\": \"Unspecified\", \"PwC License URL\": null, \"S2 Citation Count (June 2024)\": \"\", \"S2 Date\": \"\"}, \"Bibtex\": \"@Article{Liu2020ViolinAL,\\n author = {J. Liu and Wenhu Chen and Yu Cheng and Zhe Gan and Licheng Yu and Yiming Yang and Jingjing Liu},\\n booktitle = {Computer Vision and Pattern Recognition},\\n journal = {2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},\\n pages = {10897-10907},\\n title = {Violin: A Large-Scale Dataset for Video-and-Language Inference},\\n year = {2020}\\n}\\n\", \"License Use (DataProvenance)\": \"unspecified\", \"License Attribution (DataProvenance)\": 0, \"License Share Alike (DataProvenance)\": 0, \"License Use (DataProvenance IgnoreOpenAI)\": \"unspecified\", \"License Attribution (DataProvenance IgnoreOpenAI)\": 0, \"License Share Alike (DataProvenance IgnoreOpenAI)\": 0, \"License Type\": \"Unspecified\"}, {\"Unique Dataset Identifier\": \"lemma-dataset\", \"Collection\": \"lemma-dataset\", \"Collection URL\": \"https://arxiv.org/abs/2007.15781\", \"Dataset Name\": \"LEMMA: A Multi-view Dataset for LEarning Multi-agent Multi-task Activities (ECCV 2020)\", \"Paper Title\": \"LEMMA: A Multi-view Dataset for LEarning Multi-agent Multi-task Activities (ECCV 2020)\", \"Paper URL\": \"https://arxiv.org/abs/2007.15781\", \"GitHub URL\": \"https://github.com/Buzz-Beater/LEMMA\", \"Hugging Face URL\": \"\", \"Papers with Code URL\": \"https://paperswithcode.com/dataset/lemma\", \"ArXiv URL\": \"https://arxiv.org/abs/2007.15781\", \"Semantic Scholar Corpus ID\": 220634784, \"Year Released\": \"2020\", \"Text Sources\": [\"crowdsourced\", \"human\"], \"Licenses\": [{\"License\": \"Unspecified\", \"License URL\": \"\"}], \"Creators\": [\"UCLA\"], \"Countries\": [\"United States of America\"], \"License Verified By\": \"Vivek Sharma\", \"Video Hours\": 10.8, \"Taken Down\": \"False\", \"Video Sources\": \"crowdsourced\", \"Task Categories\": [\"Video Classification\"], \"Inferred Metadata\": {\"GitHub License\": \"\", \"GitHub Stars (June 2024)\": 27, \"GitHub Topics\": [], \"Github Date\": \"\", \"HF Config\": \"\", \"HF Config License\": \"\", \"HF Dataset\": \"\", \"HF Date\": \"\", \"HF Downloads (June 2024)\": \"\", \"HF Likes (June 2024)\": \"\", \"HF Yaml License\": \"\", \"PwC Date\": \"\", \"PwC Description\": \"The LEMMA dataset aims to explore the essence of complex human activities in a goal-directed, multi-agent, multi-task setting with ground-truth labels of compositional atomic-actions and their associated tasks. By quantifying the scenarios to up to two multi-step tasks with two agents, the authors strive to address human multi-task and multi-agent interactions in four scenarios: single-agent single-task (1 x 1), single-agent multi-task (1 x 2), multi-agent single-task (2 x 1), and multi-agent multi-task (2 x 2). Task instructions are only given to one agent in the 2 x 1 setting to resemble the robot-helping scenario, hoping that the learned perception models could be applied in robotic tasks (especially in HRI) in the near future.\\n\\nBoth the third-person views (TPVs) and the first-person views (FPVs) were recorded to account for different perspectives of the same activities. The authors densely annotate atomic-actions (in the form of compositional verb-noun pairs) and tasks of each atomic-action, as well as the spatial location of each participating agent (bounding boxes) to facilitate the learning of multi-agent multi-task task scheduling and assignment.\", \"PwC License Name\": \"Unspecified\", \"PwC License URL\": null, \"S2 Citation Count (June 2024)\": \"\", \"S2 Date\": \"\"}, \"Bibtex\": \"@Article{Jia2020LEMMAAM,\\n author = {Baoxiong Jia and Yixin Chen and Siyuan Huang and Yixin Zhu and Song-Chun Zhu},\\n booktitle = {European Conference on Computer Vision},\\n journal = {ArXiv},\\n title = {LEMMA: A Multi-view Dataset for Learning Multi-agent Multi-task Activities},\\n volume = {abs/2007.15781},\\n year = {2020}\\n}\\n\", \"License Use (DataProvenance)\": \"unspecified\", \"License Attribution (DataProvenance)\": 0, \"License Share Alike (DataProvenance)\": 0, \"License Use (DataProvenance IgnoreOpenAI)\": \"unspecified\", \"License Attribution (DataProvenance IgnoreOpenAI)\": 0, \"License Share Alike (DataProvenance IgnoreOpenAI)\": 0, \"License Type\": \"Unspecified\"}, {\"Unique Dataset Identifier\": \"lemma-dataset\", \"Collection\": \"lemma-dataset\", \"Collection URL\": \"https://arxiv.org/abs/2007.15781\", \"Dataset Name\": \"LEMMA: A Multi-view Dataset for LEarning Multi-agent Multi-task Activities (ECCV 2020)\", \"Paper Title\": \"LEMMA: A Multi-view Dataset for LEarning Multi-agent Multi-task Activities (ECCV 2020)\", \"Paper URL\": \"https://arxiv.org/abs/2007.15781\", \"GitHub URL\": \"https://github.com/Buzz-Beater/LEMMA\", \"Hugging Face URL\": \"\", \"Papers with Code URL\": \"https://paperswithcode.com/dataset/lemma\", \"ArXiv URL\": \"https://arxiv.org/abs/2007.15781\", \"Semantic Scholar Corpus ID\": 220634784, \"Year Released\": \"2020\", \"Text Sources\": [\"crowdsourced\", \"human\"], \"Licenses\": [{\"License\": \"Unspecified\", \"License URL\": \"\"}], \"Creators\": [\"UCLA\"], \"Countries\": [\"United States of America\"], \"License Verified By\": \"Vivek Sharma\", \"Video Hours\": 10.8, \"Taken Down\": \"False\", \"Video Sources\": \"human\", \"Task Categories\": [\"Video Classification\"], \"Inferred Metadata\": {\"GitHub License\": \"\", \"GitHub Stars (June 2024)\": 27, \"GitHub Topics\": [], \"Github Date\": \"\", \"HF Config\": \"\", \"HF Config License\": \"\", \"HF Dataset\": \"\", \"HF Date\": \"\", \"HF Downloads (June 2024)\": \"\", \"HF Likes (June 2024)\": \"\", \"HF Yaml License\": \"\", \"PwC Date\": \"\", \"PwC Description\": \"The LEMMA dataset aims to explore the essence of complex human activities in a goal-directed, multi-agent, multi-task setting with ground-truth labels of compositional atomic-actions and their associated tasks. By quantifying the scenarios to up to two multi-step tasks with two agents, the authors strive to address human multi-task and multi-agent interactions in four scenarios: single-agent single-task (1 x 1), single-agent multi-task (1 x 2), multi-agent single-task (2 x 1), and multi-agent multi-task (2 x 2). Task instructions are only given to one agent in the 2 x 1 setting to resemble the robot-helping scenario, hoping that the learned perception models could be applied in robotic tasks (especially in HRI) in the near future.\\n\\nBoth the third-person views (TPVs) and the first-person views (FPVs) were recorded to account for different perspectives of the same activities. The authors densely annotate atomic-actions (in the form of compositional verb-noun pairs) and tasks of each atomic-action, as well as the spatial location of each participating agent (bounding boxes) to facilitate the learning of multi-agent multi-task task scheduling and assignment.\", \"PwC License Name\": \"Unspecified\", \"PwC License URL\": null, \"S2 Citation Count (June 2024)\": \"\", \"S2 Date\": \"\"}, \"Bibtex\": \"@Article{Jia2020LEMMAAM,\\n author = {Baoxiong Jia and Yixin Chen and Siyuan Huang and Yixin Zhu and Song-Chun Zhu},\\n booktitle = {European Conference on Computer Vision},\\n journal = {ArXiv},\\n title = {LEMMA: A Multi-view Dataset for Learning Multi-agent Multi-task Activities},\\n volume = {abs/2007.15781},\\n year = {2020}\\n}\\n\", \"License Use (DataProvenance)\": \"unspecified\", \"License Attribution (DataProvenance)\": 0, \"License Share Alike (DataProvenance)\": 0, \"License Use (DataProvenance IgnoreOpenAI)\": \"unspecified\", \"License Attribution (DataProvenance IgnoreOpenAI)\": 0, \"License Share Alike (DataProvenance IgnoreOpenAI)\": 0, \"License Type\": \"Unspecified\"}, {\"Unique Dataset Identifier\": \"hyu-vids\", \"Collection\": \"hyu-vids\", \"Collection URL\": \"https://arxiv.org/abs/1904.11451\", \"Dataset Name\": \"HVU: Large Scale Holistic Video Understanding (ECCV 2020)\", \"Paper Title\": \"HVU: Large Scale Holistic Video Understanding (ECCV 2020)\", \"Paper URL\": \"https://arxiv.org/abs/1904.11451\", \"GitHub URL\": \"https://github.com/holistic-video-understanding/HVU-Dataset\", \"Hugging Face URL\": \"\", \"Papers with Code URL\": \"https://paperswithcode.com/paper/holistic-large-scale-video-understanding\", \"ArXiv URL\": \"https://arxiv.org/abs/1904.11451\", \"Semantic Scholar Corpus ID\": 131777079, \"Year Released\": \"2020\", \"Text Sources\": [\"youtube\"], \"Licenses\": [{\"License\": \"Custom\", \"License URL\": \"https://github.com/holistic-video-understanding/HVU-Dataset\"}], \"Creators\": [\"Karlsruhe Institute of Technology\", \"ETH Z\\u00fcrich\", \"KU Leuven\", \"University of Bonn\", \"Sensifai\"], \"Countries\": [\"Switzerland\", \"Belgium\", \"Germany\"], \"License Verified By\": \"Vivek Sharma\", \"Video Hours\": 96166.67, \"Taken Down\": \"False\", \"Video Sources\": \"youtube\", \"Task Categories\": [\"Video Classification\"], \"Inferred Metadata\": {\"GitHub License\": \"\", \"GitHub Stars (June 2024)\": 70, \"GitHub Topics\": [], \"Github Date\": \"\", \"HF Config\": \"\", \"HF Config License\": \"\", \"HF Dataset\": \"\", \"HF Date\": \"\", \"HF Downloads (June 2024)\": \"\", \"HF Likes (June 2024)\": \"\", \"HF Yaml License\": \"\", \"PwC Date\": \"\", \"PwC Description\": \"#11 best model for Action Recognition on UCF101 (3-fold Accuracy metric)\", \"PwC License Name\": \"\", \"PwC License URL\": \"\", \"S2 Citation Count (June 2024)\": \"\", \"S2 Date\": \"\"}, \"Bibtex\": \"@Article{Diba2019HolisticLS,\\n author = {Ali Diba and Mohsen Fayyaz and Vivek Sharma and Manohar Paluri and Juergen Gall and R. Stiefelhagen and L. Gool},\\n booktitle = {arXiv.org},\\n journal = {ArXiv},\\n title = {Holistic Large Scale Video Understanding},\\n volume = {abs/1904.11451},\\n year = {2019}\\n}\\n\", \"License Use (DataProvenance)\": \"academic-only\", \"License Attribution (DataProvenance)\": 1, \"License Share Alike (DataProvenance)\": 0, \"License Use (DataProvenance IgnoreOpenAI)\": \"academic-only\", \"License Attribution (DataProvenance IgnoreOpenAI)\": 1, \"License Share Alike (DataProvenance IgnoreOpenAI)\": 0, \"License Type\": \"Non-Commercial/Academic\"}, {\"Unique Dataset Identifier\": \"tiny-virat\", \"Collection\": \"tiny-virat\", \"Collection URL\": \"https://arxiv.org/abs/2007.07355\", \"Dataset Name\": \"TinyVIRAT: Low-resolution Video Action Recognition\", \"Paper Title\": \"TinyVIRAT: Low-resolution Video Action Recognition\", \"Paper URL\": \"https://arxiv.org/abs/2007.07355\", \"GitHub URL\": \"https://github.com/UgurDemir/Tiny-VIRAT\", \"Hugging Face URL\": \"\", \"Papers with Code URL\": \"https://paperswithcode.com/dataset/tinyvirat\", \"ArXiv URL\": \"https://arxiv.org/abs/2007.07355\", \"Semantic Scholar Corpus ID\": 220525685, \"Year Released\": \"2020\", \"Text Sources\": [\"crowdsourced\"], \"Licenses\": [{\"License\": \"Unspecified\", \"License URL\": \"\"}], \"Creators\": [\"University of Central Florida\"], \"Countries\": [\"United States of America\"], \"License Verified By\": \"Vivek Sharma\", \"Video Hours\": 10.83, \"Taken Down\": \"False\", \"Video Sources\": \"crowdsourced\", \"Task Categories\": [\"Video Classification\"], \"Inferred Metadata\": {\"GitHub License\": \"\", \"GitHub Stars (June 2024)\": 16, \"GitHub Topics\": [], \"Github Date\": \"\", \"HF Config\": \"\", \"HF Config License\": \"\", \"HF Dataset\": \"\", \"HF Date\": \"\", \"HF Downloads (June 2024)\": \"\", \"HF Likes (June 2024)\": \"\", \"HF Yaml License\": \"\", \"PwC Date\": \"\", \"PwC Description\": \"TinyVIRAT contains natural low-resolution activities. The actions in TinyVIRAT videos have multiple labels and they are extracted from surveillance videos which makes them realistic and more challenging.\", \"PwC License Name\": \"Unspecified\", \"PwC License URL\": null, \"S2 Citation Count (June 2024)\": \"\", \"S2 Date\": \"\"}, \"Bibtex\": \"@Article{Demir2020TinyVIRATLV,\\n author = {Ugur Demir and Y. Rawat and M. Shah},\\n booktitle = {International Conference on Pattern Recognition},\\n journal = {2020 25th International Conference on Pattern Recognition (ICPR)},\\n pages = {7387-7394},\\n title = {TinyVIRAT: Low-resolution Video Action Recognition},\\n year = {2020}\\n}\\n\", \"License Use (DataProvenance)\": \"unspecified\", \"License Attribution (DataProvenance)\": 0, \"License Share Alike (DataProvenance)\": 0, \"License Use (DataProvenance IgnoreOpenAI)\": \"unspecified\", \"License Attribution (DataProvenance IgnoreOpenAI)\": 0, \"License Share Alike (DataProvenance IgnoreOpenAI)\": 0, \"License Type\": \"Unspecified\"}, {\"Unique Dataset Identifier\": \"moviescenes\", \"Collection\": \"moviescenes\", \"Collection URL\": \"https://arxiv.org/abs/2004.02678\", \"Dataset Name\": \"MovieScenes\", \"Paper Title\": \"MovieScenes\", \"Paper URL\": \"https://arxiv.org/abs/2004.02678\", \"GitHub URL\": \"https://github.com/AnyiRao/SceneSeg\", \"Hugging Face URL\": \"\", \"Papers with Code URL\": \"https://paperswithcode.com/paper/a-local-to-global-approach-to-multi-modal\", \"ArXiv URL\": \"https://arxiv.org/abs/2004.02678\", \"Semantic Scholar Corpus ID\": 214802984, \"Year Released\": \"2020\", \"Text Sources\": [\"movies\"], \"Licenses\": [{\"License\": \"Unspecified\", \"License URL\": \"\"}], \"Creators\": [\"The Chinese University of Hong Kong\", \"UC Berkeley\"], \"Countries\": [\"China\", \"United States of America\"], \"License Verified By\": \"Vivek Sharma\", \"Video Hours\": 250.0, \"Taken Down\": \"False\", \"Video Sources\": \"movies\", \"Task Categories\": [\"Misc (Scene Segmentation)\"], \"Inferred Metadata\": {\"GitHub License\": \"\", \"GitHub Stars (June 2024)\": 213, \"GitHub Topics\": [\"boundary-detection\", \"scene\", \"segmentation\", \"video-analysis\"], \"Github Date\": \"\", \"HF Config\": \"\", \"HF Config License\": \"\", \"HF Dataset\": \"\", \"HF Date\": \"\", \"HF Downloads (June 2024)\": \"\", \"HF Likes (June 2024)\": \"\", \"HF Yaml License\": \"\", \"PwC Date\": \"\", \"PwC Description\": \"4 code implementations in PyTorch and TensorFlow. Scene, as the crucial unit of storytelling in movies, contains complex activities of actors and their interactions in a physical environment. Identifying the composition of scenes serves as a critical step towards semantic understanding of movies. This is very challenging -- compared to the videos studied in conventional vision problems, e.g. action recognition, as scenes in movies usually contain much richer temporal structures and more complex semantic information. Towards this goal, we scale up the scene segmentation task by building a large-scale video dataset MovieScenes, which contains 21K annotated scene segments from 150 movies. We further propose a local-to-global scene segmentation framework, which integrates multi-modal information across three levels, i.e. clip, segment, and movie. This framework is able to distill complex semantics from hierarchical temporal structures over a long movie, providing top-down guidance for scene segmentation. Our experiments show that the proposed network is able to segment a movie into scenes with high accuracy, consistently outperforming previous methods. We also found that pretraining on our MovieScenes can bring significant improvements to the existing approaches.\", \"PwC License Name\": \"\", \"PwC License URL\": \"\", \"S2 Citation Count (June 2024)\": \"\", \"S2 Date\": \"\"}, \"Bibtex\": \"@Article{Rao2020ALA,\\n author = {Anyi Rao and Linning Xu and Yu Xiong and Guodong Xu and Qingqiu Huang and Bolei Zhou and Dahua Lin},\\n booktitle = {Computer Vision and Pattern Recognition},\\n journal = {2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},\\n pages = {10143-10152},\\n title = {A Local-to-Global Approach to Multi-Modal Movie Scene Segmentation},\\n year = {2020}\\n}\\n\", \"License Use (DataProvenance)\": \"unspecified\", \"License Attribution (DataProvenance)\": 0, \"License Share Alike (DataProvenance)\": 0, \"License Use (DataProvenance IgnoreOpenAI)\": \"unspecified\", \"License Attribution (DataProvenance IgnoreOpenAI)\": 0, \"License Share Alike (DataProvenance IgnoreOpenAI)\": 0, \"License Type\": \"Unspecified\"}, {\"Unique Dataset Identifier\": \"spoken-moments\", \"Collection\": \"spoken-moments\", \"Collection URL\": \"https://arxiv.org/abs/2105.04489\", \"Dataset Name\": \"Spoken Moments\", \"Paper Title\": \"Spoken Moments\", \"Paper URL\": \"https://arxiv.org/abs/2105.04489\", \"GitHub URL\": \"\", \"Hugging Face URL\": \"\", \"Papers with Code URL\": \"https://paperswithcode.com/paper/spoken-moments-learning-joint-audio-visual\", \"ArXiv URL\": \"https://arxiv.org/abs/2105.04489\", \"Semantic Scholar Corpus ID\": 233992735, \"Year Released\": \"2021\", \"Text Sources\": [\"youtube\", \"flickr\", \"vine\", \"metacafe\", \"peeks\", \"vimeo\", \"videoblocks\", \"bing videos\", \"giphy\", \"weather channel\", \"getty-images\"], \"Licenses\": [{\"License\": \"Custom\", \"License URL\": \"https://docs.google.com/forms/d/e/1FAIpQLSdUOScon6uWkslgcA2-BCkAhMp86SihgKtMqX53zRf-E2fIHw/formResponse\"}], \"Creators\": [\"Massachusetts Institute of Technology\", \"IBM\", \"The University of Texas at Austin\"], \"Countries\": [\"United States of America\"], \"License Verified By\": \"Vivek Sharma\", \"Video Hours\": 416.67, \"Taken Down\": \"False\", \"Video Sources\": \"Other\", \"Task Categories\": [\"Video Captioning\"], \"Inferred Metadata\": {\"GitHub License\": \"\", \"GitHub Stars (June 2024)\": \"\", \"GitHub Topics\": \"\", \"Github Date\": \"\", \"HF Config\": \"\", \"HF Config License\": \"\", \"HF Dataset\": \"\", \"HF Date\": \"\", \"HF Downloads (June 2024)\": \"\", \"HF Likes (June 2024)\": \"\", \"HF Yaml License\": \"\", \"PwC Date\": \"\", \"PwC Description\": \"No code available yet.\", \"PwC License Name\": \"\", \"PwC License URL\": \"\", \"S2 Citation Count (June 2024)\": \"\", \"S2 Date\": \"\"}, \"Bibtex\": \"@Article{Monfort2021SpokenML,\\n author = {Mathew Monfort and SouYoung Jin},\\n booktitle = {Computer Vision and Pattern Recognition},\\n journal = {2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},\\n pages = {14866-14876},\\n title = {Spoken Moments: Learning Joint Audio-Visual Representations from Video Descriptions},\\n year = {2021}\\n}\\n\", \"License Use (DataProvenance)\": \"commercial\", \"License Attribution (DataProvenance)\": 0, \"License Share Alike (DataProvenance)\": 0, \"License Use (DataProvenance IgnoreOpenAI)\": \"commercial\", \"License Attribution (DataProvenance IgnoreOpenAI)\": 0, \"License Share Alike (DataProvenance IgnoreOpenAI)\": 0, \"License Type\": \"Commercial\"}, {\"Unique Dataset Identifier\": \"spoken-moments\", \"Collection\": \"spoken-moments\", \"Collection URL\": \"https://arxiv.org/abs/2105.04489\", \"Dataset Name\": \"Spoken Moments\", \"Paper Title\": \"Spoken Moments\", \"Paper URL\": \"https://arxiv.org/abs/2105.04489\", \"GitHub URL\": \"\", \"Hugging Face URL\": \"\", \"Papers with Code URL\": \"https://paperswithcode.com/paper/spoken-moments-learning-joint-audio-visual\", \"ArXiv URL\": \"https://arxiv.org/abs/2105.04489\", \"Semantic Scholar Corpus ID\": 233992735, \"Year Released\": \"2021\", \"Text Sources\": [\"youtube\", \"flickr\", \"vine\", \"metacafe\", \"peeks\", \"vimeo\", \"videoblocks\", \"bing videos\", \"giphy\", \"weather channel\", \"getty-images\"], \"Licenses\": [{\"License\": \"Custom\", \"License URL\": \"https://docs.google.com/forms/d/e/1FAIpQLSdUOScon6uWkslgcA2-BCkAhMp86SihgKtMqX53zRf-E2fIHw/formResponse\"}], \"Creators\": [\"Massachusetts Institute of Technology\", \"IBM\", \"The University of Texas at Austin\"], \"Countries\": [\"United States of America\"], \"License Verified By\": \"Vivek Sharma\", \"Video Hours\": 416.67, \"Taken Down\": \"False\", \"Video Sources\": \"Other\", \"Task Categories\": [\"Video Captioning\"], \"Inferred Metadata\": {\"GitHub License\": \"\", \"GitHub Stars (June 2024)\": \"\", \"GitHub Topics\": \"\", \"Github Date\": \"\", \"HF Config\": \"\", \"HF Config License\": \"\", \"HF Dataset\": \"\", \"HF Date\": \"\", \"HF Downloads (June 2024)\": \"\", \"HF Likes (June 2024)\": \"\", \"HF Yaml License\": \"\", \"PwC Date\": \"\", \"PwC Description\": \"No code available yet.\", \"PwC License Name\": \"\", \"PwC License URL\": \"\", \"S2 Citation Count (June 2024)\": \"\", \"S2 Date\": \"\"}, \"Bibtex\": \"@Article{Monfort2021SpokenML,\\n author = {Mathew Monfort and SouYoung Jin},\\n booktitle = {Computer Vision and Pattern Recognition},\\n journal = {2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},\\n pages = {14866-14876},\\n title = {Spoken Moments: Learning Joint Audio-Visual Representations from Video Descriptions},\\n year = {2021}\\n}\\n\", \"License Use (DataProvenance)\": \"commercial\", \"License Attribution (DataProvenance)\": 0, \"License Share Alike (DataProvenance)\": 0, \"License Use (DataProvenance IgnoreOpenAI)\": \"commercial\", \"License Attribution (DataProvenance IgnoreOpenAI)\": 0, \"License Share Alike (DataProvenance IgnoreOpenAI)\": 0, \"License Type\": \"Commercial\"}, {\"Unique Dataset Identifier\": \"spoken-moments\", \"Collection\": \"spoken-moments\", \"Collection URL\": \"https://arxiv.org/abs/2105.04489\", \"Dataset Name\": \"Spoken Moments\", \"Paper Title\": \"Spoken Moments\", \"Paper URL\": \"https://arxiv.org/abs/2105.04489\", \"GitHub URL\": \"\", \"Hugging Face URL\": \"\", \"Papers with Code URL\": \"https://paperswithcode.com/paper/spoken-moments-learning-joint-audio-visual\", \"ArXiv URL\": \"https://arxiv.org/abs/2105.04489\", \"Semantic Scholar Corpus ID\": 233992735, \"Year Released\": \"2021\", \"Text Sources\": [\"youtube\", \"flickr\", \"vine\", \"metacafe\", \"peeks\", \"vimeo\", \"videoblocks\", \"bing videos\", \"giphy\", \"weather channel\", \"getty-images\"], \"Licenses\": [{\"License\": \"Custom\", \"License URL\": \"https://docs.google.com/forms/d/e/1FAIpQLSdUOScon6uWkslgcA2-BCkAhMp86SihgKtMqX53zRf-E2fIHw/formResponse\"}], \"Creators\": [\"Massachusetts Institute of Technology\", \"IBM\", \"The University of Texas at Austin\"], \"Countries\": [\"United States of America\"], \"License Verified By\": \"Vivek Sharma\", \"Video Hours\": 416.67, \"Taken Down\": \"False\", \"Video Sources\": \"Other\", \"Task Categories\": [\"Video Captioning\"], \"Inferred Metadata\": {\"GitHub License\": \"\", \"GitHub Stars (June 2024)\": \"\", \"GitHub Topics\": \"\", \"Github Date\": \"\", \"HF Config\": \"\", \"HF Config License\": \"\", \"HF Dataset\": \"\", \"HF Date\": \"\", \"HF Downloads (June 2024)\": \"\", \"HF Likes (June 2024)\": \"\", \"HF Yaml License\": \"\", \"PwC Date\": \"\", \"PwC Description\": \"No code available yet.\", \"PwC License Name\": \"\", \"PwC License URL\": \"\", \"S2 Citation Count (June 2024)\": \"\", \"S2 Date\": \"\"}, \"Bibtex\": \"@Article{Monfort2021SpokenML,\\n author = {Mathew Monfort and SouYoung Jin},\\n booktitle = {Computer Vision and Pattern Recognition},\\n journal = {2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},\\n pages = {14866-14876},\\n title = {Spoken Moments: Learning Joint Audio-Visual Representations from Video Descriptions},\\n year = {2021}\\n}\\n\", \"License Use (DataProvenance)\": \"commercial\", \"License Attribution (DataProvenance)\": 0, \"License Share Alike (DataProvenance)\": 0, \"License Use (DataProvenance IgnoreOpenAI)\": \"commercial\", \"License Attribution (DataProvenance IgnoreOpenAI)\": 0, \"License Share Alike (DataProvenance IgnoreOpenAI)\": 0, \"License Type\": \"Commercial\"}, {\"Unique Dataset Identifier\": \"spoken-moments\", \"Collection\": \"spoken-moments\", \"Collection URL\": \"https://arxiv.org/abs/2105.04489\", \"Dataset Name\": \"Spoken Moments\", \"Paper Title\": \"Spoken Moments\", \"Paper URL\": \"https://arxiv.org/abs/2105.04489\", \"GitHub URL\": \"\", \"Hugging Face URL\": \"\", \"Papers with Code URL\": \"https://paperswithcode.com/paper/spoken-moments-learning-joint-audio-visual\", \"ArXiv URL\": \"https://arxiv.org/abs/2105.04489\", \"Semantic Scholar Corpus ID\": 233992735, \"Year Released\": \"2021\", \"Text Sources\": [\"youtube\", \"flickr\", \"vine\", \"metacafe\", \"peeks\", \"vimeo\", \"videoblocks\", \"bing videos\", \"giphy\", \"weather channel\", \"getty-images\"], \"Licenses\": [{\"License\": \"Custom\", \"License URL\": \"https://docs.google.com/forms/d/e/1FAIpQLSdUOScon6uWkslgcA2-BCkAhMp86SihgKtMqX53zRf-E2fIHw/formResponse\"}], \"Creators\": [\"Massachusetts Institute of Technology\", \"IBM\", \"The University of Texas at Austin\"], \"Countries\": [\"United States of America\"], \"License Verified By\": \"Vivek Sharma\", \"Video Hours\": 416.67, \"Taken Down\": \"False\", \"Video Sources\": \"Other\", \"Task Categories\": [\"Video Captioning\"], \"Inferred Metadata\": {\"GitHub License\": \"\", \"GitHub Stars (June 2024)\": \"\", \"GitHub Topics\": \"\", \"Github Date\": \"\", \"HF Config\": \"\", \"HF Config License\": \"\", \"HF Dataset\": \"\", \"HF Date\": \"\", \"HF Downloads (June 2024)\": \"\", \"HF Likes (June 2024)\": \"\", \"HF Yaml License\": \"\", \"PwC Date\": \"\", \"PwC Description\": \"No code available yet.\", \"PwC License Name\": \"\", \"PwC License URL\": \"\", \"S2 Citation Count (June 2024)\": \"\", \"S2 Date\": \"\"}, \"Bibtex\": \"@Article{Monfort2021SpokenML,\\n author = {Mathew Monfort and SouYoung Jin},\\n booktitle = {Computer Vision and Pattern Recognition},\\n journal = {2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},\\n pages = {14866-14876},\\n title = {Spoken Moments: Learning Joint Audio-Visual Representations from Video Descriptions},\\n year = {2021}\\n}\\n\", \"License Use (DataProvenance)\": \"commercial\", \"License Attribution (DataProvenance)\": 0, \"License Share Alike (DataProvenance)\": 0, \"License Use (DataProvenance IgnoreOpenAI)\": \"commercial\", \"License Attribution (DataProvenance IgnoreOpenAI)\": 0, \"License Share Alike (DataProvenance IgnoreOpenAI)\": 0, \"License Type\": \"Commercial\"}, {\"Unique Dataset Identifier\": \"spoken-moments\", \"Collection\": \"spoken-moments\", \"Collection URL\": \"https://arxiv.org/abs/2105.04489\", \"Dataset Name\": \"Spoken Moments\", \"Paper Title\": \"Spoken Moments\", \"Paper URL\": \"https://arxiv.org/abs/2105.04489\", \"GitHub URL\": \"\", \"Hugging Face URL\": \"\", \"Papers with Code URL\": \"https://paperswithcode.com/paper/spoken-moments-learning-joint-audio-visual\", \"ArXiv URL\": \"https://arxiv.org/abs/2105.04489\", \"Semantic Scholar Corpus ID\": 233992735, \"Year Released\": \"2021\", \"Text Sources\": [\"youtube\", \"flickr\", \"vine\", \"metacafe\", \"peeks\", \"vimeo\", \"videoblocks\", \"bing videos\", \"giphy\", \"weather channel\", \"getty-images\"], \"Licenses\": [{\"License\": \"Custom\", \"License URL\": \"https://docs.google.com/forms/d/e/1FAIpQLSdUOScon6uWkslgcA2-BCkAhMp86SihgKtMqX53zRf-E2fIHw/formResponse\"}], \"Creators\": [\"Massachusetts Institute of Technology\", \"IBM\", \"The University of Texas at Austin\"], \"Countries\": [\"United States of America\"], \"License Verified By\": \"Vivek Sharma\", \"Video Hours\": 416.67, \"Taken Down\": \"False\", \"Video Sources\": \"Other\", \"Task Categories\": [\"Video Captioning\"], \"Inferred Metadata\": {\"GitHub License\": \"\", \"GitHub Stars (June 2024)\": \"\", \"GitHub Topics\": \"\", \"Github Date\": \"\", \"HF Config\": \"\", \"HF Config License\": \"\", \"HF Dataset\": \"\", \"HF Date\": \"\", \"HF Downloads (June 2024)\": \"\", \"HF Likes (June 2024)\": \"\", \"HF Yaml License\": \"\", \"PwC Date\": \"\", \"PwC Description\": \"No code available yet.\", \"PwC License Name\": \"\", \"PwC License URL\": \"\", \"S2 Citation Count (June 2024)\": \"\", \"S2 Date\": \"\"}, \"Bibtex\": \"@Article{Monfort2021SpokenML,\\n author = {Mathew Monfort and SouYoung Jin},\\n booktitle = {Computer Vision and Pattern Recognition},\\n journal = {2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},\\n pages = {14866-14876},\\n title = {Spoken Moments: Learning Joint Audio-Visual Representations from Video Descriptions},\\n year = {2021}\\n}\\n\", \"License Use (DataProvenance)\": \"commercial\", \"License Attribution (DataProvenance)\": 0, \"License Share Alike (DataProvenance)\": 0, \"License Use (DataProvenance IgnoreOpenAI)\": \"commercial\", \"License Attribution (DataProvenance IgnoreOpenAI)\": 0, \"License Share Alike (DataProvenance IgnoreOpenAI)\": 0, \"License Type\": \"Commercial\"}, {\"Unique Dataset Identifier\": \"spoken-moments\", \"Collection\": \"spoken-moments\", \"Collection URL\": \"https://arxiv.org/abs/2105.04489\", \"Dataset Name\": \"Spoken Moments\", \"Paper Title\": \"Spoken Moments\", \"Paper URL\": \"https://arxiv.org/abs/2105.04489\", \"GitHub URL\": \"\", \"Hugging Face URL\": \"\", \"Papers with Code URL\": \"https://paperswithcode.com/paper/spoken-moments-learning-joint-audio-visual\", \"ArXiv URL\": \"https://arxiv.org/abs/2105.04489\", \"Semantic Scholar Corpus ID\": 233992735, \"Year Released\": \"2021\", \"Text Sources\": [\"youtube\", \"flickr\", \"vine\", \"metacafe\", \"peeks\", \"vimeo\", \"videoblocks\", \"bing videos\", \"giphy\", \"weather channel\", \"getty-images\"], \"Licenses\": [{\"License\": \"Custom\", \"License URL\": \"https://docs.google.com/forms/d/e/1FAIpQLSdUOScon6uWkslgcA2-BCkAhMp86SihgKtMqX53zRf-E2fIHw/formResponse\"}], \"Creators\": [\"Massachusetts Institute of Technology\", \"IBM\", \"The University of Texas at Austin\"], \"Countries\": [\"United States of America\"], \"License Verified By\": \"Vivek Sharma\", \"Video Hours\": 416.67, \"Taken Down\": \"False\", \"Video Sources\": \"Other\", \"Task Categories\": [\"Video Captioning\"], \"Inferred Metadata\": {\"GitHub License\": \"\", \"GitHub Stars (June 2024)\": \"\", \"GitHub Topics\": \"\", \"Github Date\": \"\", \"HF Config\": \"\", \"HF Config License\": \"\", \"HF Dataset\": \"\", \"HF Date\": \"\", \"HF Downloads (June 2024)\": \"\", \"HF Likes (June 2024)\": \"\", \"HF Yaml License\": \"\", \"PwC Date\": \"\", \"PwC Description\": \"No code available yet.\", \"PwC License Name\": \"\", \"PwC License URL\": \"\", \"S2 Citation Count (June 2024)\": \"\", \"S2 Date\": \"\"}, \"Bibtex\": \"@Article{Monfort2021SpokenML,\\n author = {Mathew Monfort and SouYoung Jin},\\n booktitle = {Computer Vision and Pattern Recognition},\\n journal = {2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},\\n pages = {14866-14876},\\n title = {Spoken Moments: Learning Joint Audio-Visual Representations from Video Descriptions},\\n year = {2021}\\n}\\n\", \"License Use (DataProvenance)\": \"commercial\", \"License Attribution (DataProvenance)\": 0, \"License Share Alike (DataProvenance)\": 0, \"License Use (DataProvenance IgnoreOpenAI)\": \"commercial\", \"License Attribution (DataProvenance IgnoreOpenAI)\": 0, \"License Share Alike (DataProvenance IgnoreOpenAI)\": 0, \"License Type\": \"Commercial\"}, {\"Unique Dataset Identifier\": \"spoken-moments\", \"Collection\": \"spoken-moments\", \"Collection URL\": \"https://arxiv.org/abs/2105.04489\", \"Dataset Name\": \"Spoken Moments\", \"Paper Title\": \"Spoken Moments\", \"Paper URL\": \"https://arxiv.org/abs/2105.04489\", \"GitHub URL\": \"\", \"Hugging Face URL\": \"\", \"Papers with Code URL\": \"https://paperswithcode.com/paper/spoken-moments-learning-joint-audio-visual\", \"ArXiv URL\": \"https://arxiv.org/abs/2105.04489\", \"Semantic Scholar Corpus ID\": 233992735, \"Year Released\": \"2021\", \"Text Sources\": [\"youtube\", \"flickr\", \"vine\", \"metacafe\", \"peeks\", \"vimeo\", \"videoblocks\", \"bing videos\", \"giphy\", \"weather channel\", \"getty-images\"], \"Licenses\": [{\"License\": \"Custom\", \"License URL\": \"https://docs.google.com/forms/d/e/1FAIpQLSdUOScon6uWkslgcA2-BCkAhMp86SihgKtMqX53zRf-E2fIHw/formResponse\"}], \"Creators\": [\"Massachusetts Institute of Technology\", \"IBM\", \"The University of Texas at Austin\"], \"Countries\": [\"United States of America\"], \"License Verified By\": \"Vivek Sharma\", \"Video Hours\": 416.67, \"Taken Down\": \"False\", \"Video Sources\": \"Other\", \"Task Categories\": [\"Video Captioning\"], \"Inferred Metadata\": {\"GitHub License\": \"\", \"GitHub Stars (June 2024)\": \"\", \"GitHub Topics\": \"\", \"Github Date\": \"\", \"HF Config\": \"\", \"HF Config License\": \"\", \"HF Dataset\": \"\", \"HF Date\": \"\", \"HF Downloads (June 2024)\": \"\", \"HF Likes (June 2024)\": \"\", \"HF Yaml License\": \"\", \"PwC Date\": \"\", \"PwC Description\": \"No code available yet.\", \"PwC License Name\": \"\", \"PwC License URL\": \"\", \"S2 Citation Count (June 2024)\": \"\", \"S2 Date\": \"\"}, \"Bibtex\": \"@Article{Monfort2021SpokenML,\\n author = {Mathew Monfort and SouYoung Jin},\\n booktitle = {Computer Vision and Pattern Recognition},\\n journal = {2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},\\n pages = {14866-14876},\\n title = {Spoken Moments: Learning Joint Audio-Visual Representations from Video Descriptions},\\n year = {2021}\\n}\\n\", \"License Use (DataProvenance)\": \"commercial\", \"License Attribution (DataProvenance)\": 0, \"License Share Alike (DataProvenance)\": 0, \"License Use (DataProvenance IgnoreOpenAI)\": \"commercial\", \"License Attribution (DataProvenance IgnoreOpenAI)\": 0, \"License Share Alike (DataProvenance IgnoreOpenAI)\": 0, \"License Type\": \"Commercial\"}, {\"Unique Dataset Identifier\": \"spoken-moments\", \"Collection\": \"spoken-moments\", \"Collection URL\": \"https://arxiv.org/abs/2105.04489\", \"Dataset Name\": \"Spoken Moments\", \"Paper Title\": \"Spoken Moments\", \"Paper URL\": \"https://arxiv.org/abs/2105.04489\", \"GitHub URL\": \"\", \"Hugging Face URL\": \"\", \"Papers with Code URL\": \"https://paperswithcode.com/paper/spoken-moments-learning-joint-audio-visual\", \"ArXiv URL\": \"https://arxiv.org/abs/2105.04489\", \"Semantic Scholar Corpus ID\": 233992735, \"Year Released\": \"2021\", \"Text Sources\": [\"youtube\", \"flickr\", \"vine\", \"metacafe\", \"peeks\", \"vimeo\", \"videoblocks\", \"bing videos\", \"giphy\", \"weather channel\", \"getty-images\"], \"Licenses\": [{\"License\": \"Custom\", \"License URL\": \"https://docs.google.com/forms/d/e/1FAIpQLSdUOScon6uWkslgcA2-BCkAhMp86SihgKtMqX53zRf-E2fIHw/formResponse\"}], \"Creators\": [\"Massachusetts Institute of Technology\", \"IBM\", \"The University of Texas at Austin\"], \"Countries\": [\"United States of America\"], \"License Verified By\": \"Vivek Sharma\", \"Video Hours\": 416.67, \"Taken Down\": \"False\", \"Video Sources\": \"Other\", \"Task Categories\": [\"Video Captioning\"], \"Inferred Metadata\": {\"GitHub License\": \"\", \"GitHub Stars (June 2024)\": \"\", \"GitHub Topics\": \"\", \"Github Date\": \"\", \"HF Config\": \"\", \"HF Config License\": \"\", \"HF Dataset\": \"\", \"HF Date\": \"\", \"HF Downloads (June 2024)\": \"\", \"HF Likes (June 2024)\": \"\", \"HF Yaml License\": \"\", \"PwC Date\": \"\", \"PwC Description\": \"No code available yet.\", \"PwC License Name\": \"\", \"PwC License URL\": \"\", \"S2 Citation Count (June 2024)\": \"\", \"S2 Date\": \"\"}, \"Bibtex\": \"@Article{Monfort2021SpokenML,\\n author = {Mathew Monfort and SouYoung Jin},\\n booktitle = {Computer Vision and Pattern Recognition},\\n journal = {2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},\\n pages = {14866-14876},\\n title = {Spoken Moments: Learning Joint Audio-Visual Representations from Video Descriptions},\\n year = {2021}\\n}\\n\", \"License Use (DataProvenance)\": \"commercial\", \"License Attribution (DataProvenance)\": 0, \"License Share Alike (DataProvenance)\": 0, \"License Use (DataProvenance IgnoreOpenAI)\": \"commercial\", \"License Attribution (DataProvenance IgnoreOpenAI)\": 0, \"License Share Alike (DataProvenance IgnoreOpenAI)\": 0, \"License Type\": \"Commercial\"}, {\"Unique Dataset Identifier\": \"homage\", \"Collection\": \"homage\", \"Collection URL\": \"https://arxiv.org/abs/2105.05226\", \"Dataset Name\": \"HOMAGE: Home Action Genome: Cooperative Compositional Action Understanding (CVPR 2021)\", \"Paper Title\": \"HOMAGE: Home Action Genome: Cooperative Compositional Action Understanding (CVPR 2021)\", \"Paper URL\": \"https://arxiv.org/abs/2105.05226\", \"GitHub URL\": \"https://github.com/nishantrai18/homage\", \"Hugging Face URL\": \"\", \"Papers with Code URL\": \"https://paperswithcode.com/paper/home-action-genome-cooperative-compositional\", \"ArXiv URL\": \"https://arxiv.org/abs/2105.05226\", \"Semantic Scholar Corpus ID\": 234357543, \"Year Released\": \"2021\", \"Text Sources\": [\"human\"], \"Licenses\": [{\"License\": \"Unspecified\", \"License URL\": \"\"}], \"Creators\": [\"Stanford University\", \"Panasonic Corporation\"], \"Countries\": [\"United States of America\"], \"License Verified By\": \"Vivek Sharma\", \"Video Hours\": 30.0, \"Taken Down\": \"False\", \"Video Sources\": \"human\", \"Task Categories\": [\"Video Classification\", \"Temporal Action Segmentation\"], \"Inferred Metadata\": {\"GitHub License\": \"MIT License\", \"GitHub Stars (June 2024)\": 17, \"GitHub Topics\": [], \"Github Date\": \"\", \"HF Config\": \"\", \"HF Config License\": \"\", \"HF Dataset\": \"\", \"HF Date\": \"\", \"HF Downloads (June 2024)\": \"\", \"HF Likes (June 2024)\": \"\", \"HF Yaml License\": \"\", \"PwC Date\": \"\", \"PwC Description\": \"\\ud83c\\udfc6 SOTA for Video Classification on Home Action Genome (Accuracy (%) metric)\", \"PwC License Name\": \"\", \"PwC License URL\": \"\", \"S2 Citation Count (June 2024)\": \"\", \"S2 Date\": \"\"}, \"Bibtex\": \"@Article{Rai2021HomeAG,\\n author = {Nishant Rai and Haofeng Chen and Jingwei Ji and Rishi Desai and K. Kozuka and Shun Ishizaka and E. Adeli and Juan Carlos Niebles},\\n booktitle = {Computer Vision and Pattern Recognition},\\n journal = {2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},\\n pages = {11179-11188},\\n title = {Home Action Genome: Cooperative Compositional Action Understanding},\\n year = {2021}\\n}\\n\", \"License Use (DataProvenance)\": \"unspecified\", \"License Attribution (DataProvenance)\": 0, \"License Share Alike (DataProvenance)\": 0, \"License Use (DataProvenance IgnoreOpenAI)\": \"unspecified\", \"License Attribution (DataProvenance IgnoreOpenAI)\": 0, \"License Share Alike (DataProvenance IgnoreOpenAI)\": 0, \"License Type\": \"Unspecified\"}, {\"Unique Dataset Identifier\": \"queryd\", \"Collection\": \"queryd\", \"Collection URL\": \"https://arxiv.org/abs/2011.11071\", \"Dataset Name\": \"QuerYD\", \"Paper Title\": \"QuerYD\", \"Paper URL\": \"https://arxiv.org/abs/2011.11071\", \"GitHub URL\": \"\", \"Hugging Face URL\": \"\", \"Papers with Code URL\": \"https://paperswithcode.com/dataset/queryd\", \"ArXiv URL\": \"https://arxiv.org/abs/2011.11071\", \"Semantic Scholar Corpus ID\": 261006321, \"Year Released\": \"2021\", \"Text Sources\": [\"youtube\", \"youdescribe\"], \"Licenses\": [{\"License\": \"Unspecified\", \"License URL\": \"\"}], \"Creators\": [\"University of Oxford\"], \"Countries\": [\"United Kingdom\"], \"License Verified By\": \"Vivek Sharma\", \"Video Hours\": 207.0, \"Taken Down\": \"False\", \"Video Sources\": \"Other\", \"Task Categories\": [\"Video Captioning\"], \"Inferred Metadata\": {\"GitHub License\": \"\", \"GitHub Stars (June 2024)\": \"\", \"GitHub Topics\": \"\", \"Github Date\": \"\", \"HF Config\": \"\", \"HF Config License\": \"\", \"HF Dataset\": \"\", \"HF Date\": \"\", \"HF Downloads (June 2024)\": \"\", \"HF Likes (June 2024)\": \"\", \"HF Yaml License\": \"\", \"PwC Date\": \"\", \"PwC Description\": \"A large-scale dataset for retrieval and event localisation in video. A unique feature of the dataset is the availability of two audio tracks for each video: the original audio, and a high-quality spoken description of the visual content.\", \"PwC License Name\": \"Unspecified\", \"PwC License URL\": null, \"S2 Citation Count (June 2024)\": \"\", \"S2 Date\": \"\"}, \"Bibtex\": \"@Article{Oncescu2021QUERYDAV,\\n author = {Andreea-Maria Oncescu and Jo\\u00e3o F. Henriques and Yang Liu and Andrew Zisserman and Samuel Albanie},\\n booktitle = {IEEE International Conference on Acoustics, Speech, and Signal Processing},\\n journal = {ICASSP 2021 - 2021 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},\\n pages = {2265-2269},\\n title = {QUERYD: A Video Dataset with High-Quality Text and Audio Narrations},\\n year = {2021}\\n}\\n\", \"License Use (DataProvenance)\": \"unspecified\", \"License Attribution (DataProvenance)\": 0, \"License Share Alike (DataProvenance)\": 0, \"License Use (DataProvenance IgnoreOpenAI)\": \"unspecified\", \"License Attribution (DataProvenance IgnoreOpenAI)\": 0, \"License Share Alike (DataProvenance IgnoreOpenAI)\": 0, \"License Type\": \"Unspecified\"}, {\"Unique Dataset Identifier\": \"queryd\", \"Collection\": \"queryd\", \"Collection URL\": \"https://arxiv.org/abs/2011.11071\", \"Dataset Name\": \"QuerYD\", \"Paper Title\": \"QuerYD\", \"Paper URL\": \"https://arxiv.org/abs/2011.11071\", \"GitHub URL\": \"\", \"Hugging Face URL\": \"\", \"Papers with Code URL\": \"https://paperswithcode.com/dataset/queryd\", \"ArXiv URL\": \"https://arxiv.org/abs/2011.11071\", \"Semantic Scholar Corpus ID\": 261006321, \"Year Released\": \"2021\", \"Text Sources\": [\"youtube\", \"youdescribe\"], \"Licenses\": [{\"License\": \"Unspecified\", \"License URL\": \"\"}], \"Creators\": [\"University of Oxford\"], \"Countries\": [\"United Kingdom\"], \"License Verified By\": \"Vivek Sharma\", \"Video Hours\": 207.0, \"Taken Down\": \"False\", \"Video Sources\": \"youtube\", \"Task Categories\": [\"Video Captioning\"], \"Inferred Metadata\": {\"GitHub License\": \"\", \"GitHub Stars (June 2024)\": \"\", \"GitHub Topics\": \"\", \"Github Date\": \"\", \"HF Config\": \"\", \"HF Config License\": \"\", \"HF Dataset\": \"\", \"HF Date\": \"\", \"HF Downloads (June 2024)\": \"\", \"HF Likes (June 2024)\": \"\", \"HF Yaml License\": \"\", \"PwC Date\": \"\", \"PwC Description\": \"A large-scale dataset for retrieval and event localisation in video. A unique feature of the dataset is the availability of two audio tracks for each video: the original audio, and a high-quality spoken description of the visual content.\", \"PwC License Name\": \"Unspecified\", \"PwC License URL\": null, \"S2 Citation Count (June 2024)\": \"\", \"S2 Date\": \"\"}, \"Bibtex\": \"@Article{Oncescu2021QUERYDAV,\\n author = {Andreea-Maria Oncescu and Jo\\u00e3o F. Henriques and Yang Liu and Andrew Zisserman and Samuel Albanie},\\n booktitle = {IEEE International Conference on Acoustics, Speech, and Signal Processing},\\n journal = {ICASSP 2021 - 2021 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},\\n pages = {2265-2269},\\n title = {QUERYD: A Video Dataset with High-Quality Text and Audio Narrations},\\n year = {2021}\\n}\\n\", \"License Use (DataProvenance)\": \"unspecified\", \"License Attribution (DataProvenance)\": 0, \"License Share Alike (DataProvenance)\": 0, \"License Use (DataProvenance IgnoreOpenAI)\": \"unspecified\", \"License Attribution (DataProvenance IgnoreOpenAI)\": 0, \"License Share Alike (DataProvenance IgnoreOpenAI)\": 0, \"License Type\": \"Unspecified\"}, {\"Unique Dataset Identifier\": \"videolt-dataset\", \"Collection\": \"videolt-dataset\", \"Collection URL\": \"https://arxiv.org/abs/2105.02668\", \"Dataset Name\": \"VideoLT: Large-scale Long-tailed Video Recognition\", \"Paper Title\": \"VideoLT: Large-scale Long-tailed Video Recognition\", \"Paper URL\": \"https://arxiv.org/abs/2105.02668\", \"GitHub URL\": \"https://github.com/17Skye17/VideoLT\", \"Hugging Face URL\": \"\", \"Papers with Code URL\": \"https://paperswithcode.com/dataset/videolt\", \"ArXiv URL\": \"https://arxiv.org/abs/2105.02668\", \"Semantic Scholar Corpus ID\": 233864776, \"Year Released\": \"2021\", \"Text Sources\": [\"youtube\"], \"Licenses\": [{\"License\": \"Non Commercial\", \"License URL\": \"https://github.com/17Skye17/VideoLT?tab=readme-ov-file#data-preparation\"}], \"Creators\": [\"Fudan University\", \"Shanghai Collaborative Innovation Center of Intelligent Visual Computing\", \"Inception Institute of Artificial Intelligence\", \"University of Maryland\"], \"Countries\": [\"China\", \"UAE\"], \"License Verified By\": \"Vivek Sharma\", \"Video Hours\": 13664.96, \"Taken Down\": \"False\", \"Video Sources\": \"youtube\", \"Task Categories\": [\"Video Classification\"], \"Inferred Metadata\": {\"GitHub License\": \"\", \"GitHub Stars (June 2024)\": 33, \"GitHub Topics\": [\"long-tailed-recognition\", \"video-classification\", \"video-dataset\"], \"Github Date\": \"\", \"HF Config\": \"\", \"HF Config License\": \"\", \"HF Dataset\": \"\", \"HF Date\": \"\", \"HF Downloads (June 2024)\": \"\", \"HF Likes (June 2024)\": \"\", \"HF Yaml License\": \"\", \"PwC Date\": \"2021-05-06\", \"PwC Description\": \"VideoLT is a large-scale long-tailed video recognition dataset that contains 256,218 untrimmed videos, annotated into 1,004 classes with a long-tailed distribution.\", \"PwC License Name\": \"Unspecified\", \"PwC License URL\": null, \"S2 Citation Count (June 2024)\": \"\", \"S2 Date\": \"\"}, \"Bibtex\": \"@Article{Zhang2021VideoLTLL,\\n author = {Xing Zhang and Zuxuan Wu and Zejia Weng and H. Fu and Jingjing Chen and Yu-Gang Jiang and Larry S. Davis},\\n booktitle = {IEEE International Conference on Computer Vision},\\n journal = {2021 IEEE/CVF International Conference on Computer Vision (ICCV)},\\n pages = {7940-7949},\\n title = {VideoLT: Large-scale Long-tailed Video Recognition},\\n year = {2021}\\n}\\n\", \"License Use (DataProvenance)\": \"non-commercial\", \"License Attribution (DataProvenance)\": 0, \"License Share Alike (DataProvenance)\": 0, \"License Use (DataProvenance IgnoreOpenAI)\": \"non-commercial\", \"License Attribution (DataProvenance IgnoreOpenAI)\": 0, \"License Share Alike (DataProvenance IgnoreOpenAI)\": 0, \"License Type\": \"Non-Commercial/Academic\"}, {\"Unique Dataset Identifier\": \"spoken-moments\", \"Collection\": \"spoken-moments\", \"Collection URL\": \"https://arxiv.org/abs/2105.04489\", \"Dataset Name\": \"Spoken Moments\", \"Paper Title\": \"Spoken Moments\", \"Paper URL\": \"https://arxiv.org/abs/2105.04489\", \"GitHub URL\": \"\", \"Hugging Face URL\": \"\", \"Papers with Code URL\": \"https://paperswithcode.com/paper/spoken-moments-learning-joint-audio-visual\", \"ArXiv URL\": \"https://arxiv.org/abs/2105.04489\", \"Semantic Scholar Corpus ID\": 233992735, \"Year Released\": \"2021\", \"Text Sources\": [\"youtube\", \"flickr\", \"vine\", \"metacafe\", \"peeks\", \"vimeo\", \"videoblocks\", \"bing videos\", \"giphy\", \"weather channel\", \"getty-images\"], \"Licenses\": [{\"License\": \"Custom\", \"License URL\": \"https://docs.google.com/forms/d/e/1FAIpQLSdUOScon6uWkslgcA2-BCkAhMp86SihgKtMqX53zRf-E2fIHw/formResponse\"}], \"Creators\": [\"Massachusetts Institute of Technology\", \"IBM\", \"The University of Texas at Austin\"], \"Countries\": [\"United States of America\"], \"License Verified By\": \"Vivek Sharma\", \"Video Hours\": 416.67, \"Taken Down\": \"False\", \"Video Sources\": \"Other\", \"Task Categories\": [\"Video Captioning\"], \"Inferred Metadata\": {\"GitHub License\": \"\", \"GitHub Stars (June 2024)\": \"\", \"GitHub Topics\": \"\", \"Github Date\": \"\", \"HF Config\": \"\", \"HF Config License\": \"\", \"HF Dataset\": \"\", \"HF Date\": \"\", \"HF Downloads (June 2024)\": \"\", \"HF Likes (June 2024)\": \"\", \"HF Yaml License\": \"\", \"PwC Date\": \"\", \"PwC Description\": \"No code available yet.\", \"PwC License Name\": \"\", \"PwC License URL\": \"\", \"S2 Citation Count (June 2024)\": \"\", \"S2 Date\": \"\"}, \"Bibtex\": \"@Article{Monfort2021SpokenML,\\n author = {Mathew Monfort and SouYoung Jin},\\n booktitle = {Computer Vision and Pattern Recognition},\\n journal = {2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},\\n pages = {14866-14876},\\n title = {Spoken Moments: Learning Joint Audio-Visual Representations from Video Descriptions},\\n year = {2021}\\n}\\n\", \"License Use (DataProvenance)\": \"commercial\", \"License Attribution (DataProvenance)\": 0, \"License Share Alike (DataProvenance)\": 0, \"License Use (DataProvenance IgnoreOpenAI)\": \"commercial\", \"License Attribution (DataProvenance IgnoreOpenAI)\": 0, \"License Share Alike (DataProvenance IgnoreOpenAI)\": 0, \"License Type\": \"Commercial\"}, {\"Unique Dataset Identifier\": \"spoken-moments\", \"Collection\": \"spoken-moments\", \"Collection URL\": \"https://arxiv.org/abs/2105.04489\", \"Dataset Name\": \"Spoken Moments\", \"Paper Title\": \"Spoken Moments\", \"Paper URL\": \"https://arxiv.org/abs/2105.04489\", \"GitHub URL\": \"\", \"Hugging Face URL\": \"\", \"Papers with Code URL\": \"https://paperswithcode.com/paper/spoken-moments-learning-joint-audio-visual\", \"ArXiv URL\": \"https://arxiv.org/abs/2105.04489\", \"Semantic Scholar Corpus ID\": 233992735, \"Year Released\": \"2021\", \"Text Sources\": [\"youtube\", \"flickr\", \"vine\", \"metacafe\", \"peeks\", \"vimeo\", \"videoblocks\", \"bing videos\", \"giphy\", \"weather channel\", \"getty-images\"], \"Licenses\": [{\"License\": \"Custom\", \"License URL\": \"https://docs.google.com/forms/d/e/1FAIpQLSdUOScon6uWkslgcA2-BCkAhMp86SihgKtMqX53zRf-E2fIHw/formResponse\"}], \"Creators\": [\"Massachusetts Institute of Technology\", \"IBM\", \"The University of Texas at Austin\"], \"Countries\": [\"United States of America\"], \"License Verified By\": \"Vivek Sharma\", \"Video Hours\": 416.67, \"Taken Down\": \"False\", \"Video Sources\": \"flickr\", \"Task Categories\": [\"Video Captioning\"], \"Inferred Metadata\": {\"GitHub License\": \"\", \"GitHub Stars (June 2024)\": \"\", \"GitHub Topics\": \"\", \"Github Date\": \"\", \"HF Config\": \"\", \"HF Config License\": \"\", \"HF Dataset\": \"\", \"HF Date\": \"\", \"HF Downloads (June 2024)\": \"\", \"HF Likes (June 2024)\": \"\", \"HF Yaml License\": \"\", \"PwC Date\": \"\", \"PwC Description\": \"No code available yet.\", \"PwC License Name\": \"\", \"PwC License URL\": \"\", \"S2 Citation Count (June 2024)\": \"\", \"S2 Date\": \"\"}, \"Bibtex\": \"@Article{Monfort2021SpokenML,\\n author = {Mathew Monfort and SouYoung Jin},\\n booktitle = {Computer Vision and Pattern Recognition},\\n journal = {2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},\\n pages = {14866-14876},\\n title = {Spoken Moments: Learning Joint Audio-Visual Representations from Video Descriptions},\\n year = {2021}\\n}\\n\", \"License Use (DataProvenance)\": \"commercial\", \"License Attribution (DataProvenance)\": 0, \"License Share Alike (DataProvenance)\": 0, \"License Use (DataProvenance IgnoreOpenAI)\": \"commercial\", \"License Attribution (DataProvenance IgnoreOpenAI)\": 0, \"License Share Alike (DataProvenance IgnoreOpenAI)\": 0, \"License Type\": \"Commercial\"}, {\"Unique Dataset Identifier\": \"hd-vila-100m\", \"Collection\": \"hd-vila-100m\", \"Collection URL\": \"https://arxiv.org/abs/2111.10337\", \"Dataset Name\": \"Advancing High-Resolution Video-Language Representation with Large-Scale Video Transcriptions\", \"Paper Title\": \"Advancing High-Resolution Video-Language Representation with Large-Scale Video Transcriptions\", \"Paper URL\": \"https://arxiv.org/abs/2111.10337\", \"GitHub URL\": \"https://github.com/microsoft/XPretrain/blob/main/hd-vila-100m/README.md\", \"Hugging Face URL\": \"\", \"Papers with Code URL\": \"https://paperswithcode.com/paper/advancing-high-resolution-video-language/review/\", \"ArXiv URL\": \"https://arxiv.org/abs/2111.10337\", \"Semantic Scholar Corpus ID\": 244462849, \"Year Released\": \"2021\", \"Text Sources\": [\"youtube\"], \"Licenses\": [{\"License\": \"Custom\", \"License URL\": \"https://github.com/microsoft/XPretrain/blob/main/hd-vila-100m/README.md\"}], \"Creators\": [\"Microsoft\"], \"Countries\": [\"United States of America\"], \"License Verified By\": \"Vivek Sharma\", \"Video Hours\": 371.5, \"Taken Down\": \"False\", \"Video Sources\": \"youtube\", \"Task Categories\": [\"Video Captioning\"], \"Inferred Metadata\": {\"GitHub License\": \"\", \"GitHub Stars (June 2024)\": 0, \"GitHub Topics\": [\"\"], \"Github Date\": \"\", \"HF Config\": \"\", \"HF Config License\": \"\", \"HF Dataset\": \"\", \"HF Date\": \"\", \"HF Downloads (June 2024)\": \"\", \"HF Likes (June 2024)\": \"\", \"HF Yaml License\": \"\", \"PwC Date\": \"\", \"PwC Description\": \"Paper tables with annotated results for Advancing High-Resolution Video-Language Representation with Large-Scale Video Transcriptions\", \"PwC License Name\": \"\", \"PwC License URL\": \"\", \"S2 Citation Count (June 2024)\": \"\", \"S2 Date\": \"\"}, \"Bibtex\": \"@Article{Xue2021AdvancingHV,\\n author = {Hongwei Xue and Tiankai Hang and Yanhong Zeng and Yuchong Sun and Bei Liu and Huan Yang and Jianlong Fu and B. Guo},\\n booktitle = {Computer Vision and Pattern Recognition},\\n journal = {2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},\\n pages = {5026-5035},\\n title = {Advancing High-Resolution Video-Language Representation with Large-Scale Video Transcriptions},\\n year = {2021}\\n}\\n\", \"License Use (DataProvenance)\": \"academic-only\", \"License Attribution (DataProvenance)\": 1, \"License Share Alike (DataProvenance)\": 0, \"License Use (DataProvenance IgnoreOpenAI)\": \"academic-only\", \"License Attribution (DataProvenance IgnoreOpenAI)\": 1, \"License Share Alike (DataProvenance IgnoreOpenAI)\": 0, \"License Type\": \"Non-Commercial/Academic\"}, {\"Unique Dataset Identifier\": \"YT-Temporal-180m\", \"Collection\": \"YT-Temporal-180m\", \"Collection URL\": \"https://arxiv.org/pdf/2106.02636\", \"Dataset Name\": \"MERLOT: Multimodal Neural Script Knowledge Models\", \"Paper Title\": \"MERLOT: Multimodal Neural Script Knowledge Models\", \"Paper URL\": \"https://arxiv.org/pdf/2106.02636\", \"GitHub URL\": \"https://github.com/rowanz/merlot\", \"Hugging Face URL\": \"https://huggingface.co/datasets/HuggingFaceM4/yttemporal180m\", \"Papers with Code URL\": \"https://paperswithcode.com/paper/merlot-multimodal-neural-script-knowledge\", \"ArXiv URL\": \"https://arxiv.org/pdf/2106.02636\", \"Semantic Scholar Corpus ID\": 235352775, \"Year Released\": \"2021\", \"Text Sources\": [\"youtube\"], \"Licenses\": [{\"License\": \"MIT License\", \"License URL\": \"https://github.com/rowanz/merlot/blob/main/LICENSE\"}], \"Creators\": [\"University of Washington\", \"Allen Institute for AI\"], \"Countries\": [\"United States of America\"], \"License Verified By\": \"Vivek Sharma\", \"Video Hours\": 1515.0, \"Taken Down\": \"False\", \"Video Sources\": \"youtube\", \"Task Categories\": [\"Video Q&A\"], \"Inferred Metadata\": null, \"Bibtex\": null, \"License Use (DataProvenance)\": \"commercial\", \"License Attribution (DataProvenance)\": 1, \"License Share Alike (DataProvenance)\": 0, \"License Use (DataProvenance IgnoreOpenAI)\": \"commercial\", \"License Attribution (DataProvenance IgnoreOpenAI)\": 1, \"License Share Alike (DataProvenance IgnoreOpenAI)\": 0, \"License Type\": \"Commercial\"}, {\"Unique Dataset Identifier\": \"mimetics-dataset\", \"Collection\": \"mimetics-dataset\", \"Collection URL\": \"https://arxiv.org/abs/1912.07249\", \"Dataset Name\": \"Mimetics: Towards Understanding Human Actions Out of Context\", \"Paper Title\": \"Mimetics: Towards Understanding Human Actions Out of Context\", \"Paper URL\": \"https://arxiv.org/abs/1912.07249\", \"GitHub URL\": \"\", \"Hugging Face URL\": \"\", \"Papers with Code URL\": \"https://paperswithcode.com/paper/mimetics-towards-understanding-human-actions\", \"ArXiv URL\": \"https://arxiv.org/abs/1912.07249\", \"Semantic Scholar Corpus ID\": 209376248, \"Year Released\": \"2021\", \"Text Sources\": [\"youtube\"], \"Licenses\": [{\"License\": \"Unspecified\", \"License URL\": \"\"}], \"Creators\": [\"Naver\"], \"Countries\": [\"South Korea\"], \"License Verified By\": \"Vivek Sharma\", \"Video Hours\": 0.99, \"Taken Down\": \"False\", \"Video Sources\": \"youtube\", \"Task Categories\": [\"Video Classification\"], \"Inferred Metadata\": {\"GitHub License\": \"\", \"GitHub Stars (June 2024)\": \"\", \"GitHub Topics\": \"\", \"Github Date\": \"\", \"HF Config\": \"\", \"HF Config License\": \"\", \"HF Dataset\": \"\", \"HF Date\": \"\", \"HF Downloads (June 2024)\": \"\", \"HF Likes (June 2024)\": \"\", \"HF Yaml License\": \"\", \"PwC Date\": \"\", \"PwC Description\": \"No code available yet.\", \"PwC License Name\": \"\", \"PwC License URL\": \"\", \"S2 Citation Count (June 2024)\": \"\", \"S2 Date\": \"\"}, \"Bibtex\": \"@Article{Weinzaepfel2019MimeticsTU,\\n author = {Philippe Weinzaepfel and Gr\\u00e9gory Rogez},\\n booktitle = {International Journal of Computer Vision},\\n journal = {International Journal of Computer Vision},\\n pages = {1675 - 1690},\\n title = {Mimetics: Towards Understanding Human Actions Out of Context},\\n volume = {129},\\n year = {2019}\\n}\\n\", \"License Use (DataProvenance)\": \"unspecified\", \"License Attribution (DataProvenance)\": 0, \"License Share Alike (DataProvenance)\": 0, \"License Use (DataProvenance IgnoreOpenAI)\": \"unspecified\", \"License Attribution (DataProvenance IgnoreOpenAI)\": 0, \"License Share Alike (DataProvenance IgnoreOpenAI)\": 0, \"License Type\": \"Unspecified\"}, {\"Unique Dataset Identifier\": \"apes\", \"Collection\": \"apes\", \"Collection URL\": \"https://arxiv.org/pdf/2106.01667\", \"Dataset Name\": \"Apes\", \"Paper Title\": \"Apes\", \"Paper URL\": \"https://arxiv.org/pdf/2106.01667\", \"GitHub URL\": \"https://github.com/fuankarion/audiovisual-person-search\", \"Hugging Face URL\": \"\", \"Papers with Code URL\": \"https://paperswithcode.com/paper/apes-audiovisual-person-search-in-untrimmed/review/\", \"ArXiv URL\": \"https://arxiv.org/pdf/2106.01667\", \"Semantic Scholar Corpus ID\": 235313698, \"Year Released\": \"2021\", \"Text Sources\": [\"movies\"], \"Licenses\": [{\"License\": \"Unspecified\", \"License URL\": \"\"}], \"Creators\": [\"Universidad de los Andes\", \"Adobe Research\", \"King Abdullah University of Science and Technology (KAUST)\"], \"Countries\": [\"Chile\", \"United States of America\", \"Saudi Arabia\"], \"License Verified By\": \"Vivek Sharma\", \"Video Hours\": 36.0, \"Taken Down\": \"False\", \"Video Sources\": \"movies\", \"Task Categories\": [\"Video Classification\"], \"Inferred Metadata\": {\"GitHub License\": \"\", \"GitHub Stars (June 2024)\": 4, \"GitHub Topics\": [], \"Github Date\": \"\", \"HF Config\": \"\", \"HF Config License\": \"\", \"HF Dataset\": \"\", \"HF Date\": \"\", \"HF Downloads (June 2024)\": \"\", \"HF Likes (June 2024)\": \"\", \"HF Yaml License\": \"\", \"PwC Date\": \"\", \"PwC Description\": \"Paper tables with annotated results for APES: Audiovisual Person Search in Untrimmed Video\", \"PwC License Name\": \"\", \"PwC License URL\": \"\", \"S2 Citation Count (June 2024)\": \"\", \"S2 Date\": \"\"}, \"Bibtex\": \"@Article{Alcazar2021APESAP,\\n author = {Juan Leon Alcazar and Long Mai and Federico Perazzi and Joon-Young Lee and Pablo Arbel\\u00e1ez and Bernard Ghanem and Fabian Caba Heilbron},\\n booktitle = {2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)},\\n journal = {2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)},\\n pages = {1720-1729},\\n title = {APES: Audiovisual Person Search in Untrimmed Video},\\n year = {2021}\\n}\\n\", \"License Use (DataProvenance)\": \"unspecified\", \"License Attribution (DataProvenance)\": 0, \"License Share Alike (DataProvenance)\": 0, \"License Use (DataProvenance IgnoreOpenAI)\": \"unspecified\", \"License Attribution (DataProvenance IgnoreOpenAI)\": 0, \"License Share Alike (DataProvenance IgnoreOpenAI)\": 0, \"License Type\": \"Unspecified\"}, {\"Unique Dataset Identifier\": \"WebVid\", \"Collection\": \"WebVid\", \"Collection URL\": \"https://arxiv.org/abs/2104.00650\", \"Dataset Name\": \"WebVid\", \"Paper Title\": \"WebVid\", \"Paper URL\": \"https://arxiv.org/abs/2104.00650\", \"GitHub URL\": \"https://github.com/m-bain/webvid\", \"Hugging Face URL\": \"https://huggingface.co/datasets/TempoFunk/webvid-10M\", \"Papers with Code URL\": \"https://paperswithcode.com/dataset/webvid\", \"ArXiv URL\": \"https://arxiv.org/abs/2104.00650\", \"Semantic Scholar Corpus ID\": 232478955, \"Year Released\": \"2021\", \"Text Sources\": [\"undisclosed web\"], \"Licenses\": [{\"License\": \"Custom\", \"License URL\": \"https://github.com/m-bain/webvid/blob/main/TERMS.md\"}], \"Creators\": [\"University of Oxford\", \"CNRS\"], \"Countries\": [\"United Kingdom\", \"France\"], \"License Verified By\": \"Vivek Sharma\", \"Video Hours\": 13000.0, \"Taken Down\": \"True\", \"Video Sources\": \"undisclosed web\", \"Task Categories\": [\"Video Captioning\"], \"Inferred Metadata\": {\"GitHub License\": \"\", \"GitHub Stars (June 2024)\": 528, \"GitHub Topics\": [], \"Github Date\": \"\", \"HF Config\": \"default\", \"HF Config License\": \"\", \"HF Dataset\": \"TempoFunk/webvid-10M\", \"HF Date\": \"2023-06-16\", \"HF Downloads (June 2024)\": 360, \"HF Likes (June 2024)\": 28, \"HF Yaml License\": \"GNU General Public License v3.0\", \"PwC Date\": \"2021-04-01\", \"PwC Description\": \"WebVid contains 10 million video clips with captions, sourced from the web. The videos are diverse and rich in their content.\\n\\nBoth the full 10M set and a 2.5M subset is available for download:\\nhttps://github.com/m-bain/webvid-dataset\", \"PwC License Name\": \"Various\", \"PwC License URL\": \"https://github.com/m-bain/webvid/blob/main/TERMS.md\", \"S2 Citation Count (June 2024)\": \"\", \"S2 Date\": \"\"}, \"Bibtex\": \"@Article{Bain2021FrozenIT,\\n author = {Max Bain and Arsha Nagrani and G\\u00fcl Varol and Andrew Zisserman},\\n booktitle = {IEEE International Conference on Computer Vision},\\n journal = {2021 IEEE/CVF International Conference on Computer Vision (ICCV)},\\n pages = {1708-1718},\\n title = {Frozen in Time: A Joint Video and Image Encoder for End-to-End Retrieval},\\n year = {2021}\\n}\\n\", \"License Use (DataProvenance)\": \"academic-only\", \"License Attribution (DataProvenance)\": 0, \"License Share Alike (DataProvenance)\": 0, \"License Use (DataProvenance IgnoreOpenAI)\": \"academic-only\", \"License Attribution (DataProvenance IgnoreOpenAI)\": 0, \"License Share Alike (DataProvenance IgnoreOpenAI)\": 0, \"License Type\": \"Non-Commercial/Academic\"}, {\"Unique Dataset Identifier\": \"multi-moments-in-time-dataset\", \"Collection\": \"multi-moments-in-time-dataset\", \"Collection URL\": \"https://arxiv.org/pdf/1911.00232.pdf\", \"Dataset Name\": \"Multi-Moments in Time: Learning and Interpreting Models for Multi-Action Video Understanding\", \"Paper Title\": \"Multi-Moments in Time: Learning and Interpreting Models for Multi-Action Video Understanding\", \"Paper URL\": \"https://arxiv.org/pdf/1911.00232.pdf\", \"GitHub URL\": \"\", \"Hugging Face URL\": \"\", \"Papers with Code URL\": \"\", \"ArXiv URL\": \"https://arxiv.org/pdf/1911.00232.pdf\", \"Semantic Scholar Corpus ID\": 207780280, \"Year Released\": \"2021\", \"Text Sources\": [\"crowdsourced\", \"undisclosed web\"], \"Licenses\": [{\"License\": \"Unspecified\", \"License URL\": \"\"}], \"Creators\": [\"Massachusetts Institute of Technology\"], \"Countries\": [\"United States of America\"], \"License Verified By\": \"Vivek Sharma\", \"Video Hours\": 833.0, \"Taken Down\": \"False\", \"Video Sources\": \"crowdsourced\", \"Task Categories\": [\"Video Classification\"], \"Inferred Metadata\": {\"GitHub License\": \"\", \"GitHub Stars (June 2024)\": \"\", \"GitHub Topics\": \"\", \"Github Date\": \"\", \"HF Config\": \"\", \"HF Config License\": \"\", \"HF Dataset\": \"\", \"HF Date\": \"\", \"HF Downloads (June 2024)\": \"\", \"HF Likes (June 2024)\": \"\", \"HF Yaml License\": \"\", \"PwC Date\": \"\", \"PwC Description\": \"\", \"PwC License Name\": \"\", \"PwC License URL\": \"\", \"S2 Citation Count (June 2024)\": \"\", \"S2 Date\": \"\"}, \"Bibtex\": \"@Article{Monfort2019MultiMomentsIT,\\n author = {Mathew Monfort and K. Ramakrishnan and A. Andonian and Barry A. McNamara and A. Lascelles and Bowen Pan and Quanfu Fan and Dan Gutfreund and R. Feris and A. Oliva},\\n booktitle = {IEEE Transactions on Pattern Analysis and Machine Intelligence},\\n journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},\\n pages = {1-1},\\n title = {Multi-Moments in Time: Learning and Interpreting Models for Multi-Action Video Understanding},\\n volume = {PP},\\n year = {2019}\\n}\\n\", \"License Use (DataProvenance)\": \"unspecified\", \"License Attribution (DataProvenance)\": 0, \"License Share Alike (DataProvenance)\": 0, \"License Use (DataProvenance IgnoreOpenAI)\": \"unspecified\", \"License Attribution (DataProvenance IgnoreOpenAI)\": 0, \"License Share Alike (DataProvenance IgnoreOpenAI)\": 0, \"License Type\": \"Unspecified\"}, {\"Unique Dataset Identifier\": \"spoken-moments\", \"Collection\": \"spoken-moments\", \"Collection URL\": \"https://arxiv.org/abs/2105.04489\", \"Dataset Name\": \"Spoken Moments\", \"Paper Title\": \"Spoken Moments\", \"Paper URL\": \"https://arxiv.org/abs/2105.04489\", \"GitHub URL\": \"\", \"Hugging Face URL\": \"\", \"Papers with Code URL\": \"https://paperswithcode.com/paper/spoken-moments-learning-joint-audio-visual\", \"ArXiv URL\": \"https://arxiv.org/abs/2105.04489\", \"Semantic Scholar Corpus ID\": 233992735, \"Year Released\": \"2021\", \"Text Sources\": [\"youtube\", \"flickr\", \"vine\", \"metacafe\", \"peeks\", \"vimeo\", \"videoblocks\", \"bing videos\", \"giphy\", \"weather channel\", \"getty-images\"], \"Licenses\": [{\"License\": \"Custom\", \"License URL\": \"https://docs.google.com/forms/d/e/1FAIpQLSdUOScon6uWkslgcA2-BCkAhMp86SihgKtMqX53zRf-E2fIHw/formResponse\"}], \"Creators\": [\"Massachusetts Institute of Technology\", \"IBM\", \"The University of Texas at Austin\"], \"Countries\": [\"United States of America\"], \"License Verified By\": \"Vivek Sharma\", \"Video Hours\": 416.67, \"Taken Down\": \"False\", \"Video Sources\": \"youtube\", \"Task Categories\": [\"Video Captioning\"], \"Inferred Metadata\": {\"GitHub License\": \"\", \"GitHub Stars (June 2024)\": \"\", \"GitHub Topics\": \"\", \"Github Date\": \"\", \"HF Config\": \"\", \"HF Config License\": \"\", \"HF Dataset\": \"\", \"HF Date\": \"\", \"HF Downloads (June 2024)\": \"\", \"HF Likes (June 2024)\": \"\", \"HF Yaml License\": \"\", \"PwC Date\": \"\", \"PwC Description\": \"No code available yet.\", \"PwC License Name\": \"\", \"PwC License URL\": \"\", \"S2 Citation Count (June 2024)\": \"\", \"S2 Date\": \"\"}, \"Bibtex\": \"@Article{Monfort2021SpokenML,\\n author = {Mathew Monfort and SouYoung Jin},\\n booktitle = {Computer Vision and Pattern Recognition},\\n journal = {2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},\\n pages = {14866-14876},\\n title = {Spoken Moments: Learning Joint Audio-Visual Representations from Video Descriptions},\\n year = {2021}\\n}\\n\", \"License Use (DataProvenance)\": \"commercial\", \"License Attribution (DataProvenance)\": 0, \"License Share Alike (DataProvenance)\": 0, \"License Use (DataProvenance IgnoreOpenAI)\": \"commercial\", \"License Attribution (DataProvenance IgnoreOpenAI)\": 0, \"License Share Alike (DataProvenance IgnoreOpenAI)\": 0, \"License Type\": \"Commercial\"}, {\"Unique Dataset Identifier\": \"uav-human\", \"Collection\": \"uav-human\", \"Collection URL\": \"https://arxiv.org/abs/2104.00946\", \"Dataset Name\": \"UAV-Human: A Large Benchmark for Human Behavior Understanding with Unmanned Aerial Vehicles\", \"Paper Title\": \"UAV-Human: A Large Benchmark for Human Behavior Understanding with Unmanned Aerial Vehicles\", \"Paper URL\": \"https://arxiv.org/abs/2104.00946\", \"GitHub URL\": \"https://github.com/sutdcv/UAV-Human\", \"Hugging Face URL\": \"\", \"Papers with Code URL\": \"https://paperswithcode.com/paper/uav-human-a-large-benchmark-for-human\", \"ArXiv URL\": \"https://arxiv.org/abs/2104.00946\", \"Semantic Scholar Corpus ID\": 233004700, \"Year Released\": \"2021\", \"Text Sources\": [\"human\"], \"Licenses\": [{\"License\": \"Custom\", \"License URL\": \"https://sutdcv.github.io/uav-human-web/\"}], \"Creators\": [\"Shandong University\", \"Singapore University of Technology and Design\"], \"Countries\": [\"Singapore\", \"China\"], \"License Verified By\": \"Vivek Sharma\", \"Video Hours\": 18.34, \"Taken Down\": \"False\", \"Video Sources\": \"human\", \"Task Categories\": [\"Video Classification\"], \"Inferred Metadata\": {\"GitHub License\": \"\", \"GitHub Stars (June 2024)\": 180, \"GitHub Topics\": [\"action-recognition\", \"dataset\", \"uav\"], \"Github Date\": \"\", \"HF Config\": \"\", \"HF Config License\": \"\", \"HF Dataset\": \"\", \"HF Date\": \"\", \"HF Downloads (June 2024)\": \"\", \"HF Likes (June 2024)\": \"\", \"HF Yaml License\": \"\", \"PwC Date\": \"\", \"PwC Description\": \"Implemented in 2 code libraries.\", \"PwC License Name\": \"\", \"PwC License URL\": \"\", \"S2 Citation Count (June 2024)\": \"\", \"S2 Date\": \"\"}, \"Bibtex\": \"@Article{Li2021UAVHumanAL,\\n author = {Tianjiao Li and Jun Liu and Wei Zhang and Yun Ni and Wenqian Wang and Zhiheng Li},\\n booktitle = {Computer Vision and Pattern Recognition},\\n journal = {2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},\\n pages = {16261-16270},\\n title = {UAV-Human: A Large Benchmark for Human Behavior Understanding with Unmanned Aerial Vehicles},\\n year = {2021}\\n}\\n\", \"License Use (DataProvenance)\": \"academic-only\", \"License Attribution (DataProvenance)\": 1, \"License Share Alike (DataProvenance)\": 0, \"License Use (DataProvenance IgnoreOpenAI)\": \"academic-only\", \"License Attribution (DataProvenance IgnoreOpenAI)\": 1, \"License Share Alike (DataProvenance IgnoreOpenAI)\": 0, \"License Type\": \"Non-Commercial/Academic\"}, {\"Unique Dataset Identifier\": \"multi-moments-in-time-dataset\", \"Collection\": \"multi-moments-in-time-dataset\", \"Collection URL\": \"https://arxiv.org/pdf/1911.00232.pdf\", \"Dataset Name\": \"Multi-Moments in Time: Learning and Interpreting Models for Multi-Action Video Understanding\", \"Paper Title\": \"Multi-Moments in Time: Learning and Interpreting Models for Multi-Action Video Understanding\", \"Paper URL\": \"https://arxiv.org/pdf/1911.00232.pdf\", \"GitHub URL\": \"\", \"Hugging Face URL\": \"\", \"Papers with Code URL\": \"\", \"ArXiv URL\": \"https://arxiv.org/pdf/1911.00232.pdf\", \"Semantic Scholar Corpus ID\": 207780280, \"Year Released\": \"2021\", \"Text Sources\": [\"crowdsourced\", \"undisclosed web\"], \"Licenses\": [{\"License\": \"Unspecified\", \"License URL\": \"\"}], \"Creators\": [\"Massachusetts Institute of Technology\"], \"Countries\": [\"United States of America\"], \"License Verified By\": \"Vivek Sharma\", \"Video Hours\": 833.0, \"Taken Down\": \"False\", \"Video Sources\": \"undisclosed web\", \"Task Categories\": [\"Video Classification\"], \"Inferred Metadata\": {\"GitHub License\": \"\", \"GitHub Stars (June 2024)\": \"\", \"GitHub Topics\": \"\", \"Github Date\": \"\", \"HF Config\": \"\", \"HF Config License\": \"\", \"HF Dataset\": \"\", \"HF Date\": \"\", \"HF Downloads (June 2024)\": \"\", \"HF Likes (June 2024)\": \"\", \"HF Yaml License\": \"\", \"PwC Date\": \"\", \"PwC Description\": \"\", \"PwC License Name\": \"\", \"PwC License URL\": \"\", \"S2 Citation Count (June 2024)\": \"\", \"S2 Date\": \"\"}, \"Bibtex\": \"@Article{Monfort2019MultiMomentsIT,\\n author = {Mathew Monfort and K. Ramakrishnan and A. Andonian and Barry A. McNamara and A. Lascelles and Bowen Pan and Quanfu Fan and Dan Gutfreund and R. Feris and A. Oliva},\\n booktitle = {IEEE Transactions on Pattern Analysis and Machine Intelligence},\\n journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},\\n pages = {1-1},\\n title = {Multi-Moments in Time: Learning and Interpreting Models for Multi-Action Video Understanding},\\n volume = {PP},\\n year = {2019}\\n}\\n\", \"License Use (DataProvenance)\": \"unspecified\", \"License Attribution (DataProvenance)\": 0, \"License Share Alike (DataProvenance)\": 0, \"License Use (DataProvenance IgnoreOpenAI)\": \"unspecified\", \"License Attribution (DataProvenance IgnoreOpenAI)\": 0, \"License Share Alike (DataProvenance IgnoreOpenAI)\": 0, \"License Type\": \"Unspecified\"}, {\"Unique Dataset Identifier\": \"YT-Temporal-1B\", \"Collection\": \"YT-Temporal-1B\", \"Collection URL\": \"https://arxiv.org/pdf/2201.02639\", \"Dataset Name\": \"MERLOT Reserve: Multimodal Neural Script Knowledge through Vision and Language and Sound\", \"Paper Title\": \"MERLOT Reserve: Multimodal Neural Script Knowledge through Vision and Language and Sound\", \"Paper URL\": \"https://arxiv.org/pdf/2201.02639\", \"GitHub URL\": \"\", \"Hugging Face URL\": \"\", \"Papers with Code URL\": \"https://paperswithcode.com/paper/merlot-reserve-neural-script-knowledge\", \"ArXiv URL\": \"https://arxiv.org/pdf/2201.02639\", \"Semantic Scholar Corpus ID\": 245837609, \"Year Released\": \"2022\", \"Text Sources\": [\"youtube\"], \"Licenses\": [{\"License\": \"MIT License\", \"License URL\": \"https://github.com/rowanz/merlot_reserve/blob/main/LICENSE\"}], \"Creators\": [\"University of Washington\", \"Allen Institute for AI\", \"University of Edinburgh\"], \"Countries\": [\"United States of America\", \"Scotland\"], \"License Verified By\": \"Vivek Sharma\", \"Video Hours\": 55555.0, \"Taken Down\": \"False\", \"Video Sources\": \"youtube\", \"Task Categories\": [\"Video Q&A\"], \"Inferred Metadata\": null, \"Bibtex\": null, \"License Use (DataProvenance)\": \"commercial\", \"License Attribution (DataProvenance)\": 1, \"License Share Alike (DataProvenance)\": 0, \"License Use (DataProvenance IgnoreOpenAI)\": \"commercial\", \"License Attribution (DataProvenance IgnoreOpenAI)\": 1, \"License Share Alike (DataProvenance IgnoreOpenAI)\": 0, \"License Type\": \"Commercial\"}, {\"Unique Dataset Identifier\": \"mad\", \"Collection\": \"mad\", \"Collection URL\": \"https://arxiv.org/abs/2112.00431\", \"Dataset Name\": \"MAD: A Scalable Dataset for Language Grounding in Videos from Movie Audio Descriptions\", \"Paper Title\": \"MAD: A Scalable Dataset for Language Grounding in Videos from Movie Audio Descriptions\", \"Paper URL\": \"https://arxiv.org/abs/2112.00431\", \"GitHub URL\": \"https://github.com/Soldelli/MAD\", \"Hugging Face URL\": \"\", \"Papers with Code URL\": \"https://paperswithcode.com/dataset/mad\", \"ArXiv URL\": \"https://arxiv.org/abs/2112.00431\", \"Semantic Scholar Corpus ID\": 244773187, \"Year Released\": \"2022\", \"Text Sources\": [\"undisclosed web\"], \"Licenses\": [{\"License\": \"Custom\", \"License URL\": \"https://docs.google.com/forms/d/e/1FAIpQLSdtUV3uweS0u7AHAMIJAL_dRRdZ5MHpJS3fdZVbhnVt-Yb4NA/viewform\"}], \"Creators\": [\"King Abdullah University of Science and Technology (KAUST)\"], \"Countries\": [\"Saudi Arabia\"], \"License Verified By\": \"Vivek Sharma\", \"Video Hours\": 1207.3, \"Taken Down\": \"False\", \"Video Sources\": \"undisclosed web\", \"Task Categories\": [\"Video Captioning\"], \"Inferred Metadata\": {\"GitHub License\": \"MIT License\", \"GitHub Stars (June 2024)\": 138, \"GitHub Topics\": [], \"Github Date\": \"\", \"HF Config\": \"\", \"HF Config License\": \"\", \"HF Dataset\": \"\", \"HF Date\": \"\", \"HF Downloads (June 2024)\": \"\", \"HF Likes (June 2024)\": \"\", \"HF Yaml License\": \"\", \"PwC Date\": \"2021-12-01\", \"PwC Description\": \"MAD (Movie Audio Descriptions) is an automatically curated large-scale dataset for the task of natural language grounding in videos or natural language moment retrieval.\\nMAD exploits available audio descriptions of mainstream movies. Such audio descriptions are redacted for visually impaired audiences and are therefore highly descriptive of the visual content being displayed. \\nMAD contains over 384,000 natural language sentences grounded in over 1,200 hours of video, and provides a unique setup for video grounding as the visual stream is truly untrimmed with an average video duration of 110 minutes. 2 orders of magnitude longer than legacy datasets. \\n\\nTake a look at the paper for additional information.\\n\\nFrom the authors on availability: \\\"Due to copyright constraints, MAD\\u2019s videos will not be publicly released. However, we will provide all necessary features for our experiments\\u2019 reproducibility and promote future research in this direction\\\"\", \"PwC License Name\": \"Unspecified\", \"PwC License URL\": null, \"S2 Citation Count (June 2024)\": \"\", \"S2 Date\": \"\"}, \"Bibtex\": \"@Article{Soldan2021MADAS,\\n author = {Mattia Soldan and A. Pardo and Juan Le'on Alc'azar and Fabian Caba Heilbron and Chen Zhao and Silvio Giancola and Bernard Ghanem},\\n booktitle = {Computer Vision and Pattern Recognition},\\n journal = {2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},\\n pages = {5016-5025},\\n title = {MAD: A Scalable Dataset for Language Grounding in Videos from Movie Audio Descriptions},\\n year = {2021}\\n}\\n\", \"License Use (DataProvenance)\": \"academic-only\", \"License Attribution (DataProvenance)\": 1, \"License Share Alike (DataProvenance)\": 0, \"License Use (DataProvenance IgnoreOpenAI)\": \"academic-only\", \"License Attribution (DataProvenance IgnoreOpenAI)\": 1, \"License Share Alike (DataProvenance IgnoreOpenAI)\": 0, \"License Type\": \"Non-Commercial/Academic\"}, {\"Unique Dataset Identifier\": \"ferv39k-dataset\", \"Collection\": \"ferv39k-dataset\", \"Collection URL\": \"https://arxiv.org/abs/2203.09463\", \"Dataset Name\": \"FERV39k: A Large-Scale Multi-Scene Dataset for Facial Expression Recognition in Videos (CVPR 2022)\", \"Paper Title\": \"FERV39k: A Large-Scale Multi-Scene Dataset for Facial Expression Recognition in Videos (CVPR 2022)\", \"Paper URL\": \"https://arxiv.org/abs/2203.09463\", \"GitHub URL\": \"https://github.com/wangyanckxx/FERV39k\", \"Hugging Face URL\": \"\", \"Papers with Code URL\": \"https://paperswithcode.com/paper/ferv39k-a-large-scale-multi-scene-dataset-for\", \"ArXiv URL\": \"https://arxiv.org/abs/2203.09463\", \"Semantic Scholar Corpus ID\": 247518747, \"Year Released\": \"2022\", \"Text Sources\": [\"undisclosed web\"], \"Licenses\": [{\"License\": \"CC BY-NC 4.0\", \"License URL\": \"https://wangyanckxx.github.io/Proj_CVPR2022_FERV39k.html\"}], \"Creators\": [\"Fudan University\"], \"Countries\": [\"China\"], \"License Verified By\": \"Vivek Sharma\", \"Video Hours\": 16.47, \"Taken Down\": \"False\", \"Video Sources\": \"undisclosed web\", \"Task Categories\": [\"Video Classification\"], \"Inferred Metadata\": {\"GitHub License\": \"\", \"GitHub Stars (June 2024)\": 52, \"GitHub Topics\": [], \"Github Date\": \"\", \"HF Config\": \"\", \"HF Config License\": \"\", \"HF Dataset\": \"\", \"HF Date\": \"\", \"HF Downloads (June 2024)\": \"\", \"HF Likes (June 2024)\": \"\", \"HF Yaml License\": \"\", \"PwC Date\": \"\", \"PwC Description\": \"No code available yet.\", \"PwC License Name\": \"\", \"PwC License URL\": \"\", \"S2 Citation Count (June 2024)\": \"\", \"S2 Date\": \"\"}, \"Bibtex\": \"@Article{Wang2022FERV39kAL,\\n author = {Yan Wang and Yixuan Sun and Yiwen Huang and Zhongying Liu and Shuyong Gao and Wei Zhang and Weifeng Ge and Wenqiang Zhang},\\n booktitle = {Computer Vision and Pattern Recognition},\\n journal = {2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},\\n pages = {20890-20899},\\n title = {FERV39k: A Large-Scale Multi-Scene Dataset for Facial Expression Recognition in Videos},\\n year = {2022}\\n}\\n\", \"License Use (DataProvenance)\": \"non-commercial\", \"License Attribution (DataProvenance)\": 1, \"License Share Alike (DataProvenance)\": 0, \"License Use (DataProvenance IgnoreOpenAI)\": \"non-commercial\", \"License Attribution (DataProvenance IgnoreOpenAI)\": 1, \"License Share Alike (DataProvenance IgnoreOpenAI)\": 0, \"License Type\": \"Non-Commercial/Academic\"}, {\"Unique Dataset Identifier\": \"cacd\", \"Collection\": \"cacd\", \"Collection URL\": \"https://openaccess.thecvf.com/content/CVPR2022W/L3D-IVU/papers/Xiang_CDAD_A_Common_Daily_Action_Dataset_With_Collected_Hard_Negative_CVPRW_2022_paper.pdf\", \"Dataset Name\": \"CDAD: A Common Daily Action Dataset with Collected Hard Negative Samples (CVPR 2022)\", \"Paper Title\": \"CDAD: A Common Daily Action Dataset with Collected Hard Negative Samples (CVPR 2022)\", \"Paper URL\": \"https://openaccess.thecvf.com/content/CVPR2022W/L3D-IVU/papers/Xiang_CDAD_A_Common_Daily_Action_Dataset_With_Collected_Hard_Negative_CVPRW_2022_paper.pdf\", \"GitHub URL\": \"https://github.com/MartinXM/CDAD\", \"Hugging Face URL\": \"\", \"Papers with Code URL\": \"\", \"ArXiv URL\": \"https://openaccess.thecvf.com/content/CVPR2022W/L3D-IVU/papers/Xiang_CDAD_A_Common_Daily_Action_Dataset_With_Collected_Hard_Negative_CVPRW_2022_paper.pdf\", \"Semantic Scholar Corpus ID\": 251035434, \"Year Released\": \"2022\", \"Text Sources\": [\"crowdsourced\"], \"Licenses\": [{\"License\": \"Unspecified\", \"License URL\": \"\"}], \"Creators\": [\"The Hong Kong Polytechnic University\", \"Alibaba Group\"], \"Countries\": [\"China\"], \"License Verified By\": \"Vivek Sharma\", \"Video Hours\": 215.0, \"Taken Down\": \"False\", \"Video Sources\": \"crowdsourced\", \"Task Categories\": [\"Video Classification\"], \"Inferred Metadata\": {\"GitHub License\": \"\", \"GitHub Stars (June 2024)\": 6, \"GitHub Topics\": [], \"Github Date\": \"\", \"HF Config\": \"\", \"HF Config License\": \"\", \"HF Dataset\": \"\", \"HF Date\": \"\", \"HF Downloads (June 2024)\": \"\", \"HF Likes (June 2024)\": \"\", \"HF Yaml License\": \"\", \"PwC Date\": \"\", \"PwC Description\": \"\", \"PwC License Name\": \"\", \"PwC License URL\": \"\", \"S2 Citation Count (June 2024)\": \"\", \"S2 Date\": \"\"}, \"Bibtex\": \"@Article{Xiang2022CDADAC,\\n author = {Wangmeng Xiang and C. Li and Ke Li and Biao Wang and Xiangpei Hua and Lei Zhang},\\n booktitle = {2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)},\\n journal = {2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)},\\n pages = {3920-3929},\\n title = {CDAD: A Common Daily Action Dataset with Collected Hard Negative Samples},\\n year = {2022}\\n}\\n\", \"License Use (DataProvenance)\": \"unspecified\", \"License Attribution (DataProvenance)\": 0, \"License Share Alike (DataProvenance)\": 0, \"License Use (DataProvenance IgnoreOpenAI)\": \"unspecified\", \"License Attribution (DataProvenance IgnoreOpenAI)\": 0, \"License Share Alike (DataProvenance IgnoreOpenAI)\": 0, \"License Type\": \"Unspecified\"}, {\"Unique Dataset Identifier\": \"ego-4d\", \"Collection\": \"ego-4d\", \"Collection URL\": \"https://arxiv.org/abs/2110.07058\", \"Dataset Name\": \"Ego4D: Around the World in 3,000 Hours of Egocentric Video\", \"Paper Title\": \"Ego4D: Around the World in 3,000 Hours of Egocentric Video\", \"Paper URL\": \"https://arxiv.org/abs/2110.07058\", \"GitHub URL\": \"https://github.com/EGO4D/forecasting\", \"Hugging Face URL\": \"\", \"Papers with Code URL\": \"https://paperswithcode.com/paper/ego4d-around-the-world-in-3000-hours-of\", \"ArXiv URL\": \"https://arxiv.org/abs/2110.07058\", \"Semantic Scholar Corpus ID\": 238856888, \"Year Released\": \"2022\", \"Text Sources\": [\"human\"], \"Licenses\": [{\"License\": \"Custom\", \"License URL\": \"https://ego4d-data.org/pdfs/Ego4D-Licenses-Draft.pdf\"}], \"Creators\": [\"Facebook AI Research\"], \"Countries\": [\"United States of America\"], \"License Verified By\": \"Vivek Sharma\", \"Video Hours\": 3670.0, \"Taken Down\": \"False\", \"Video Sources\": \"human\", \"Task Categories\": [\"Video Classification\", \"Temporal Action Segmentation\", \"Video Captioning\", \"Misc\"], \"Inferred Metadata\": {\"GitHub License\": \"MIT License\", \"GitHub Stars (June 2024)\": 63, \"GitHub Topics\": [], \"Github Date\": \"\", \"HF Config\": \"\", \"HF Config License\": \"\", \"HF Dataset\": \"\", \"HF Date\": \"\", \"HF Downloads (June 2024)\": \"\", \"HF Likes (June 2024)\": \"\", \"HF Yaml License\": \"\", \"PwC Date\": \"\", \"PwC Description\": \"Implemented in 8 code libraries.\", \"PwC License Name\": \"\", \"PwC License URL\": \"\", \"S2 Citation Count (June 2024)\": \"\", \"S2 Date\": \"\"}, \"Bibtex\": \"@Article{Grauman2021Ego4DAT,\\n author = {K. Grauman and Andrew Westbury and Eugene Byrne and Zachary Chavis and Antonino Furnari and Rohit Girdhar and Jackson Hamburger and Hao Jiang and Miao Liu and Xingyu Liu and Miguel Martin and Tushar Nagarajan and Ilija Radosavovic and Santhosh K. Ramakrishnan and Fiona Ryan and J. Sharma and Michael Wray and Mengmeng Xu and Eric Z. Xu and Chen Zhao and Siddhant Bansal and Dhruv Batra and Vincent Cartillier and S. Crane and Tien Do and Morrie Doulaty and Akshay Erapalli and Christoph Feichtenhofer and A. Fragomeni and Qichen Fu and Christian Fuegen and A. Gebreselasie and Cristina Gonz\\u00e1lez and James M. Hillis and Xuhua Huang and Yifei Huang and Wenqi Jia and Weslie Khoo and J. Kol\\u00e1r and Satwik Kottur and Anurag Kumar and F. Landini and Chao Li and Yanghao Li and Zhenqiang Li and K. Mangalam and Raghava Modhugu and Jonathan Munro and Tullie Murrell and Takumi Nishiyasu and Will Price and Paola Ruiz Puentes and Merey Ramazanova and Leda Sari and K. Somasundaram and Audrey Southerland and Yusuke Sugano and Ruijie Tao and Minh Vo and Yuchen Wang and Xindi Wu and Takuma Yagi and Yunyi Zhu and P. Arbel\\u00e1ez and David J. Crandall and D. Damen and G. Farinella and Bernard Ghanem and V. Ithapu and C. V. Jawahar and H. Joo and Kris Kitani and Haizhou Li and Richard A. Newcombe and A. Oliva and H. Park and James M. Rehg and Yoichi Sato and Jianbo Shi and Mike Zheng Shou and A. Torralba and L. Torresani and Mingfei Yan and J. Malik},\\n booktitle = {Computer Vision and Pattern Recognition},\\n journal = {2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},\\n pages = {18973-18990},\\n title = {Ego4D: Around the World in 3,000 Hours of Egocentric Video},\\n year = {2021}\\n}\\n\", \"License Use (DataProvenance)\": \"commercial\", \"License Attribution (DataProvenance)\": 1, \"License Share Alike (DataProvenance)\": 0, \"License Use (DataProvenance IgnoreOpenAI)\": \"commercial\", \"License Attribution (DataProvenance IgnoreOpenAI)\": 1, \"License Share Alike (DataProvenance IgnoreOpenAI)\": 0, \"License Type\": \"Commercial\"}, {\"Unique Dataset Identifier\": \"project-aria-digital-twin-dataset\", \"Collection\": \"project-aria-digital-twin-dataset\", \"Collection URL\": \"https://arxiv.org/abs/2306.06362\", \"Dataset Name\": \"Aria Digital Twin\", \"Paper Title\": \"Aria Digital Twin\", \"Paper URL\": \"https://arxiv.org/abs/2306.06362\", \"GitHub URL\": \"https://github.com/facebookresearch/projectaria_tools\", \"Hugging Face URL\": \"\", \"Papers with Code URL\": \"https://paperswithcode.com/paper/aria-digital-twin-a-new-benchmark-dataset-for\", \"ArXiv URL\": \"https://arxiv.org/abs/2306.06362\", \"Semantic Scholar Corpus ID\": 261243365, \"Year Released\": \"2023\", \"Text Sources\": [\"human\"], \"Licenses\": [{\"License\": \"Apache License 2.0\", \"License URL\": \"https://github.com/facebookresearch/projectaria_tools/blob/main/LICENSE\"}], \"Creators\": [\"Meta\"], \"Countries\": [\"United States of America\"], \"License Verified By\": \"Vivek Sharma\", \"Video Hours\": 6.6, \"Taken Down\": \"False\", \"Video Sources\": \"human\", \"Task Categories\": [\"Video Object Detection\", \"Video Segmentation\"], \"Inferred Metadata\": {\"GitHub License\": \"Apache License 2.0\", \"GitHub Stars (June 2024)\": 361, \"GitHub Topics\": [], \"Github Date\": \"\", \"HF Config\": \"\", \"HF Config License\": \"\", \"HF Dataset\": \"\", \"HF Date\": \"\", \"HF Downloads (June 2024)\": \"\", \"HF Likes (June 2024)\": \"\", \"HF Yaml License\": \"\", \"PwC Date\": \"\", \"PwC Description\": \"Implemented in one code library.\", \"PwC License Name\": \"\", \"PwC License URL\": \"\", \"S2 Citation Count (June 2024)\": \"\", \"S2 Date\": \"\"}, \"Bibtex\": \"@Article{Somasundaram2023ProjectAA,\\n author = {K. Somasundaram and Jing Dong and Huixuan Tang and Julian Straub and Mingfei Yan and M. Goesele and Jakob J. Engel and R. D. Nardi and Richard A. Newcombe},\\n booktitle = {arXiv.org},\\n journal = {ArXiv},\\n title = {Project Aria: A New Tool for Egocentric Multi-Modal AI Research},\\n volume = {abs/2308.13561},\\n year = {2023}\\n}\\n\", \"License Use (DataProvenance)\": \"commercial\", \"License Attribution (DataProvenance)\": 1, \"License Share Alike (DataProvenance)\": 0, \"License Use (DataProvenance IgnoreOpenAI)\": \"commercial\", \"License Attribution (DataProvenance IgnoreOpenAI)\": 1, \"License Share Alike (DataProvenance IgnoreOpenAI)\": 0, \"License Type\": \"Commercial\"}, {\"Unique Dataset Identifier\": \"ego-exo4D\", \"Collection\": \"ego-exo4D\", \"Collection URL\": \"https://arxiv.org/abs/2311.18259\", \"Dataset Name\": \"Ego-Exo4D\", \"Paper Title\": \"Ego-Exo4D\", \"Paper URL\": \"https://arxiv.org/abs/2311.18259\", \"GitHub URL\": \"https://github.com/facebookresearch/Ego4d\", \"Hugging Face URL\": \"\", \"Papers with Code URL\": \"https://paperswithcode.com/paper/ego-exo4d-understanding-skilled-human/review/\", \"ArXiv URL\": \"https://arxiv.org/abs/2311.18259\", \"Semantic Scholar Corpus ID\": 265506384, \"Year Released\": \"2023\", \"Text Sources\": [\"human\"], \"Licenses\": [{\"License\": \"MIT License\", \"License URL\": \"https://github.com/facebookresearch/Ego4d/blob/main/LICENSE\"}], \"Creators\": [\"Meta\"], \"Countries\": [\"United States of America\"], \"License Verified By\": \"Vivek Sharma\", \"Video Hours\": 1422.0, \"Taken Down\": \"False\", \"Video Sources\": \"human\", \"Task Categories\": [\"Pose Estimation\", \"Video Classification\", \"Video Captioning\", \"Misc\"], \"Inferred Metadata\": {\"GitHub License\": \"MIT License\", \"GitHub Stars (June 2024)\": 297, \"GitHub Topics\": [\"computer-vision\", \"dataset\", \"feature-extraction\", \"video\", \"visuzalization\"], \"Github Date\": \"\", \"HF Config\": \"\", \"HF Config License\": \"\", \"HF Dataset\": \"\", \"HF Date\": \"\", \"HF Downloads (June 2024)\": \"\", \"HF Likes (June 2024)\": \"\", \"HF Yaml License\": \"\", \"PwC Date\": \"\", \"PwC Description\": \"Paper tables with annotated results for Ego-Exo4D: Understanding Skilled Human Activity from First- and Third-Person Perspectives\", \"PwC License Name\": \"\", \"PwC License URL\": \"\", \"S2 Citation Count (June 2024)\": \"\", \"S2 Date\": \"\"}, \"Bibtex\": \"@Article{Grauman2023EgoExo4DUS,\\n author = {K. Grauman and Andrew Westbury and L. Torresani and Kris Kitani and Jitendra Malik and Triantafyllos Afouras and Kumar Ashutosh and Vijay Baiyya and Siddhant Bansal and Bikram Boote and Eugene Byrne and Zachary Chavis and Joya Chen and Feng Cheng and Fu-Jen Chu and Sean Crane and Avijit Dasgupta and Jing Dong and Mar\\u00eda Escobar and Cristhian Forigua and A. Gebreselasie and S. Haresh and Jing Huang and Md Mohaiminul Islam and S. Jain and Rawal Khirodkar and Devansh Kukreja and Kevin J Liang and Jia-Wei Liu and Sagnik Majumder and Yongsen Mao and Miguel Martin and E. Mavroudi and Tushar Nagarajan and Francesco Ragusa and Santhosh K. Ramakrishnan and Luigi Seminara and Arjun Somayazulu and Yale Song and Shan Su and Zihui Xue and Edward Zhang and Jinxu Zhang and Angela Castillo and Changan Chen and Xinzhu Fu and Ryosuke Furuta and Cristina Gonzalez and Prince Gupta and Jiabo Hu and Yifei Huang and Yiming Huang and Weslie Khoo and Anush Kumar and Robert Kuo and Sach Lakhavani and Miao Liu and M. Luo and Zhengyi Luo and Brighid Meredith and Austin Miller and Oluwatumininu Oguntola and Xiaqing Pan and Penny Peng and Shraman Pramanick and Merey Ramazanova and Fiona Ryan and Wei Shan and Kiran Somasundaram and Chenan Song and Audrey Southerland and Masatoshi Tateno and Huiyu Wang and Yuchen Wang and Takuma Yagi and Mingfei Yan and Xitong Yang and Zecheng Yu and S. Zha and Chen Zhao and Ziwei Zhao and Zhifan Zhu and Jeff Zhuo and Pablo Arbel\\u00e1ez and Gedas Bertasius and David J. Crandall and D. Damen and J. Engel and G. Farinella and Antonino Furnari and Bernard Ghanem and Judy Hoffman and C. V. Jawahar and Richard A. Newcombe and Hyun Soo Park and James M. Rehg and Yoichi Sato and M. Savva and Jianbo Shi and Mike Zheng Shou and Michael Wray},\\n booktitle = {arXiv.org},\\n journal = {ArXiv},\\n title = {Ego-Exo4D: Understanding Skilled Human Activity from First- and Third-Person Perspectives},\\n volume = {abs/2311.18259},\\n year = {2023}\\n}\\n\", \"License Use (DataProvenance)\": \"commercial\", \"License Attribution (DataProvenance)\": 1, \"License Share Alike (DataProvenance)\": 0, \"License Use (DataProvenance IgnoreOpenAI)\": \"commercial\", \"License Attribution (DataProvenance IgnoreOpenAI)\": 1, \"License Share Alike (DataProvenance IgnoreOpenAI)\": 0, \"License Type\": \"Commercial\"}, {\"Unique Dataset Identifier\": \"cinepile\", \"Collection\": \"cinepile\", \"Collection URL\": \"https://arxiv.org/pdf/2405.08813\", \"Dataset Name\": \"CinePile: A Long Video Question Answering Dataset and Benchmark\", \"Paper Title\": \"CinePile: A Long Video Question Answering Dataset and Benchmark\", \"Paper URL\": \"https://arxiv.org/pdf/2405.08813\", \"GitHub URL\": \"\", \"Hugging Face URL\": \"https://huggingface.co/datasets/tomg-group-umd/cinepile\", \"Papers with Code URL\": \"https://paperswithcode.com/dataset/cinepile\", \"ArXiv URL\": \"https://arxiv.org/pdf/2405.08813\", \"Semantic Scholar Corpus ID\": 269761335, \"Year Released\": \"2024\", \"Text Sources\": [\"youtube\", \"movies\"], \"Licenses\": [{\"License\": \"CC BY 4.0\", \"License URL\": \"https://creativecommons.org/licenses/by/4.0/\"}], \"Creators\": [\"University of Maryland\", \"Weizmann Institute of Science\"], \"Countries\": [\"United States of America\", \"Israel\"], \"License Verified By\": \"Vivek Sharma\", \"Video Hours\": 417.6, \"Taken Down\": \"False\", \"Video Sources\": \"movies\", \"Task Categories\": [\"Video Q&A\"], \"Inferred Metadata\": null, \"Bibtex\": null, \"License Use (DataProvenance)\": \"commercial\", \"License Attribution (DataProvenance)\": 1, \"License Share Alike (DataProvenance)\": 0, \"License Use (DataProvenance IgnoreOpenAI)\": \"commercial\", \"License Attribution (DataProvenance IgnoreOpenAI)\": 1, \"License Share Alike (DataProvenance IgnoreOpenAI)\": 0, \"License Type\": \"Commercial\"}, {\"Unique Dataset Identifier\": \"cinepile\", \"Collection\": \"cinepile\", \"Collection URL\": \"https://arxiv.org/pdf/2405.08813\", \"Dataset Name\": \"CinePile: A Long Video Question Answering Dataset and Benchmark\", \"Paper Title\": \"CinePile: A Long Video Question Answering Dataset and Benchmark\", \"Paper URL\": \"https://arxiv.org/pdf/2405.08813\", \"GitHub URL\": \"\", \"Hugging Face URL\": \"https://huggingface.co/datasets/tomg-group-umd/cinepile\", \"Papers with Code URL\": \"https://paperswithcode.com/dataset/cinepile\", \"ArXiv URL\": \"https://arxiv.org/pdf/2405.08813\", \"Semantic Scholar Corpus ID\": 269761335, \"Year Released\": \"2024\", \"Text Sources\": [\"youtube\", \"movies\"], \"Licenses\": [{\"License\": \"CC BY 4.0\", \"License URL\": \"https://creativecommons.org/licenses/by/4.0/\"}], \"Creators\": [\"University of Maryland\", \"Weizmann Institute of Science\"], \"Countries\": [\"United States of America\", \"Israel\"], \"License Verified By\": \"Vivek Sharma\", \"Video Hours\": 417.6, \"Taken Down\": \"False\", \"Video Sources\": \"youtube\", \"Task Categories\": [\"Video Q&A\"], \"Inferred Metadata\": null, \"Bibtex\": null, \"License Use (DataProvenance)\": \"commercial\", \"License Attribution (DataProvenance)\": 1, \"License Share Alike (DataProvenance)\": 0, \"License Use (DataProvenance IgnoreOpenAI)\": \"commercial\", \"License Attribution (DataProvenance IgnoreOpenAI)\": 1, \"License Share Alike (DataProvenance IgnoreOpenAI)\": 0, \"License Type\": \"Commercial\"}, {\"Unique Dataset Identifier\": \"project-aria-dataset\", \"Collection\": \"project-aria-dataset\", \"Collection URL\": \"https://arxiv.org/pdf/2402.13349\", \"Dataset Name\": \"Aria Everyday Activities Dataset\", \"Paper Title\": \"Aria Everyday Activities Dataset\", \"Paper URL\": \"https://arxiv.org/pdf/2402.13349\", \"GitHub URL\": \"https://github.com/facebookresearch/projectaria_tools\", \"Hugging Face URL\": \"\", \"Papers with Code URL\": \"https://paperswithcode.com/paper/aria-everyday-activities-dataset\", \"ArXiv URL\": \"https://arxiv.org/pdf/2402.13349\", \"Semantic Scholar Corpus ID\": 267770215, \"Year Released\": \"2024\", \"Text Sources\": [\"human\"], \"Licenses\": [{\"License\": \"Apache License 2.0\", \"License URL\": \"https://github.com/facebookresearch/Aria_data_tools/blob/main/LICENSE\"}], \"Creators\": [\"Meta\"], \"Countries\": [\"United States of America\"], \"License Verified By\": \"Vivek Sharma\", \"Video Hours\": 1400.0, \"Taken Down\": \"False\", \"Video Sources\": \"human\", \"Task Categories\": [\"Misc (Scene Reconstruction)\"], \"Inferred Metadata\": {\"GitHub License\": \"Apache License 2.0\", \"GitHub Stars (June 2024)\": 361, \"GitHub Topics\": [], \"Github Date\": \"\", \"HF Config\": \"\", \"HF Config License\": \"\", \"HF Dataset\": \"\", \"HF Date\": \"\", \"HF Downloads (June 2024)\": \"\", \"HF Likes (June 2024)\": \"\", \"HF Yaml License\": \"\", \"PwC Date\": \"\", \"PwC Description\": \"Implemented in one code library.\", \"PwC License Name\": \"\", \"PwC License URL\": \"\", \"S2 Citation Count (June 2024)\": \"\", \"S2 Date\": \"\"}, \"Bibtex\": \"@Article{Lv2024AriaEA,\\n author = {Zhaoyang Lv and Nickolas Charron and Pierre Moulon and Alexander Gamino and Cheng Peng and Chris Sweeney and Edward Miller and Huixuan Tang and Jeff Meissner and Jing Dong and Kiran Somasundaram and Luis Pesqueira and Mark Schwesinger and Omkar M. Parkhi and Qiao Gu and R. D. Nardi and Shangyi Cheng and Steve Saarinen and Vijay Baiyya and Yuyang Zou and Richard A. Newcombe and J. Engel and Xiaqing Pan and Carl Ren},\\n booktitle = {arXiv.org},\\n journal = {ArXiv},\\n title = {Aria Everyday Activities Dataset},\\n volume = {abs/2402.13349},\\n year = {2024}\\n}\\n\", \"License Use (DataProvenance)\": \"commercial\", \"License Attribution (DataProvenance)\": 1, \"License Share Alike (DataProvenance)\": 0, \"License Use (DataProvenance IgnoreOpenAI)\": \"commercial\", \"License Attribution (DataProvenance IgnoreOpenAI)\": 1, \"License Share Alike (DataProvenance IgnoreOpenAI)\": 0, \"License Type\": \"Commercial\"}, {\"Unique Dataset Identifier\": \"egopet\", \"Collection\": \"egopet\", \"Collection URL\": \"https://arxiv.org/pdf/2404.09991\", \"Dataset Name\": \"EgoPet: Egomotion and Interaction Data from an Animal's Perspective\", \"Paper Title\": \"EgoPet: Egomotion and Interaction Data from an Animal's Perspective\", \"Paper URL\": \"https://arxiv.org/pdf/2404.09991\", \"GitHub URL\": \"\", \"Hugging Face URL\": \"\", \"Papers with Code URL\": \"https://paperswithcode.com/paper/egopet-egomotion-and-interaction-data-from-an\", \"ArXiv URL\": \"https://arxiv.org/pdf/2404.09991\", \"Semantic Scholar Corpus ID\": 269148727, \"Year Released\": \"2024\", \"Text Sources\": [\"tiktok\", \"youtube\"], \"Licenses\": [{\"License\": \"CC BY-NC 4.0\", \"License URL\": \"https://github.com/DannyTran123/egopet/blob/main/LICENSE\"}], \"Creators\": [\"Tel Aviv University\", \"UC Berkeley\", \"New York University\"], \"Countries\": [\"United States of America\", \"Israel\"], \"License Verified By\": \"Vivek Sharma\", \"Video Hours\": 84.0, \"Taken Down\": \"False\", \"Video Sources\": \"Other\", \"Task Categories\": [\"Misc (Locomotion Prediction\", \"Visual Interaction Prediction\", \"Vision to Proprioception Prediction)\"], \"Inferred Metadata\": null, \"Bibtex\": null, \"License Use (DataProvenance)\": \"non-commercial\", \"License Attribution (DataProvenance)\": 1, \"License Share Alike (DataProvenance)\": 0, \"License Use (DataProvenance IgnoreOpenAI)\": \"non-commercial\", \"License Attribution (DataProvenance IgnoreOpenAI)\": 1, \"License Share Alike (DataProvenance IgnoreOpenAI)\": 0, \"License Type\": \"Non-Commercial/Academic\"}, {\"Unique Dataset Identifier\": \"egoschema\", \"Collection\": \"egoschema\", \"Collection URL\": \"https://arxiv.org/pdf/2308.09126\", \"Dataset Name\": \"EgoSchema: A Diagnostic Benchmark for Very Long-form Video Language Understanding\", \"Paper Title\": \"EgoSchema: A Diagnostic Benchmark for Very Long-form Video Language Understanding\", \"Paper URL\": \"https://arxiv.org/pdf/2308.09126\", \"GitHub URL\": \"\", \"Hugging Face URL\": \"\", \"Papers with Code URL\": \"https://paperswithcode.com/paper/egoschema-a-diagnostic-benchmark-for-very-1\", \"ArXiv URL\": \"https://arxiv.org/pdf/2308.09126\", \"Semantic Scholar Corpus ID\": 261031047, \"Year Released\": \"2024\", \"Text Sources\": [\"human\"], \"Licenses\": [{\"License\": \"Unspecified\", \"License URL\": \"\"}], \"Creators\": [\"UC Berkeley\"], \"Countries\": [\"United States of America\"], \"License Verified By\": \"Vivek Sharma\", \"Video Hours\": 250.0, \"Taken Down\": \"False\", \"Video Sources\": \"human\", \"Task Categories\": [\"Video Q&A\"], \"Inferred Metadata\": null, \"Bibtex\": null, \"License Use (DataProvenance)\": \"unspecified\", \"License Attribution (DataProvenance)\": 0, \"License Share Alike (DataProvenance)\": 0, \"License Use (DataProvenance IgnoreOpenAI)\": \"unspecified\", \"License Attribution (DataProvenance IgnoreOpenAI)\": 0, \"License Share Alike (DataProvenance IgnoreOpenAI)\": 0, \"License Type\": \"Unspecified\"}, {\"Unique Dataset Identifier\": \"egopet\", \"Collection\": \"egopet\", \"Collection URL\": \"https://arxiv.org/pdf/2404.09991\", \"Dataset Name\": \"EgoPet: Egomotion and Interaction Data from an Animal's Perspective\", \"Paper Title\": \"EgoPet: Egomotion and Interaction Data from an Animal's Perspective\", \"Paper URL\": \"https://arxiv.org/pdf/2404.09991\", \"GitHub URL\": \"\", \"Hugging Face URL\": \"\", \"Papers with Code URL\": \"https://paperswithcode.com/paper/egopet-egomotion-and-interaction-data-from-an\", \"ArXiv URL\": \"https://arxiv.org/pdf/2404.09991\", \"Semantic Scholar Corpus ID\": 269148727, \"Year Released\": \"2024\", \"Text Sources\": [\"tiktok\", \"youtube\"], \"Licenses\": [{\"License\": \"CC BY-NC 4.0\", \"License URL\": \"https://github.com/DannyTran123/egopet/blob/main/LICENSE\"}], \"Creators\": [\"Tel Aviv University\", \"UC Berkeley\", \"New York University\"], \"Countries\": [\"United States of America\", \"Israel\"], \"License Verified By\": \"Vivek Sharma\", \"Video Hours\": 84.0, \"Taken Down\": \"False\", \"Video Sources\": \"youtube\", \"Task Categories\": [\"Misc (Locomotion Prediction\", \"Visual Interaction Prediction\", \"Vision to Proprioception Prediction)\"], \"Inferred Metadata\": null, \"Bibtex\": null, \"License Use (DataProvenance)\": \"non-commercial\", \"License Attribution (DataProvenance)\": 1, \"License Share Alike (DataProvenance)\": 0, \"License Use (DataProvenance IgnoreOpenAI)\": \"non-commercial\", \"License Attribution (DataProvenance IgnoreOpenAI)\": 1, \"License Share Alike (DataProvenance IgnoreOpenAI)\": 0, \"License Type\": \"Non-Commercial/Academic\"}], \"data-ad31ef507ef239cdd64fdbe14568e0d2\": [{\"Year Released\": \"<2004\", \"Video Sources\": \"Other\", \"Cumulative Hours\": 0.0}, {\"Year Released\": \"<2004\", \"Video Sources\": \"crowdsourced\", \"Cumulative Hours\": 0.0}, {\"Year Released\": \"<2004\", \"Video Sources\": \"flickr\", \"Cumulative Hours\": 0.0}, {\"Year Released\": \"<2004\", \"Video Sources\": \"human\", \"Cumulative Hours\": 0.0}, {\"Year Released\": \"<2004\", \"Video Sources\": \"movies\", \"Cumulative Hours\": 0.0}, {\"Year Released\": \"<2004\", \"Video Sources\": \"undisclosed web\", \"Cumulative Hours\": 0.0}, {\"Year Released\": \"<2004\", \"Video Sources\": \"youtube\", \"Cumulative Hours\": 0.0}, {\"Year Released\": \"2004\", \"Video Sources\": \"youtube\", \"Cumulative Hours\": 0.0}, {\"Year Released\": \"2004\", \"Video Sources\": \"undisclosed web\", \"Cumulative Hours\": 0.0}, {\"Year Released\": \"2004\", \"Video Sources\": \"movies\", \"Cumulative Hours\": 0.0}, {\"Year Released\": \"2004\", \"Video Sources\": \"crowdsourced\", \"Cumulative Hours\": 0.0}, {\"Year Released\": \"2004\", \"Video Sources\": \"flickr\", \"Cumulative Hours\": 0.0}, {\"Year Released\": \"2004\", \"Video Sources\": \"Other\", \"Cumulative Hours\": 0.0}, {\"Year Released\": \"2004\", \"Video Sources\": \"human\", \"Cumulative Hours\": 0.0}, {\"Year Released\": \"2005\", \"Video Sources\": \"undisclosed web\", \"Cumulative Hours\": 0.0}, {\"Year Released\": \"2005\", \"Video Sources\": \"youtube\", \"Cumulative Hours\": 0.0}, {\"Year Released\": \"2005\", \"Video Sources\": \"human\", \"Cumulative Hours\": 0.0}, {\"Year Released\": \"2005\", \"Video Sources\": \"movies\", \"Cumulative Hours\": 0.0}, {\"Year Released\": \"2005\", \"Video Sources\": \"crowdsourced\", \"Cumulative Hours\": 0.0}, {\"Year Released\": \"2005\", \"Video Sources\": \"Other\", \"Cumulative Hours\": 0.0}, {\"Year Released\": \"2005\", \"Video Sources\": \"flickr\", \"Cumulative Hours\": 0.0}, {\"Year Released\": \"2006\", \"Video Sources\": \"youtube\", \"Cumulative Hours\": 0.0}, {\"Year Released\": \"2006\", \"Video Sources\": \"movies\", \"Cumulative Hours\": 0.0}, {\"Year Released\": \"2006\", \"Video Sources\": \"human\", \"Cumulative Hours\": 0.0}, {\"Year Released\": \"2006\", \"Video Sources\": \"undisclosed web\", \"Cumulative Hours\": 0.0}, {\"Year Released\": \"2006\", \"Video Sources\": \"crowdsourced\", \"Cumulative Hours\": 0.0}, {\"Year Released\": \"2006\", \"Video Sources\": \"Other\", \"Cumulative Hours\": 0.0}, {\"Year Released\": \"2006\", \"Video Sources\": \"flickr\", \"Cumulative Hours\": 0.0}, {\"Year Released\": \"2007\", \"Video Sources\": \"undisclosed web\", \"Cumulative Hours\": 0.0}, {\"Year Released\": \"2007\", \"Video Sources\": \"Other\", \"Cumulative Hours\": 0.0}, {\"Year Released\": \"2007\", \"Video Sources\": \"crowdsourced\", \"Cumulative Hours\": 0.0}, {\"Year Released\": \"2007\", \"Video Sources\": \"flickr\", \"Cumulative Hours\": 0.0}, {\"Year Released\": \"2007\", \"Video Sources\": \"human\", \"Cumulative Hours\": 0.0}, {\"Year Released\": \"2007\", \"Video Sources\": \"movies\", \"Cumulative Hours\": 0.0}, {\"Year Released\": \"2007\", \"Video Sources\": \"youtube\", \"Cumulative Hours\": 0.0}, {\"Year Released\": \"2008\", \"Video Sources\": \"undisclosed web\", \"Cumulative Hours\": 0.0}, {\"Year Released\": \"2008\", \"Video Sources\": \"youtube\", \"Cumulative Hours\": 0.0}, {\"Year Released\": \"2008\", \"Video Sources\": \"movies\", \"Cumulative Hours\": 0.0}, {\"Year Released\": \"2008\", \"Video Sources\": \"crowdsourced\", \"Cumulative Hours\": 0.0}, {\"Year Released\": \"2008\", \"Video Sources\": \"flickr\", \"Cumulative Hours\": 0.0}, {\"Year Released\": \"2008\", \"Video Sources\": \"human\", \"Cumulative Hours\": 0.0}, {\"Year Released\": \"2008\", \"Video Sources\": \"Other\", \"Cumulative Hours\": 0.0}, {\"Year Released\": \"2009\", \"Video Sources\": \"Other\", \"Cumulative Hours\": 0.0}, {\"Year Released\": \"2009\", \"Video Sources\": \"crowdsourced\", \"Cumulative Hours\": 0.0}, {\"Year Released\": \"2009\", \"Video Sources\": \"flickr\", \"Cumulative Hours\": 0.0}, {\"Year Released\": \"2009\", \"Video Sources\": \"human\", \"Cumulative Hours\": 0.1}, {\"Year Released\": \"2009\", \"Video Sources\": \"movies\", \"Cumulative Hours\": 20.1}, {\"Year Released\": \"2009\", \"Video Sources\": \"undisclosed web\", \"Cumulative Hours\": 0.0}, {\"Year Released\": \"2009\", \"Video Sources\": \"youtube\", \"Cumulative Hours\": 0.0}, {\"Year Released\": \"2010\", \"Video Sources\": \"undisclosed web\", \"Cumulative Hours\": 0.0}, {\"Year Released\": \"2010\", \"Video Sources\": \"movies\", \"Cumulative Hours\": 20.1}, {\"Year Released\": \"2010\", \"Video Sources\": \"human\", \"Cumulative Hours\": 0.1}, {\"Year Released\": \"2010\", \"Video Sources\": \"youtube\", \"Cumulative Hours\": 0.0}, {\"Year Released\": \"2010\", \"Video Sources\": \"crowdsourced\", \"Cumulative Hours\": 0.0}, {\"Year Released\": \"2010\", \"Video Sources\": \"Other\", \"Cumulative Hours\": 0.0}, {\"Year Released\": \"2010\", \"Video Sources\": \"flickr\", \"Cumulative Hours\": 0.0}, {\"Year Released\": \"2011\", \"Video Sources\": \"undisclosed web\", \"Cumulative Hours\": 7000.0}, {\"Year Released\": \"2011\", \"Video Sources\": \"movies\", \"Cumulative Hours\": 7020.1}, {\"Year Released\": \"2011\", \"Video Sources\": \"human\", \"Cumulative Hours\": 0.1}, {\"Year Released\": \"2011\", \"Video Sources\": \"youtube\", \"Cumulative Hours\": 7000.0}, {\"Year Released\": \"2011\", \"Video Sources\": \"crowdsourced\", \"Cumulative Hours\": 0.0}, {\"Year Released\": \"2011\", \"Video Sources\": \"Other\", \"Cumulative Hours\": 14000.0}, {\"Year Released\": \"2011\", \"Video Sources\": \"flickr\", \"Cumulative Hours\": 0.0}, {\"Year Released\": \"2012\", \"Video Sources\": \"Other\", \"Cumulative Hours\": 14000.0}, {\"Year Released\": \"2012\", \"Video Sources\": \"crowdsourced\", \"Cumulative Hours\": 0.0}, {\"Year Released\": \"2012\", \"Video Sources\": \"flickr\", \"Cumulative Hours\": 0.0}, {\"Year Released\": \"2012\", \"Video Sources\": \"human\", \"Cumulative Hours\": 0.1}, {\"Year Released\": \"2012\", \"Video Sources\": \"movies\", \"Cumulative Hours\": 7020.1}, {\"Year Released\": \"2012\", \"Video Sources\": \"undisclosed web\", \"Cumulative Hours\": 7000.0}, {\"Year Released\": \"2012\", \"Video Sources\": \"youtube\", \"Cumulative Hours\": 7026.0}, {\"Year Released\": \"2013\", \"Video Sources\": \"movies\", \"Cumulative Hours\": 7020.1}, {\"Year Released\": \"2013\", \"Video Sources\": \"undisclosed web\", \"Cumulative Hours\": 7000.0}, {\"Year Released\": \"2013\", \"Video Sources\": \"human\", \"Cumulative Hours\": 40.1}, {\"Year Released\": \"2013\", \"Video Sources\": \"youtube\", \"Cumulative Hours\": 8026.0}, {\"Year Released\": \"2013\", \"Video Sources\": \"crowdsourced\", \"Cumulative Hours\": 0.0}, {\"Year Released\": \"2013\", \"Video Sources\": \"flickr\", \"Cumulative Hours\": 0.0}, {\"Year Released\": \"2013\", \"Video Sources\": \"Other\", \"Cumulative Hours\": 14000.0}, {\"Year Released\": \"2014\", \"Video Sources\": \"youtube\", \"Cumulative Hours\": 114784.41}, {\"Year Released\": \"2014\", \"Video Sources\": \"Other\", \"Cumulative Hours\": 14000.0}, {\"Year Released\": \"2014\", \"Video Sources\": \"crowdsourced\", \"Cumulative Hours\": 1.11}, {\"Year Released\": \"2014\", \"Video Sources\": \"flickr\", \"Cumulative Hours\": 0.0}, {\"Year Released\": \"2014\", \"Video Sources\": \"human\", \"Cumulative Hours\": 117.1}, {\"Year Released\": \"2014\", \"Video Sources\": \"movies\", \"Cumulative Hours\": 7036.150000000001}, {\"Year Released\": \"2014\", \"Video Sources\": \"undisclosed web\", \"Cumulative Hours\": 7000.0}, {\"Year Released\": \"2015\", \"Video Sources\": \"youtube\", \"Cumulative Hours\": 114788.01000000001}, {\"Year Released\": \"2015\", \"Video Sources\": \"undisclosed web\", \"Cumulative Hours\": 7849.0}, {\"Year Released\": \"2015\", \"Video Sources\": \"movies\", \"Cumulative Hours\": 7417.150000000001}, {\"Year Released\": \"2015\", \"Video Sources\": \"Other\", \"Cumulative Hours\": 14000.0}, {\"Year Released\": \"2015\", \"Video Sources\": \"flickr\", \"Cumulative Hours\": 0.0}, {\"Year Released\": \"2015\", \"Video Sources\": \"crowdsourced\", \"Cumulative Hours\": 1.11}, {\"Year Released\": \"2015\", \"Video Sources\": \"human\", \"Cumulative Hours\": 144.1}, {\"Year Released\": \"2016\", \"Video Sources\": \"movies\", \"Cumulative Hours\": 7417.150000000001}, {\"Year Released\": \"2016\", \"Video Sources\": \"undisclosed web\", \"Cumulative Hours\": 8103.4}, {\"Year Released\": \"2016\", \"Video Sources\": \"youtube\", \"Cumulative Hours\": 464795.01}, {\"Year Released\": \"2016\", \"Video Sources\": \"human\", \"Cumulative Hours\": 253.64}, {\"Year Released\": \"2016\", \"Video Sources\": \"crowdsourced\", \"Cumulative Hours\": 75.21}, {\"Year Released\": \"2016\", \"Video Sources\": \"Other\", \"Cumulative Hours\": 14086.1}, {\"Year Released\": \"2016\", \"Video Sources\": \"flickr\", \"Cumulative Hours\": 0.0}, {\"Year Released\": \"2017\", \"Video Sources\": \"undisclosed web\", \"Cumulative Hours\": 8112.7}, {\"Year Released\": \"2017\", \"Video Sources\": \"movies\", \"Cumulative Hours\": 7631.650000000001}, {\"Year Released\": \"2017\", \"Video Sources\": \"human\", \"Cumulative Hours\": 1104.32}, {\"Year Released\": \"2017\", \"Video Sources\": \"youtube\", \"Cumulative Hours\": 467108.11}, {\"Year Released\": \"2017\", \"Video Sources\": \"crowdsourced\", \"Cumulative Hours\": 266.66999999999996}, {\"Year Released\": \"2017\", \"Video Sources\": \"Other\", \"Cumulative Hours\": 14086.1}, {\"Year Released\": \"2017\", \"Video Sources\": \"flickr\", \"Cumulative Hours\": 0.0}, {\"Year Released\": \"2018\", \"Video Sources\": \"Other\", \"Cumulative Hours\": 15647.2}, {\"Year Released\": \"2018\", \"Video Sources\": \"crowdsourced\", \"Cumulative Hours\": 266.66999999999996}, {\"Year Released\": \"2018\", \"Video Sources\": \"flickr\", \"Cumulative Hours\": 275.0}, {\"Year Released\": \"2018\", \"Video Sources\": \"human\", \"Cumulative Hours\": 1273.6499999999999}, {\"Year Released\": \"2018\", \"Video Sources\": \"movies\", \"Cumulative Hours\": 7725.55}, {\"Year Released\": \"2018\", \"Video Sources\": \"undisclosed web\", \"Cumulative Hours\": 8112.7}, {\"Year Released\": \"2018\", \"Video Sources\": \"youtube\", \"Cumulative Hours\": 471120.63}, {\"Year Released\": \"2019\", \"Video Sources\": \"youtube\", \"Cumulative Hours\": 608225.21}, {\"Year Released\": \"2019\", \"Video Sources\": \"undisclosed web\", \"Cumulative Hours\": 9112.7}, {\"Year Released\": \"2019\", \"Video Sources\": \"movies\", \"Cumulative Hours\": 8241.550000000001}, {\"Year Released\": \"2019\", \"Video Sources\": \"crowdsourced\", \"Cumulative Hours\": 279.66999999999996}, {\"Year Released\": \"2019\", \"Video Sources\": \"flickr\", \"Cumulative Hours\": 1108.0}, {\"Year Released\": \"2019\", \"Video Sources\": \"Other\", \"Cumulative Hours\": 24182.7}, {\"Year Released\": \"2019\", \"Video Sources\": \"human\", \"Cumulative Hours\": 1642.23}, {\"Year Released\": \"2020\", \"Video Sources\": \"undisclosed web\", \"Cumulative Hours\": 10211.83}, {\"Year Released\": \"2020\", \"Video Sources\": \"movies\", \"Cumulative Hours\": 12919.55}, {\"Year Released\": \"2020\", \"Video Sources\": \"human\", \"Cumulative Hours\": 1658.51}, {\"Year Released\": \"2020\", \"Video Sources\": \"youtube\", \"Cumulative Hours\": 724739.74}, {\"Year Released\": \"2020\", \"Video Sources\": \"crowdsourced\", \"Cumulative Hours\": 304.21}, {\"Year Released\": \"2020\", \"Video Sources\": \"Other\", \"Cumulative Hours\": 50848.7}, {\"Year Released\": \"2020\", \"Video Sources\": \"flickr\", \"Cumulative Hours\": 1108.0}, {\"Year Released\": \"2021\", \"Video Sources\": \"Other\", \"Cumulative Hours\": 54805.73}, {\"Year Released\": \"2021\", \"Video Sources\": \"crowdsourced\", \"Cumulative Hours\": 1137.21}, {\"Year Released\": \"2021\", \"Video Sources\": \"flickr\", \"Cumulative Hours\": 1524.67}, {\"Year Released\": \"2021\", \"Video Sources\": \"human\", \"Cumulative Hours\": 1706.85}, {\"Year Released\": \"2021\", \"Video Sources\": \"movies\", \"Cumulative Hours\": 12955.55}, {\"Year Released\": \"2021\", \"Video Sources\": \"undisclosed web\", \"Cumulative Hours\": 24044.83}, {\"Year Released\": \"2021\", \"Video Sources\": \"youtube\", \"Cumulative Hours\": 740915.86}, {\"Year Released\": \"2022\", \"Video Sources\": \"youtube\", \"Cumulative Hours\": 796470.86}, {\"Year Released\": \"2022\", \"Video Sources\": \"undisclosed web\", \"Cumulative Hours\": 25268.6}, {\"Year Released\": \"2022\", \"Video Sources\": \"movies\", \"Cumulative Hours\": 12955.55}, {\"Year Released\": \"2022\", \"Video Sources\": \"crowdsourced\", \"Cumulative Hours\": 1352.21}, {\"Year Released\": \"2022\", \"Video Sources\": \"flickr\", \"Cumulative Hours\": 1524.67}, {\"Year Released\": \"2022\", \"Video Sources\": \"Other\", \"Cumulative Hours\": 54805.73}, {\"Year Released\": \"2022\", \"Video Sources\": \"human\", \"Cumulative Hours\": 5376.85}, {\"Year Released\": \"2023\", \"Video Sources\": \"youtube\", \"Cumulative Hours\": 796470.86}, {\"Year Released\": \"2023\", \"Video Sources\": \"movies\", \"Cumulative Hours\": 12955.55}, {\"Year Released\": \"2023\", \"Video Sources\": \"human\", \"Cumulative Hours\": 6805.45}, {\"Year Released\": \"2023\", \"Video Sources\": \"undisclosed web\", \"Cumulative Hours\": 25268.6}, {\"Year Released\": \"2023\", \"Video Sources\": \"crowdsourced\", \"Cumulative Hours\": 1352.21}, {\"Year Released\": \"2023\", \"Video Sources\": \"Other\", \"Cumulative Hours\": 54805.73}, {\"Year Released\": \"2023\", \"Video Sources\": \"flickr\", \"Cumulative Hours\": 1524.67}, {\"Year Released\": \"2024\", \"Video Sources\": \"undisclosed web\", \"Cumulative Hours\": 25268.6}, {\"Year Released\": \"2024\", \"Video Sources\": \"Other\", \"Cumulative Hours\": 54889.73}, {\"Year Released\": \"2024\", \"Video Sources\": \"crowdsourced\", \"Cumulative Hours\": 1352.21}, {\"Year Released\": \"2024\", \"Video Sources\": \"flickr\", \"Cumulative Hours\": 1524.67}, {\"Year Released\": \"2024\", \"Video Sources\": \"human\", \"Cumulative Hours\": 8455.45}, {\"Year Released\": \"2024\", \"Video Sources\": \"movies\", \"Cumulative Hours\": 13373.15}, {\"Year Released\": \"2024\", \"Video Sources\": \"youtube\", \"Cumulative Hours\": 796972.46}]}}, {\"mode\": \"vega-lite\"});\n",
       "</script>"
      ],
      "text/plain": [
       "alt.HConcatChart(...)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chart_sourceyearcombined = alt.hconcat(\n",
    "    base_sourceyear,\n",
    "    chart_sourceyearhours\n",
    ").configure_axis(\n",
    "    labelFontSize=FONT_SIZE,\n",
    "    titleFontSize=FONT_SIZE,\n",
    "    grid=False\n",
    ").configure_legend(\n",
    "    labelFontSize=FONT_SIZE,\n",
    "    titleFontSize=FONT_SIZE,\n",
    "    labelLimit=MAX_LABELLIMIT\n",
    ").configure_header(\n",
    "    titleFontSize=FONT_SIZE,\n",
    "    labelFontSize=FONT_SIZE\n",
    ").configure_title(\n",
    "    fontSize=FONT_SIZE\n",
    ").resolve_scale(\n",
    "    x=\"independent\",\n",
    "    y=\"independent\"\n",
    ")\n",
    "\n",
    "if PLOT_TOFILE:\n",
    "    chart_sourceyearcombined.save(\n",
    "        os.path.join(PLOT_DIR, \"video_sourcecategories-yearscombined.png\"),\n",
    "        ppi=PLOT_PPI\n",
    "    )\n",
    "\n",
    "chart_sourceyearcombined"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Video Task Vs Year Release "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'type' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[39], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m task_categories: \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m]\u001b[49m] \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# categories['Video Q&A'] = ['Video Question Answering', \"Video Summarization\", \"Video Q&A\", \"Video Captioning\"]\u001b[39;00m\n\u001b[1;32m      3\u001b[0m task_categories[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mVideo Q&A\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mVideo Question Answering\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "\u001b[0;31mTypeError\u001b[0m: 'type' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "\n",
    "task_categories: dict[str, list[str]] = {}\n",
    "# categories['Video Q&A'] = ['Video Question Answering', \"Video Summarization\", \"Video Q&A\", \"Video Captioning\"]\n",
    "task_categories['Video Q&A'] = [\"Video Question Answering\"]\n",
    "task_categories['Misc'] = [\"Misc (Hand/Object Detection)\", \"Misc\", \"Misc (Scene Reconstruction)\", \"Misc (video retrieval\", \n",
    "                        \"Misc (Locomotion Prediction\", \"Misc\", \"interaction understanding via ordering\", \n",
    "                        \"reason prediction)\", \"Misc (Scene Segmentation)\", ]\n",
    "\n",
    "# reverse the categories\n",
    "task_categories = invert_dict_of_lists(task_categories)\n",
    "\n",
    "# Task Categories are a list of categories, but applu the mapping to each item in the list\n",
    "df_video['Task Categories'] = df_video['Task Categories'].apply(\n",
    "    lambda x: [task_categories.get(item, item) for item in x]\n",
    ")\n",
    "print(df_video['Task Categories'].explode().value_counts())\n",
    "\n",
    "INCLUDE_TOP_N_CATEGORIES = 6\n",
    "df_videotaskyears = df_video.explode(\"Task Categories\")\n",
    "df_videotaskyears = reduce_categories_to_topk(df_videotaskyears, \"Task Categories\", INCLUDE_TOP_N_CATEGORIES)\n",
    "\n",
    "df_videotaskyears = df_videotaskyears.sort_values(by=\"Year Released\")\n",
    "df_videotaskyears.head()['Year Released']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df_videotaskyears' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[40], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m base_taskyear \u001b[38;5;241m=\u001b[39m alt\u001b[38;5;241m.\u001b[39mChart(\n\u001b[0;32m----> 2\u001b[0m     \u001b[43mdf_videotaskyears\u001b[49m\n\u001b[1;32m      3\u001b[0m )\u001b[38;5;241m.\u001b[39mmark_bar()\u001b[38;5;241m.\u001b[39mencode(\n\u001b[1;32m      4\u001b[0m     x\u001b[38;5;241m=\u001b[39malt\u001b[38;5;241m.\u001b[39mX(\n\u001b[1;32m      5\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYear Released:N\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      6\u001b[0m         title\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYear Released\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      7\u001b[0m         sort\u001b[38;5;241m=\u001b[39mYEARS_ORDER,\n\u001b[1;32m      8\u001b[0m         axis\u001b[38;5;241m=\u001b[39malt\u001b[38;5;241m.\u001b[39mAxis(labelAngle\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m30\u001b[39m)\n\u001b[1;32m      9\u001b[0m     ),\n\u001b[1;32m     10\u001b[0m     y\u001b[38;5;241m=\u001b[39malt\u001b[38;5;241m.\u001b[39mY(\n\u001b[1;32m     11\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcount():Q\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     12\u001b[0m         stack\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnormalize\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     13\u001b[0m         axis\u001b[38;5;241m=\u001b[39malt\u001b[38;5;241m.\u001b[39mAxis(\u001b[38;5;28mformat\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m     14\u001b[0m         title\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPct. Datasets\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     15\u001b[0m     ),\n\u001b[1;32m     16\u001b[0m     color\u001b[38;5;241m=\u001b[39malt\u001b[38;5;241m.\u001b[39mColor(\n\u001b[1;32m     17\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTask Categories:N\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     18\u001b[0m         title\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mVideo Task Categories\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     19\u001b[0m     )\n\u001b[1;32m     20\u001b[0m )\u001b[38;5;241m.\u001b[39mproperties(\n\u001b[1;32m     21\u001b[0m     width\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m600\u001b[39m,\n\u001b[1;32m     22\u001b[0m     height\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m160\u001b[39m\n\u001b[1;32m     23\u001b[0m )\n\u001b[1;32m     25\u001b[0m text_sourceyear \u001b[38;5;241m=\u001b[39m alt\u001b[38;5;241m.\u001b[39mChart(df_videotaskyears)\u001b[38;5;241m.\u001b[39mmark_text(\n\u001b[1;32m     26\u001b[0m     dy\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m90\u001b[39m,\n\u001b[1;32m     27\u001b[0m     align\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcenter\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     36\u001b[0m     text\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcount():Q\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     37\u001b[0m )\n\u001b[1;32m     39\u001b[0m chart_taskyear \u001b[38;5;241m=\u001b[39m (base_taskyear \u001b[38;5;241m+\u001b[39m text_sourceyear)\u001b[38;5;241m.\u001b[39mconfigure_axis(\n\u001b[1;32m     40\u001b[0m     labelFontSize\u001b[38;5;241m=\u001b[39mFONT_SIZE,\n\u001b[1;32m     41\u001b[0m     titleFontSize\u001b[38;5;241m=\u001b[39mFONT_SIZE\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     47\u001b[0m     labelLimit\u001b[38;5;241m=\u001b[39mMAX_LABELLIMIT\n\u001b[1;32m     48\u001b[0m )\n",
      "\u001b[0;31mNameError\u001b[0m: name 'df_videotaskyears' is not defined"
     ]
    }
   ],
   "source": [
    "base_taskyear = alt.Chart(\n",
    "    df_videotaskyears\n",
    ").mark_bar().encode(\n",
    "    x=alt.X(\n",
    "        \"Year Released:N\",\n",
    "        title=\"Year Released\",\n",
    "        sort=YEARS_ORDER,\n",
    "        axis=alt.Axis(labelAngle=-30)\n",
    "    ),\n",
    "    y=alt.Y(\n",
    "        \"count():Q\",\n",
    "        stack=\"normalize\",\n",
    "        axis=alt.Axis(format=\"%\"),\n",
    "        title=\"Pct. Datasets\"\n",
    "    ),\n",
    "    color=alt.Color(\n",
    "        \"Task Categories:N\",\n",
    "        title=\"Video Task Categories\"\n",
    "    )\n",
    ").properties(\n",
    "    width=600,\n",
    "    height=160\n",
    ")\n",
    "\n",
    "text_sourceyear = alt.Chart(df_videotaskyears).mark_text(\n",
    "    dy=-90,\n",
    "    align=\"center\",\n",
    "    baseline=\"top\",\n",
    "    fontSize=12\n",
    ").encode(\n",
    "    x=alt.X(\n",
    "        \"Year Released:N\",\n",
    "        title=\"Year Released\",\n",
    "        sort=YEARS_ORDER\n",
    "    ),\n",
    "    text=\"count():Q\"\n",
    ")\n",
    "\n",
    "chart_taskyear = (base_taskyear + text_sourceyear).configure_axis(\n",
    "    labelFontSize=FONT_SIZE,\n",
    "    titleFontSize=FONT_SIZE\n",
    ").configure_legend(\n",
    "    labelFontSize=FONT_SIZE,\n",
    "    titleFontSize=FONT_SIZE,\n",
    "    orient=LEGEND_POSITION,\n",
    "    columns=4,\n",
    "    labelLimit=MAX_LABELLIMIT\n",
    ")\n",
    "\n",
    "\n",
    "if PLOT_TOFILE:\n",
    "    chart_taskyear.save(\n",
    "        os.path.join(PLOT_DIR, \"video_taskcategories-years.png\"),\n",
    "        ppi=PLOT_PPI\n",
    "    )\n",
    "\n",
    "chart_taskyear"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Source Category (YouTube or Other) by License Type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# By count\n",
    "df_counts_by_license_source = df_video.explode(\"Video Sources\").groupby([\"License Type\", \"Video Sources\"]).size().reset_index(name=\"Count\")\n",
    "df_counts_by_license_source = df_counts_by_license_source.sort_values(by=\"Count\")\n",
    "df_counts_by_license_source[\"YouTube\"] = df_counts_by_license_source[\"Video Sources\"].map(\n",
    "    lambda x: \"YouTube\" if \"youtube\" in x.lower() else \"Other\"\n",
    ")\n",
    "\n",
    "# # By hours\n",
    "# df_hours_by_license_source = df_speech.explode(\"Source\").groupby([\"License Type\", \"Source\"])[\"Hours\"].sum().reset_index(name=\"Total Hours\")\n",
    "# df_hours_by_license_source = df_hours_by_license_source.sort_values(by=\"Total Hours\")\n",
    "# df_hours_by_license_source[\"YouTube\"] = df_hours_by_license_source[\"Source\"].map(\n",
    "#     lambda x: \"YouTube\" if \"youtube\" in x.lower() else \"Other\"\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creator Categories by Year\n",
    "\n",
    "Note: we use the original annotations here instead of the DPI constants, for a different view."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of License Type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     CC BY-NC-SA 4.0\n",
       "27             Custom\n",
       "65    CC BY-NC-SA 4.0\n",
       "32             Custom\n",
       "62             Custom\n",
       "           ...       \n",
       "28          CC BY 4.0\n",
       "26        MIT License\n",
       "25          CC BY 4.0\n",
       "63          CC BY 4.0\n",
       "48        MIT License\n",
       "Name: Licenses, Length: 98, dtype: object"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_video['Licenses'].apply(lambda x: [item['License'] for item in x][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Count</th>\n",
       "      <th>Pct.</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Licenses</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Unspecified</th>\n",
       "      <td>37</td>\n",
       "      <td>37.76</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Custom</th>\n",
       "      <td>27</td>\n",
       "      <td>27.55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CC BY 4.0</th>\n",
       "      <td>9</td>\n",
       "      <td>9.18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MIT License</th>\n",
       "      <td>8</td>\n",
       "      <td>8.16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CC BY-NC 4.0</th>\n",
       "      <td>5</td>\n",
       "      <td>5.10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CC BY-NC-SA 4.0</th>\n",
       "      <td>3</td>\n",
       "      <td>3.06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Apache License 2.0</th>\n",
       "      <td>3</td>\n",
       "      <td>3.06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Non Commercial</th>\n",
       "      <td>2</td>\n",
       "      <td>2.04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CC BY 3.0</th>\n",
       "      <td>2</td>\n",
       "      <td>2.04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Various</th>\n",
       "      <td>1</td>\n",
       "      <td>1.02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BSD 2-Clause License</th>\n",
       "      <td>1</td>\n",
       "      <td>1.02</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      Count   Pct.\n",
       "Licenses                          \n",
       "Unspecified              37  37.76\n",
       "Custom                   27  27.55\n",
       "CC BY 4.0                 9   9.18\n",
       "MIT License               8   8.16\n",
       "CC BY-NC 4.0              5   5.10\n",
       "CC BY-NC-SA 4.0           3   3.06\n",
       "Apache License 2.0        3   3.06\n",
       "Non Commercial            2   2.04\n",
       "CC BY 3.0                 2   2.04\n",
       "Various                   1   1.02\n",
       "BSD 2-Clause License      1   1.02"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "licensetype_counts = df_video[\"Licenses\"].value_counts()\n",
    "licensetype_counts = df_video['Licenses'].apply(lambda x: [item['License'] for item in x][0]).value_counts()\n",
    "df_licensetypes = pd.concat([\n",
    "    licensetype_counts,\n",
    "    (licensetype_counts / licensetype_counts.sum()).round(4) * 100\n",
    "], axis=1)\n",
    "\n",
    "df_licensetypes.columns = [\"Count\", \"Pct.\"]\n",
    "\n",
    "df_licensetypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Video Task Categories by Licence Type"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Youtuve to Licence Type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>License Type</th>\n",
       "      <th>YouTube</th>\n",
       "      <th>Count</th>\n",
       "      <th>Pct.</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Unspecified</td>\n",
       "      <td>YouTube</td>\n",
       "      <td>12</td>\n",
       "      <td>32.43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Non-Commercial/Academic</td>\n",
       "      <td>YouTube</td>\n",
       "      <td>13</td>\n",
       "      <td>41.94</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Commercial</td>\n",
       "      <td>YouTube</td>\n",
       "      <td>13</td>\n",
       "      <td>43.33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Commercial</td>\n",
       "      <td>Other</td>\n",
       "      <td>17</td>\n",
       "      <td>56.67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Non-Commercial/Academic</td>\n",
       "      <td>Other</td>\n",
       "      <td>18</td>\n",
       "      <td>58.06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Unspecified</td>\n",
       "      <td>Other</td>\n",
       "      <td>25</td>\n",
       "      <td>67.57</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              License Type  YouTube  Count   Pct.\n",
       "3              Unspecified  YouTube     12  32.43\n",
       "1  Non-Commercial/Academic  YouTube     13  41.94\n",
       "5               Commercial  YouTube     13  43.33\n",
       "4               Commercial    Other     17  56.67\n",
       "0  Non-Commercial/Academic    Other     18  58.06\n",
       "2              Unspecified    Other     25  67.57"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# By count\n",
    "df_video[\"YouTube\"] = df_video[\"Video Sources\"].map(\n",
    "    lambda x: \"YouTube\" if any(\"youtube\" in xi.lower() for xi in x) else \"Other\"\n",
    ")\n",
    "\n",
    "df_youtube = df_video.groupby([\"License Type\", \"YouTube\"]).size().reset_index(name=\"Count\")\n",
    "df_youtube = df_youtube.sort_values(by=\"Count\")\n",
    "df_youtube[\"Pct.\"] = df_youtube.groupby(\"License Type\")[\"Count\"].transform(lambda x: (x / x.sum()).round(4) * 100)\n",
    "\n",
    "df_youtube"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Video Hours by Licence Type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>License Type</th>\n",
       "      <th>YouTube</th>\n",
       "      <th>Total Hours</th>\n",
       "      <th>Pct.</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Unspecified</td>\n",
       "      <td>Other</td>\n",
       "      <td>8380.77</td>\n",
       "      <td>1.68</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Commercial</td>\n",
       "      <td>Other</td>\n",
       "      <td>9847.75</td>\n",
       "      <td>5.07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Non-Commercial/Academic</td>\n",
       "      <td>Other</td>\n",
       "      <td>16920.19</td>\n",
       "      <td>12.18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Non-Commercial/Academic</td>\n",
       "      <td>YouTube</td>\n",
       "      <td>121946.43</td>\n",
       "      <td>87.82</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Commercial</td>\n",
       "      <td>YouTube</td>\n",
       "      <td>184436.86</td>\n",
       "      <td>94.93</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Unspecified</td>\n",
       "      <td>YouTube</td>\n",
       "      <td>490589.17</td>\n",
       "      <td>98.32</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              License Type  YouTube  Total Hours   Pct.\n",
       "2              Unspecified    Other      8380.77   1.68\n",
       "4               Commercial    Other      9847.75   5.07\n",
       "0  Non-Commercial/Academic    Other     16920.19  12.18\n",
       "1  Non-Commercial/Academic  YouTube    121946.43  87.82\n",
       "5               Commercial  YouTube    184436.86  94.93\n",
       "3              Unspecified  YouTube    490589.17  98.32"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# By hours\n",
    "df_youtubehours = df_video.groupby([\"License Type\", \"YouTube\"])[\"Video Hours\"].sum().reset_index(name=\"Total Hours\")\n",
    "df_youtubehours = df_youtubehours.sort_values(by=\"Total Hours\")\n",
    "df_youtubehours[\"Pct.\"] = df_youtubehours.groupby(\"License Type\")[\"Total Hours\"].transform(lambda x: (x / x.sum()).round(4) * 100)\n",
    "\n",
    "df_youtubehours"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tables of Creator Categories (By Count and Hour)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Creators</th>\n",
       "      <th>Count</th>\n",
       "      <th>Pct.</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Academic</td>\n",
       "      <td>129</td>\n",
       "      <td>70.11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Corporation</td>\n",
       "      <td>20</td>\n",
       "      <td>10.87</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Government</td>\n",
       "      <td>1</td>\n",
       "      <td>0.54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Industry Lab</td>\n",
       "      <td>17</td>\n",
       "      <td>9.24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Other</td>\n",
       "      <td>5</td>\n",
       "      <td>2.72</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Research Group</td>\n",
       "      <td>11</td>\n",
       "      <td>5.98</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Startup</td>\n",
       "      <td>1</td>\n",
       "      <td>0.54</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Creators  Count   Pct.\n",
       "0        Academic    129  70.11\n",
       "1     Corporation     20  10.87\n",
       "2      Government      1   0.54\n",
       "3    Industry Lab     17   9.24\n",
       "4           Other      5   2.72\n",
       "5  Research Group     11   5.98\n",
       "6         Startup      1   0.54"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_creatorcategories = df_video.explode(\"Creators\").groupby(\"Creators\").size().reset_index(name=\"Count\")\n",
    "df_creatorcategories[\"Pct.\"] = df_creatorcategories[\"Count\"].transform(lambda x: (x / x.sum()).round(4) * 100)\n",
    "\n",
    "df_creatorcategories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Creators</th>\n",
       "      <th>Video Hours</th>\n",
       "      <th>Pct.</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Academic</td>\n",
       "      <td>1056460.71</td>\n",
       "      <td>58.13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Corporation</td>\n",
       "      <td>6518.77</td>\n",
       "      <td>0.36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Government</td>\n",
       "      <td>1000.00</td>\n",
       "      <td>0.06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Industry Lab</td>\n",
       "      <td>359428.15</td>\n",
       "      <td>19.78</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Other</td>\n",
       "      <td>148627.99</td>\n",
       "      <td>8.18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Research Group</td>\n",
       "      <td>149141.24</td>\n",
       "      <td>8.21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Startup</td>\n",
       "      <td>96166.67</td>\n",
       "      <td>5.29</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Creators  Video Hours   Pct.\n",
       "0        Academic   1056460.71  58.13\n",
       "1     Corporation      6518.77   0.36\n",
       "2      Government      1000.00   0.06\n",
       "3    Industry Lab    359428.15  19.78\n",
       "4           Other    148627.99   8.18\n",
       "5  Research Group    149141.24   8.21\n",
       "6         Startup     96166.67   5.29"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_creatorcategories = df_video.explode(\"Creators\").groupby(\"Creators\")[\"Video Hours\"].sum().reset_index(name=\"Video Hours\")\n",
    "df_creatorcategories[\"Pct.\"] = df_creatorcategories[\"Video Hours\"].transform(lambda x: (x / x.sum()).round(4) * 100)\n",
    "df_creatorcategories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Video Sources</th>\n",
       "      <th>Total Hours</th>\n",
       "      <th>Pct.</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Not Prohibited</td>\n",
       "      <td>38.5</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>tiktok</td>\n",
       "      <td>84.0</td>\n",
       "      <td>0.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>tumblr</td>\n",
       "      <td>86.1</td>\n",
       "      <td>0.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>youdescribe</td>\n",
       "      <td>207.0</td>\n",
       "      <td>0.02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>bbc</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>0.11</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Video Sources  Total Hours  Pct.\n",
       "0   Not Prohibited         38.5  0.00\n",
       "15          tiktok         84.0  0.01\n",
       "16          tumblr         86.1  0.01\n",
       "22     youdescribe        207.0  0.02\n",
       "1              bbc       1000.0  0.11"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# do video hours vs. video source categories\n",
    "df_videohours = df_video.explode(\"Video Sources\")\n",
    "df_videohours = df_videohours.groupby(\"Video Sources\")[\"Video Hours\"].sum().reset_index(name=\"Total Hours\")\n",
    "df_videohours = df_videohours.sort_values(by=\"Total Hours\")\n",
    "df_videohours[\"Pct.\"] = df_videohours[\"Total Hours\"].transform(lambda x: (x / x.sum()).round(4) * 100)\n",
    "df_videohours.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "271834be-94ee-4002-8ced-73a0790fbf16",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import copy\n",
    "from datetime import datetime\n",
    "from collections import Counter, defaultdict\n",
    "\n",
    "# Visualization packages\n",
    "import altair as alt\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from matplotlib.patches import Patch\n",
    "\n",
    "# Append system path\n",
    "sys.path = [p for p in sys.path if not p.endswith('../..')]  # Cleans duplicated '../..'\n",
    "sys.path.insert(0, '../')  # This adds `src` to the path\n",
    "\n",
    "from helpers import io, filters, constants\n",
    "from analysis import analysis_util, analysis_constants, visualization_util\n",
    "from web_analysis import parse_robots\n",
    "from web_analysis import robots_util\n",
    "\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbe935ae-6338-4e3b-bd10-13123b0a978a",
   "metadata": {},
   "source": [
    "### Define Paths to all relevant files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2133323-eca3-44bc-9694-d8b0fbd51069",
   "metadata": {},
   "outputs": [],
   "source": [
    "FPATH_TO_RELEVANT_URL_TOKENS = 'pretrain_data/relevant_url_token_counts.csv'\n",
    "FPATH_to_HEAD_ROBOTS = \"robots_data/temporal_robots_head.json\"\n",
    "FPATH_TO_RAND_ROBOTS = \"robots_data/temporal_robots_rand_10k.json\"\n",
    "FPATH_TO_TOS_DATA = \"robots_data/tos_ai_scraping_policies.json\"\n",
    "DIRPATHS_TO_ANNOTATED_TASKS = [\"annotated_websites/Task 1\", \"annotated_websites/Task 2\"]\n",
    "FPATH_SNAPSHOT_DATA = \"robots_data/temporal_main_sites_current.json\"\n",
    "\n",
    "ALL_COMPANIES_TO_TRACK = [\"Google\", \"OpenAI\", \"Anthropic\", \"Cohere\", \"Common Crawl\", \"Meta\", \"Internet Archive\", \"Google Search\", \"False Anthropic\"]\n",
    "COMPANIES_TO_ANALYZE = [\"Google\", \"OpenAI\", \"Anthropic\", \"Cohere\", \"Common Crawl\", \"Meta\"]\n",
    "TEMPORAL_ANALYSIS_START_DATE = '2016-01-01'\n",
    "TEMPORAL_ANALYSIS_END_DATE = '2024-04-30'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee96fd8b-6342-476c-bff6-72a525e1da36",
   "metadata": {},
   "source": [
    "### Load all URL splits (top vs random) and maps to Token Counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a3b2a93-5a35-4c7e-b087-73dd4db273f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "url_token_lookup = robots_util.URLTokenLookup(FPATH_TO_RELEVANT_URL_TOKENS) # 'c4', 'rf', 'dolma'\n",
    "c4_url_to_counts = url_token_lookup.get_url_to_token_map(\"c4\")\n",
    "rf_url_to_counts = url_token_lookup.get_url_to_token_map(\"rf\")\n",
    "dolma_url_to_counts = url_token_lookup.get_url_to_token_map(\"dolma\")\n",
    "top_c4_urls = url_token_lookup.top_k_urls(\"c4\", 2000)\n",
    "top_rf_urls = url_token_lookup.top_k_urls(\"rf\", 2000)\n",
    "top_dolma_urls = url_token_lookup.top_k_urls(\"dolma\", 2000)\n",
    "random_10k_urls = url_token_lookup.get_10k_random_sample()\n",
    "all_urls = set(random_10k_urls + top_c4_urls + top_rf_urls + top_dolma_urls)\n",
    "\n",
    "# Load website snapshots for relevant URLs\n",
    "website_snapshots = robots_util.read_snapshots(FPATH_SNAPSHOT_DATA, all_urls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02e6f2de-3bb0-43ac-b0a0-3dbcd5c4f82d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4a54f01e-6c3a-4f48-a889-ba765dc9450f",
   "metadata": {},
   "source": [
    "### Define Agents and Agent Groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb8b750e-3c52-4578-a486-b8ad1a2a88a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_groups_to_track = robots_util.get_bot_groups(ALL_COMPANIES_TO_TRACK)\n",
    "agents_to_track = robots_util.get_bots()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b09644e3-d075-4a46-8445-d8179ae29bc9",
   "metadata": {},
   "source": [
    "### Load Robots.txt info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41e969a9-1d0b-4b95-9ccf-ff15840d94e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# URL -> Date -> Robots.txt raw text\n",
    "head_robots = io.read_json(FPATH_to_HEAD_ROBOTS)\n",
    "random_10k_robots = io.read_json(FPATH_TO_RAND_ROBOTS)\n",
    "joined_robots = copy.deepcopy(head_robots)\n",
    "joined_robots.update(random_10k_robots)\n",
    "robots_util.print_out_robots_info(head_robots)\n",
    "robots_util.print_out_robots_info(random_10k_robots)\n",
    "\n",
    "# {URL --> Date --> Agent --> Status}\n",
    "url_robots_summary, agent_counter_df = robots_util.compute_url_date_agent_status(\n",
    "    data=joined_robots, \n",
    "    # relevant_agents=agents_to_track)\n",
    "    relevant_agents=[v for vs in agent_groups_to_track.values() for v in vs])\n",
    "\n",
    "agent_counter_df.to_csv(\"all_agents_counter.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a3988a4-0a7c-427f-a311-4b36cd8b5774",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d5e26ade-d9e9-4900-a3b3-a01af4a698b1",
   "metadata": {},
   "source": [
    "### Load ToS info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70351b94-b889-4f16-9c31-56fd61f43464",
   "metadata": {},
   "outputs": [],
   "source": [
    "# URL --> Date --> ToS-suburl --> {\"verdict\": X, \"evidence\": Y}\n",
    "tos_policies = io.read_json(FPATH_TO_TOS_DATA)\n",
    "print(f\"Num ToS URLs: {len(tos_policies)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36901240-db00-4c04-9ac3-6f5377233736",
   "metadata": {},
   "source": [
    "### Load Manual Pretraining Annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31e586df-d0ba-44f0-ba5c-e260380107c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "url_to_info = analysis_util.extract_url_annotations(DIRPATHS_TO_ANNOTATED_TASKS)\n",
    "url_results_df = analysis_util.process_url_annotations(url_to_info)\n",
    "url_results_df = analysis_util.encode_size_columns(url_results_df, url_token_lookup)\n",
    "url_results_df = robots_util.encode_latest_tos_robots_into_df(\n",
    "    url_results_df, tos_policies, url_robots_summary,\n",
    "    COMPANIES_TO_ANALYZE\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e3002a9-b499-4a0f-bee1-9e433c4cf5e3",
   "metadata": {},
   "source": [
    "# Create Plots"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "298fa261-5171-4c9a-9b1a-de5a70cc8490",
   "metadata": {},
   "source": [
    "### Preprocessing for Robots Head & Random URL splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15b876db-605f-424d-af51-7e3b7cb98242",
   "metadata": {},
   "outputs": [],
   "source": [
    "### DECISION POINT: Use C4, Dolma, or RefinedWeb here?\n",
    "\n",
    "CHOSEN_CORPUS = \"c4\" # 'c4', 'rf', 'dolma'\n",
    "if CHOSEN_CORPUS == \"c4\":\n",
    "    HEAD_URL_SET = top_c4_urls\n",
    "    URL_TO_COUNTS = c4_url_to_counts\n",
    "elif CHOSEN_CORPUS == \"rf\":\n",
    "    HEAD_URL_SET = top_rf_urls\n",
    "    URL_TO_COUNTS = rf_url_to_counts\n",
    "elif CHOSEN_CORPUS == \"dolma\":\n",
    "    HEAD_URL_SET = top_dolma_urls\n",
    "    URL_TO_COUNTS = dolma_url_to_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95af6c29-6b79-44c0-b98f-199376387ebf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "840d10e9-9586-46e2-91a5-a4910e1340ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "url_robots_summary_head = {url: url_robots_summary[url] for url in HEAD_URL_SET if url in url_robots_summary}\n",
    "url_robots_summary_rand = {url: url_robots_summary[url] for url in random_10k_urls if url in url_robots_summary}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd4de08b-09be-4247-8354-a4b9120e9d5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HEAD URL SPLIT\n",
    "# {Period --> Agent --> Status --> set(URLs)}\n",
    "robots_filled_status_head_summary = robots_util.prepare_robots_temporal_summary(\n",
    "    url_robots_summary=url_robots_summary_head, \n",
    "    # group_to_agents={k: [k] for k in agents_to_track},\n",
    "    group_to_agents=agent_groups_to_track,\n",
    "    start_time=TEMPORAL_ANALYSIS_START_DATE, \n",
    "    end_time=TEMPORAL_ANALYSIS_END_DATE,\n",
    "    time_frequency=\"M\",\n",
    "    website_start_dates=website_snapshots,\n",
    ")\n",
    "# RANDOM URL SPLIT\n",
    "robots_filled_status_rand_summary = robots_util.prepare_robots_temporal_summary(\n",
    "    url_robots_summary=url_robots_summary_rand, \n",
    "    # group_to_agents={k: [k] for k in agents_to_track},\n",
    "    group_to_agents=agent_groups_to_track,\n",
    "    start_time=TEMPORAL_ANALYSIS_START_DATE, \n",
    "    end_time=TEMPORAL_ANALYSIS_END_DATE,\n",
    "    time_frequency=\"M\",\n",
    "    website_start_dates=website_snapshots,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0239870-9790-48bb-8222-b2174a4f0ec1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataframe w/ [Period, Agent, Status, count(URLs or tokens)]\n",
    "robots_temporal_head_summary = robots_util.robots_temporal_to_df(\n",
    "    robots_filled_status_head_summary,\n",
    "    url_to_counts=c4_url_to_counts,\n",
    ")\n",
    "# Dataframe w/ [Period, Agent, Status, count(URLs), count(tokens)]\n",
    "robots_temporal_rand_summary = robots_util.robots_temporal_to_df(\n",
    "    robots_filled_status_rand_summary,\n",
    "    url_to_counts=URL_TO_COUNTS,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76a0f2c9-9b93-4e7b-8fdd-49e67f0de4db",
   "metadata": {},
   "source": [
    "### Preprocessing for ToS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7620dcbd-c869-42d0-b12b-83986a65ffb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# URL --> time --> ToS verdict string. \n",
    "url_to_time_to_tos_verdict = robots_util.get_tos_url_time_verdicts(tos_policies)\n",
    "# Period --> Status --> set(URLs)\n",
    "period_tos_verdict_urls = robots_util.prepare_tos_temporal_summary(\n",
    "    url_to_time_to_tos_verdict,\n",
    "    start_time=TEMPORAL_ANALYSIS_START_DATE, \n",
    "    end_time=TEMPORAL_ANALYSIS_END_DATE,\n",
    "    time_frequency=\"M\",\n",
    "    website_start_dates=website_snapshots,\n",
    ")\n",
    "# Dataframe: [Period, Status, Count, Tokens]\n",
    "tos_summary_df = robots_util.tos_temporal_to_df(\n",
    "    period_tos_verdict_urls,\n",
    "    url_set=HEAD_URL_SET,\n",
    "    url_to_counts=URL_TO_COUNTS,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "749d2654-6968-4da2-b065-013ca1e4a6eb",
   "metadata": {},
   "source": [
    "## TODO: Add Forecasting code here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2907248a-b12d-462b-b0cc-575e0ffa9278",
   "metadata": {},
   "outputs": [],
   "source": [
    "# robots_temporal_head_summary --> function that extends for forecasting?\n",
    "# robots_temporal_rand_summary --> function that extends for forecasting?\n",
    "# tos_summary_df --> function that extends for forecasting?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f58c102",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import altair as alt\n",
    "from statsmodels.tsa.ar_model import AutoReg\n",
    "from prophet import Prophet\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
    "\n",
    "#disable user warning\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "\n",
    "\n",
    "def create_lagged_features(df, lags):\n",
    "    for lag in lags:\n",
    "        df[f'lag_{lag}'] = df['y'].shift(lag)\n",
    "    return df.dropna()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0baa7e5",
   "metadata": {},
   "source": [
    "### Autoregressive robots temporal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2cb15b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forecast_and_plot(df, agent, lags):\n",
    "    # Filter the DataFrame for the specific agent\n",
    "    agent_df = df[df['agent'] == agent].copy()\n",
    "    \n",
    "    # Convert 'period' to timestamp if it's a Period object\n",
    "    agent_df.loc[:, 'period'] = agent_df['period'].apply(lambda x: x.to_timestamp() if isinstance(x, pd.Period) else x)\n",
    "    \n",
    "    # Reshape the data\n",
    "    pivoted_df = agent_df.pivot_table(index='period', columns='status', values='count')\n",
    "    \n",
    "    # Normalize the counts to percentages\n",
    "    pivoted_df = pivoted_df.div(pivoted_df.sum(axis=1), axis=0) * 100\n",
    "    \n",
    "    # Doing these for each status individually\n",
    "    status_dfs = {}\n",
    "    for status in pivoted_df.columns:\n",
    "        status_df = pivoted_df[[status]].reset_index()\n",
    "        status_df.columns = ['ds', 'y']\n",
    "        status_df.set_index('ds', inplace=True)\n",
    "        status_dfs[status] = status_df\n",
    "    \n",
    "    # Fit model\n",
    "    models = {}\n",
    "    for status, status_df in status_dfs.items():\n",
    "        model = AutoReg(status_df['y'], lags=lags)\n",
    "        models[status] = model.fit()\n",
    "    \n",
    "    # Periods to predict (months)\n",
    "    n_periods = 12  \n",
    "    \n",
    "    # Make future predictions\n",
    "    future_periods = pd.date_range(start=agent_df['period'].max(), periods=n_periods, freq='M')\n",
    "    predictions = {}\n",
    "    conf_intervals = {}\n",
    "    for status, model in models.items():\n",
    "        forecast = model.predict(start=len(status_df), end=len(status_df) + n_periods - 1)\n",
    "        conf_int = model.get_prediction(start=len(status_df), end=len(status_df) + n_periods - 1).conf_int()\n",
    "        predictions[status] = forecast.values\n",
    "        conf_intervals[status] = conf_int\n",
    "    # Combine the predictions into a single DataFrame\n",
    "    predicted_df = pd.DataFrame(predictions, index=future_periods)\n",
    "    predicted_df = predicted_df.reset_index().melt(id_vars='index', var_name='status', value_name='count')\n",
    "    predicted_df.columns = ['period', 'status', 'count']\n",
    "\n",
    "    predicted_df['agent'] = agent\n",
    "    \n",
    "    # Concatenate the original and predicted DataFrames\n",
    "    combined_df = pd.concat([agent_df, predicted_df], ignore_index=True)\n",
    "    \n",
    "    # Define the color scheme for the statuses\n",
    "    status_colors = {'no_robots': 'gray', 'none': 'blue', 'some': 'orange', 'all': 'red'}\n",
    "    \n",
    "    chart = robots_util.plot_robots_time_map_altair(\n",
    "        combined_df, \n",
    "        agent_type=agent, \n",
    "        period_col='period', \n",
    "        status_col='status', \n",
    "        val_col='count', \n",
    "        title='Restriction Status over Time', \n",
    "        ordered_statuses=['no_robots', 'none', 'some', 'all'], \n",
    "        status_colors=status_colors,\n",
    "        datetime_swap=True,\n",
    "    )\n",
    "     # map the confidence intervals to the predicted df\n",
    "    for status, conf_int in conf_intervals.items():\n",
    "        # Ensure the length of the confidence intervals matches the number of future periods\n",
    "        predicted_df.loc[predicted_df['status'] == status, 'lower'] = conf_int['lower'].values\n",
    "        predicted_df.loc[predicted_df['status'] == status, 'upper'] = conf_int['upper'].values\n",
    "\n",
    "    return chart, predicted_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6568c4f",
   "metadata": {},
   "source": [
    "### Prophet "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51904ded",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forecast_and_plot_prophet(df, agent, lags):\n",
    "    # Pick agent\n",
    "    agent_df = df[df['agent'] == agent].copy()\n",
    "    \n",
    "    # Convert 'period' to timestamp if it's a Period object\n",
    "    agent_df.loc[:, 'period'] = agent_df['period'].apply(lambda x: x.to_timestamp() if isinstance(x, pd.Period) else x)\n",
    "    \n",
    "    # Reshape the data\n",
    "    pivoted_df = agent_df.pivot_table(index='period', columns='status', values='count')\n",
    "    \n",
    "    # Normalize the counts to percentages\n",
    "    pivoted_df = pivoted_df.div(pivoted_df.sum(axis=1), axis=0) * 100\n",
    "    \n",
    "    # Create separate DataFrames for each status\n",
    "    status_dfs = {}\n",
    "    for status in pivoted_df.columns:\n",
    "        status_df = pivoted_df[[status]].reset_index()\n",
    "        status_df.columns = ['ds', 'y']\n",
    "        status_df = create_lagged_features(status_df, lags)\n",
    "        status_dfs[status] = status_df\n",
    "    \n",
    "    # Train time series models for each status\n",
    "    models = {}\n",
    "    for status, status_df in status_dfs.items():\n",
    "        model = Prophet()\n",
    "        for lag in lags:\n",
    "            model.add_regressor(f'lag_{lag}')\n",
    "        model.fit(status_df)\n",
    "        models[status] = model\n",
    "    \n",
    "    # Define the number of future periods\n",
    "    n_periods = 12 \n",
    "    \n",
    "    # Make future predictions\n",
    "    future_periods = pd.date_range(start=agent_df['period'].max(), periods=n_periods, freq='M')\n",
    "    predictions = {}\n",
    "    for status, model in models.items():\n",
    "        future_df = pd.DataFrame({'ds': future_periods})\n",
    "        for lag in lags:\n",
    "            future_df[f'lag_{lag}'] = status_dfs[status][f'lag_{lag}'].iloc[-1]\n",
    "        forecast = model.predict(future_df)\n",
    "        predictions[status] = forecast['yhat'].values\n",
    "    \n",
    "    # Combine the predictions into a single DataFrame\n",
    "    predicted_df = pd.DataFrame(predictions, index=future_periods)\n",
    "    predicted_df = predicted_df.reset_index().melt(id_vars='index', var_name='status', value_name='predicted_value')\n",
    "    predicted_df.columns = ['period', 'status', 'predicted_value']\n",
    "    # add agent column\n",
    "    predicted_df['agent'] = agent\n",
    "    # add tokens column\n",
    "    predicted_df['tokens'] = predicted_df['predicted_value']\n",
    "    # Define the color scheme for the statuses\n",
    "    status_colors = {'no_robots': 'gray', 'none': 'blue', 'some': 'orange', 'all': 'red'}\n",
    "    \n",
    "    chart = robots_util.plot_robots_time_map_altair(\n",
    "        predicted_df, \n",
    "        agent_type=agent, \n",
    "        period_col='period', \n",
    "        status_col='status', \n",
    "        val_col='tokens',  # \"count\" / \"tokens\"\n",
    "        title='Restriction Status over Time', \n",
    "        ordered_statuses=['no_robots', 'none', 'some', 'all'], \n",
    "        status_colors=status_colors,\n",
    "        datetime_swap=True,\n",
    "    )\n",
    "    \n",
    "    return chart"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11e7cf06",
   "metadata": {},
   "source": [
    "### ARIMA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea4c656f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forecast_and_plot_arima(df, agent, lags):\n",
    "    # Pick agent\n",
    "    agent_df = df[df['agent'] == agent].copy()\n",
    "    \n",
    "    # Convert 'period' to timestamp if it's a Period object\n",
    "    agent_df.loc[:, 'period'] = agent_df['period'].apply(lambda x: x.to_timestamp() if isinstance(x, pd.Period) else x)\n",
    "    \n",
    "    # Reshape the data\n",
    "    pivoted_df = agent_df.pivot_table(index='period', columns='status', values='count')\n",
    "    \n",
    "    # Normalize the counts to percentages\n",
    "    pivoted_df = pivoted_df.div(pivoted_df.sum(axis=1), axis=0) * 100\n",
    "    \n",
    "    # Create separate DataFrames for each status\n",
    "    status_dfs = {}\n",
    "    for status in pivoted_df.columns:\n",
    "        status_df = pivoted_df[[status]].reset_index()\n",
    "        status_df.columns = ['ds', 'y']\n",
    "        status_df = create_lagged_features(status_df, lags)\n",
    "        status_dfs[status] = status_df\n",
    "    \n",
    "    # Train ARIMA models for each status\n",
    "    models = {}\n",
    "    for status, status_df in status_dfs.items():\n",
    "        model = ARIMA(status_df['y'], order=(max(lags), 0, 0))\n",
    "        models[status] = model.fit()\n",
    "    \n",
    "    # Define the number of future periods\n",
    "    n_periods = 12 \n",
    "    \n",
    "    # Make future predictions\n",
    "    future_periods = pd.date_range(start=agent_df['period'].max(), periods=n_periods, freq='M')\n",
    "    predictions = {}\n",
    "    for status, model in models.items():\n",
    "        forecast = model.forecast(steps=n_periods)\n",
    "        predictions[status] = forecast.values\n",
    "    \n",
    "    # Combine the predictions into a single DataFrame\n",
    "    predicted_df = pd.DataFrame(predictions, index=future_periods)\n",
    "    predicted_df = predicted_df.reset_index().melt(id_vars='index', var_name='status', value_name='predicted_value')\n",
    "    predicted_df.columns = ['period', 'status', 'predicted_value']\n",
    "    # add agent column\n",
    "    predicted_df['agent'] = agent\n",
    "    # add tokens column\n",
    "    predicted_df['tokens'] = predicted_df['predicted_value']\n",
    "    # Define the color scheme for the statuses\n",
    "    status_colors = {'no_robots': 'gray', 'none': 'blue', 'some': 'orange', 'all': 'red'}\n",
    "    \n",
    "    chart = robots_util.plot_robots_time_map_altair(\n",
    "        predicted_df, \n",
    "        agent_type=agent, \n",
    "        period_col='period', \n",
    "        status_col='status', \n",
    "        val_col='tokens', \n",
    "        title='Restriction Status over Time', \n",
    "        ordered_statuses=['no_robots', 'none', 'some', 'all'], \n",
    "        status_colors=status_colors,\n",
    "        datetime_swap=True,\n",
    "    )\n",
    "    \n",
    "    return chart\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf8d6e4c",
   "metadata": {},
   "source": [
    "### SARIMA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1791a90",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forecast_and_plot_sarima(df, agent, lags, seasonal_order):\n",
    "    # Pick agent\n",
    "    agent_df = df[df['agent'] == agent].copy()\n",
    "    \n",
    "    # Convert 'period' to timestamp if it's a Period object\n",
    "    agent_df.loc[:, 'period'] = agent_df['period'].apply(lambda x: x.to_timestamp() if isinstance(x, pd.Period) else x)\n",
    "    \n",
    "    # Reshape the data\n",
    "    pivoted_df = agent_df.pivot_table(index='period', columns='status', values='count')\n",
    "    \n",
    "    # Normalize the counts to percentages\n",
    "    pivoted_df = pivoted_df.div(pivoted_df.sum(axis=1), axis=0) * 100\n",
    "    \n",
    "    # Create separate DataFrames for each status\n",
    "    status_dfs = {}\n",
    "    for status in pivoted_df.columns:\n",
    "        status_df = pivoted_df[[status]].reset_index()\n",
    "        status_df.columns = ['ds', 'y']\n",
    "        status_df = create_lagged_features(status_df, lags)\n",
    "        status_dfs[status] = status_df\n",
    "    \n",
    "    # Train SARIMA models for each status\n",
    "    models = {}\n",
    "    for status, status_df in status_dfs.items():\n",
    "        model = SARIMAX(status_df['y'], order=(max(lags), 0, 0), seasonal_order=seasonal_order)\n",
    "        models[status] = model.fit(disp=False)\n",
    "    \n",
    "    # Define the number of future periods\n",
    "    n_periods = 12 \n",
    "    \n",
    "    # Make future predictions\n",
    "    future_periods = pd.date_range(start=agent_df['period'].max(), periods=n_periods, freq='M')\n",
    "    predictions = {}\n",
    "    for status, model in models.items():\n",
    "        forecast = model.get_forecast(steps=n_periods)\n",
    "        predictions[status] = forecast.predicted_mean.values\n",
    "    \n",
    "    # Combine the predictions into a single DataFrame\n",
    "    predicted_df = pd.DataFrame(predictions, index=future_periods)\n",
    "    predicted_df = predicted_df.reset_index().melt(id_vars='index', var_name='status', value_name='predicted_value')\n",
    "    predicted_df.columns = ['period', 'status', 'predicted_value']\n",
    "    # add agent column\n",
    "    predicted_df['agent'] = agent\n",
    "    # add tokens column\n",
    "    predicted_df['tokens'] = predicted_df['predicted_value']\n",
    "    # Define the color scheme for the statuses\n",
    "    status_colors = {'no_robots': 'gray', 'none': 'blue', 'some': 'orange', 'all': 'red'}\n",
    "    \n",
    "    chart = robots_util.plot_robots_time_map_altair(\n",
    "        predicted_df, \n",
    "        agent_type=agent, \n",
    "        period_col='period', \n",
    "        status_col='status', \n",
    "        val_col='tokens', \n",
    "        title='Restriction Status over Time', \n",
    "        ordered_statuses=['no_robots', 'none', 'some', 'all'], \n",
    "        status_colors=status_colors,\n",
    "        datetime_swap=True,\n",
    "    )\n",
    "    \n",
    "    return chart\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27870094",
   "metadata": {},
   "source": [
    "### Run Forecasts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "951e9ef5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_robots(df, analysis_type, lags, seasonal_order=None, display=False):\n",
    "    \"\"\"\n",
    "    Analyzes robot data for different agents using specified forecasting methods.\n",
    "\n",
    "    Parameters:\n",
    "    - analysis_type (str): Type of analysis to perform. Options are 'autoregression', 'prophet', 'arima', 'sarima'.\n",
    "    - lags (list): List of lag values to be used in the forecasting models.\n",
    "    - seasonal_order (tuple, optional): Seasonal order parameters for SARIMA model. Default is None.\n",
    "    - display (bool, optional): If True, displays the predicted DataFrame. Default is False.\n",
    "\n",
    "    Returns:\n",
    "    - None: Displays the chart for each agent.\n",
    "    \"\"\"\n",
    "    agents = df['agent'].unique()\n",
    "    \n",
    "    for agent in agents:\n",
    "        print(f\"CHOSEN_CORPUS: {CHOSEN_CORPUS}\")\n",
    "        print(f\"AGENT: {agent}\")\n",
    "        \n",
    "        if analysis_type == 'autoregression':\n",
    "            chart, predicted_df = forecast_and_plot(df, agent, lags)\n",
    "        elif analysis_type == 'prophet':\n",
    "            chart = forecast_and_plot_prophet(df, agent, lags)\n",
    "        elif analysis_type == 'arima':\n",
    "            chart = forecast_and_plot_arima(df, agent, lags)\n",
    "        elif analysis_type == 'sarima':\n",
    "            chart = forecast_and_plot_sarima(df, agent, lags, seasonal_order)\n",
    "        else:\n",
    "            raise ValueError(\"Invalid analysis type specified.\")\n",
    "        \n",
    "        if display & (analysis_type == 'autoregression'):\n",
    "            display(predicted_df)\n",
    "        chart.show()\n",
    "\n",
    "# Example usage:\n",
    "df_to_analyze = robots_temporal_head_summary  # or robots_temporal_rand_summary\n",
    "analyze_robots(df_to_analyze, analysis_type='arima', lags=[1, 3, 6, 9])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9e1d2cb-22af-4b5e-854b-ed7fb58c3f36",
   "metadata": {},
   "source": [
    "### Plot Temporal Robots Area Chart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20bc2244-2f4f-49ff-ac0d-7fdd177bd796",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for agent in agents_to_track:\n",
    "#     robots_util.plot_robots_time_map(robots_temporal_summary, agent)\n",
    "\n",
    "for group in agent_groups_to_track:\n",
    "    print(group)\n",
    "    chart = robots_util.plot_robots_time_map_altair(\n",
    "        robots_temporal_head_summary, \n",
    "        agent_type=group, \n",
    "        period_col='period', \n",
    "        status_col='status', \n",
    "        val_col='tokens',  # \"count\" / \"tokens\"\n",
    "        title='Restriction Status over Time', \n",
    "        ordered_statuses=['no_robots', 'none', 'some', 'all'], \n",
    "        status_colors={'no_robots': 'gray', 'none': 'blue', 'some': 'orange', 'all': 'red'}\n",
    "    )\n",
    "    chart.show()\n",
    "#     robots_util.plot_robots_time_map(robots_temporal_head_summary, group, val_key=\"count\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "341c6e1b-b7d7-4e25-abf7-45ad343de81a",
   "metadata": {},
   "source": [
    "### Plot Temporal ToS Area Chart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e61c5fa-a05d-46a3-87c9-850da9ed7cc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "chart = robots_util.plot_temporal_area_map_altair(\n",
    "    tos_summary_df,\n",
    "    period_col='period', \n",
    "    status_col='status', \n",
    "    val_col='tokens',  # \"count\" / \"tokens\"\n",
    "    title='Restriction Status over Time', \n",
    "    ordered_statuses=['No Terms Pages', 'No Restrictions', 'Conditional Restrictions', 'Prohibits AI', 'Prohibits Scraping', 'Prohibits Scraping & AI'], \n",
    "    status_colors= {\n",
    "        'No Terms Pages': 'gray', 'No Restrictions': 'blue', \n",
    "        'Conditional Restrictions': 'yellow', 'Prohibits AI': 'orange', \n",
    "        'Prohibits Scraping': 'red', 'Prohibits Scraping & AI': 'red'\n",
    "    },\n",
    ")\n",
    "chart.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9234e3ba-e2e5-492f-b0f4-265c4efdcdb2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4bdfd16-c68d-4f0a-ad41-243834ca8d0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# robots_temporal_head_summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deefc077-ecb9-4bc9-81c7-52e82c85dd26",
   "metadata": {},
   "source": [
    "Restrictions by Company"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ddd078b-0c8c-4e41-90c3-36eed0c674b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Dataframe w/ [Period, Agent, Status, count(URLs), count(tokens)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a42fe8b-15e0-48ba-a751-ee4d1c22d44f",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_color_mapping = {\n",
    "    \"Google\": \"#1f77b4\",          # blue\n",
    "    \"OpenAI\": \"#ff7f0e\",          # orange\n",
    "    \"Anthropic\": \"#2ca02c\",       # green\n",
    "    \"Cohere\": \"#d62728\",          # red\n",
    "    \"Common Crawl\": \"#9467bd\",    # purple\n",
    "    \"Meta\": \"#8c564b\",            # brown\n",
    "    \"Internet Archive\": \"#e377c2\",# pink\n",
    "    \"Google Search\": \"#7f7f7f\",   # gray\n",
    "    \"False Anthropic\": \"#bcbd22\"  # yellow\n",
    "}\n",
    "\n",
    "visualization_util.plot_company_comparisons_altair(\n",
    "    robots_temporal_head_summary, color_mapping=agent_color_mapping)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbbbfb6f-de79-4007-ad4f-1370cb54ee6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# log scale\n",
    "# x-axis fix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e29a9266-3668-4cfe-882a-e4a6dd77c57f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0285bc1f-4626-4ee3-a119-1dfacf58eb98",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "580ad72c-c29e-493a-add3-3bf000b5e423",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7311473-89f4-46f0-969e-eb3a80933843",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8cd36f9-0110-48f9-ac78-087076d9de87",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "65002141-7f02-45f7-9edb-4d7395366e67",
   "metadata": {},
   "source": [
    "### Create Correlations Tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cf458b4-89c7-41dd-838e-c4413f0062ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "url_correlation_df = analysis_util.analyze_url_variable_correlations(url_results_df, [100, 500, 2000], \"dolma\")\n",
    "\n",
    "# Convert the dataframe to a LaTeX table\n",
    "latex_table = url_correlation_df.to_latex(index=True, escape=True, float_format=\"{:.1f}\".format)\n",
    "print(latex_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de77b84a-3b07-4768-917b-883b559f6b6f",
   "metadata": {},
   "source": [
    "### Robots & ToS Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9fa9195-b1e7-491b-b3b6-3e0cc156058b",
   "metadata": {},
   "outputs": [],
   "source": [
    "robots_util.prepare_tos_robots_confusion_matrix(\n",
    "    tos_policies,\n",
    "    url_robots_summary,\n",
    "    COMPANIES_TO_ANALYZE,\n",
    "    url_token_lookup,\n",
    "    corpora_choice=\"dolma\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4f3a452-1450-4da7-af4d-08631ebd03ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# companies = [\"Google\", \"OpenAI\", \"Anthropic\", \"Cohere\", \"Common Crawl\", \"Meta\"]\n",
    "# for company in companies:\n",
    "#     # {URL --> Date --> Agent --> Status}\n",
    "#     url_robots_summary[url][date][company]\n",
    "#     # get latest\n",
    "#     # df: [company1, company2, status1, status2]\n",
    "#     # plot confusion matrix.\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "033d5aea-be98-4838-b28d-0b2b57864b3a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40451dc5-6f9b-4e7c-bebe-19d9b305be7b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "00f8bdfc-23e0-4eb2-9b11-84ff8c4b927a",
   "metadata": {},
   "source": [
    "### Plot Num Tokens against Robots Restrictions per Company"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55a94479-ef4a-44ce-9f0b-b33929436965",
   "metadata": {},
   "outputs": [],
   "source": [
    "size_bucket_to_urls = robots_util.bucket_urls_by_size(\n",
    "    c4_url_to_counts, \n",
    "    bucket_boundaries=[0, 1000, 10000, 50000, 1000000, 10000000, 50000000, 9999999999999999]\n",
    ")\n",
    "robots_util.plot_size_against_restrictions(\n",
    "    url_robots_summary,\n",
    "    size_bucket_to_urls,\n",
    "    \"OpenAI\",\n",
    "    setting=\"train\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac8b36da-3bd6-44e6-8927-29c5f7fec032",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb7a1f44-2f69-45ae-8bf1-a569251efa79",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "81e66e2f-e5f0-4c13-a41a-ae66d2f2fc20",
   "metadata": {},
   "source": [
    "# Scratch / Notes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85134ae0-036b-41c3-b8e5-9f65b71c0624",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "add08973-a0a0-44a0-90dd-07808ee16cee",
   "metadata": {},
   "outputs": [],
   "source": [
    "Notes:\n",
    "\n",
    "* Take All Agents and subtract it from the other plots to see the diffs between agents.\n",
    "* How do these charts vary with different quantiles for number of tokens. (Behavior diffs for token rich and token poor)\n",
    "\n",
    "* Incompatability between Robots.txt and ToS? Robots.txt is an encoded ToS for scrapers. Is there more intention detailed in the ToS than the robots?\n",
    "* Robots and ToS update rate. --> How often that the other is updated within T time (robots and ToS).\n",
    "* Analysis: Of all websites that restrict at least one AI bot, what other bots do they restrict? E.g. if you restrict cohere, you probaby also restrict OpenAI\n",
    "P(Cohere restricted | OpenAI restricted)\n",
    "* Include Midjourney, CCBot, IAbot, etc\n",
    "* Vertical lines that show when bots get introduced.\n",
    "\n",
    "* Restrictions are rising across the board.\n",
    "* Company-wise restrictions\n",
    "* Compare for each company their scraping restrictions vs RAG restrictions vs AI bot restrictions (OpenAI, Anthropic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff15b8d4-6ec2-4a02-8a98-f7120e7aa3a9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0c7c342-c06a-4da0-826a-807845598cce",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b94143e-3aa5-45e0-af3e-7216117067e2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fc42112-fe35-4d6c-b2d2-8e0fc08a5eab",
   "metadata": {},
   "outputs": [],
   "source": [
    "Data Sources Analysis:\n",
    "\n",
    "0. Table: Methodology -- what metadata we collected/annotated (and how). [WIP]\n",
    "1. Figure: Temporal changes in Robots / ToS (somehow over a collection or multiple companies?) [Waiting on ToS]\n",
    "    C4/RF/(Dolma) vs (2k-Head)/Random vs Company(6)\n",
    "2. Figure: Robots / ToS contradiction matrix [Waiting on ToS]\n",
    "    Creative Commons.\n",
    "3. Table: Robots permission differences by company + Stella(pairwise differences by confusion matrix) [WIP]\n",
    "4. Table: Robots / ToS / other indicators variability by website num tokens (i.e. head vs tail) [WIP]\n",
    "5. Figure: Commercial/market copyright concerns and comparison to WildChat + EU AI Act [WIP, waiting on WildChat]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c963221b-e1f2-48a7-b8d2-30e5d87a3926",
   "metadata": {},
   "outputs": [],
   "source": [
    "Datasets Analysis:\n",
    "\n",
    "(License: NC, C, Unspecified) x (Terms: Unspecified/None, NC, C).\n",
    "Text =~ 200\n",
    "Video = 11\n",
    "Speech ~= 50\n",
    "\n",
    "0. Tables for Text, Video, Speech [Will: WIP]\n",
    "1. License & source (terms) restriction differences between Text, Video, Speech. (normalized stacked bar chart) [WIP]\n",
    "2. Source domains by Text, Video, Speech: scraped, synthetic, crowdsourced, ....  (normalized stacked bar chart) [WIP]\n",
    "2b. Include pretraining (broken down by modality + user content + illicit content)\n",
    "3. Creator distribution for Text, Video, Speech by geography and organization type [WIP]\n",
    "\n",
    "Extension: \n",
    "\n",
    "\n",
    "4. Temporal breakdown of license restriction categories by Text, Video, Speech [WIP]\n",
    "5. License Type by modality for Text, Video, Speech [WIP]\n",
    "6. Languages / Tasks.\n",
    "(Will run statistical tests checking if diffs between Text,Video,Speech are significant on all of these. [WIP]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38d020db-98ae-45a8-9fdd-c344b13d7d31",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# People who restrict Anthropic but not OpenAI. <-- public awareness of organizations is the driving force here. \n",
    "# Ordered by notoriety. (Confusion matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2172ae3a-8d2b-474b-9f02-c662511792c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "nayan = pd.read_csv(\"test_data/wildchat_annotations.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be02a05f-cec5-4e6e-8778-7275d1b7e1fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# nayan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78dfdc6f-be9b-4009-89cc-dc0db5b85791",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_nayan(df, i):\n",
    "    row = df.iloc[i]\n",
    "    prompt = row[\"WildChat Example Prompt\"]\n",
    "    response = row[\"WildChat Example Response\"]\n",
    "    typs = row[\"Types of Service\"]\n",
    "    cd = row[\"Content Domain\"]\n",
    "    print(prompt)\n",
    "    # print(\"*************\" + response[:100])\n",
    "    print(cd)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b71a3a69-d7f8-4d45-b3b4-c22b834d338c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in range(30, 60):\n",
    "#     sample_nayan(nayan, i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79e40b72-92c7-41c6-ac55-8a0b0ac3bda8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a179f1e-ff84-49b1-8aad-81ff92b39896",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

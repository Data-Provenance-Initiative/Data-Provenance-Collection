{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1d1016c1-0c59-4bfb-a7a4-14a8914e99ab",
   "metadata": {},
   "source": [
    "# Market Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6e620a7-0d5d-4762-8ced-38b9fde5c293",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install altair seaborn scipy scikit-learn plotly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c53ccd02-5651-40de-bb16-e22108c631c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import copy\n",
    "import ast\n",
    "from collections import Counter\n",
    "\n",
    "# Visualization packages\n",
    "import altair as alt\n",
    "\n",
    "# Append system path\n",
    "sys.path = [p for p in sys.path if not p.endswith('../..')]  # Cleans duplicated '../..'\n",
    "sys.path.insert(0, '../')  # This adds `src` to the path\n",
    "\n",
    "from helpers import io\n",
    "from analysis import analysis_util\n",
    "from web_analysis import robots_util\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "641190df-d54c-47c0-a0bf-09107119f192",
   "metadata": {},
   "outputs": [],
   "source": [
    "EXCEL_FNAME_ROBOTS = \"data/forecasted_robots_data.xlsx\"\n",
    "EXCEL_FNAME_TOS = \"data/forecasted_tos_data.xlsx\"\n",
    "EXCEL_FNAME_FINAL = \"data/forecasted_final_data.xlsx\"\n",
    "FPATH_TO_RELEVANT_URL_TOKENS = 'data/pretrain_data/relevant_url_token_counts.csv'\n",
    "FPATH_to_HEAD_ROBOTS = \"data/robots/temporal_robots_head.json\"\n",
    "FPATH_TO_RAND_ROBOTS = \"data/robots/temporal_robots_rand_10k.json\"\n",
    "FPATH_TO_TOS_DATA = \"data/GPT_analysis_results/tos_ai_scraping_policies.json\"\n",
    "FPATH_TO_TOS_LICENSE_DATA = \"data/GPT_analysis_results/tos_license_policies.json\"\n",
    "FPATH_TO_TOS_COMPETE_DATA = \"data/GPT_analysis_results/tos_competing_services_policies.json\"\n",
    "FPATH_TO_C4_TOKEN_ESTIMATES = \"data/raw_annotations/c4_total_token_estimates.csv\" \n",
    "FPATH_TO_DOLMA_TOKEN_ESTIMATES = \"data/raw_annotations/dolma_total_token_estimates.csv\"\n",
    "FPATH_TO_RF_TOKEN_ESTIMATES = \"data/raw_annotations/rf_total_token_estimates.csv\" \n",
    "DIRPATHS_TO_ANNOTATED_TASKS = [\"data/raw_annotations/task_1\", \"data/raw_annotations/task_2\"]\n",
    "START_DATES = \"data/raw_annotations/domain_start_dates.json\"\n",
    "FPATH_WILDCHAT_RESULTS = \"data/raw_annotations/wildchat_analysis_results.csv\"\n",
    "\n",
    "ALL_COMPANIES_TO_TRACK = [\"Google\", \"OpenAI\", \"Anthropic\", \"Cohere\", \"Common Crawl\", \"Meta\", \"Internet Archive\", \"Google Search\", \"False Anthropic\"]\n",
    "COMPANIES_TO_ANALYZE = [\"Google\", \"OpenAI\", \"Anthropic\", \"Cohere\", \"Common Crawl\", \"Meta\"]\n",
    "TEMPORAL_ANALYSIS_START_DATE = '2016-01-01'\n",
    "TEMPORAL_ANALYSIS_END_DATE = '2024-04-30'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c838df1f-315b-47c2-b834-15b4932691b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_groups_to_track = robots_util.get_bot_groups(COMPANIES_TO_ANALYZE)\n",
    "agents_to_track = robots_util.get_bots()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2478fc6-70af-4b04-b410-46448293724e",
   "metadata": {},
   "outputs": [],
   "source": [
    "url_token_lookup = robots_util.URLTokenLookup(FPATH_TO_RELEVANT_URL_TOKENS) # 'c4', 'rf', 'dolma'\n",
    "c4_url_to_counts = url_token_lookup.get_url_to_token_map(\"c4\")\n",
    "rf_url_to_counts = url_token_lookup.get_url_to_token_map(\"rf\")\n",
    "dolma_url_to_counts = url_token_lookup.get_url_to_token_map(\"dolma\")\n",
    "top_c4_urls = url_token_lookup.top_k_urls(\"c4\", 2000)\n",
    "top_rf_urls = url_token_lookup.top_k_urls(\"rf\", 2000)\n",
    "top_dolma_urls = url_token_lookup.top_k_urls(\"dolma\", 2000)\n",
    "random_10k_urls = url_token_lookup.get_10k_random_sample()\n",
    "all_urls = set(random_10k_urls + top_c4_urls + top_rf_urls + top_dolma_urls)\n",
    "\n",
    "# Load website snapshots for relevant URLs\n",
    "# website_snapshots = robots_util.read_snapshots(FPATH_SNAPSHOT_DATA, all_urls)\n",
    "website_start_dates = robots_util.read_start_dates(START_DATES, all_urls) # THIS WON'T WORK FOR THE 10k SAMPLE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14b5d6d8-d3e5-461e-8349-944915dc31b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# URL -> Date -> Robots.txt raw text\n",
    "head_robots = io.read_json(FPATH_to_HEAD_ROBOTS)\n",
    "random_10k_robots = io.read_json(FPATH_TO_RAND_ROBOTS)\n",
    "joined_robots = copy.deepcopy(head_robots)\n",
    "joined_robots.update(random_10k_robots)\n",
    "robots_util.print_out_robots_info(head_robots)\n",
    "robots_util.print_out_robots_info(random_10k_robots)\n",
    "\n",
    "# {URL --> Date --> Agent --> Status}\n",
    "url_robots_summary, agent_counter_df = robots_util.compute_url_date_agent_status(\n",
    "    data=joined_robots, \n",
    "    # relevant_agents=agents_to_track)\n",
    "    relevant_agents=[v for vs in agent_groups_to_track.values() for v in vs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80f765bd-e39b-4c3f-bc07-b62b9868a242",
   "metadata": {},
   "outputs": [],
   "source": [
    "# URL --> Date --> ToS-suburl --> {\"verdict\": X, \"evidence\": Y}\n",
    "# tos_policies = io.read_json(FPATH_TO_TOS_DATA)\n",
    "# print(f\"Num ToS URLs: {len(tos_policies)}\")\n",
    "\n",
    "# URL --> Date --> ToS-suburl --> {\"verdict\": X, \"evidence\": Y}\n",
    "tos_policies = io.read_json(FPATH_TO_TOS_DATA)\n",
    "tos_license_policies = io.read_json(FPATH_TO_TOS_LICENSE_DATA)\n",
    "tos_compete_policies = io.read_json(FPATH_TO_TOS_COMPETE_DATA)\n",
    "# tos_license_policies = robots_util.switch_dates_yearly_to_monthly(tos_license_policies)\n",
    "print(f\"Num ToS AI/Scraping URLs: {len(tos_policies)}\")\n",
    "print(f\"Num ToS License URLs: {len(tos_license_policies)}\")\n",
    "print(f\"Num ToS Compete URLs: {len(tos_compete_policies)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a1ae3f0-3342-40d4-8fd3-6742ab9c5a8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "url_to_info, unannotated_urls = analysis_util.extract_url_annotations(DIRPATHS_TO_ANNOTATED_TASKS)\n",
    "url_results_df = analysis_util.process_url_annotations(url_to_info)\n",
    "url_results_df = analysis_util.encode_size_columns(url_results_df, url_token_lookup)\n",
    "url_results_df = robots_util.encode_latest_tos_robots_into_df(\n",
    "    url_results_df, tos_policies, tos_license_policies, tos_compete_policies,\n",
    "    url_robots_summary, COMPANIES_TO_ANALYZE\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e99e7d42-d8dd-4f6a-b5e7-53d6abc31e11",
   "metadata": {},
   "outputs": [],
   "source": [
    "ALL_VARS = [\n",
    "    'User Content', 'Paywall', 'Ads','Modality: Image', 'Modality: Video', 'Modality: Audio',\n",
    "    'Sensitive Content', 'services_Academic', 'services_Blogs',\n",
    "    'services_E-Commerce', 'services_Encyclopedia/Database',\n",
    "    'services_Government', 'services_News/Periodicals',\n",
    "    'services_Organization/Personal Website', 'services_Other',\n",
    "    'services_Social Media/Forums', 'Restrictive Robots.txt', 'Restrictive Terms'\n",
    "]\n",
    "\n",
    "c4_estimates = analysis_util.run_population_analysis(\n",
    "    url_results_df, \n",
    "    url_token_lookup, \n",
    "    \"c4\", \n",
    "    ALL_VARS,\n",
    "    top_c4_urls,\n",
    "    verbose=False\n",
    ")\n",
    "rf_estimates = analysis_util.run_population_analysis(\n",
    "    url_results_df, \n",
    "    url_token_lookup, \n",
    "    \"rf\", \n",
    "    ALL_VARS,\n",
    "    top_rf_urls,\n",
    "    verbose=False\n",
    ")\n",
    "dolma_estimates = analysis_util.run_population_analysis(\n",
    "    url_results_df, \n",
    "    url_token_lookup, \n",
    "    \"dolma\", \n",
    "    ALL_VARS,\n",
    "    top_dolma_urls,\n",
    "    verbose=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bb684d2-344d-4af7-a2da-4fbc743dfbd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = analysis_util.analyze_url_variable_correlations(\n",
    "    url_results_df,\n",
    "    ALL_VARS,\n",
    "    [100, 500, 1000],\n",
    ")\n",
    "\n",
    "# Convert the dataframe to a LaTeX table\n",
    "latex_table = results_df.to_latex(index=True, escape=True, float_format=\"{:.1f}\".format)\n",
    "print(latex_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d91d5c2f-998b-4eed-8fc9-8da5ce879717",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_altair_stacked_services_paywall(\n",
    "    merged_df, \n",
    "    x_axis, \n",
    "    y_axis,\n",
    "    stack_axis,\n",
    "    stack_order,\n",
    "    stack_colors,\n",
    "    color=\"#1f77b4\",\n",
    "    legend_orient=\"top\",\n",
    "    axis_fontsize=10,\n",
    "    labels_fontsize=10,\n",
    "    font=\"arial\",\n",
    "    title=\"\",\n",
    "    title_x=None,\n",
    "    title_y=None,\n",
    "    labels_map=None,\n",
    "    percentages=False,\n",
    "    width=400, \n",
    "    height=400,\n",
    "):\n",
    "    \"\"\"\n",
    "    Plots a horizontal stacked bar chart with service categories on the y-axis\n",
    "    and the specified x-value (instance rate or total tokens) on the x-axis.\n",
    "    The bars are stacked based on the paywall and ads values.\n",
    "\n",
    "    Args:\n",
    "        df (pandas.DataFrame): The input dataframe with columns 'url', 'tokens',\n",
    "            'services', 'paywall', and 'ads'.\n",
    "        x_value (str, optional): The value to use for the x-axis. Can be 'instance_rate'\n",
    "            or 'total_tokens'. Defaults to 'instance_rate'.\n",
    "        width (int, optional): The width of the chart. Defaults to 800.\n",
    "        height (int, optional): The height of the chart. Defaults to 400.\n",
    "\n",
    "    Returns:\n",
    "        altair.Chart: The stacked bar chart.\n",
    "    \"\"\"\n",
    "    # Create a categorical data type with the desired ordering\n",
    "    merged_df[stack_axis] = pd.Categorical(\n",
    "        merged_df[stack_axis],\n",
    "        categories=stack_order,\n",
    "        ordered=True\n",
    "    )\n",
    "    merged_df = merged_df.sort_values(by=stack_axis)\n",
    "    \n",
    "    # Create the chart\n",
    "    chart = alt.Chart(merged_df).mark_bar(color=color).encode(\n",
    "        x=alt.X(\n",
    "            f\"{x_axis}:Q\",\n",
    "            title=x_axis.replace('_', ' ').title() if title_x is None else title_x,\n",
    "            axis=alt.Axis(\n",
    "                format=\".0%\" if percentages else \"d\",\n",
    "                titleFontSize=axis_fontsize,\n",
    "                titleFont=font,\n",
    "                labelFontSize=labels_fontsize,\n",
    "                labelFont=font,\n",
    "            )\n",
    "        ),\n",
    "        y=alt.Y(\n",
    "            f'{y_axis}:N', \n",
    "            sort='-x',\n",
    "            # sort=services_ordered,\n",
    "            title=title_y,\n",
    "            axis=alt.Axis(\n",
    "                titleFontSize=axis_fontsize,\n",
    "                titleFont=font,\n",
    "                labelFontSize=labels_fontsize,\n",
    "                labelFont=font,\n",
    "                labelExpr=\"datum.value\" if (labels_map is None) else f\"{labels_map}[datum.value]\"\n",
    "            )\n",
    "        ),\n",
    "        order=\"order:Q\",\n",
    "    ).properties(\n",
    "        width=width,\n",
    "        height=height,\n",
    "        title=title\n",
    "    )\n",
    "\n",
    "    if stack_colors:\n",
    "        chart = chart.encode(\n",
    "            color=alt.Color(f\"{stack_axis}:N\",\n",
    "                            title=None,\n",
    "                            sort=stack_order,\n",
    "                            scale=alt.Scale(\n",
    "                                domain=stack_order,\n",
    "                                range=stack_colors,\n",
    "                            ),\n",
    "                            legend=alt.Legend(\n",
    "                                orient=legend_orient,\n",
    "                                title=None,\n",
    "                                labelFontSize=12,\n",
    "                                labelFont=font,\n",
    "                                labelPadding=5,\n",
    "                                labelColor='black',\n",
    "                                labelFontWeight='bold'\n",
    "                            )\n",
    "                           )\n",
    "        )\n",
    "\n",
    "    return chart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78301cb9-ea5c-4e9c-a18e-b5bb81fdea10",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_altair_stacked_services_paywall(\n",
    "    df,\n",
    "    x_value='instance_count', \n",
    "    token_key='c4 tokens',\n",
    "    paywall_ads_colors=['#8dd3c7', '#fb8072', '#ffffb3', '#bebada'],\n",
    "    legend_orient=\"bottom-right\",\n",
    "    axis_fontsize=10,\n",
    "    labels_fontsize=10,\n",
    "    font=\"arial\",\n",
    "    title=\"\",\n",
    "    title_x=\"Total Tokens\",\n",
    "    title_y=\"Service Categories\",\n",
    "    labels_map=None,\n",
    "    percentages=True,\n",
    "    remove=[],\n",
    "    width=800, \n",
    "    height=400,\n",
    "):\n",
    "    # Create a new column for the paywall/ads combination with custom labels\n",
    "    df['paywall_ads'] = df.apply(lambda row: 'No Ads or Paywall' if not row['Paywall'] and not row['Ads'] else\n",
    "                                             'Ads' if not row['Paywall'] and row['Ads'] else\n",
    "                                             'Paywall' if row['Paywall'] and not row['Ads'] else\n",
    "                                             'Both Paywall & Ads', axis=1)\n",
    "\n",
    "    # Explode the 'services' column to create a row for each service\n",
    "    df = df.explode('Services')\n",
    "\n",
    "    # Calculate the instance rate and total tokens\n",
    "    instance_counts = df.groupby(['Services', 'paywall_ads']).size().reset_index(name='instance_count')\n",
    "    total_tokens = df.groupby(['Services', 'paywall_ads'])[token_key].sum().reset_index(name='total_tokens')\n",
    "\n",
    "    # Merge the instance counts and total tokens\n",
    "    merged_df = instance_counts.merge(total_tokens, on=['Services', 'paywall_ads'])\n",
    "\n",
    "    merged_df[\"pct_tokens\"] = merged_df[\"total_tokens\"] / merged_df.total_tokens.sum()\n",
    "\n",
    "    merged_df['percent'] = merged_df.groupby(['Services', 'paywall_ads'])[x_value].transform('sum') / merged_df.groupby('Services')[x_value].transform('sum') * 100\n",
    "    services_ordered = merged_df.groupby('Services')[x_value].sum().sort_values(ascending=False).index.tolist()\n",
    "\n",
    "    paywall_ads_order = ['No Ads or Paywall', 'Ads', 'Paywall', 'Both Paywall & Ads']\n",
    "\n",
    "    # Remove given columns\n",
    "    merged_df.drop(merged_df[merged_df[\"Services\"].isin(remove)].index, inplace=True)\n",
    "    return plot_altair_stacked_services_paywall(\n",
    "        merged_df,\n",
    "        x_axis=\"pct_tokens\", # Otherwise x_value\n",
    "        y_axis=\"Services\",\n",
    "        stack_axis=\"paywall_ads\",\n",
    "        stack_order=paywall_ads_order,\n",
    "        stack_colors=paywall_ads_colors,\n",
    "        legend_orient=legend_orient,\n",
    "        axis_fontsize=axis_fontsize,\n",
    "        labels_fontsize=labels_fontsize,\n",
    "        font=font,\n",
    "        title=title,\n",
    "        title_x=title_x,\n",
    "        title_y=title_y,\n",
    "        labels_map=labels_map,\n",
    "        percentages=percentages,\n",
    "        width=width,\n",
    "        height=height\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d024f76-9823-4d2b-bfd5-a7e3fd8fbd7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_altair_wildchat(\n",
    "    fpath,\n",
    "    color=\"#1f77b4\",\n",
    "    axis_fontsize=10,\n",
    "    labels_fontsize=10,\n",
    "    font=\"arial\",\n",
    "    title=\"\",\n",
    "    title_x=\"Count\",\n",
    "    title_y=\"Service Categories\",\n",
    "    labels_map=None,\n",
    "    percentages=False,\n",
    "    remove=[],\n",
    "    width=400, \n",
    "    height=400,\n",
    "):\n",
    "    df = pd.read_csv(fpath)\n",
    "    category_map = {\n",
    "        'Coding Composition': [\n",
    "            'Coding composition (fixing, debugging, or help)',\n",
    "            'Coding composition',\n",
    "        ],\n",
    "        'Explanation and Reasoning': [\n",
    "            'Asking for an explanation, reasoning, or help solving a puzzle, or math problem',\n",
    "            'Asking for an explanation, reasoning, or help solving a puzzle or math problem',\n",
    "        ],\n",
    "        'Creative Composition': [\n",
    "            'Creative composition',\n",
    "            'Creative composition (such as role-playing, fictional story writing or continuation)'\n",
    "        ],\n",
    "        'Academic Composition': [\n",
    "            'Academic composition (such as non-fiction essay writing, continuation, or fixing)',\n",
    "            'Academic composition'\n",
    "        ],\n",
    "        'General Information': ['General informational requests'],\n",
    "        'Sexual/Illegal Content': ['Sexual or illegal content requests'],\n",
    "        'Translation': ['Translation'],\n",
    "        'Brainstorming and Planning': ['Brainstorming, planning, or ideation'],\n",
    "        'Self-help & self-harm': [\n",
    "            'Self-help, advice seeking, or self-harm',\n",
    "            'Self-help, advice seeking'\n",
    "        ],\n",
    "        'Organization Information': [\n",
    "            'Information requests specifically about organizations, companies, or persons'\n",
    "        ],\n",
    "        'E-commerce Information': [\n",
    "            'E-commerce or information requests about products and purchasing'\n",
    "        ],\n",
    "        'News': [\n",
    "            'News or recent events informational requests',\n",
    "        ],\n",
    "        'Other': ['Other', np.nan],\n",
    "    }\n",
    "    RELEVANT_CATEGORIES = {\n",
    "        \"News\": \"Relevance to Websites\",\n",
    "        \"E-commerce Information\": \"Relevance to Websites\",\n",
    "        \"Organization Information\": \"Relevance to Websites\",\n",
    "        \"Academic Composition\": \"Relevance to Websites\",\n",
    "        \"Sexual/Illegal Content\": \"NSFW Content\",\n",
    "    }\n",
    "    \n",
    "    inverse_mapper = {v: k for k, vs in category_map.items() for v in vs}\n",
    "\n",
    "    def safe_literal_eval(val):\n",
    "        if \"[\" not in val:\n",
    "            return [val]\n",
    "        else:\n",
    "            try:\n",
    "                return ast.literal_eval(val)\n",
    "            except (ValueError, SyntaxError):\n",
    "                print(val)\n",
    "                return None  # or some other default value or handling mechanism\n",
    "    \n",
    "    df['Types of Service'] = df['Types of Service'].apply(safe_literal_eval)\n",
    "\n",
    "    # df['Types of Service'] = df['Types of Service'].apply(ast.literal_eval)\n",
    "    df = df.explode('Types of Service')\n",
    "\n",
    "    cat_df = []\n",
    "    for category, count in Counter(df['Types of Service'].tolist()).most_common():\n",
    "        if isinstance(category, float) and np.isnan(category):\n",
    "            category = np.nan\n",
    "        cat_df.append({\n",
    "            \"category\": inverse_mapper[category],\n",
    "            \"count\": count,\n",
    "            \"stack\": RELEVANT_CATEGORIES.get(inverse_mapper[category], \"Less Relevant to Websites\"),\n",
    "        })\n",
    "    cat_df = pd.DataFrame(cat_df)\n",
    "\n",
    "    cat_df[\"pct_count\"] = cat_df[\"count\"] / cat_df[\"count\"].sum()\n",
    "    \n",
    "    # Remove given columns\n",
    "    cat_df.drop(cat_df[cat_df[\"category\"].isin(remove)].index, inplace=True)\n",
    "    return plot_altair_stacked_services_paywall(\n",
    "        cat_df,\n",
    "        x_axis=\"pct_count\", \n",
    "        y_axis=\"category\", \n",
    "        stack_axis=\"stack\", \n",
    "        stack_order=[\"Relevance to Websites\", \"Less Relevant to Websites\", \"NSFW Content\"], \n",
    "        stack_colors=[],\n",
    "        color=color,\n",
    "        axis_fontsize=axis_fontsize,\n",
    "        labels_fontsize=labels_fontsize,\n",
    "        font=font,\n",
    "        title=title,\n",
    "        title_x=title_x,\n",
    "        title_y=title_y,\n",
    "        labels_map=labels_map,\n",
    "        percentages=percentages,\n",
    "        width=width,\n",
    "        height=height\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a3a58bd-17ad-4994-9774-00899c956194",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CHANGE HERE HOW THE PLOT LOOKS\n",
    "services_paywall_chart = prepare_altair_stacked_services_paywall(\n",
    "    url_results_df,\n",
    "    x_value='total_tokens',\n",
    "    paywall_ads_colors=['#ffffd1', '#add8b7', '#64b4c2', '#335da3'],\n",
    "    legend_orient=\"bottom-right\",\n",
    "    axis_fontsize=18,\n",
    "    labels_fontsize=15,\n",
    "    font=\"times\",\n",
    "    title=\"(a) Web Domain Services\",\n",
    "    title_x=\"% of Total Tokens\",\n",
    "    title_y=\"Service Categories\",\n",
    "    labels_map={\"News/Periodicals\": \"News\", \"Encyclopedia/Database\": \"Encyclopedia\", \"Organization/Personal Website\": \"Org Site\", \"Social Media/Forums\": \"Social Media\", \"Other\": \"Other\", \"Academic\": \"Academic\", \"E-Commerce\": \"E-Commerce\", \"Blogs\": \"Blogs\", \"Government\": \"Government\"},\n",
    "    percentages=True,\n",
    "    remove=[],\n",
    "    width=350, \n",
    "    height=150,\n",
    ")\n",
    "services_paywall_chart.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0c7943d-b804-4ef4-93f6-593b5d5fba6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CHANGE HERE HOW THE PLOT LOOKS\n",
    "wildchat_chart = prepare_altair_wildchat(\n",
    "    FPATH_WILDCHAT_RESULTS,\n",
    "    color=\"#ac8298\",\n",
    "    axis_fontsize=18,\n",
    "    labels_fontsize=15,\n",
    "    font=\"times\",\n",
    "    title=\"(b) Real ChatGPT Uses\",\n",
    "    title_x=\"Estimated % of Queries\",\n",
    "    title_y=None, # Manuel: I'm removing it only in the second plot because it looks better when concatenated, but you can add it back\n",
    "    labels_map={\"Creative Composition\": \"Creative Composition\", \"Sexual/Illegal Content\": \"Sexual Content\", \"Brainstorming and Planning\": \"Brainstorming & Planning\", \"Explanation and Reasoning\": \"Explanation & Reasoning\", \"General Information\": \"General Information\", \"Coding Composition\": \"Coding Composition\", \"Academic Composition\": \"Academic Composition\", \"Translation\": \"Translation\", \"Organization Information\": \"Organization Info\", \"E-commerce Information\": \"E-commerce Info\", \"Self-help & self-harm\": \"Self-help/harm\", \"News\": \"News\"},\n",
    "    percentages=True,\n",
    "    remove=[\"Other\", \"Self-help & self-harm\"],\n",
    "    width=350, \n",
    "    height=150,\n",
    ")\n",
    "wildchat_chart.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c5df177-22ab-4728-aa70-785176c49b5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CHANGE HERE THE ORDER OF THE PLOTS\n",
    "alt.hconcat(services_paywall_chart, wildchat_chart).configure_axis(\n",
    "    grid=False\n",
    ").configure_view(\n",
    "    strokeWidth=0 # Remove the frame around the chart\n",
    ").resolve_legend(\n",
    "    color='independent'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd30bb26-9ba5-4bd2-9f29-6458b18bbd7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate underlying percents. Look at random/head, dolma/rf/c4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12669a66-4f3d-4397-9d9d-0b2bdcb1497b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_uc = url_results_df[url_results_df[\"sample\"] != \"random\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9717283e-c1b9-4e13-9727-2977e1d3c612",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Normalize by token count\n",
    "# # ax = plot_modality_stacked_bar(url_results_df, normalize_by='token_count')\n",
    "\n",
    "# # Normalize by URL count\n",
    "# ax = plot_modality_stacked_bar(df_uc, normalize_by='url_count')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e90b511-051a-4767-a2d9-8850798710a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explode the 'domains' column\n",
    "dfx = url_results_df.explode('Domains')\n",
    "\n",
    "# Get the unique domain categories\n",
    "domain_categories = dfx['Domains'].unique()\n",
    "\n",
    "# Create a new column for each domain category with True/False values\n",
    "for category in domain_categories:\n",
    "    url_results_df[f'domain_{category}'] = url_results_df['Domains'].apply(lambda x: category in x)\n",
    "\n",
    "# Filter the DataFrame to only include rows with video modality\n",
    "df_with_videos = url_results_df[url_results_df['Modality: Video']]\n",
    "\n",
    "# Calculate the portion of URLs with videos for each domain category\n",
    "portions_by_category = {}\n",
    "for category in domain_categories:\n",
    "    category_col = f'domain_{category}'\n",
    "    total_videos = len(df_with_videos)\n",
    "    videos_in_category = df_with_videos[category_col].sum()\n",
    "    portion = videos_in_category / total_videos\n",
    "    portions_by_category[category] = portion\n",
    "\n",
    "# Print the results\n",
    "tot_portions = sum(portions_by_category.values())\n",
    "for category, portion in portions_by_category.items():\n",
    "    print(f\"{category}: {100 * portion / tot_portions :.2f}%\")\n",
    "# print(sum(tot_portions))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
    "hc3_en-finance": {
        "Unique Dataset Identifier": "hc3_en-finance",
        "Dataset Name": "hc3_en_finance",
        "Collection": "HC3 (English)",
        "Collection URL": "https://huggingface.co/datasets/Hello-SimpleAI/HC3",
        "Dataset URL": "https://github.com/Hello-SimpleAI/chatgpt-comparison-detection",
        "GitHub URL": "https://github.com/Hello-SimpleAI/chatgpt-comparison-detection",
        "Hugging Face URL": "https://huggingface.co/datasets/Hello-SimpleAI/HC3",
        "Paper Title": "How Close is ChatGPT to Human Experts? Comparison Corpus, Evaluation, and Detection",
        "Papers with Code URL": "https://paperswithcode.com/dataset/hc3",
        "ArXiv URL": "https://arxiv.org/abs/2301.07597",
        "Semantic Scholar Corpus ID": 255998637,
        "Languages": [
            "English"
        ],
        "Task Categories": [
            "Question Answering",
            "Response Ranking",
            "Data Analysis",
            "Open-form Text Generation",
            "Creativity"
        ],
        "Format": [
            "Response Ranking"
        ],
        "Text Metrics": {
            "Num Dialogs": 3933,
            "Mean Inputs Length": 62.0646,
            "Mean Targets Length": 1110.1625,
            "Max Inputs Length": 166,
            "Max Targets Length": 10070,
            "Min Inputs Length": 15,
            "Min Targets Length": 11,
            "Min Dialog Turns": 3,
            "Max Dialog Turns": 3,
            "Mean Dialog Turns": 3.0
        },
        "Text Sources": [
            "crowdsourced",
            "web exams",
            "wikipedia.org",
            "iclinic.com",
            "healthcaremagic.com",
            "financial news",
            "financial blogs"
        ],
        "Model Generated": [
            "OpenAI ChatGPT"
        ],
        "Creators": [
            "Shanghai University of Finance and Economics",
            "Harbin Institute of Technology",
            "Beijing Language and Culture University",
            "Xidian University",
            "Queen’s University",
            "Wind Information Co."
        ],
        "Licenses": [
            {
                "License": "CC BY-SA 4.0",
                "License URL": "https://github.com/Hello-SimpleAI/chatgpt-comparison-detection#dataset-copyright"
            },
            {
                "License": "Unspecified",
                "License URL": "https://sites.google.com/view/fiqa/"
            },
            {
                "License": "OpenAI",
                "License URL": "https://arxiv.org/abs/2301.07597"
            }
        ],
        "License Notes": "Provides human and ChatGPT responses for different domains and languages",
        "License Verified By": "Shayne",
        "Dataset Filter IDs": [
            "finance"
        ],
        "Inferred Metadata": {
            "HF Dataset": "Hello-SimpleAI/HC3",
            "HF Config": "all",
            "HF Config License": "",
            "HF Yaml License": "",
            "PwC License Name": "",
            "PwC License URL": "",
            "PwC Date": "",
            "S2 Date": "2023-01-18",
            "GitHub License": "",
            "Text Topics": [
                "Personal finance",
                "Finance",
                "Economics",
                "Stock market",
                "Retirement planning",
                "Taxation",
                "Investing",
                "Investment strategies",
                "Financial analysis",
                "Real estate investment"
            ],
            "Github Date": "",
            "HF Date": "2023-01-18",
            "HF Downloads (September 2023)": 3363,
            "HF Likes (September 2023)": 110,
            "PwC Description": "The HC3 (Human ChatGPT Comparison Corpus) dataset consists of nearly 40K questions and their corresponding human/ChatGPT answers. The motivation for this dataset was to study ChatGPT's answers in contrast to human's answers. The questions range from a wide variety of domains, including open-domain, financial, medical, legal, and psychological areas.",
            "S2 Citation Count (September 2023)": 121,
            "GitHub Stars": "",
            "GitHub Topics": []
        },
        "Derived from Datasets": [
            "ELI5 dataset",
            "WikiQA dataset",
            "Wikipedia",
            "Medical Dialog dataset",
            "FiQA dataset",
            "WebTextQA dataset",
            "BaikeQA dataset",
            "BaiduBake",
            "NLPCC-DBQA dataset",
            "Chinese Psychological Exams",
            "LegalQA dataset",
            "FinanceZhidao dataset"
        ],
        "Human Annotation": "Yes",
        "Bibtex": "@Article{Guo2023HowCI,\n author = {Biyang Guo and Xin Zhang and Ziyuan Wang and Minqi Jiang and Jinran Nie and Yuxuan Ding and Jianwei Yue and Yupeng Wu},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {How Close is ChatGPT to Human Experts? Comparison Corpus, Evaluation, and Detection},\n volume = {abs/2301.07597},\n year = {2023}\n}\n"
    },
    "hc3_en-medicine": {
        "Unique Dataset Identifier": "hc3_en-medicine",
        "Dataset Name": "hc3_en_medicine",
        "Collection": "HC3 (English)",
        "Collection URL": "https://huggingface.co/datasets/Hello-SimpleAI/HC3",
        "Dataset URL": "https://github.com/Hello-SimpleAI/chatgpt-comparison-detection",
        "GitHub URL": "https://github.com/Hello-SimpleAI/chatgpt-comparison-detection",
        "Hugging Face URL": "https://huggingface.co/datasets/Hello-SimpleAI/HC3",
        "Paper Title": "How Close is ChatGPT to Human Experts? Comparison Corpus, Evaluation, and Detection",
        "Papers with Code URL": "https://paperswithcode.com/dataset/hc3",
        "ArXiv URL": "https://arxiv.org/abs/2301.07597",
        "Semantic Scholar Corpus ID": 255998637,
        "Languages": [
            "English"
        ],
        "Task Categories": [
            "Question Answering",
            "Dialogue Generation",
            "Response Ranking",
            "Data Analysis",
            "Open-form Text Generation",
            "Creativity"
        ],
        "Format": [
            "Response Ranking"
        ],
        "Text Metrics": {
            "Num Dialogs": 1248,
            "Mean Inputs Length": 360.4383,
            "Mean Targets Length": 822.1616,
            "Max Inputs Length": 599,
            "Max Targets Length": 2696,
            "Min Inputs Length": 78,
            "Min Targets Length": 5,
            "Min Dialog Turns": 2,
            "Max Dialog Turns": 3,
            "Mean Dialog Turns": 2.9984
        },
        "Text Sources": [
            "crowdsourced",
            "web exams",
            "wikipedia.org",
            "iclinic.com",
            "healthcaremagic.com",
            "financial news",
            "financial blogs"
        ],
        "Model Generated": [
            "OpenAI ChatGPT"
        ],
        "Creators": [
            "Shanghai University of Finance and Economics",
            "Harbin Institute of Technology",
            "Beijing Language and Culture University",
            "Xidian University",
            "Queen’s University",
            "Wind Information Co."
        ],
        "Licenses": [
            {
                "License": "CC BY-SA 4.0",
                "License URL": "https://github.com/Hello-SimpleAI/chatgpt-comparison-detection#dataset-copyright"
            },
            {
                "License": "Unspecified",
                "License URL": "https://github.com/UCSD-AI4H/Medical-Dialogue-System"
            },
            {
                "License": "OpenAI",
                "License URL": "https://arxiv.org/abs/2301.07597"
            }
        ],
        "License Notes": "Provides human and ChatGPT responses for different domains and languages",
        "License Verified By": "Shayne",
        "Dataset Filter IDs": [
            "medicine"
        ],
        "Inferred Metadata": {
            "HF Dataset": "Hello-SimpleAI/HC3",
            "HF Config": "all",
            "HF Config License": "",
            "HF Yaml License": "",
            "PwC License Name": "",
            "PwC License URL": "",
            "PwC Date": "",
            "S2 Date": "2023-01-18",
            "GitHub License": "",
            "Text Topics": [
                "Health",
                "Sexual health",
                "Health and Wellness",
                "Pain management",
                "Dermatology",
                "Pediatric health",
                "Skin conditions",
                "Women's health",
                "Health and medication",
                "Health and wellness"
            ],
            "Github Date": "",
            "HF Date": "2023-01-18",
            "HF Downloads (September 2023)": 3363,
            "HF Likes (September 2023)": 110,
            "PwC Description": "The HC3 (Human ChatGPT Comparison Corpus) dataset consists of nearly 40K questions and their corresponding human/ChatGPT answers. The motivation for this dataset was to study ChatGPT's answers in contrast to human's answers. The questions range from a wide variety of domains, including open-domain, financial, medical, legal, and psychological areas.",
            "S2 Citation Count (September 2023)": 121,
            "GitHub Stars": "",
            "GitHub Topics": []
        },
        "Derived from Datasets": [
            "ELI5 dataset",
            "WikiQA dataset",
            "Wikipedia",
            "Medical Dialog dataset",
            "FiQA dataset",
            "WebTextQA dataset",
            "BaikeQA dataset",
            "BaiduBake",
            "NLPCC-DBQA dataset",
            "Chinese Psychological Exams",
            "LegalQA dataset",
            "FinanceZhidao dataset"
        ],
        "Human Annotation": "Yes",
        "Bibtex": "@Article{Guo2023HowCI,\n author = {Biyang Guo and Xin Zhang and Ziyuan Wang and Minqi Jiang and Jinran Nie and Yuxuan Ding and Jianwei Yue and Yupeng Wu},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {How Close is ChatGPT to Human Experts? Comparison Corpus, Evaluation, and Detection},\n volume = {abs/2301.07597},\n year = {2023}\n}\n"
    },
    "hc3_en-wikiqa": {
        "Unique Dataset Identifier": "hc3_en-wikiqa",
        "Dataset Name": "hc3_en_wikiqa",
        "Collection": "HC3 (English)",
        "Collection URL": "https://huggingface.co/datasets/Hello-SimpleAI/HC3",
        "Dataset URL": "https://github.com/Hello-SimpleAI/chatgpt-comparison-detection",
        "GitHub URL": "https://github.com/Hello-SimpleAI/chatgpt-comparison-detection",
        "Hugging Face URL": "https://huggingface.co/datasets/Hello-SimpleAI/HC3",
        "Paper Title": "How Close is ChatGPT to Human Experts? Comparison Corpus, Evaluation, and Detection",
        "Papers with Code URL": "https://paperswithcode.com/dataset/hc3",
        "ArXiv URL": "https://arxiv.org/abs/2301.07597",
        "Semantic Scholar Corpus ID": 255998637,
        "Languages": [
            "English"
        ],
        "Task Categories": [
            "Question Answering",
            "Response Ranking",
            "Data Analysis",
            "Open-form Text Generation",
            "Creativity"
        ],
        "Format": [
            "Response Ranking"
        ],
        "Text Metrics": {
            "Num Dialogs": 1187,
            "Mean Inputs Length": 32.973,
            "Mean Targets Length": 449.1917,
            "Max Inputs Length": 96,
            "Max Targets Length": 3176,
            "Min Inputs Length": 13,
            "Min Targets Length": 9,
            "Min Dialog Turns": 3,
            "Max Dialog Turns": 3,
            "Mean Dialog Turns": 3.0
        },
        "Text Sources": [
            "crowdsourced",
            "web exams",
            "wikipedia.org",
            "iclinic.com",
            "healthcaremagic.com",
            "financial news",
            "financial blogs"
        ],
        "Model Generated": [
            "OpenAI ChatGPT"
        ],
        "Creators": [
            "Shanghai University of Finance and Economics",
            "Harbin Institute of Technology",
            "Beijing Language and Culture University",
            "Xidian University",
            "Queen’s University",
            "Wind Information Co."
        ],
        "Licenses": [
            {
                "License": "CC BY-SA 4.0",
                "License URL": "https://github.com/Hello-SimpleAI/chatgpt-comparison-detection#dataset-copyright"
            },
            {
                "License": "Microsoft Data Licensing Agreement",
                "License URL": ""
            },
            {
                "License": "OpenAI",
                "License URL": "https://arxiv.org/abs/2301.07597"
            }
        ],
        "License Notes": "Provides human and ChatGPT responses for different domains and languages",
        "License Verified By": "Shayne",
        "Dataset Filter IDs": [
            "open_qa"
        ],
        "Inferred Metadata": {
            "HF Dataset": "Hello-SimpleAI/HC3",
            "HF Config": "all",
            "HF Config License": "",
            "HF Yaml License": "",
            "PwC License Name": "",
            "PwC License URL": "",
            "PwC Date": "",
            "S2 Date": "2023-01-18",
            "GitHub License": "",
            "Text Topics": [
                "History",
                "Geography",
                "Sports",
                "Biology",
                "Music",
                "Science",
                "Politics",
                "Religion",
                "Engineering",
                "General knowledge"
            ],
            "Github Date": "",
            "HF Date": "2023-01-18",
            "HF Downloads (September 2023)": 3363,
            "HF Likes (September 2023)": 110,
            "PwC Description": "The HC3 (Human ChatGPT Comparison Corpus) dataset consists of nearly 40K questions and their corresponding human/ChatGPT answers. The motivation for this dataset was to study ChatGPT's answers in contrast to human's answers. The questions range from a wide variety of domains, including open-domain, financial, medical, legal, and psychological areas.",
            "S2 Citation Count (September 2023)": 121,
            "GitHub Stars": "",
            "GitHub Topics": []
        },
        "Derived from Datasets": [
            "ELI5 dataset",
            "WikiQA dataset",
            "Wikipedia",
            "Medical Dialog dataset",
            "FiQA dataset",
            "WebTextQA dataset",
            "BaikeQA dataset",
            "BaiduBake",
            "NLPCC-DBQA dataset",
            "Chinese Psychological Exams",
            "LegalQA dataset",
            "FinanceZhidao dataset"
        ],
        "Human Annotation": "Yes",
        "Bibtex": "@Article{Guo2023HowCI,\n author = {Biyang Guo and Xin Zhang and Ziyuan Wang and Minqi Jiang and Jinran Nie and Yuxuan Ding and Jianwei Yue and Yupeng Wu},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {How Close is ChatGPT to Human Experts? Comparison Corpus, Evaluation, and Detection},\n volume = {abs/2301.07597},\n year = {2023}\n}\n"
    },
    "hc3_en-reddit_eli5": {
        "Unique Dataset Identifier": "hc3_en-reddit_eli5",
        "Dataset Name": "hc3_en_eli5",
        "Collection": "HC3 (English)",
        "Collection URL": "https://huggingface.co/datasets/Hello-SimpleAI/HC3",
        "Dataset URL": "https://github.com/Hello-SimpleAI/chatgpt-comparison-detection",
        "GitHub URL": "https://github.com/Hello-SimpleAI/chatgpt-comparison-detection",
        "Hugging Face URL": "https://huggingface.co/datasets/Hello-SimpleAI/HC3",
        "Paper Title": "How Close is ChatGPT to Human Experts? Comparison Corpus, Evaluation, and Detection",
        "Papers with Code URL": "https://paperswithcode.com/dataset/hc3",
        "ArXiv URL": "https://arxiv.org/abs/2301.07597",
        "Semantic Scholar Corpus ID": 255998637,
        "Languages": [
            "English"
        ],
        "Task Categories": [
            "Question Answering",
            "Response Ranking",
            "Data Analysis",
            "Open-form Text Generation",
            "Creativity"
        ],
        "Format": [
            "Response Ranking"
        ],
        "Text Metrics": {
            "Num Dialogs": 17112,
            "Mean Inputs Length": 190.9521,
            "Mean Targets Length": 874.0087,
            "Max Inputs Length": 611,
            "Max Targets Length": 36793,
            "Min Inputs Length": 58,
            "Min Targets Length": 25,
            "Min Dialog Turns": 2,
            "Max Dialog Turns": 3,
            "Mean Dialog Turns": 2.9734
        },
        "Text Sources": [
            "crowdsourced",
            "web exams",
            "wikipedia.org",
            "iclinic.com",
            "healthcaremagic.com",
            "financial news",
            "financial blogs"
        ],
        "Model Generated": [
            "OpenAI ChatGPT"
        ],
        "Creators": [
            "Shanghai University of Finance and Economics",
            "Harbin Institute of Technology",
            "Beijing Language and Culture University",
            "Xidian University",
            "Queen’s University",
            "Wind Information Co."
        ],
        "Licenses": [
            {
                "License": "CC BY-SA 4.0",
                "License URL": "https://github.com/Hello-SimpleAI/chatgpt-comparison-detection#dataset-copyright"
            },
            {
                "License": "Unspecified",
                "License URL": "https://github.com/facebookresearch/ELI5"
            },
            {
                "License": "OpenAI",
                "License URL": "https://arxiv.org/abs/2301.07597"
            }
        ],
        "License Notes": "Provides human and ChatGPT responses for different domains and languages",
        "License Verified By": "Shayne",
        "Dataset Filter IDs": [
            "reddit_eli5"
        ],
        "Inferred Metadata": {
            "HF Dataset": "Hello-SimpleAI/HC3",
            "HF Config": "all",
            "HF Config License": "",
            "HF Yaml License": "",
            "PwC License Name": "",
            "PwC License URL": "",
            "PwC Date": "",
            "S2 Date": "2023-01-18",
            "GitHub License": "",
            "Text Topics": [
                "Biology",
                "Economics",
                "Web development",
                "Linguistics",
                "Health",
                "Education policy",
                "Labor unions",
                "School funding",
                "Biology - The topic of melanin and its role in protecting our skin from UV light",
                "Health - The topic of sunburn and the damage it causes to our skin"
            ],
            "Github Date": "",
            "HF Date": "2023-01-18",
            "HF Downloads (September 2023)": 3363,
            "HF Likes (September 2023)": 110,
            "PwC Description": "The HC3 (Human ChatGPT Comparison Corpus) dataset consists of nearly 40K questions and their corresponding human/ChatGPT answers. The motivation for this dataset was to study ChatGPT's answers in contrast to human's answers. The questions range from a wide variety of domains, including open-domain, financial, medical, legal, and psychological areas.",
            "S2 Citation Count (September 2023)": 121,
            "GitHub Stars": "",
            "GitHub Topics": []
        },
        "Derived from Datasets": [
            "ELI5 dataset",
            "WikiQA dataset",
            "Wikipedia",
            "Medical Dialog dataset",
            "FiQA dataset",
            "WebTextQA dataset",
            "BaikeQA dataset",
            "BaiduBake",
            "NLPCC-DBQA dataset",
            "Chinese Psychological Exams",
            "LegalQA dataset",
            "FinanceZhidao dataset"
        ],
        "Human Annotation": "Yes",
        "Bibtex": "@Article{Guo2023HowCI,\n author = {Biyang Guo and Xin Zhang and Ziyuan Wang and Minqi Jiang and Jinran Nie and Yuxuan Ding and Jianwei Yue and Yupeng Wu},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {How Close is ChatGPT to Human Experts? Comparison Corpus, Evaluation, and Detection},\n volume = {abs/2301.07597},\n year = {2023}\n}\n"
    },
    "hc3_en-wiki_csai": {
        "Unique Dataset Identifier": "hc3_en-wiki_csai",
        "Dataset Name": "hc3_en_wiki_csai",
        "Collection": "HC3 (English)",
        "Collection URL": "https://huggingface.co/datasets/Hello-SimpleAI/HC3",
        "Dataset URL": "https://github.com/Hello-SimpleAI/chatgpt-comparison-detection",
        "GitHub URL": "https://github.com/Hello-SimpleAI/chatgpt-comparison-detection",
        "Hugging Face URL": "https://huggingface.co/datasets/Hello-SimpleAI/HC3",
        "Paper Title": "How Close is ChatGPT to Human Experts? Comparison Corpus, Evaluation, and Detection",
        "Papers with Code URL": "https://paperswithcode.com/dataset/hc3",
        "ArXiv URL": "https://arxiv.org/abs/2301.07597",
        "Semantic Scholar Corpus ID": 255998637,
        "Languages": [
            "English"
        ],
        "Task Categories": [
            "Question Answering",
            "Response Ranking",
            "Data Analysis",
            "Open-form Text Generation",
            "Creativity"
        ],
        "Format": [
            "Response Ranking"
        ],
        "Text Metrics": {
            "Num Dialogs": 842,
            "Mean Inputs Length": 42.9561,
            "Mean Targets Length": 1239.7292,
            "Max Inputs Length": 60,
            "Max Targets Length": 4448,
            "Min Inputs Length": 32,
            "Min Targets Length": 64,
            "Min Dialog Turns": 3,
            "Max Dialog Turns": 3,
            "Mean Dialog Turns": 3.0
        },
        "Text Sources": [
            "crowdsourced",
            "web exams",
            "wikipedia.org",
            "iclinic.com",
            "healthcaremagic.com",
            "financial news",
            "financial blogs"
        ],
        "Model Generated": [
            "OpenAI ChatGPT"
        ],
        "Creators": [
            "Shanghai University of Finance and Economics",
            "Harbin Institute of Technology",
            "Beijing Language and Culture University",
            "Xidian University",
            "Queen’s University",
            "Wind Information Co."
        ],
        "Licenses": [
            {
                "License": "CC BY-SA 4.0",
                "License URL": "https://github.com/Hello-SimpleAI/chatgpt-comparison-detection#dataset-copyright"
            },
            {
                "License": "OpenAI",
                "License URL": "https://arxiv.org/abs/2301.07597"
            }
        ],
        "License Notes": "Provides human and ChatGPT responses for different domains and languages",
        "License Verified By": "Shayne",
        "Dataset Filter IDs": [
            "wiki_csai"
        ],
        "Inferred Metadata": {
            "HF Dataset": "Hello-SimpleAI/HC3",
            "HF Config": "all",
            "HF Config License": "",
            "HF Yaml License": "",
            "PwC License Name": "",
            "PwC License URL": "",
            "PwC Date": "",
            "S2 Date": "2023-01-18",
            "GitHub License": "",
            "Text Topics": [
                "Computer Science",
                "Artificial Intelligence",
                "Technology",
                "Mathematics",
                "Artificial intelligence",
                "Statistics",
                "Machine learning",
                "Machine Learning",
                "Biology",
                "Neuroscience"
            ],
            "Github Date": "",
            "HF Date": "2023-01-18",
            "HF Downloads (September 2023)": 3363,
            "HF Likes (September 2023)": 110,
            "PwC Description": "The HC3 (Human ChatGPT Comparison Corpus) dataset consists of nearly 40K questions and their corresponding human/ChatGPT answers. The motivation for this dataset was to study ChatGPT's answers in contrast to human's answers. The questions range from a wide variety of domains, including open-domain, financial, medical, legal, and psychological areas.",
            "S2 Citation Count (September 2023)": 121,
            "GitHub Stars": "",
            "GitHub Topics": []
        },
        "Derived from Datasets": [
            "ELI5 dataset",
            "WikiQA dataset",
            "Wikipedia",
            "Medical Dialog dataset",
            "FiQA dataset",
            "WebTextQA dataset",
            "BaikeQA dataset",
            "BaiduBake",
            "NLPCC-DBQA dataset",
            "Chinese Psychological Exams",
            "LegalQA dataset",
            "FinanceZhidao dataset"
        ],
        "Human Annotation": "Yes",
        "Bibtex": "@Article{Guo2023HowCI,\n author = {Biyang Guo and Xin Zhang and Ziyuan Wang and Minqi Jiang and Jinran Nie and Yuxuan Ding and Jianwei Yue and Yupeng Wu},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {How Close is ChatGPT to Human Experts? Comparison Corpus, Evaluation, and Detection},\n volume = {abs/2301.07597},\n year = {2023}\n}\n"
    }
}
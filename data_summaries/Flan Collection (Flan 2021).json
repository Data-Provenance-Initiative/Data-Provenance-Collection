{
    "fc-flan-aeslc": {
        "Unique Dataset Identifier": "fc-flan-aeslc",
        "Dataset Name": "aeslc",
        "Collection": "Flan Collection (Flan 2021)",
        "Collection URL": "https://github.com/google-research/FLAN/tree/main/flan/v2",
        "Dataset URL": "https://github.com/ryanzhumich/AESLC",
        "GitHub URL": "https://github.com/ryanzhumich/AESLC",
        "Hugging Face URL": "https://huggingface.co/datasets/aeslc",
        "Paper Title": "This Email Could Save Your Life: Introducing the Task of Email Subject Line Generation",
        "Papers with Code URL": "https://paperswithcode.com/dataset/aeslc",
        "ArXiv URL": "https://arxiv.org/abs/1906.03497",
        "Semantic Scholar Corpus ID": 182953152,
        "Languages": [
            "English"
        ],
        "Task Categories": [
            "Summarization",
            "Inverted Summarization"
        ],
        "Format": [
            "Zero-shot",
            "Few-shot"
        ],
        "Text Metrics": {
            "Num Dialogs": 51924,
            "Mean Inputs Length": 1916.877,
            "Mean Targets Length": 132.8782,
            "Max Inputs Length": 41521,
            "Max Targets Length": 35926,
            "Min Inputs Length": 42,
            "Min Targets Length": 1,
            "Min Dialog Turns": 2,
            "Max Dialog Turns": 2,
            "Mean Dialog Turns": 2.0
        },
        "Text Sources": [
            "enron corporation"
        ],
        "Model Generated": [],
        "Creators": [
            "Yale University",
            "Grammarly"
        ],
        "Licenses": [
            {
                "License": "CC BY-NC-SA 4.0",
                "License URL": "https://github.com/ryanzhumich/AESLC"
            }
        ],
        "License Notes": "",
        "License Verified By": "Shayne",
        "Dataset Filter IDs": [
            "aeslc:1.0.0"
        ],
        "Inferred Metadata": {
            "HF Dataset": "aeslc",
            "HF Config": "default",
            "HF Config License": "",
            "HF Yaml License": "Unspecified",
            "PwC License Name": "",
            "PwC License URL": "",
            "PwC Date": "",
            "S2 Date": "2019-06-08",
            "GitHub License": "",
            "Github Date": "",
            "HF Date": "2022-01-25",
            "HF Downloads (September 2023)": 2396,
            "HF Likes (September 2023)": 4,
            "PwC Description": "To study the task of email subject line generation: automatically generating an email subject line from the email body.",
            "S2 Citation Count (September 2023)": 48,
            "GitHub Stars": 22,
            "GitHub Topics": [],
            "Text Topics": []
        },
        "Derived from Datasets": [],
        "Human Annotation": "Yes",
        "Bibtex": "@Article{Zhang2019ThisEC,\n author = {Rui Zhang and Joel R. Tetreault},\n booktitle = {Annual Meeting of the Association for Computational Linguistics},\n pages = {446-456},\n title = {This Email Could Save Your Life: Introducing the Task of Email Subject Line Generation},\n year = {2019}\n}\n"
    },
    "fc-flan-ag_news_subset": {
        "Unique Dataset Identifier": "fc-flan-ag_news_subset",
        "Dataset Name": "ag_news_subset",
        "Collection": "Flan Collection (Flan 2021)",
        "Collection URL": "https://github.com/google-research/FLAN/tree/main/flan/v2",
        "Dataset URL": "http://www.di.unipi.it/~gulli/AG_corpus_of_news_articles.html",
        "GitHub URL": "",
        "Hugging Face URL": "https://huggingface.co/datasets/ag_news",
        "Paper Title": "Ranking a stream of news",
        "Papers with Code URL": "https://paperswithcode.com/dataset/ag-news",
        "ArXiv URL": "",
        "Semantic Scholar Corpus ID": 12445509,
        "Languages": [
            "English"
        ],
        "Task Categories": [
            "Inverted Topic Classification",
            "Text Classification"
        ],
        "Format": [
            "Zero-shot",
            "Few-shot"
        ],
        "Text Metrics": {
            "Num Dialogs": 108707,
            "Mean Inputs Length": 724.7042,
            "Mean Targets Length": 13.4394,
            "Max Inputs Length": 2972,
            "Max Targets Length": 100,
            "Min Inputs Length": 45,
            "Min Targets Length": 2,
            "Min Dialog Turns": 2,
            "Max Dialog Turns": 2,
            "Mean Dialog Turns": 2.0
        },
        "Text Sources": [],
        "Model Generated": [],
        "Creators": [
            "Univerisity of Pisa"
        ],
        "Licenses": [
            {
                "License": "Custom",
                "License URL": "http://groups.di.unipi.it/~gulli/AG_corpus_of_news_articles.html"
            }
        ],
        "License Notes": "",
        "License Verified By": "Shayne",
        "Dataset Filter IDs": [
            "ag_news_subset:1.0.0"
        ],
        "Inferred Metadata": {
            "HF Dataset": "ag_news",
            "HF Config": "default",
            "HF Config License": "",
            "HF Yaml License": "",
            "PwC License Name": "Non Commercial",
            "PwC License URL": "http://groups.di.unipi.it/~gulli/AG_corpus_of_news_articles.html",
            "PwC Date": "2015-01-01",
            "S2 Date": "2005-05-10",
            "GitHub License": "",
            "Github Date": "",
            "HF Date": "2022-01-25",
            "HF Downloads (September 2023)": 76013,
            "HF Likes (September 2023)": 66,
            "PwC Description": "AG News (AG’s News Corpus) is a subdataset of AG's corpus of news articles constructed by assembling titles and description fields of articles from the 4 largest classes (“World”, “Sports”, “Business”, “Sci/Tech”) of AG’s Corpus. The AG News contains 30,000 training and 1,900 test samples per class.",
            "S2 Citation Count (September 2023)": 191,
            "GitHub Stars": "",
            "GitHub Topics": [],
            "Text Topics": []
        },
        "Human Annotation": "No",
        "Derived from Datasets": [],
        "Bibtex": "@Article{Corso2005RankingAS,\n author = {G. D. Corso and Antonio Gullì and F. Romani},\n booktitle = {The Web Conference},\n pages = {97-106},\n title = {Ranking a stream of news},\n year = {2005}\n}\n"
    },
    "fc-flan-anli_r1": {
        "Unique Dataset Identifier": "fc-flan-anli_r1",
        "Dataset Name": "anli_r1",
        "Collection": "Flan Collection (Flan 2021)",
        "Collection URL": "https://github.com/google-research/FLAN/tree/main/flan/v2",
        "Dataset URL": "https://github.com/facebookresearch/anli",
        "GitHub URL": "https://github.com/facebookresearch/anli",
        "Hugging Face URL": "https://huggingface.co/datasets/anli",
        "Paper Title": "Adversarial NLI: A New Benchmark for Natural Language Understanding",
        "Papers with Code URL": "https://paperswithcode.com/dataset/anli",
        "ArXiv URL": "https://arxiv.org/abs/1910.14599",
        "Semantic Scholar Corpus ID": 207756753,
        "Languages": [
            "English"
        ],
        "Task Categories": [
            "Inverted Natural Language Inference",
            "Natural Language Inference"
        ],
        "Format": [
            "Zero-shot",
            "Few-shot"
        ],
        "Text Metrics": {
            "Num Dialogs": 61455,
            "Mean Inputs Length": 1170.3943,
            "Mean Targets Length": 59.8644,
            "Max Inputs Length": 3513,
            "Max Targets Length": 753,
            "Min Inputs Length": 36,
            "Min Targets Length": 2,
            "Min Dialog Turns": 2,
            "Max Dialog Turns": 2,
            "Mean Dialog Turns": 2.0
        },
        "Text Sources": [
            "wikipedia.org",
            "commoncrawl.org",
            "wikihow.com",
            "crowdsourced",
            "project gutenberg"
        ],
        "Model Generated": [],
        "Creators": [
            "UNC Chapel Hill",
            "Facebook AI Research"
        ],
        "Licenses": [
            {
                "License": "CC BY-NC 4.0",
                "License URL": "https://github.com/facebookresearch/anli/blob/main/LICENSE"
            }
        ],
        "License Notes": "",
        "License Verified By": "Shayne",
        "Dataset Filter IDs": [
            "anli/r1:0.1.0"
        ],
        "Inferred Metadata": {
            "HF Dataset": "anli",
            "HF Config": "plain_text",
            "HF Config License": "",
            "HF Yaml License": "",
            "PwC License Name": "CC BY-NC 4.0",
            "PwC License URL": "https://github.com/facebookresearch/anli/blob/master/LICENSE",
            "PwC Date": "2019-01-01",
            "S2 Date": "2019-10-31",
            "GitHub License": "",
            "Github Date": "",
            "HF Date": "2022-01-25",
            "HF Downloads (September 2023)": 19294,
            "HF Likes (September 2023)": 24,
            "PwC Description": "The Adversarial Natural Language Inference (ANLI, Nie et al.) is a new large-scale NLI benchmark dataset, collected via an iterative, adversarial human-and-model-in-the-loop procedure. Particular, the data is selected to be difficult to the state-of-the-art models, including BERT and RoBERTa.",
            "S2 Citation Count (September 2023)": 601,
            "GitHub Stars": 364,
            "GitHub Topics": [],
            "Text Topics": []
        },
        "Derived from Datasets": [
            "StoryCloze",
            "The Children’s Book Test dataset",
            "RTE5",
            "Manually Annotated Sub-Corpus (MASC) of the Open American National Corpus"
        ],
        "Human Annotation": "Yes",
        "Bibtex": "@Article{Nie2019AdversarialNA,\n author = {Yixin Nie and Adina Williams and Emily Dinan and Mohit Bansal and J. Weston and Douwe Kiela},\n booktitle = {Annual Meeting of the Association for Computational Linguistics},\n journal = {ArXiv},\n title = {Adversarial NLI: A New Benchmark for Natural Language Understanding},\n volume = {abs/1910.14599},\n year = {2019}\n}\n"
    },
    "fc-flan-anli_r2": {
        "Unique Dataset Identifier": "fc-flan-anli_r2",
        "Dataset Name": "anli_r2",
        "Collection": "Flan Collection (Flan 2021)",
        "Collection URL": "https://github.com/google-research/FLAN/tree/main/flan/v2",
        "Dataset URL": "https://github.com/facebookresearch/anli",
        "GitHub URL": "https://github.com/facebookresearch/anli",
        "Hugging Face URL": "https://huggingface.co/datasets/anli",
        "Paper Title": "Adversarial NLI: A New Benchmark for Natural Language Understanding",
        "Papers with Code URL": "https://paperswithcode.com/dataset/anli",
        "ArXiv URL": "https://arxiv.org/abs/1910.14599",
        "Semantic Scholar Corpus ID": 207756753,
        "Languages": [
            "English"
        ],
        "Task Categories": [
            "Inverted Natural Language Inference",
            "Natural Language Inference"
        ],
        "Format": [
            "Zero-shot",
            "Few-shot"
        ],
        "Text Metrics": {
            "Num Dialogs": 108838,
            "Mean Inputs Length": 1157.3478,
            "Mean Targets Length": 59.562,
            "Max Inputs Length": 3496,
            "Max Targets Length": 790,
            "Min Inputs Length": 36,
            "Min Targets Length": 2,
            "Min Dialog Turns": 2,
            "Max Dialog Turns": 2,
            "Mean Dialog Turns": 2.0
        },
        "Text Sources": [
            "wikipedia.org",
            "commoncrawl.org",
            "wikihow.com",
            "crowdsourced",
            "project gutenberg"
        ],
        "Model Generated": [],
        "Creators": [
            "UNC Chapel Hill",
            "Facebook AI Research"
        ],
        "Licenses": [
            {
                "License": "CC BY-NC 4.0",
                "License URL": "https://github.com/facebookresearch/anli/blob/main/LICENSE"
            }
        ],
        "License Notes": "",
        "License Verified By": "Shayne",
        "Dataset Filter IDs": [
            "anli/r2:0.1.0"
        ],
        "Inferred Metadata": {
            "HF Dataset": "anli",
            "HF Config": "plain_text",
            "HF Config License": "",
            "HF Yaml License": "",
            "PwC License Name": "CC BY-NC 4.0",
            "PwC License URL": "https://github.com/facebookresearch/anli/blob/master/LICENSE",
            "PwC Date": "2019-01-01",
            "S2 Date": "2019-10-31",
            "GitHub License": "",
            "Github Date": "",
            "HF Date": "2022-01-25",
            "HF Downloads (September 2023)": 19294,
            "HF Likes (September 2023)": 24,
            "PwC Description": "The Adversarial Natural Language Inference (ANLI, Nie et al.) is a new large-scale NLI benchmark dataset, collected via an iterative, adversarial human-and-model-in-the-loop procedure. Particular, the data is selected to be difficult to the state-of-the-art models, including BERT and RoBERTa.",
            "S2 Citation Count (September 2023)": 601,
            "GitHub Stars": 364,
            "GitHub Topics": [],
            "Text Topics": []
        },
        "Derived from Datasets": [
            "StoryCloze",
            "The Children’s Book Test dataset",
            "RTE5",
            "Manually Annotated Sub-Corpus (MASC) of the Open American National Corpus"
        ],
        "Human Annotation": "Yes",
        "Bibtex": "@Article{Nie2019AdversarialNA,\n author = {Yixin Nie and Adina Williams and Emily Dinan and Mohit Bansal and J. Weston and Douwe Kiela},\n booktitle = {Annual Meeting of the Association for Computational Linguistics},\n journal = {ArXiv},\n title = {Adversarial NLI: A New Benchmark for Natural Language Understanding},\n volume = {abs/1910.14599},\n year = {2019}\n}\n"
    },
    "fc-flan-anli_r3": {
        "Unique Dataset Identifier": "fc-flan-anli_r3",
        "Dataset Name": "anli_r3",
        "Collection": "Flan Collection (Flan 2021)",
        "Collection URL": "https://github.com/google-research/FLAN/tree/main/flan/v2",
        "Dataset URL": "https://github.com/facebookresearch/anli",
        "GitHub URL": "https://github.com/facebookresearch/anli",
        "Hugging Face URL": "https://huggingface.co/datasets/anli",
        "Paper Title": "Adversarial NLI: A New Benchmark for Natural Language Understanding",
        "Papers with Code URL": "https://paperswithcode.com/dataset/anli",
        "ArXiv URL": "https://arxiv.org/abs/1910.14599",
        "Semantic Scholar Corpus ID": 207756753,
        "Languages": [
            "English"
        ],
        "Task Categories": [
            "Inverted Natural Language Inference",
            "Natural Language Inference"
        ],
        "Format": [
            "Zero-shot",
            "Few-shot"
        ],
        "Text Metrics": {
            "Num Dialogs": 108830,
            "Mean Inputs Length": 1115.9112,
            "Mean Targets Length": 57.2937,
            "Max Inputs Length": 4128,
            "Max Targets Length": 961,
            "Min Inputs Length": 36,
            "Min Targets Length": 2,
            "Min Dialog Turns": 2,
            "Max Dialog Turns": 2,
            "Mean Dialog Turns": 2.0
        },
        "Text Sources": [
            "wikipedia.org",
            "commoncrawl.org",
            "wikihow.com",
            "crowdsourced",
            "project gutenberg"
        ],
        "Model Generated": [],
        "Creators": [
            "UNC Chapel Hill",
            "Facebook AI Research"
        ],
        "Licenses": [
            {
                "License": "CC BY-NC 4.0",
                "License URL": "https://github.com/facebookresearch/anli/blob/main/LICENSE"
            }
        ],
        "License Notes": "",
        "License Verified By": "Shayne",
        "Dataset Filter IDs": [
            "anli/r3:0.1.0"
        ],
        "Inferred Metadata": {
            "HF Dataset": "anli",
            "HF Config": "plain_text",
            "HF Config License": "",
            "HF Yaml License": "",
            "PwC License Name": "CC BY-NC 4.0",
            "PwC License URL": "https://github.com/facebookresearch/anli/blob/master/LICENSE",
            "PwC Date": "2019-01-01",
            "S2 Date": "2019-10-31",
            "GitHub License": "",
            "Github Date": "",
            "HF Date": "2022-01-25",
            "HF Downloads (September 2023)": 19294,
            "HF Likes (September 2023)": 24,
            "PwC Description": "The Adversarial Natural Language Inference (ANLI, Nie et al.) is a new large-scale NLI benchmark dataset, collected via an iterative, adversarial human-and-model-in-the-loop procedure. Particular, the data is selected to be difficult to the state-of-the-art models, including BERT and RoBERTa.",
            "S2 Citation Count (September 2023)": 601,
            "GitHub Stars": 364,
            "GitHub Topics": [],
            "Text Topics": []
        },
        "Derived from Datasets": [
            "StoryCloze",
            "The Children’s Book Test dataset",
            "RTE5",
            "Manually Annotated Sub-Corpus (MASC) of the Open American National Corpus"
        ],
        "Human Annotation": "Yes",
        "Bibtex": "@Article{Nie2019AdversarialNA,\n author = {Yixin Nie and Adina Williams and Emily Dinan and Mohit Bansal and J. Weston and Douwe Kiela},\n booktitle = {Annual Meeting of the Association for Computational Linguistics},\n journal = {ArXiv},\n title = {Adversarial NLI: A New Benchmark for Natural Language Understanding},\n volume = {abs/1910.14599},\n year = {2019}\n}\n"
    },
    "fc-flan-arc_challenge": {
        "Unique Dataset Identifier": "fc-flan-arc_challenge",
        "Dataset Name": "arc_challenge",
        "Collection": "Flan Collection (Flan 2021)",
        "Collection URL": "https://github.com/google-research/FLAN/tree/main/flan/v2",
        "Dataset URL": "https://ai2-public-datasets.s3.amazonaws.com/arc/ARC-V1-Feb2018.zip",
        "GitHub URL": "",
        "Hugging Face URL": "https://huggingface.co/datasets/ai2_arc",
        "Paper Title": "Think you have Solved Question Answering? Try ARC, the AI2 Reasoning Challenge",
        "Papers with Code URL": "https://paperswithcode.com/dataset/arc",
        "ArXiv URL": "https://arxiv.org/abs/1803.05457",
        "Semantic Scholar Corpus ID": 3922816,
        "Languages": [
            "English"
        ],
        "Task Categories": [
            "Closed-Book Question Answering",
            "Inverted Closed-Book QA"
        ],
        "Format": [
            "Zero-shot",
            "Few-shot"
        ],
        "Text Metrics": {
            "Num Dialogs": 3231,
            "Mean Inputs Length": 479.2185,
            "Mean Targets Length": 75.8954,
            "Max Inputs Length": 2177,
            "Max Targets Length": 683,
            "Min Inputs Length": 25,
            "Min Targets Length": 1,
            "Min Dialog Turns": 2,
            "Max Dialog Turns": 2,
            "Mean Dialog Turns": 2.0
        },
        "Text Sources": [
            "web exams",
            "undisclosed web"
        ],
        "Model Generated": [],
        "Creators": [
            "AI2"
        ],
        "Licenses": [
            {
                "License": "CC BY-SA 4.0",
                "License URL": "https://creativecommons.org/licenses/by-sa/4.0/"
            }
        ],
        "License Notes": "",
        "License Verified By": "Shayne",
        "Dataset Filter IDs": [
            "ai2_arc/ARC-Challenge:1.0.0"
        ],
        "Inferred Metadata": {
            "HF Dataset": "ai2_arc",
            "HF Config": "ARC-Challenge",
            "HF Config License": "",
            "HF Yaml License": "",
            "PwC License Name": "CC BY-SA 4.0",
            "PwC License URL": "https://allenai.org/data/arc",
            "PwC Date": "2018-01-01",
            "S2 Date": "2018-03-14",
            "GitHub License": "",
            "Github Date": "",
            "HF Date": "2022-01-25",
            "HF Downloads (September 2023)": 171232,
            "HF Likes (September 2023)": 21,
            "PwC Description": "The AI2’s Reasoning Challenge (ARC) dataset is a multiple-choice question-answering dataset, containing questions from science exams from grade 3 to grade 9. The dataset is split in two partitions: Easy and Challenge, where the latter partition contains the more difficult questions that require reasoning. Most of the questions have 4 answer choices, with <1% of all the questions having either 3 or 5 answer choices. ARC includes a supporting KB of 14.3M unstructured text passages.",
            "S2 Citation Count (September 2023)": 458,
            "GitHub Stars": "",
            "GitHub Topics": [],
            "Text Topics": []
        },
        "Derived from Datasets": [],
        "Human Annotation": "No",
        "Bibtex": "@Article{Clark2018ThinkYH,\n author = {Peter Clark and Isaac Cowhey and Oren Etzioni and Tushar Khot and Ashish Sabharwal and Carissa Schoenick and Oyvind Tafjord},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {Think you have Solved Question Answering? Try ARC, the AI2 Reasoning Challenge},\n volume = {abs/1803.05457},\n year = {2018}\n}\n"
    },
    "fc-flan-arc_easy": {
        "Unique Dataset Identifier": "fc-flan-arc_easy",
        "Dataset Name": "arc_easy",
        "Collection": "Flan Collection (Flan 2021)",
        "Collection URL": "https://github.com/google-research/FLAN/tree/main/flan/v2",
        "Dataset URL": "https://ai2-public-datasets.s3.amazonaws.com/arc/ARC-V1-Feb2018.zip",
        "GitHub URL": "",
        "Hugging Face URL": "https://huggingface.co/datasets/ai2_arc",
        "Paper Title": "Think you have Solved Question Answering? Try ARC, the AI2 Reasoning Challenge",
        "Papers with Code URL": "https://paperswithcode.com/dataset/arc",
        "ArXiv URL": "https://arxiv.org/abs/1803.05457",
        "Semantic Scholar Corpus ID": 3922816,
        "Languages": [
            "English"
        ],
        "Task Categories": [
            "Closed-Book Question Answering",
            "Inverted Closed-Book QA"
        ],
        "Format": [
            "Zero-shot",
            "Few-shot"
        ],
        "Text Metrics": {
            "Num Dialogs": 7473,
            "Mean Inputs Length": 423.5624,
            "Mean Targets Length": 64.976,
            "Max Inputs Length": 1903,
            "Max Targets Length": 586,
            "Min Inputs Length": 32,
            "Min Targets Length": 2,
            "Min Dialog Turns": 2,
            "Max Dialog Turns": 2,
            "Mean Dialog Turns": 2.0
        },
        "Text Sources": [
            "web exams",
            "undisclosed web"
        ],
        "Model Generated": [],
        "Creators": [
            "AI2"
        ],
        "Licenses": [
            {
                "License": "CC BY-SA 4.0",
                "License URL": "https://creativecommons.org/licenses/by-sa/4.0/"
            }
        ],
        "License Notes": "",
        "License Verified By": "Shayne",
        "Dataset Filter IDs": [
            "ai2_arc/ARC-Easy:1.0.0"
        ],
        "Inferred Metadata": {
            "HF Dataset": "ai2_arc",
            "HF Config": "ARC-Challenge",
            "HF Config License": "",
            "HF Yaml License": "",
            "PwC License Name": "CC BY-SA 4.0",
            "PwC License URL": "https://allenai.org/data/arc",
            "PwC Date": "2018-01-01",
            "S2 Date": "2018-03-14",
            "GitHub License": "",
            "Github Date": "",
            "HF Date": "2022-01-25",
            "HF Downloads (September 2023)": 171232,
            "HF Likes (September 2023)": 21,
            "PwC Description": "The AI2’s Reasoning Challenge (ARC) dataset is a multiple-choice question-answering dataset, containing questions from science exams from grade 3 to grade 9. The dataset is split in two partitions: Easy and Challenge, where the latter partition contains the more difficult questions that require reasoning. Most of the questions have 4 answer choices, with <1% of all the questions having either 3 or 5 answer choices. ARC includes a supporting KB of 14.3M unstructured text passages.",
            "S2 Citation Count (September 2023)": 458,
            "GitHub Stars": "",
            "GitHub Topics": [],
            "Text Topics": []
        },
        "Derived from Datasets": [],
        "Human Annotation": "No",
        "Bibtex": "@Article{Clark2018ThinkYH,\n author = {Peter Clark and Isaac Cowhey and Oren Etzioni and Tushar Khot and Ashish Sabharwal and Carissa Schoenick and Oyvind Tafjord},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {Think you have Solved Question Answering? Try ARC, the AI2 Reasoning Challenge},\n volume = {abs/1803.05457},\n year = {2018}\n}\n"
    },
    "fc-flan-bool_q": {
        "Unique Dataset Identifier": "fc-flan-bool_q",
        "Dataset Name": "bool_q",
        "Collection": "Flan Collection (Flan 2021)",
        "Collection URL": "https://github.com/google-research/FLAN/tree/main/flan/v2",
        "Dataset URL": "https://github.com/google-research-datasets/boolean-questions",
        "GitHub URL": "https://github.com/google-research-datasets/boolean-questions",
        "Hugging Face URL": "https://huggingface.co/datasets/boolq",
        "Paper Title": "BoolQ: Exploring the Surprising Difficulty of Natural Yes/No Questions",
        "Papers with Code URL": "https://paperswithcode.com/dataset/boolq",
        "ArXiv URL": "https://arxiv.org/abs/1905.10044",
        "Semantic Scholar Corpus ID": 165163607,
        "Languages": [
            "English"
        ],
        "Task Categories": [
            "Inverted Multiple-Choice QA (no trivia knowledge required)",
            "Multiple Choice Question Answering"
        ],
        "Format": [
            "Zero-shot",
            "Few-shot"
        ],
        "Text Metrics": {
            "Num Dialogs": 33293,
            "Mean Inputs Length": 1618.7465,
            "Mean Targets Length": 2.7744,
            "Max Inputs Length": 7696,
            "Max Targets Length": 5,
            "Min Inputs Length": 86,
            "Min Targets Length": 2,
            "Min Dialog Turns": 2,
            "Max Dialog Turns": 2,
            "Mean Dialog Turns": 2.0
        },
        "Text Sources": [
            "wikipedia.org",
            "google search"
        ],
        "Model Generated": [],
        "Creators": [
            "University of Washington",
            "Google Research"
        ],
        "Licenses": [
            {
                "License": "CC BY-SA 3.0",
                "License URL": "https://creativecommons.org/licenses/by-sa/3.0/"
            }
        ],
        "License Notes": "",
        "License Verified By": "Shayne",
        "Dataset Filter IDs": [
            "bool_q:1.0.0"
        ],
        "Inferred Metadata": {
            "HF Dataset": "boolq",
            "HF Config": "default",
            "HF Config License": "",
            "HF Yaml License": "",
            "PwC License Name": "CC BY-SA 3.0",
            "PwC License URL": "https://creativecommons.org/licenses/by-sa/3.0/",
            "PwC Date": "2019-01-01",
            "S2 Date": "2019-05-01",
            "GitHub License": "",
            "Github Date": "",
            "HF Date": "2022-01-25",
            "HF Downloads (September 2023)": 12122,
            "HF Likes (September 2023)": 22,
            "PwC Description": "BoolQ is a question answering dataset for yes/no questions containing 15942 examples. These questions are naturally occurring – they are generated in unprompted and unconstrained settings.\nEach example is a triplet of (question, passage, answer), with the title of the page as optional additional context.\n\nQuestions are gathered from anonymized, aggregated queries to the Google search engine. Queries that are likely to be yes/no questions are heuristically identified and questions are only kept if a Wikipedia page is returned as one of the first five results, in which case the question and Wikipedia page are given to a human annotator for further processing. Annotators label question/article pairs in a three-step process. First, they decide if the question is good, meaning it is comprehensible, unambiguous, and requesting factual information. This judgment is made before the annotator sees the Wikipedia page. Next, for good questions, annotators find a passage within the document that contains enough information to answer the question. Annotators can mark questions as “not answerable” if the Wikipedia article does not contain the requested information. Finally, annotators mark whether the question’s answer is “yes” or “no”. Only questions that were marked as having a yes/no answer are used, and each question is paired with the selected passage instead of the entire document.",
            "S2 Citation Count (September 2023)": 457,
            "GitHub Stars": "",
            "GitHub Topics": [],
            "Text Topics": []
        },
        "Derived from Datasets": [],
        "Human Annotation": "Yes",
        "Bibtex": "@Article{Clark2019BoolQET,\n author = {Christopher Clark and Kenton Lee and Ming-Wei Chang and T. Kwiatkowski and Michael Collins and Kristina Toutanova},\n booktitle = {North American Chapter of the Association for Computational Linguistics},\n journal = {ArXiv},\n title = {BoolQ: Exploring the Surprising Difficulty of Natural Yes/No Questions},\n volume = {abs/1905.10044},\n year = {2019}\n}\n"
    },
    "fc-flan-cb": {
        "Unique Dataset Identifier": "fc-flan-cb",
        "Dataset Name": "cb",
        "Collection": "Flan Collection (Flan 2021)",
        "Collection URL": "https://github.com/google-research/FLAN/tree/main/flan/v2",
        "Dataset URL": "https://github.com/mcdm/CommitmentBank",
        "GitHub URL": "https://github.com/mcdm/CommitmentBank",
        "Hugging Face URL": "https://huggingface.co/datasets/super_glue",
        "Paper Title": "The CommitmentBank: Investigating projection in naturally occurring discourse",
        "Papers with Code URL": "",
        "ArXiv URL": "",
        "Semantic Scholar Corpus ID": 203595067,
        "Languages": [
            "English"
        ],
        "Task Categories": [
            "Inverted Natural Language Inference",
            "Natural Language Inference",
            "Textual Entailment"
        ],
        "Format": [
            "Zero-shot",
            "Few-shot"
        ],
        "Text Metrics": {
            "Num Dialogs": 745,
            "Mean Inputs Length": 967.4389,
            "Mean Targets Length": 38.6201,
            "Max Inputs Length": 3749,
            "Max Targets Length": 773,
            "Min Inputs Length": 36,
            "Min Targets Length": 2,
            "Min Dialog Turns": 2,
            "Max Dialog Turns": 2,
            "Mean Dialog Turns": 2.0
        },
        "Text Sources": [],
        "Model Generated": [],
        "Creators": [
            "The Ohio State University",
            "Carnegie Mellon University"
        ],
        "Licenses": [
            {
                "License": "Unspecified",
                "License URL": "unknown"
            }
        ],
        "License Notes": "",
        "License Verified By": "Shayne",
        "Dataset Filter IDs": [
            "super_glue/cb:1.0.2"
        ],
        "Inferred Metadata": {
            "HF Dataset": "super_glue",
            "HF Config": "cb",
            "HF Config License": "",
            "HF Yaml License": "",
            "PwC License Name": "",
            "PwC License URL": "",
            "PwC Date": "",
            "S2 Date": "2019-07-25",
            "GitHub License": "",
            "Github Date": "",
            "HF Date": "2022-01-25",
            "HF Downloads (September 2023)": 199405,
            "HF Likes (September 2023)": 111,
            "PwC Description": "",
            "S2 Citation Count (September 2023)": 216,
            "GitHub Stars": "",
            "GitHub Topics": [],
            "Text Topics": []
        },
        "Human Annotation": "No",
        "Derived from Datasets": [],
        "Bibtex": "@Inproceedings{Marneffe2019TheCI,\n author = {M. Marneffe and M. Simons and Judith Tonhauser},\n pages = {107-124},\n title = {The CommitmentBank: Investigating projection in naturally occurring discourse},\n volume = {23},\n year = {2019}\n}\n"
    },
    "fc-flan-cnn_dailymail": {
        "Unique Dataset Identifier": "fc-flan-cnn_dailymail",
        "Dataset Name": "cnn_dailymail",
        "Collection": "Flan Collection (Flan 2021)",
        "Collection URL": "https://github.com/google-research/FLAN/tree/main/flan/v2",
        "Dataset URL": "https://github.com/abisee/cnn-dailymail",
        "GitHub URL": "https://github.com/abisee/cnn-dailymail",
        "Hugging Face URL": "https://huggingface.co/datasets/cnn_dailymail",
        "Paper Title": "Get To The Point: Summarization with Pointer-Generator Networks",
        "Papers with Code URL": "https://paperswithcode.com/dataset/cnn-daily-mail-1",
        "ArXiv URL": "https://arxiv.org/abs/1704.04368",
        "Semantic Scholar Corpus ID": 8314118,
        "Languages": [
            "English"
        ],
        "Task Categories": [
            "Summarization",
            "Inverted Summarization"
        ],
        "Format": [
            "Zero-shot",
            "Few-shot"
        ],
        "Text Metrics": {
            "Num Dialogs": 108213,
            "Mean Inputs Length": 4636.1305,
            "Mean Targets Length": 1278.7201,
            "Max Inputs Length": 11429,
            "Max Targets Length": 14588,
            "Min Inputs Length": 84,
            "Min Targets Length": 33,
            "Min Dialog Turns": 2,
            "Max Dialog Turns": 2,
            "Mean Dialog Turns": 2.0
        },
        "Text Sources": [
            "cnn.com",
            "dailymail.co.uk"
        ],
        "Model Generated": [],
        "Creators": [
            "Stanford University",
            "Google Research"
        ],
        "Licenses": [
            {
                "License": "MIT License",
                "License URL": "https://github.com/abisee/cnn-dailymail"
            }
        ],
        "License Notes": "",
        "License Verified By": "Shayne",
        "Dataset Filter IDs": [
            "cnn_dailymail:3.4.0"
        ],
        "Inferred Metadata": {
            "HF Dataset": "cnn_dailymail",
            "HF Config": "3.0.0",
            "HF Config License": "",
            "HF Yaml License": "Apache License 2.0",
            "PwC License Name": "MIT License",
            "PwC License URL": "https://github.com/abisee/cnn-dailymail",
            "PwC Date": "2016-08-16",
            "S2 Date": "2017-04-01",
            "GitHub License": "MIT License",
            "Github Date": "",
            "HF Date": "2022-01-25",
            "HF Downloads (September 2023)": 69742,
            "HF Likes (September 2023)": 99,
            "PwC Description": "CNN/Daily Mail is a dataset for text summarization. Human generated abstractive summary bullets were generated from news stories in CNN and Daily Mail websites as questions (with one of the entities hidden), and stories as the corresponding passages from which the system is expected to answer the fill-in the-blank question. The authors released the scripts that crawl, extract and generate pairs of passages and questions from these websites.\n\nIn all, the corpus has 286,817 training pairs, 13,368 validation pairs and 11,487 test pairs, as defined by their scripts. The source documents in the training set have 766 words spanning 29.74 sentences on an average while the summaries consist of 53 words and 3.72 sentences.",
            "S2 Citation Count (September 2023)": 3215,
            "GitHub Stars": 601,
            "GitHub Topics": [],
            "Text Topics": []
        },
        "Derived from Datasets": [],
        "Human Annotation": "No",
        "Bibtex": "@Article{See2017GetTT,\n author = {A. See and Peter J. Liu and Christopher D. Manning},\n booktitle = {Annual Meeting of the Association for Computational Linguistics},\n journal = {ArXiv},\n title = {Get To The Point: Summarization with Pointer-Generator Networks},\n volume = {abs/1704.04368},\n year = {2017}\n}\n"
    },
    "fc-flan-cola": {
        "Unique Dataset Identifier": "fc-flan-cola",
        "Dataset Name": "cola",
        "Collection": "Flan Collection (Flan 2021)",
        "Collection URL": "https://github.com/google-research/FLAN/tree/main/flan/v2",
        "Dataset URL": "https://nyu-mll.github.io/CoLA/",
        "GitHub URL": "https://nyu-mll.github.io/CoLA/",
        "Hugging Face URL": "https://huggingface.co/datasets/linxinyuan/cola",
        "Paper Title": "Neural Network Acceptability Judgments",
        "Papers with Code URL": "https://paperswithcode.com/dataset/cola",
        "ArXiv URL": "https://arxiv.org/abs/1805.12471",
        "Semantic Scholar Corpus ID": 44072099,
        "Languages": [
            "English"
        ],
        "Task Categories": [
            "Inverted Grammatical Acceptability",
            "Grammatical Acceptability",
            "Text Quality Evaluation"
        ],
        "Format": [
            "Zero-shot",
            "Few-shot"
        ],
        "Text Metrics": {
            "Num Dialogs": 30028,
            "Mean Inputs Length": 339.7683,
            "Mean Targets Length": 21.761,
            "Max Inputs Length": 1213,
            "Max Targets Length": 190,
            "Min Inputs Length": 59,
            "Min Targets Length": 2,
            "Min Dialog Turns": 2,
            "Max Dialog Turns": 2,
            "Mean Dialog Turns": 2.0
        },
        "Text Sources": [
            "syntax textbooks"
        ],
        "Model Generated": [],
        "Creators": [
            "Stanford University",
            "Google Research"
        ],
        "Licenses": [
            {
                "License": "Unspecified",
                "License URL": "https://nyu-mll.github.io/CoLA/"
            }
        ],
        "License Notes": "Fair use only",
        "License Verified By": "Shayne",
        "Dataset Filter IDs": [
            "glue/cola:2.0.0"
        ],
        "Inferred Metadata": {
            "HF Dataset": "linxinyuan/cola",
            "HF Config": "default",
            "HF Config License": "",
            "HF Yaml License": "",
            "PwC License Name": "Custom",
            "PwC License URL": "https://nyu-mll.github.io/CoLA/",
            "PwC Date": "2018-01-01",
            "S2 Date": "2018-05-31",
            "GitHub License": "",
            "Github Date": "",
            "HF Date": "2022-06-08",
            "HF Downloads (September 2023)": 427,
            "HF Likes (September 2023)": 1,
            "PwC Description": "The Corpus of Linguistic Acceptability (CoLA) consists of 10657 sentences from 23 linguistics publications, expertly annotated for acceptability (grammaticality) by their original authors. The public version contains 9594 sentences belonging to training and development sets, and excludes 1063 sentences belonging to a held out test set.",
            "S2 Citation Count (September 2023)": 874,
            "GitHub Stars": "",
            "GitHub Topics": [],
            "Text Topics": []
        },
        "Derived from Datasets": [],
        "Human Annotation": "Yes",
        "Bibtex": "@Article{Warstadt2018NeuralNA,\n author = {Alex Warstadt and Amanpreet Singh and Samuel R. Bowman},\n booktitle = {Transactions of the Association for Computational Linguistics},\n journal = {Transactions of the Association for Computational Linguistics},\n pages = {625-641},\n title = {Neural Network Acceptability Judgments},\n volume = {7},\n year = {2018}\n}\n"
    },
    "fc-flan-common_gen": {
        "Unique Dataset Identifier": "fc-flan-common_gen",
        "Dataset Name": "common_gen",
        "Collection": "Flan Collection (Flan 2021)",
        "Collection URL": "https://github.com/google-research/FLAN/tree/main/flan/v2",
        "Dataset URL": "https://gem-benchmark.com/data_cards/common_gen",
        "GitHub URL": "",
        "Hugging Face URL": "https://huggingface.co/datasets/common_gen",
        "Paper Title": "CommonGen: A Constrained Text Generation Challenge for Generative Commonsense Reasoning",
        "Papers with Code URL": "https://paperswithcode.com/dataset/commongen",
        "ArXiv URL": "https://arxiv.org/abs/1911.03705",
        "Semantic Scholar Corpus ID": 218500588,
        "Languages": [
            "English"
        ],
        "Task Categories": [
            "Structured Data to Text",
            "Inverted Structured Data to Text"
        ],
        "Format": [
            "Zero-shot",
            "Few-shot"
        ],
        "Text Metrics": {
            "Num Dialogs": 108647,
            "Mean Inputs Length": 303.826,
            "Mean Targets Length": 52.0416,
            "Max Inputs Length": 945,
            "Max Targets Length": 174,
            "Min Inputs Length": 59,
            "Min Targets Length": 11,
            "Min Dialog Turns": 2,
            "Max Dialog Turns": 2,
            "Mean Dialog Turns": 2.0
        },
        "Text Sources": [
            "flickr",
            "crowdsourced",
            "undisclosed web",
            "amazon.com"
        ],
        "Model Generated": [],
        "Creators": [
            "University of Southern California",
            "AI2",
            "University of Washington"
        ],
        "Licenses": [
            {
                "License": "MIT License",
                "License URL": "https://gem-benchmark.com/data_cards/common_gen"
            }
        ],
        "License Notes": "",
        "License Verified By": "Shayne",
        "Dataset Filter IDs": [
            "gem/common_gen:1.1.0"
        ],
        "Inferred Metadata": {
            "HF Dataset": "common_gen",
            "HF Config": "default",
            "HF Config License": "",
            "HF Yaml License": "MIT License",
            "PwC License Name": "Custom",
            "PwC License URL": "https://inklab.usc.edu/CommonGen/",
            "PwC Date": "",
            "S2 Date": "2020-02-14",
            "GitHub License": "",
            "Github Date": "",
            "HF Date": "2022-01-25",
            "HF Downloads (September 2023)": 6767,
            "HF Likes (September 2023)": 13,
            "PwC Description": "CommonGen is constructed through a combination of crowdsourced and existing caption corpora, consists of 79k commonsense descriptions over 35k unique concept-sets.",
            "S2 Citation Count (September 2023)": 222,
            "GitHub Stars": "",
            "GitHub Topics": [],
            "Text Topics": []
        },
        "Derived from Datasets": [
            "Flickr30k",
            "MSCOCO",
            "Conceptual Captions",
            "LSMDC video captions",
            "ActivityNet",
            "VATEX"
        ],
        "Human Annotation": "Yes",
        "Bibtex": "@Article{Lin2020CommonGenAC,\n author = {Bill Yuchen Lin and Minghan Shen and Wangchunshu Zhou and Pei Zhou and Chandra Bhagavatula and Yejin Choi and Xiang Ren},\n booktitle = {Findings},\n pages = {1823-1840},\n title = {CommonGen: A Constrained Text Generation Challenge for Generative Commonsense Reasoning},\n year = {2020}\n}\n"
    },
    "fc-flan-copa": {
        "Unique Dataset Identifier": "fc-flan-copa",
        "Dataset Name": "copa",
        "Collection": "Flan Collection (Flan 2021)",
        "Collection URL": "https://github.com/google-research/FLAN/tree/main/flan/v2",
        "Dataset URL": "https://people.ict.usc.edu/~gordon/copa.html",
        "GitHub URL": "",
        "Hugging Face URL": "https://huggingface.co/datasets/gimmaru/super_glue-copa",
        "Paper Title": "",
        "Papers with Code URL": "https://paperswithcode.com/dataset/copa",
        "ArXiv URL": "https://arxiv.org/abs/1911.03705",
        "Semantic Scholar Corpus ID": 434646,
        "Languages": [
            "English"
        ],
        "Task Categories": [
            "Inverted Next Sentence Prediction",
            "Next Sentence Prediction",
            "Cause Effect Classification"
        ],
        "Format": [
            "Zero-shot",
            "Few-shot"
        ],
        "Text Metrics": {
            "Num Dialogs": 1302,
            "Mean Inputs Length": 328.4009,
            "Mean Targets Length": 27.2366,
            "Max Inputs Length": 1175,
            "Max Targets Length": 102,
            "Min Inputs Length": 17,
            "Min Targets Length": 2,
            "Min Dialog Turns": 2,
            "Max Dialog Turns": 2,
            "Mean Dialog Turns": 2.0
        },
        "Text Sources": [
            "flickr",
            "crowdsourced",
            "undisclosed web",
            "amazon.com"
        ],
        "Model Generated": [],
        "Creators": [
            "Indiana University",
            "University of Southern California"
        ],
        "Licenses": [
            {
                "License": "BSD 2-Clause License",
                "License URL": "https://people.ict.usc.edu/~gordon/copa.html"
            }
        ],
        "License Notes": "",
        "License Verified By": "Shayne",
        "Dataset Filter IDs": [
            "super_glue/copa:1.0.2"
        ],
        "Inferred Metadata": {
            "HF Dataset": "gimmaru/super_glue-copa",
            "HF Config": "gimmaru--super_glue-copa",
            "HF Config License": "",
            "HF Yaml License": "",
            "PwC License Name": "BSD 2-Clause License",
            "PwC License URL": "https://people.ict.usc.edu/~gordon/copa.html",
            "PwC Date": "2011-01-01",
            "S2 Date": "2011-03-20",
            "GitHub License": "",
            "Github Date": "",
            "HF Date": "2023-05-08",
            "HF Downloads (September 2023)": 21,
            "HF Likes (September 2023)": 0,
            "PwC Description": "The Choice Of Plausible Alternatives (COPA) evaluation provides researchers with a tool for assessing progress in open-domain commonsense causal reasoning. COPA consists of 1000 questions, split equally into development and test sets of 500 questions each. Each question is composed of a premise and two alternatives, where the task is to select the alternative that more plausibly has a causal relation with the premise. The correct alternative is randomized so that the expected performance of randomly guessing is 50%.",
            "S2 Citation Count (September 2023)": 442,
            "GitHub Stars": "",
            "GitHub Topics": [],
            "Text Topics": []
        },
        "Derived from Datasets": [
            "Flickr30k",
            "MSCOCO",
            "Conceptual Captions",
            "LSMDC video captions",
            "ActivityNet",
            "VATEX"
        ],
        "Human Annotation": "Yes",
        "Bibtex": "@Article{Gordon2011ChoiceOP,\n author = {A. Gordon and Zornitsa Kozareva and Melissa Roemmele},\n booktitle = {AAAI Spring Symposium: Logical Formalizations of Commonsense Reasoning},\n title = {Choice of Plausible Alternatives: An Evaluation of Commonsense Causal Reasoning},\n year = {2011}\n}\n"
    },
    "fc-flan-coqa": {
        "Unique Dataset Identifier": "fc-flan-coqa",
        "Dataset Name": "coqa",
        "Collection": "Flan Collection (Flan 2021)",
        "Collection URL": "https://github.com/google-research/FLAN/tree/main/flan/v2",
        "Dataset URL": "CoQA",
        "GitHub URL": "",
        "Hugging Face URL": "https://huggingface.co/datasets/coqa",
        "Paper Title": " CoQA: A Conversational Question Answering Challenge ",
        "Papers with Code URL": "https://paperswithcode.com/dataset/coqa",
        "ArXiv URL": "https://arxiv.org/abs/1808.07042",
        "Semantic Scholar Corpus ID": 52055325,
        "Languages": [
            "English"
        ],
        "Task Categories": [
            "Inverted Multiple-Choice QA (no trivia knowledge required)",
            "Multiple Choice Question Answering",
            "Wrong Candidate Generation"
        ],
        "Format": [
            "Zero-shot",
            "Few-shot"
        ],
        "Text Metrics": {
            "Num Dialogs": 25808,
            "Mean Inputs Length": 4352.7675,
            "Mean Targets Length": 307.1403,
            "Max Inputs Length": 8711,
            "Max Targets Length": 2159,
            "Min Inputs Length": 774,
            "Min Targets Length": 4,
            "Min Dialog Turns": 2,
            "Max Dialog Turns": 2,
            "Mean Dialog Turns": 2.0
        },
        "Text Sources": [
            "mctest",
            "project gutenberg",
            "race exams",
            "cnn.com",
            "wikipedia.org",
            "reddit",
            "ai2 science questions"
        ],
        "Model Generated": [],
        "Creators": [
            "Stanford University"
        ],
        "Licenses": [
            {
                "License": "Custom",
                "License URL": "https://www.cs.cmu.edu/~glai1/data/race/#:~:text=notes"
            }
        ],
        "License Notes": "CoQA contains passages from several domains. Literature and Wikipedia passages are shared under CC BY-SA 4.0 license. Children's stories are collected from MCTest which comes with MSR-LA license. Middle/High school exam passages are collected from RACE which comes with its own license. News passages are collected from the DeepMind CNN dataset which comes with Apache license. Only RACE is non-commercial and research only.",
        "License Verified By": "Shayne",
        "Dataset Filter IDs": [
            "coqa:1.0.0"
        ],
        "Inferred Metadata": {
            "HF Dataset": "coqa",
            "HF Config": "default",
            "HF Config License": "",
            "HF Yaml License": "Unspecified",
            "PwC License Name": "Various",
            "PwC License URL": "https://stanfordnlp.github.io/coqa/#:~:text=License",
            "PwC Date": "2018-01-01",
            "S2 Date": "2018-08-21",
            "GitHub License": "",
            "Github Date": "",
            "HF Date": "2022-01-25",
            "HF Downloads (September 2023)": 1383,
            "HF Likes (September 2023)": 21,
            "PwC Description": "CoQA is a large-scale dataset for building Conversational Question Answering systems. The goal of the CoQA challenge is to measure the ability of machines to understand a text passage and answer a series of interconnected questions that appear in a conversation.\n\nCoQA contains 127,000+ questions with answers collected from 8000+ conversations. Each conversation is collected by pairing two crowdworkers to chat about a passage in the form of questions and answers. The unique features of CoQA include 1) the questions are conversational; 2) the answers can be free-form text; 3) each answer also comes with an evidence subsequence highlighted in the passage; and 4) the passages are collected from seven diverse domains. CoQA has a lot of challenging phenomena not present in existing reading comprehension datasets, e.g., coreference and pragmatic reasoning.",
            "S2 Citation Count (September 2023)": 830,
            "GitHub Stars": "",
            "GitHub Topics": [],
            "Text Topics": []
        },
        "Derived from Datasets": [],
        "Human Annotation": "Yes",
        "Bibtex": "@Article{Reddy2018CoQAAC,\n author = {Siva Reddy and Danqi Chen and Christopher D. Manning},\n booktitle = {Transactions of the Association for Computational Linguistics},\n journal = {Transactions of the Association for Computational Linguistics},\n pages = {249-266},\n title = {CoQA: A Conversational Question Answering Challenge},\n volume = {7},\n year = {2018}\n}\n"
    },
    "fc-flan-cosmos_qa": {
        "Unique Dataset Identifier": "fc-flan-cosmos_qa",
        "Dataset Name": "cosmos_qa",
        "Collection": "Flan Collection (Flan 2021)",
        "Collection URL": "https://github.com/google-research/FLAN/tree/main/flan/v2",
        "Dataset URL": "https://wilburone.github.io/cosmos/",
        "GitHub URL": "https://wilburone.github.io/cosmos/",
        "Hugging Face URL": "https://huggingface.co/datasets/cosmos_qa",
        "Paper Title": "Cosmos QA: Machine Reading Comprehension with Contextual Commonsense Reasoning",
        "Papers with Code URL": "https://paperswithcode.com/dataset/cosmosqa",
        "ArXiv URL": "https://arxiv.org/abs/1909.00277",
        "Semantic Scholar Corpus ID": 202540590,
        "Languages": [
            "English"
        ],
        "Task Categories": [
            "Inverted Multiple-Choice QA (no trivia knowledge required)",
            "Multiple Choice Question Answering"
        ],
        "Format": [
            "Zero-shot",
            "Few-shot"
        ],
        "Text Metrics": {
            "Num Dialogs": 90432,
            "Mean Inputs Length": 1340.1141,
            "Mean Targets Length": 50.7225,
            "Max Inputs Length": 4922,
            "Max Targets Length": 689,
            "Min Inputs Length": 155,
            "Min Targets Length": 1,
            "Min Dialog Turns": 2,
            "Max Dialog Turns": 2,
            "Mean Dialog Turns": 2.0
        },
        "Text Sources": [
            "undisclosed web",
            "icwsm.org/data"
        ],
        "Model Generated": [],
        "Creators": [
            "University of Illinois Urbana-Champaign",
            "AI2",
            "University of Washington"
        ],
        "Licenses": [
            {
                "License": "CC BY 4.0",
                "License URL": "https://creativecommons.org/licenses/by/4.0/"
            }
        ],
        "License Notes": "https://huggingface.co/datasets/cosmos_qa",
        "License Verified By": "Shayne",
        "Dataset Filter IDs": [
            "cosmos_qa:1.0.0"
        ],
        "Inferred Metadata": {
            "HF Dataset": "cosmos_qa",
            "HF Config": "default",
            "HF Config License": "CC BY 4.0",
            "HF Yaml License": "CC BY 4.0",
            "PwC License Name": "Unspecified",
            "PwC License URL": "",
            "PwC Date": "2019-01-01",
            "S2 Date": "2019-08-31",
            "GitHub License": "",
            "Github Date": "",
            "HF Date": "2022-01-25",
            "HF Downloads (September 2023)": 6765,
            "HF Likes (September 2023)": 6,
            "PwC Description": "CosmosQA is a large-scale dataset of 35.6K problems that require commonsense-based reading comprehension, formulated as multiple-choice questions. It focuses on reading between the lines over a diverse collection of people’s everyday narratives, asking questions concerning on the likely causes or effects of events that require reasoning beyond the exact text spans in the context.",
            "S2 Citation Count (September 2023)": 295,
            "GitHub Stars": "",
            "GitHub Topics": [],
            "Text Topics": []
        },
        "Derived from Datasets": [],
        "Human Annotation": "Yes",
        "Bibtex": "@Article{Huang2019CosmosQM,\n author = {Lifu Huang and Ronan Le Bras and Chandra Bhagavatula and Yejin Choi},\n booktitle = {Conference on Empirical Methods in Natural Language Processing},\n pages = {2391-2401},\n title = {Cosmos QA: Machine Reading Comprehension with Contextual Commonsense Reasoning},\n year = {2019}\n}\n"
    },
    "fc-flan-dart": {
        "Unique Dataset Identifier": "fc-flan-dart",
        "Dataset Name": "dart",
        "Collection": "Flan Collection (Flan 2021)",
        "Collection URL": "https://github.com/google-research/FLAN/tree/main/flan/v2",
        "Dataset URL": "https://gem-benchmark.com/data_cards/dart",
        "GitHub URL": "",
        "Hugging Face URL": "https://huggingface.co/datasets/dart",
        "Paper Title": "DART: Open-Domain Structured Data Record to Text Generation",
        "Papers with Code URL": "https://paperswithcode.com/dataset/dart",
        "ArXiv URL": "https://arxiv.org/abs/2007.02871",
        "Semantic Scholar Corpus ID": 220364230,
        "Languages": [
            "English"
        ],
        "Task Categories": [
            "Structured Data to Text",
            "Inverted Structured Data to Text",
            "Information Extraction"
        ],
        "Format": [
            "Zero-shot",
            "Few-shot"
        ],
        "Text Metrics": {
            "Num Dialogs": 108557,
            "Mean Inputs Length": 661.0621,
            "Mean Targets Length": 109.5974,
            "Max Inputs Length": 2840,
            "Max Targets Length": 800,
            "Min Inputs Length": 45,
            "Min Targets Length": 1,
            "Min Dialog Turns": 2,
            "Max Dialog Turns": 2,
            "Mean Dialog Turns": 2.0
        },
        "Text Sources": [
            "wikipedia.org",
            "crowdsourced",
            "dbpedia"
        ],
        "Model Generated": [
            "wikisql"
        ],
        "Creators": [
            "Yale University",
            "Salesforce Research",
            "Penn State University",
            "The University of Hong Kong",
            "Massachusetts Institute of Technology"
        ],
        "Licenses": [
            {
                "License": "MIT License",
                "License URL": "https://gem-benchmark.com/data_cards/dart"
            }
        ],
        "License Notes": "",
        "License Verified By": "Shayne",
        "Dataset Filter IDs": [
            "gem/dart:1.1.0"
        ],
        "Inferred Metadata": {
            "HF Dataset": "dart",
            "HF Config": "default",
            "HF Config License": "",
            "HF Yaml License": "MIT License",
            "PwC License Name": "Unspecified",
            "PwC License URL": "",
            "PwC Date": "",
            "S2 Date": "2020-07-06",
            "GitHub License": "",
            "Github Date": "",
            "HF Date": "2022-01-25",
            "HF Downloads (September 2023)": 2855,
            "HF Likes (September 2023)": 3,
            "PwC Description": "DART is a large dataset for open-domain structured data record to text generation. DART consists of 82,191 examples across different domains with each input being a semantic RDF triple set derived from data records in tables and the tree ontology of the schema, annotated with sentence descriptions that cover all facts in the triple set.",
            "S2 Citation Count (September 2023)": 119,
            "GitHub Stars": "",
            "GitHub Topics": [],
            "Text Topics": []
        },
        "Derived from Datasets": [
            "WikiSQL",
            "WikiTableQuestions",
            "WebNLG 2017",
            "Cleaned E2E"
        ],
        "Human Annotation": "Yes",
        "Bibtex": "@Article{Radev2020DARTOS,\n author = {Dragomir R. Radev and Rui Zhang and Amrit Rau and Abhinand Sivaprasad and Chia-Hsuan Hsieh and Nazneen Rajani and Xiangru Tang and Aadit Vyas and Neha Verma and P. Krishna and Yangxiaokang Liu and Nadia Irwanto and Jessica Pan and Faiaz Rahman and A. Zaidi and Murori Mutuma and Yasin Tarabar and Ankit Gupta and Tao Yu and Y. Tan and Xi Victoria Lin and Caiming Xiong and R. Socher},\n booktitle = {North American Chapter of the Association for Computational Linguistics},\n journal = {ArXiv},\n title = {DART: Open-Domain Structured Data Record to Text Generation},\n volume = {abs/2007.02871},\n year = {2020}\n}\n"
    },
    "fc-flan-definite_pronoun_resolution": {
        "Unique Dataset Identifier": "fc-flan-definite_pronoun_resolution",
        "Dataset Name": "definite_pronoun_resolution",
        "Collection": "Flan Collection (Flan 2021)",
        "Collection URL": "https://github.com/google-research/FLAN/tree/main/flan/v2",
        "Dataset URL": "https://www.hlt.utdallas.edu/~vince/data/emnlp12/",
        "GitHub URL": "",
        "Hugging Face URL": "https://huggingface.co/datasets/definite_pronoun_resolution",
        "Paper Title": "Resolving Complex Cases of Definite Pronouns: The Winograd Schema Challenge",
        "Papers with Code URL": "https://paperswithcode.com/dataset/definite-pronoun-resolution-dataset",
        "ArXiv URL": "",
        "Semantic Scholar Corpus ID": 15274877,
        "Languages": [
            "English"
        ],
        "Task Categories": [
            "Coreference Resolution",
            "Inverted Coreference Resolution"
        ],
        "Format": [
            "Zero-shot",
            "Few-shot"
        ],
        "Text Metrics": {
            "Num Dialogs": 4043,
            "Mean Inputs Length": 397.3302,
            "Mean Targets Length": 8.4724,
            "Max Inputs Length": 1354,
            "Max Targets Length": 31,
            "Min Inputs Length": 49,
            "Min Targets Length": 2,
            "Min Dialog Turns": 2,
            "Max Dialog Turns": 2,
            "Mean Dialog Turns": 2.0
        },
        "Text Sources": [],
        "Model Generated": [],
        "Creators": [
            "The University of Texas at Austin"
        ],
        "Licenses": [
            {
                "License": "Unspecified",
                "License URL": "Unspecified"
            }
        ],
        "License Notes": "",
        "License Verified By": "Shayne",
        "Dataset Filter IDs": [
            "definite_pronoun_resolution:1.1.0"
        ],
        "Inferred Metadata": {
            "HF Dataset": "definite_pronoun_resolution",
            "HF Config": "plain_text",
            "HF Config License": "",
            "HF Yaml License": "Unspecified",
            "PwC License Name": "",
            "PwC License URL": "",
            "PwC Date": "",
            "S2 Date": "2012-07-12",
            "GitHub License": "",
            "Github Date": "",
            "HF Date": "2022-01-25",
            "HF Downloads (September 2023)": 594,
            "HF Likes (September 2023)": 3,
            "PwC Description": "Composes sentence pairs (i.e., twin sentences).",
            "S2 Citation Count (September 2023)": 174,
            "GitHub Stars": "",
            "GitHub Topics": [],
            "Text Topics": []
        },
        "Human Annotation": "No",
        "Derived from Datasets": [],
        "Bibtex": "@Article{Rahman2012ResolvingCC,\n author = {Altaf Rahman and Vincent Ng},\n booktitle = {Conference on Empirical Methods in Natural Language Processing},\n pages = {777-789},\n title = {Resolving Complex Cases of Definite Pronouns: The Winograd Schema Challenge},\n year = {2012}\n}\n"
    },
    "fc-flan-drop": {
        "Unique Dataset Identifier": "fc-flan-drop",
        "Dataset Name": "drop",
        "Collection": "Flan Collection (Flan 2021)",
        "Collection URL": "https://github.com/google-research/FLAN/tree/main/flan/v2",
        "Dataset URL": "https://allenai.org/data/drop",
        "GitHub URL": "",
        "Hugging Face URL": "https://huggingface.co/datasets/drop",
        "Paper Title": "DROP: A Reading Comprehension Benchmark Requiring Discrete Reasoning Over Paragraphs",
        "Papers with Code URL": "https://paperswithcode.com/dataset/drop",
        "ArXiv URL": "https://arxiv.org/abs/1903.00161",
        "Semantic Scholar Corpus ID": 67855846,
        "Languages": [
            "English"
        ],
        "Task Categories": [
            "Algebraic Expression Evaluation",
            "Inverted Mathematical QA",
            "Question Answering"
        ],
        "Format": [
            "Zero-shot",
            "Few-shot"
        ],
        "Text Metrics": {
            "Num Dialogs": 108783,
            "Mean Inputs Length": 3086.2389,
            "Mean Targets Length": 118.9147,
            "Max Inputs Length": 9793,
            "Max Targets Length": 9691,
            "Min Inputs Length": 69,
            "Min Targets Length": 0,
            "Min Dialog Turns": 2,
            "Max Dialog Turns": 2,
            "Mean Dialog Turns": 2.0
        },
        "Text Sources": [
            "wikipedia.org",
            "crowdsourced"
        ],
        "Model Generated": [],
        "Creators": [
            "UC Irvine",
            "Peking University",
            "AI2",
            "University of Washington"
        ],
        "Licenses": [
            {
                "License": "CC BY 4.0",
                "License URL": "https://allenai.org/data/drop"
            }
        ],
        "License Notes": "",
        "License Verified By": "Shayne",
        "Dataset Filter IDs": [
            "drop:2.0.0"
        ],
        "Inferred Metadata": {
            "HF Dataset": "drop",
            "HF Config": "default",
            "HF Config License": "",
            "HF Yaml License": "CC BY-SA 4.0",
            "PwC License Name": "CC BY-SA 4.0",
            "PwC License URL": "https://creativecommons.org/licenses/by-sa/4.0/legalcode",
            "PwC Date": "2019-01-01",
            "S2 Date": "2019-03-01",
            "GitHub License": "",
            "Github Date": "",
            "HF Date": "2022-01-25",
            "HF Downloads (September 2023)": 2782,
            "HF Likes (September 2023)": 8,
            "PwC Description": "Discrete Reasoning Over Paragraphs DROP is a crowdsourced, adversarially-created, 96k-question benchmark, in which a system must resolve references in a question, perhaps to multiple input positions, and perform discrete operations over them (such as addition, counting, or sorting). These operations require a much more comprehensive understanding of the content of paragraphs than what was necessary for prior datasets. The questions consist of passages extracted from Wikipedia articles. The dataset is split into a training set of about 77,000 questions, a development set of around 9,500 questions and a hidden test set similar in size to the development set.",
            "S2 Citation Count (September 2023)": 552,
            "GitHub Stars": "",
            "GitHub Topics": [],
            "Text Topics": []
        },
        "Derived from Datasets": [],
        "Human Annotation": "Yes",
        "Bibtex": "@Article{Dua2019DROPAR,\n author = {Dheeru Dua and Yizhong Wang and Pradeep Dasigi and Gabriel Stanovsky and Sameer Singh and Matt Gardner},\n booktitle = {North American Chapter of the Association for Computational Linguistics},\n pages = {2368-2378},\n title = {DROP: A Reading Comprehension Benchmark Requiring Discrete Reasoning Over Paragraphs},\n year = {2019}\n}\n"
    },
    "fc-flan-e2e_nlg": {
        "Unique Dataset Identifier": "fc-flan-e2e_nlg",
        "Dataset Name": "e2e_nlg",
        "Collection": "Flan Collection (Flan 2021)",
        "Collection URL": "https://github.com/google-research/FLAN/tree/main/flan/v2",
        "Dataset URL": "https://gem-benchmark.com/data_cards/e2e_nlg",
        "GitHub URL": "",
        "Hugging Face URL": "https://huggingface.co/datasets/e2e_nlg",
        "Paper Title": "The E2E Dataset: New Challenges For End-to-End Generation",
        "Papers with Code URL": "https://paperswithcode.com/dataset/e2e",
        "ArXiv URL": "https://arxiv.org/abs/1706.09254",
        "Semantic Scholar Corpus ID": 19662556,
        "Languages": [
            "English"
        ],
        "Task Categories": [
            "Structured Data to Text",
            "Inverted Structured Data to Text"
        ],
        "Format": [
            "Zero-shot",
            "Few-shot"
        ],
        "Text Metrics": {
            "Num Dialogs": 108900,
            "Mean Inputs Length": 612.3828,
            "Mean Targets Length": 111.5562,
            "Max Inputs Length": 2111,
            "Max Targets Length": 345,
            "Min Inputs Length": 65,
            "Min Targets Length": 4,
            "Min Dialog Turns": 2,
            "Max Dialog Turns": 2,
            "Mean Dialog Turns": 2.0
        },
        "Text Sources": [
            "crowdsourced"
        ],
        "Model Generated": [],
        "Creators": [
            "Heriot-Watt University"
        ],
        "Licenses": [
            {
                "License": "CC BY-SA 4.0",
                "License URL": "https://gem-benchmark.com/data_cards/e2e_nlg"
            }
        ],
        "License Notes": "",
        "License Verified By": "Shayne",
        "Dataset Filter IDs": [
            "gem/e2e_nlg:1.1.0"
        ],
        "Inferred Metadata": {
            "HF Dataset": "e2e_nlg",
            "HF Config": "default",
            "HF Config License": "",
            "HF Yaml License": "CC BY-SA 4.0",
            "PwC License Name": "CC BY-SA 4.0",
            "PwC License URL": "https://creativecommons.org/licenses/by-sa/4.0/",
            "PwC Date": "2017-06-28",
            "S2 Date": "2017-06-28",
            "GitHub License": "",
            "Github Date": "",
            "HF Date": "2022-01-25",
            "HF Downloads (September 2023)": 6741,
            "HF Likes (September 2023)": 5,
            "PwC Description": "End-to-End NLG Challenge (E2E) aims to assess whether recent end-to-end NLG systems can generate more complex output by learning from datasets containing higher lexical richness, syntactic complexity and diverse discourse phenomena.",
            "S2 Citation Count (September 2023)": 319,
            "GitHub Stars": "",
            "GitHub Topics": [],
            "Text Topics": []
        },
        "Derived from Datasets": [],
        "Human Annotation": "No",
        "Bibtex": "@Article{Novikova2017TheED,\n author = {Jekaterina Novikova and Ondrej Dusek and Verena Rieser},\n booktitle = {SIGDIAL Conference},\n journal = {ArXiv},\n title = {The E2E Dataset: New Challenges For End-to-End Generation},\n volume = {abs/1706.09254},\n year = {2017}\n}\n"
    },
    "fc-flan-fix_punct": {
        "Unique Dataset Identifier": "fc-flan-fix_punct",
        "Dataset Name": "fix_punct",
        "Collection": "Flan Collection (Flan 2021)",
        "Collection URL": "https://github.com/google-research/FLAN/tree/main/flan/v2",
        "Dataset URL": "https://www.paracrawl.eu/",
        "GitHub URL": "",
        "Hugging Face URL": "https://huggingface.co/datasets/para_crawl",
        "Paper Title": "ParaCrawl: Web-Scale Acquisition of Parallel Corpora",
        "Papers with Code URL": "https://paperswithcode.com/dataset/paracrawl",
        "ArXiv URL": "https://aclanthology.org/2020.acl-main.417/",
        "Semantic Scholar Corpus ID": 219165306,
        "Languages": [
            "English"
        ],
        "Task Categories": [
            "Punctuation Fixing",
            "Inverted Punctuation Fixing"
        ],
        "Format": [
            "Zero-shot",
            "Few-shot"
        ],
        "Text Metrics": {
            "Num Dialogs": 102101,
            "Mean Inputs Length": 599.4123,
            "Mean Targets Length": 123.5962,
            "Max Inputs Length": 4552,
            "Max Targets Length": 949,
            "Min Inputs Length": 29,
            "Min Targets Length": 12,
            "Min Dialog Turns": 2,
            "Max Dialog Turns": 2,
            "Mean Dialog Turns": 2.0
        },
        "Text Sources": [
            "undisclosed web"
        ],
        "Model Generated": [],
        "Creators": [
            "University of Edinburgh",
            "University of Alicante",
            "Johns Hopkins University",
            "Omniscien Technologies"
        ],
        "Licenses": [
            {
                "License": "CC0 1.0",
                "License URL": "https://creativecommons.org/share-your-work/public-domain/cc0/"
            }
        ],
        "License Notes": "",
        "License Verified By": "Shayne",
        "Dataset Filter IDs": [
            "fix_punct"
        ],
        "Inferred Metadata": {
            "HF Dataset": "para_crawl",
            "HF Config": "enbg",
            "HF Config License": "",
            "HF Yaml License": "",
            "PwC License Name": "CC0 1.0",
            "PwC License URL": "https://creativecommons.org/share-your-work/public-domain/cc0/",
            "PwC Date": "2020-07-01",
            "S2 Date": "2020-07-01",
            "GitHub License": "",
            "Github Date": "",
            "HF Date": "2022-01-25",
            "HF Downloads (September 2023)": 8060,
            "HF Likes (September 2023)": 8,
            "PwC Description": "ParaCrawl v.7.1 is a parallel dataset with 41 language pairs primarily aligned with English (39 out of 41) and mined using the parallel-data-crawling tool Bitextor which includes downloading documents, preprocessing and normalization, aligning documents and segments, and filtering noisy data via Bicleaner. ParaCrawl focuses on European languages, but also includes 9 lower-resource, non-European language pairs in v7.1.",
            "S2 Citation Count (September 2023)": 141,
            "GitHub Stars": "",
            "GitHub Topics": [],
            "Text Topics": [
                "Translation",
                "Travel",
                "Punctuation",
                "Language and grammar",
                "Religion",
                "Transportation",
                "Business and marketing"
            ]
        },
        "Derived from Datasets": [],
        "Human Annotation": "No",
        "Bibtex": "@Article{Bañón2020ParaCrawlWA,\n author = {Marta Bañón and Pinzhen Chen and B. Haddow and Kenneth Heafield and Hieu D. Hoang and Miquel Esplà-GomisF and Mikel ForcadaF and Amir Kamran and Faheem Kirefu and Philipp Koehn and Sergio Ortiz-Rojas and Leopoldo PlaF and Gema Ramírez-Sánchez and Elsa Sarrı́asF and Marek Střelec and Brian Thompson and W. Waites and Dion WigginsN and Jaume Zaragoza},\n booktitle = {Annual Meeting of the Association for Computational Linguistics},\n pages = {4555-4567},\n title = {ParaCrawl: Web-Scale Acquisition of Parallel Corpora},\n year = {2020}\n}\n"
    },
    "fc-flan-gigaword": {
        "Unique Dataset Identifier": "fc-flan-gigaword",
        "Dataset Name": "gigaword",
        "Collection": "Flan Collection (Flan 2021)",
        "Collection URL": "https://github.com/google-research/FLAN/tree/main/flan/v2",
        "Dataset URL": "https://github.com/harvardnlp/sent-summary",
        "GitHub URL": "https://github.com/harvardnlp/sent-summary",
        "Hugging Face URL": "https://huggingface.co/datasets/gigaword",
        "Paper Title": "A Neural Attention Model for Abstractive Sentence Summarization ",
        "Papers with Code URL": "",
        "ArXiv URL": "https://arxiv.org/abs/1509.00685",
        "Semantic Scholar Corpus ID": 1918428,
        "Languages": [
            "English"
        ],
        "Task Categories": [
            "Summarization",
            "Inverted Summarization",
            "Text Matching"
        ],
        "Format": [
            "Zero-shot",
            "Few-shot"
        ],
        "Text Metrics": {
            "Num Dialogs": 108321,
            "Mean Inputs Length": 635.2803,
            "Mean Targets Length": 74.5999,
            "Max Inputs Length": 2287,
            "Max Targets Length": 443,
            "Min Inputs Length": 47,
            "Min Targets Length": 14,
            "Min Dialog Turns": 2,
            "Max Dialog Turns": 2,
            "Mean Dialog Turns": 2.0
        },
        "Text Sources": [
            "agence france-presse",
            "ap news",
            "central news agency of taiwan",
            "los angeles times",
            "washington post",
            "bloomberg news",
            "nytmes",
            "xinhua news agency"
        ],
        "Model Generated": [],
        "Creators": [
            "Facebook AI Research"
        ],
        "Licenses": [
            {
                "License": "MIT License",
                "License URL": "https://github.com/harvardnlp/sent-summary"
            }
        ],
        "License Notes": "",
        "License Verified By": "Shayne",
        "Dataset Filter IDs": [
            "gigaword:1.2.0"
        ],
        "Inferred Metadata": {
            "HF Dataset": "gigaword",
            "HF Config": "default",
            "HF Config License": "",
            "HF Yaml License": "MIT License",
            "PwC License Name": "",
            "PwC License URL": "",
            "PwC Date": "",
            "S2 Date": "2015-09-02",
            "GitHub License": "MIT License",
            "Github Date": "",
            "HF Date": "2022-01-25",
            "HF Downloads (September 2023)": 5083,
            "HF Likes (September 2023)": 17,
            "PwC Description": "",
            "S2 Citation Count (September 2023)": 2443,
            "GitHub Stars": 296,
            "GitHub Topics": [],
            "Text Topics": []
        },
        "Derived from Datasets": [
            "Gigaword"
        ],
        "Human Annotation": "No",
        "Bibtex": "@Article{Rush2015ANA,\n author = {Alexander M. Rush and S. Chopra and J. Weston},\n booktitle = {Conference on Empirical Methods in Natural Language Processing},\n pages = {379-389},\n title = {A Neural Attention Model for Abstractive Sentence Summarization},\n year = {2015}\n}\n"
    },
    "fc-flan-glue_mrpc": {
        "Unique Dataset Identifier": "fc-flan-glue_mrpc",
        "Dataset Name": "glue_mrpc",
        "Collection": "Flan Collection (Flan 2021)",
        "Collection URL": "https://github.com/google-research/FLAN/tree/main/flan/v2",
        "Dataset URL": "https://www.microsoft.com/en-us/download/details.aspx?id=52398",
        "GitHub URL": "",
        "Hugging Face URL": "https://huggingface.co/datasets/glue",
        "Paper Title": "GLUE: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding",
        "Papers with Code URL": "https://paperswithcode.com/dataset/glue",
        "ArXiv URL": "https://arxiv.org/abs/1804.07461",
        "Semantic Scholar Corpus ID": 5034059,
        "Languages": [
            "English"
        ],
        "Task Categories": [
            "Inverted Coreference Resolution",
            "Paraphrase Detection"
        ],
        "Format": [
            "Zero-shot",
            "Few-shot"
        ],
        "Text Metrics": {
            "Num Dialogs": 12676,
            "Mean Inputs Length": 789.8204,
            "Mean Targets Length": 2.9683,
            "Max Inputs Length": 2354,
            "Max Targets Length": 7,
            "Min Inputs Length": 134,
            "Min Targets Length": 2,
            "Min Dialog Turns": 2,
            "Max Dialog Turns": 2,
            "Mean Dialog Turns": 2.0
        },
        "Text Sources": [
            "videos",
            "glosses",
            "forum posts",
            "twitter",
            "student answers",
            "wikipedia.org",
            "image descriptions"
        ],
        "Model Generated": [],
        "Creators": [
            "University of Washington",
            "DeepMind"
        ],
        "Licenses": [
            {
                "License": "Unspecified",
                "License URL": "Unspecified"
            }
        ],
        "License Notes": "",
        "License Verified By": "Shayne",
        "Dataset Filter IDs": [
            "glue/mrpc:2.0.0"
        ],
        "Inferred Metadata": {
            "HF Dataset": "glue",
            "HF Config": "mrpc",
            "HF Config License": "",
            "HF Yaml License": "",
            "PwC License Name": "Various",
            "PwC License URL": "https://gluebenchmark.com/faq",
            "PwC Date": "2019-01-01",
            "S2 Date": "2018-04-20",
            "GitHub License": "",
            "Github Date": "",
            "HF Date": "2022-01-25",
            "HF Downloads (September 2023)": 1105572,
            "HF Likes (September 2023)": 223,
            "PwC Description": "General Language Understanding Evaluation (GLUE) benchmark is a collection of nine natural language understanding tasks, including single-sentence tasks CoLA and SST-2, similarity and paraphrasing tasks MRPC, STS-B and QQP, and natural language inference tasks MNLI, QNLI, RTE and WNLI.",
            "S2 Citation Count (September 2023)": 4367,
            "GitHub Stars": "",
            "GitHub Topics": [],
            "Text Topics": []
        },
        "Derived from Datasets": [],
        "Human Annotation": "Yes",
        "Bibtex": "@Article{Wang2018GLUEAM,\n author = {Alex Wang and Amanpreet Singh and Julian Michael and Felix Hill and Omer Levy and Samuel R. Bowman},\n booktitle = {BlackboxNLP@EMNLP},\n pages = {353-355},\n title = {GLUE: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding},\n year = {2018}\n}\n"
    },
    "fc-flan-glue_qqp": {
        "Unique Dataset Identifier": "fc-flan-glue_qqp",
        "Dataset Name": "glue_qqp",
        "Collection": "Flan Collection (Flan 2021)",
        "Collection URL": "https://github.com/google-research/FLAN/tree/main/flan/v2",
        "Dataset URL": "https://quoradata.quora.com/First-Quora-Dataset-Release-Question-Pairs",
        "GitHub URL": "",
        "Hugging Face URL": "https://huggingface.co/datasets/glue",
        "Paper Title": "GLUE: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding",
        "Papers with Code URL": "https://paperswithcode.com/dataset/glue",
        "ArXiv URL": "https://arxiv.org/abs/1804.07461",
        "Semantic Scholar Corpus ID": 5034059,
        "Languages": [
            "English"
        ],
        "Task Categories": [
            "Inverted Coreference Resolution",
            "Paraphrase Detection"
        ],
        "Format": [
            "Zero-shot",
            "Few-shot"
        ],
        "Text Metrics": {
            "Num Dialogs": 108175,
            "Mean Inputs Length": 478.9584,
            "Mean Targets Length": 2.5494,
            "Max Inputs Length": 2686,
            "Max Targets Length": 5,
            "Min Inputs Length": 69,
            "Min Targets Length": 2,
            "Min Dialog Turns": 2,
            "Max Dialog Turns": 2,
            "Mean Dialog Turns": 2.0
        },
        "Text Sources": [
            "quora"
        ],
        "Model Generated": [],
        "Creators": [
            "University of Washington",
            "DeepMind"
        ],
        "Licenses": [
            {
                "License": "Custom",
                "License URL": "https://www.quora.com/about/tos"
            }
        ],
        "License Notes": "Blog Post mentions non-commercial use",
        "License Verified By": "Shayne",
        "Dataset Filter IDs": [
            "glue/qqp:2.0.0"
        ],
        "Inferred Metadata": {
            "HF Dataset": "glue",
            "HF Config": "qqp",
            "HF Config License": "",
            "HF Yaml License": "",
            "PwC License Name": "Various",
            "PwC License URL": "https://gluebenchmark.com/faq",
            "PwC Date": "2019-01-01",
            "S2 Date": "2018-04-20",
            "GitHub License": "",
            "Github Date": "",
            "HF Date": "2022-01-25",
            "HF Downloads (September 2023)": 1105572,
            "HF Likes (September 2023)": 223,
            "PwC Description": "General Language Understanding Evaluation (GLUE) benchmark is a collection of nine natural language understanding tasks, including single-sentence tasks CoLA and SST-2, similarity and paraphrasing tasks MRPC, STS-B and QQP, and natural language inference tasks MNLI, QNLI, RTE and WNLI.",
            "S2 Citation Count (September 2023)": 4367,
            "GitHub Stars": "",
            "GitHub Topics": [],
            "Text Topics": []
        },
        "Derived from Datasets": [],
        "Human Annotation": "Yes",
        "Bibtex": "@Article{Wang2018GLUEAM,\n author = {Alex Wang and Amanpreet Singh and Julian Michael and Felix Hill and Omer Levy and Samuel R. Bowman},\n booktitle = {BlackboxNLP@EMNLP},\n pages = {353-355},\n title = {GLUE: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding},\n year = {2018}\n}\n"
    },
    "fc-flan-hellaswag": {
        "Unique Dataset Identifier": "fc-flan-hellaswag",
        "Dataset Name": "hellaswag",
        "Collection": "Flan Collection (Flan 2021)",
        "Collection URL": "https://github.com/google-research/FLAN/tree/main/flan/v2",
        "Dataset URL": "https://paperswithcode.com/dataset/hellaswag",
        "GitHub URL": "",
        "Hugging Face URL": "https://huggingface.co/datasets/hellaswag",
        "Paper Title": "HellaSwag: Can a Machine Really Finish Your Sentence?",
        "Papers with Code URL": "https://paperswithcode.com/dataset/hellaswag",
        "ArXiv URL": "https://arxiv.org/abs/1905.07830",
        "Semantic Scholar Corpus ID": 159041722,
        "Languages": [
            "English"
        ],
        "Task Categories": [
            "Inverted Next Sentence Prediction",
            "Next Sentence Prediction",
            "Fill in The Blank"
        ],
        "Format": [
            "Zero-shot",
            "Few-shot"
        ],
        "Text Metrics": {
            "Num Dialogs": 108752,
            "Mean Inputs Length": 1676.192,
            "Mean Targets Length": 109.2056,
            "Max Inputs Length": 8368,
            "Max Targets Length": 436,
            "Min Inputs Length": 50,
            "Min Targets Length": 2,
            "Min Dialog Turns": 2,
            "Max Dialog Turns": 2,
            "Mean Dialog Turns": 2.0
        },
        "Text Sources": [
            "activitynet",
            "wikihow.com",
            "model generated negative candidates using adversarial filtering"
        ],
        "Model Generated": [
            "Other"
        ],
        "Creators": [
            "University of Washington",
            "AI2"
        ],
        "Licenses": [
            {
                "License": "MIT License",
                "License URL": "https://github.com/rowanz/hellaswag/blob/master/LICENSE"
            }
        ],
        "License Notes": "",
        "License Verified By": "Shayne",
        "Dataset Filter IDs": [
            "hellaswag:1.1.0"
        ],
        "Inferred Metadata": {
            "HF Dataset": "hellaswag",
            "HF Config": "default",
            "HF Config License": "",
            "HF Yaml License": "",
            "PwC License Name": "MIT License",
            "PwC License URL": "https://github.com/rowanz/hellaswag/blob/master/LICENSE",
            "PwC Date": "",
            "S2 Date": "2019-05-01",
            "GitHub License": "",
            "Github Date": "",
            "HF Date": "2022-01-25",
            "HF Downloads (September 2023)": 193793,
            "HF Likes (September 2023)": 25,
            "PwC Description": "HellaSwag is a challenge dataset for evaluating commonsense NLI that is specially hard for state-of-the-art models, though its questions are trivial for humans (>95% accuracy).",
            "S2 Citation Count (September 2023)": 462,
            "GitHub Stars": "",
            "GitHub Topics": [],
            "Text Topics": []
        },
        "Derived from Datasets": [],
        "Human Annotation": "Yes",
        "Bibtex": "@Article{Zellers2019HellaSwagCA,\n author = {Rowan Zellers and Ari Holtzman and Yonatan Bisk and Ali Farhadi and Yejin Choi},\n booktitle = {Annual Meeting of the Association for Computational Linguistics},\n pages = {4791-4800},\n title = {HellaSwag: Can a Machine Really Finish Your Sentence?},\n year = {2019}\n}\n"
    },
    "fc-flan-imdb_reviews": {
        "Unique Dataset Identifier": "fc-flan-imdb_reviews",
        "Dataset Name": "imdb_reviews",
        "Collection": "Flan Collection (Flan 2021)",
        "Collection URL": "https://github.com/google-research/FLAN/tree/main/flan/v2",
        "Dataset URL": "http://ai.stanford.edu/~amaas/data/sentiment/",
        "GitHub URL": "",
        "Hugging Face URL": "https://huggingface.co/datasets/imdb",
        "Paper Title": "Learning Word Vectors for Sentiment Analysis",
        "Papers with Code URL": "https://paperswithcode.com/dataset/imdb-movie-reviews",
        "ArXiv URL": "https://aclanthology.org/P11-1015/",
        "Semantic Scholar Corpus ID": 1428702,
        "Languages": [
            "English"
        ],
        "Task Categories": [
            "Inverted Sentiment Analysis",
            "Sentiment Analysis"
        ],
        "Format": [
            "Zero-shot",
            "Few-shot"
        ],
        "Text Metrics": {
            "Num Dialogs": 89321,
            "Mean Inputs Length": 2855.137,
            "Mean Targets Length": 496.3132,
            "Max Inputs Length": 13696,
            "Max Targets Length": 13602,
            "Min Inputs Length": 30,
            "Min Targets Length": 2,
            "Min Dialog Turns": 2,
            "Max Dialog Turns": 2,
            "Mean Dialog Turns": 2.0
        },
        "Text Sources": [
            "imdb.com"
        ],
        "Model Generated": [],
        "Creators": [
            "Stanford University"
        ],
        "Licenses": [
            {
                "License": "Unspecified",
                "License URL": "Unspecified"
            }
        ],
        "License Notes": "",
        "License Verified By": "Shayne",
        "Dataset Filter IDs": [
            "imdb_reviews/plain_text:1.0.0"
        ],
        "Inferred Metadata": {
            "HF Dataset": "imdb",
            "HF Config": "plain_text",
            "HF Config License": "",
            "HF Yaml License": "",
            "PwC License Name": "Unspecified",
            "PwC License URL": "",
            "PwC Date": "2011-01-01",
            "S2 Date": "2011-06-19",
            "GitHub License": "",
            "Github Date": "",
            "HF Date": "2022-01-25",
            "HF Downloads (September 2023)": 229363,
            "HF Likes (September 2023)": 115,
            "PwC Description": "The IMDb Movie Reviews dataset is a binary sentiment analysis dataset consisting of 50,000 reviews from the Internet Movie Database (IMDb) labeled as positive or negative. The dataset contains an even number of positive and negative reviews. Only highly polarizing reviews are considered. A negative review has a score ≤ 4 out of 10, and a positive review has a score ≥ 7 out of 10. No more than 30 reviews are included per movie. The dataset contains additional unlabeled data.",
            "S2 Citation Count (September 2023)": 3941,
            "GitHub Stars": "",
            "GitHub Topics": [],
            "Text Topics": []
        },
        "Derived from Datasets": [],
        "Human Annotation": "No",
        "Bibtex": "@Article{Maas2011LearningWV,\n author = {Andrew L. Maas and Raymond E. Daly and Peter T. Pham and Dan Huang and A. Ng and Christopher Potts},\n booktitle = {Annual Meeting of the Association for Computational Linguistics},\n pages = {142-150},\n title = {Learning Word Vectors for Sentiment Analysis},\n year = {2011}\n}\n"
    },
    "fc-flan-lambada": {
        "Unique Dataset Identifier": "fc-flan-lambada",
        "Dataset Name": "lambada",
        "Collection": "Flan Collection (Flan 2021)",
        "Collection URL": "https://github.com/google-research/FLAN/tree/main/flan/v2",
        "Dataset URL": "https://zenodo.org/record/2630551#.Y9RFsOzMKDV",
        "GitHub URL": "",
        "Hugging Face URL": "https://huggingface.co/datasets/lambada",
        "Paper Title": "The LAMBADA dataset: Word prediction requiring a broad discourse context",
        "Papers with Code URL": "https://paperswithcode.com/dataset/lambada",
        "ArXiv URL": "https://arxiv.org/abs/1606.06031",
        "Semantic Scholar Corpus ID": 2381275,
        "Languages": [
            "English"
        ],
        "Task Categories": [
            "Inverted Language Modeling",
            "Language Modeling"
        ],
        "Format": [
            "Zero-shot",
            "Few-shot"
        ],
        "Text Metrics": {
            "Num Dialogs": 17536,
            "Mean Inputs Length": 970.4334,
            "Mean Targets Length": 6.1352,
            "Max Inputs Length": 2938,
            "Max Targets Length": 23,
            "Min Inputs Length": 225,
            "Min Targets Length": 1,
            "Min Dialog Turns": 2,
            "Max Dialog Turns": 2,
            "Mean Dialog Turns": 2.0
        },
        "Text Sources": [
            "undisclosed web"
        ],
        "Model Generated": [],
        "Creators": [
            "University of Trento",
            "University of Amsterdam"
        ],
        "Licenses": [
            {
                "License": "CC BY 4.0",
                "License URL": "https://creativecommons.org/licenses/by/4.0/legalcode"
            }
        ],
        "License Notes": "",
        "License Verified By": "Shayne",
        "Dataset Filter IDs": [
            "lambada:1.0.0"
        ],
        "Inferred Metadata": {
            "HF Dataset": "lambada",
            "HF Config": "plain_text",
            "HF Config License": "",
            "HF Yaml License": "CC BY 4.0",
            "PwC License Name": "CC BY 4.0",
            "PwC License URL": "https://creativecommons.org/licenses/by/4.0/legalcode",
            "PwC Date": "2016-01-01",
            "S2 Date": "2016-06-20",
            "GitHub License": "",
            "Github Date": "",
            "HF Date": "2022-01-25",
            "HF Downloads (September 2023)": 36590,
            "HF Likes (September 2023)": 28,
            "PwC Description": "The LAMBADA (LAnguage Modeling Broadened to Account for Discourse Aspects) benchmark is an open-ended cloze task which consists of about 10,000 passages from BooksCorpus where a missing target word is predicted in the last sentence of each passage. The missing word is constrained to always be the last word of the last sentence and there are no candidate words to choose from. Examples were filtered by humans to ensure they were possible to guess given the context, i.e., the sentences in the passage leading up to the last sentence. Examples were further filtered to ensure that missing words could not be guessed without the context, ensuring that models attempting the dataset would need to reason over the entire paragraph to answer questions.",
            "S2 Citation Count (September 2023)": 261,
            "GitHub Stars": "",
            "GitHub Topics": [],
            "Text Topics": []
        },
        "Derived from Datasets": [
            "Book Corpus dataset"
        ],
        "Human Annotation": "Yes",
        "Bibtex": "@Article{Paperno2016TheLD,\n author = {Denis Paperno and Germán Kruszewski and Angeliki Lazaridou and Q. N. Pham and R. Bernardi and Sandro Pezzelle and Marco Baroni and Gemma Boleda and R. Fernández},\n booktitle = {Annual Meeting of the Association for Computational Linguistics},\n journal = {ArXiv},\n title = {The LAMBADA dataset: Word prediction requiring a broad discourse context},\n volume = {abs/1606.06031},\n year = {2016}\n}\n"
    },
    "fc-flan-math_dataset": {
        "Unique Dataset Identifier": "fc-flan-math_dataset",
        "Dataset Name": "math_dataset",
        "Collection": "Flan Collection (Flan 2021)",
        "Collection URL": "https://github.com/google-research/FLAN/tree/main/flan/v2",
        "Dataset URL": "https://github.com/deepmind/mathematics_dataset",
        "GitHub URL": "https://github.com/deepmind/mathematics_dataset",
        "Hugging Face URL": "https://huggingface.co/datasets/math_dataset",
        "Paper Title": "Analysing Mathematical Reasoning Abilities of Neural Models",
        "Papers with Code URL": "https://paperswithcode.com/dataset/mathematics",
        "ArXiv URL": "https://arxiv.org/abs/1904.01557",
        "Semantic Scholar Corpus ID": 85504763,
        "Languages": [
            "English"
        ],
        "Task Categories": [
            "Algebraic Expression Evaluation",
            "Inverted Mathematical QA"
        ],
        "Format": [
            "Zero-shot",
            "Few-shot"
        ],
        "Text Metrics": {
            "Num Dialogs": 108892,
            "Mean Inputs Length": 178.726,
            "Mean Targets Length": 1.9797,
            "Max Inputs Length": 546,
            "Max Targets Length": 4,
            "Min Inputs Length": 22,
            "Min Targets Length": 1,
            "Min Dialog Turns": 2,
            "Max Dialog Turns": 2,
            "Mean Dialog Turns": 2.0
        },
        "Text Sources": [
            "grammar-based"
        ],
        "Model Generated": [
            "Other"
        ],
        "Creators": [
            "DeepMind"
        ],
        "Licenses": [
            {
                "License": "Apache License 2.0",
                "License URL": "https://www.apache.org/licenses/LICENSE-2.0.txt"
            }
        ],
        "License Notes": "",
        "License Verified By": "Shayne",
        "Dataset Filter IDs": [
            "math_dataset/algebra__linear_1d:1.0.0"
        ],
        "Inferred Metadata": {
            "HF Dataset": "math_dataset",
            "HF Config": "algebra__linear_1d",
            "HF Config License": "",
            "HF Yaml License": "",
            "PwC License Name": "Apache License 2.0",
            "PwC License URL": "https://github.com/deepmind/mathematics_dataset/blob/master/LICENSE",
            "PwC Date": "2019-04-02",
            "S2 Date": "2019-04-02",
            "GitHub License": "Apache License 2.0",
            "Github Date": "",
            "HF Date": "2022-01-25",
            "HF Downloads (September 2023)": 26973,
            "HF Likes (September 2023)": 34,
            "PwC Description": "This dataset code generates mathematical question and answer pairs, from a range of question types at roughly school-level difficulty. This is designed to test the mathematical learning and algebraic reasoning skills of learning models.",
            "S2 Citation Count (September 2023)": 256,
            "GitHub Stars": 1643,
            "GitHub Topics": [],
            "Text Topics": []
        },
        "Derived from Datasets": [],
        "Human Annotation": "No",
        "Bibtex": "@Article{Saxton2019AnalysingMR,\n author = {D. Saxton and Edward Grefenstette and Felix Hill and Pushmeet Kohli},\n booktitle = {International Conference on Learning Representations},\n journal = {ArXiv},\n title = {Analysing Mathematical Reasoning Abilities of Neural Models},\n volume = {abs/1904.01557},\n year = {2019}\n}\n"
    },
    "fc-flan-mnli": {
        "Unique Dataset Identifier": "fc-flan-mnli",
        "Dataset Name": "mnli",
        "Collection": "Flan Collection (Flan 2021)",
        "Collection URL": "https://github.com/google-research/FLAN/tree/main/flan/v2",
        "Dataset URL": "https://cims.nyu.edu/~sbowman/multinli/",
        "GitHub URL": "",
        "Hugging Face URL": "https://huggingface.co/datasets/SetFit/mnli",
        "Paper Title": "A Broad-Coverage Challenge Corpus for Sentence Understanding through Inference",
        "Papers with Code URL": "https://paperswithcode.com/dataset/multinli",
        "ArXiv URL": "https://arxiv.org/abs/1704.05426",
        "Semantic Scholar Corpus ID": 3432876,
        "Languages": [
            "English"
        ],
        "Task Categories": [
            "Inverted Natural Language Inference",
            "Natural Language Inference"
        ],
        "Format": [
            "Zero-shot",
            "Few-shot"
        ],
        "Text Metrics": {
            "Num Dialogs": 216560,
            "Mean Inputs Length": 699.9516,
            "Mean Targets Length": 9.3879,
            "Max Inputs Length": 3738,
            "Max Targets Length": 34,
            "Min Inputs Length": 41,
            "Min Targets Length": 2,
            "Min Dialog Turns": 2,
            "Max Dialog Turns": 2,
            "Mean Dialog Turns": 2.0
        },
        "Text Sources": [
            "anc.org"
        ],
        "Model Generated": [],
        "Creators": [
            "New York University"
        ],
        "Licenses": [
            {
                "License": "OANC",
                "License URL": "https://cims.nyu.edu/~sbowman/multinli/paper.pdf"
            }
        ],
        "License Notes": "",
        "License Verified By": "Shayne",
        "Dataset Filter IDs": [
            "glue/mnli:2.0.0"
        ],
        "Inferred Metadata": {
            "HF Dataset": "SetFit/mnli",
            "HF Config": "SetFit--mnli",
            "HF Config License": "",
            "HF Yaml License": "",
            "PwC License Name": "Various",
            "PwC License URL": "https://cims.nyu.edu/~sbowman/multinli/paper.pdf",
            "PwC Date": "2018-01-01",
            "S2 Date": "2017-04-18",
            "GitHub License": "",
            "Github Date": "",
            "HF Date": "2022-02-28",
            "HF Downloads (September 2023)": 1430,
            "HF Likes (September 2023)": 2,
            "PwC Description": "The Multi-Genre Natural Language Inference (MultiNLI) dataset has 433K sentence pairs. Its size and mode of collection are modeled closely like SNLI. MultiNLI offers ten distinct genres (Face-to-face, Telephone, 9/11, Travel, Letters, Oxford University Press, Slate, Verbatim, Goverment and Fiction) of written and spoken English data. There are matched dev/test sets which are derived from the same sources as those in the training set, and mismatched sets which do not closely resemble any seen at training time.",
            "S2 Citation Count (September 2023)": 3105,
            "GitHub Stars": "",
            "GitHub Topics": [],
            "Text Topics": []
        },
        "Derived from Datasets": [],
        "Human Annotation": "Yes",
        "Bibtex": "@Article{Williams2017ABC,\n author = {Adina Williams and Nikita Nangia and Samuel R. Bowman},\n booktitle = {North American Chapter of the Association for Computational Linguistics},\n pages = {1112-1122},\n title = {A Broad-Coverage Challenge Corpus for Sentence Understanding through Inference},\n year = {2017}\n}\n"
    },
    "fc-flan-multi_news": {
        "Unique Dataset Identifier": "fc-flan-multi_news",
        "Dataset Name": "multi_news",
        "Collection": "Flan Collection (Flan 2021)",
        "Collection URL": "https://github.com/google-research/FLAN/tree/main/flan/v2",
        "Dataset URL": "https://github.com/Alex-Fabbri/Multi-News",
        "GitHub URL": "https://github.com/Alex-Fabbri/Multi-News",
        "Hugging Face URL": "https://huggingface.co/datasets/multi_news",
        "Paper Title": "Multi-News: a Large-Scale Multi-Document Summarization Dataset and Abstractive Hierarchical Model.",
        "Papers with Code URL": "https://paperswithcode.com/dataset/multi-news",
        "ArXiv URL": "https://arxiv.org/abs/1906.01749",
        "Semantic Scholar Corpus ID": 174799390,
        "Languages": [
            "English"
        ],
        "Task Categories": [
            "Summarization",
            "Inverted Summarization"
        ],
        "Format": [
            "Zero-shot",
            "Few-shot"
        ],
        "Text Metrics": {
            "Num Dialogs": 58937,
            "Mean Inputs Length": 8951.6456,
            "Mean Targets Length": 3381.8263,
            "Max Inputs Length": 1049066,
            "Max Targets Length": 280744,
            "Min Inputs Length": 158,
            "Min Targets Length": 190,
            "Min Dialog Turns": 2,
            "Max Dialog Turns": 2,
            "Mean Dialog Turns": 2.0
        },
        "Text Sources": [
            "news"
        ],
        "Model Generated": [],
        "Creators": [
            "Yale University"
        ],
        "Licenses": [
            {
                "License": "Custom",
                "License URL": "https://github.com/Alex-Fabbri/Multi-News/blob/master/LICENSE.txt"
            }
        ],
        "License Notes": "",
        "License Verified By": "Shayne",
        "Dataset Filter IDs": [
            "multi_news:1.0.0"
        ],
        "Inferred Metadata": {
            "HF Dataset": "multi_news",
            "HF Config": "default",
            "HF Config License": "Academic Research Purposes Only",
            "HF Yaml License": "Unspecified",
            "PwC License Name": "Custom",
            "PwC License URL": "https://github.com/Alex-Fabbri/Multi-News",
            "PwC Date": "2019-01-01",
            "S2 Date": "2019-06-04",
            "GitHub License": "",
            "Github Date": "",
            "HF Date": "2022-01-25",
            "HF Downloads (September 2023)": 6214,
            "HF Likes (September 2023)": 24,
            "PwC Description": "Multi-News, consists of news articles and human-written summaries of these articles from the site newser.com. Each summary is professionally written by editors and includes links to the original articles cited.",
            "S2 Citation Count (September 2023)": 330,
            "GitHub Stars": 248,
            "GitHub Topics": [
                "multi-document-summarization",
                "multi-news",
                "summarization"
            ],
            "Text Topics": []
        },
        "Derived from Datasets": [],
        "Human Annotation": "Yes",
        "Bibtex": "@Article{Fabbri2019MultiNewsAL,\n author = {Alexander R. Fabbri and Irene Li and Tianwei She and Suyi Li and Dragomir R. Radev},\n booktitle = {Annual Meeting of the Association for Computational Linguistics},\n pages = {1074-1084},\n title = {Multi-News: A Large-Scale Multi-Document Summarization Dataset and Abstractive Hierarchical Model},\n year = {2019}\n}\n"
    },
    "fc-flan-multirc": {
        "Unique Dataset Identifier": "fc-flan-multirc",
        "Dataset Name": "multirc",
        "Collection": "Flan Collection (Flan 2021)",
        "Collection URL": "https://github.com/google-research/FLAN/tree/main/flan/v2",
        "Dataset URL": "https://github.com/CogComp/multirc",
        "GitHub URL": "https://github.com/CogComp/multirc",
        "Hugging Face URL": "https://huggingface.co/datasets/stjokerli/TextToText_multirc_seqio",
        "Paper Title": "Looking Beyond the Surface: A Challenge Set for Reading Comprehension over Multiple Sentences",
        "Papers with Code URL": "https://paperswithcode.com/dataset/multirc",
        "ArXiv URL": "https://aclanthology.org/N18-1023/",
        "Semantic Scholar Corpus ID": 5112038,
        "Languages": [
            "English"
        ],
        "Task Categories": [
            "Inverted Multiple-Choice QA (no trivia knowledge required)",
            "Multiple Choice Question Answering",
            "Answer Verification"
        ],
        "Format": [
            "Zero-shot",
            "Few-shot"
        ],
        "Text Metrics": {
            "Num Dialogs": 97857,
            "Mean Inputs Length": 3965.1976,
            "Mean Targets Length": 17.9376,
            "Max Inputs Length": 9047,
            "Max Targets Length": 280,
            "Min Inputs Length": 550,
            "Min Targets Length": 2,
            "Min Dialog Turns": 2,
            "Max Dialog Turns": 2,
            "Mean Dialog Turns": 2.0
        },
        "Text Sources": [
            "crowdsourced",
            "cnn.com",
            "wikipedia.org",
            "wsj",
            "nytmes",
            "articles on society",
            "law and justice",
            "articles on history and anthropology",
            "9/11 reports",
            "project gutenberg",
            "mctest",
            "cmu movie plots",
            "ck12.org"
        ],
        "Model Generated": [],
        "Creators": [
            "University of Pennsylvania",
            "UC Santa Cruz",
            "Saarland University"
        ],
        "Licenses": [
            {
                "License": "Custom",
                "License URL": "https://github.com/CogComp/multirc/blob/master/LICENSE"
            }
        ],
        "License Notes": "",
        "License Verified By": "Shayne",
        "Dataset Filter IDs": [
            "super_glue/multirc:1.0.2"
        ],
        "Inferred Metadata": {
            "HF Dataset": "stjokerli/TextToText_multirc_seqio",
            "HF Config": "stjokerli--TextToText_multirc_seqio",
            "HF Config License": "",
            "HF Yaml License": "",
            "PwC License Name": "Academic Research Purposes Only",
            "PwC License URL": "",
            "PwC Date": "2018-01-01",
            "S2 Date": "2018-06-01",
            "GitHub License": "",
            "Github Date": "",
            "HF Date": "2022-03-13",
            "HF Downloads (September 2023)": 24,
            "HF Likes (September 2023)": 0,
            "PwC Description": "MultiRC (Multi-Sentence Reading Comprehension) is a dataset of short paragraphs and multi-sentence questions, i.e., questions that can be answered by combining information from multiple sentences of the paragraph.\nThe dataset was designed with three key challenges in mind:\n* The number of correct answer-options for each question is not pre-specified. This removes the over-reliance on answer-options and forces them to decide on the correctness of each candidate answer independently of others. In other words, the task is not to simply identify the best answer-option, but to evaluate the correctness of each answer-option individually.\n* The correct answer(s) is not required to be a span in the text.\n* The paragraphs in the dataset have diverse provenance by being extracted from 7 different domains such as news, fiction, historical text etc., and hence are expected to be more diverse in their contents as compared to single-domain datasets.\nThe entire corpus consists of around 10K questions (including about 6K multiple-sentence questions). The 60% of the data is released as training and development data. The rest of the data is saved for evaluation and every few months a new unseen additional data is included for evaluation to prevent unintentional overfitting over time.",
            "S2 Citation Count (September 2023)": 358,
            "GitHub Stars": 30,
            "GitHub Topics": [
                "natural-language-processing",
                "question-answering",
                "reasoning"
            ],
            "Text Topics": []
        },
        "Derived from Datasets": [],
        "Human Annotation": "Yes",
        "Bibtex": "@Article{Khashabi2018LookingBT,\n author = {Daniel Khashabi and Snigdha Chaturvedi and Michael Roth and Shyam Upadhyay and D. Roth},\n booktitle = {North American Chapter of the Association for Computational Linguistics},\n pages = {252-262},\n title = {Looking Beyond the Surface: A Challenge Set for Reading Comprehension over Multiple Sentences},\n year = {2018}\n}\n"
    },
    "fc-flan-natural_questions": {
        "Unique Dataset Identifier": "fc-flan-natural_questions",
        "Dataset Name": "natural_questions",
        "Collection": "Flan Collection (Flan 2021)",
        "Collection URL": "https://github.com/google-research/FLAN/tree/main/flan/v2",
        "Dataset URL": "https://github.com/google-research-datasets/natural-questions/tree/master/nq_open",
        "GitHub URL": "https://github.com/google-research-datasets/natural-questions/tree/master/nq_open",
        "Hugging Face URL": "https://huggingface.co/datasets/natural_questions",
        "Paper Title": "Natural Questions: A Benchmark for Question Answering Research",
        "Papers with Code URL": "https://paperswithcode.com/dataset/natural-questions",
        "ArXiv URL": "https://aclanthology.org/Q19-1026/?utm_campaign=NLP%20News&utm_medium=email&utm_source=Revue%20newsletter",
        "Semantic Scholar Corpus ID": 86611921,
        "Languages": [
            "English"
        ],
        "Task Categories": [
            "Closed-Book Question Answering",
            "Inverted Closed-Book QA",
            "Question Answering"
        ],
        "Format": [
            "Zero-shot",
            "Few-shot"
        ],
        "Text Metrics": {
            "Num Dialogs": 108494,
            "Mean Inputs Length": 208.3739,
            "Mean Targets Length": 12.82,
            "Max Inputs Length": 675,
            "Max Targets Length": 63,
            "Min Inputs Length": 30,
            "Min Targets Length": 1,
            "Min Dialog Turns": 2,
            "Max Dialog Turns": 2,
            "Mean Dialog Turns": 2.0
        },
        "Text Sources": [
            "wikipedia.org",
            "crowdsourced"
        ],
        "Model Generated": [],
        "Creators": [
            "Google Research"
        ],
        "Licenses": [
            {
                "License": "CC BY-SA 3.0",
                "License URL": "https://github.com/google-research-datasets/natural-questions/tree/master/nq_open"
            }
        ],
        "License Notes": "",
        "License Verified By": "Shayne",
        "Dataset Filter IDs": [
            "natural_questions_open:1.0.0"
        ],
        "Inferred Metadata": {
            "HF Dataset": "natural_questions",
            "HF Config": "default",
            "HF Config License": "",
            "HF Yaml License": "CC BY-SA 3.0",
            "PwC License Name": "CC BY-SA 3.0",
            "PwC License URL": "https://creativecommons.org/licenses/by-sa/3.0/",
            "PwC Date": "2019-01-01",
            "S2 Date": "2019-08-01",
            "GitHub License": "Apache License 2.0",
            "Github Date": "",
            "HF Date": "2022-01-25",
            "HF Downloads (September 2023)": 1225,
            "HF Likes (September 2023)": 18,
            "PwC Description": "The Natural Questions corpus is a question answering dataset containing 307,373 training examples, 7,830 development examples, and 7,842 test examples. Each example is comprised of a google.com query and a corresponding Wikipedia page. Each Wikipedia page has a passage (or long answer) annotated on the page that answers the question and one or more short spans from the annotated passage containing the actual answer. The long and the short answer annotations can however be empty. If they are both empty, then there is no answer on the page at all. If the long answer annotation is non-empty, but the short answer annotation is empty, then the annotated passage answers the question but no explicit short answer could be found. Finally 1% of the documents have a passage annotated with a short answer that is “yes” or “no”, instead of a list of short spans.",
            "S2 Citation Count (September 2023)": 1523,
            "GitHub Stars": 821,
            "GitHub Topics": [],
            "Text Topics": []
        },
        "Derived from Datasets": [],
        "Human Annotation": "Yes",
        "Bibtex": "@Article{Kwiatkowski2019NaturalQA,\n author = {T. Kwiatkowski and J. Palomaki and Olivia Redfield and Michael Collins and Ankur P. Parikh and Chris Alberti and D. Epstein and Illia Polosukhin and Jacob Devlin and Kenton Lee and Kristina Toutanova and Llion Jones and Matthew Kelcey and Ming-Wei Chang and Andrew M. Dai and Jakob Uszkoreit and Quoc V. Le and Slav Petrov},\n booktitle = {Transactions of the Association for Computational Linguistics},\n journal = {Transactions of the Association for Computational Linguistics},\n pages = {453-466},\n title = {Natural Questions: A Benchmark for Question Answering Research},\n volume = {7},\n year = {2019}\n}\n"
    },
    "fc-flan-newsroom": {
        "Unique Dataset Identifier": "fc-flan-newsroom",
        "Dataset Name": "newsroom",
        "Collection": "Flan Collection (Flan 2021)",
        "Collection URL": "https://github.com/google-research/FLAN/tree/main/flan/v2",
        "Dataset URL": "https://lil.nlp.cornell.edu/newsroom/",
        "GitHub URL": "",
        "Hugging Face URL": "https://huggingface.co/datasets/newsroom",
        "Paper Title": "Newsroom: A Dataset of 1.3 Million Summaries with Diverse Extractive Strategies",
        "Papers with Code URL": "https://paperswithcode.com/dataset/newsroom",
        "ArXiv URL": "https://arxiv.org/abs/1804.11283",
        "Semantic Scholar Corpus ID": 13752552,
        "Languages": [
            "English"
        ],
        "Task Categories": [
            "Summarization",
            "Inverted Summarization"
        ],
        "Format": [
            "Zero-shot",
            "Few-shot"
        ],
        "Text Metrics": {
            "Num Dialogs": 108907,
            "Mean Inputs Length": 4951.5759,
            "Mean Targets Length": 338.7905,
            "Max Inputs Length": 196852,
            "Max Targets Length": 117793,
            "Min Inputs Length": 35,
            "Min Targets Length": 0,
            "Min Dialog Turns": 2,
            "Max Dialog Turns": 2,
            "Mean Dialog Turns": 2.0
        },
        "Text Sources": [
            "undisclosed web"
        ],
        "Model Generated": [],
        "Creators": [
            "Cornell University"
        ],
        "Licenses": [
            {
                "License": "Unspecified",
                "License URL": "Unspecified"
            }
        ],
        "License Notes": "",
        "License Verified By": "Shayne",
        "Dataset Filter IDs": [
            "newsroom:1.0.0"
        ],
        "Inferred Metadata": {
            "HF Dataset": "newsroom",
            "HF Config": "default",
            "HF Config License": "",
            "HF Yaml License": "Unspecified",
            "PwC License Name": "Non Commercial",
            "PwC License URL": "https://cornell.qualtrics.com/jfe/form/SV_6YA3HQ2p75XH4IR",
            "PwC Date": "2018-04-30",
            "S2 Date": "2018-04-30",
            "GitHub License": "",
            "Github Date": "",
            "HF Date": "2022-01-25",
            "HF Downloads (September 2023)": 810,
            "HF Likes (September 2023)": 6,
            "PwC Description": "CORNELL NEWSROOM is a large dataset for training and evaluating summarization systems. It contains 1.3 million articles and summaries written by authors and editors in the newsrooms of 38 major publications. The summaries are obtained from search and social metadata between 1998 and 2017 and use a variety of summarization strategies combining extraction and abstraction.",
            "S2 Citation Count (September 2023)": 409,
            "GitHub Stars": "",
            "GitHub Topics": [],
            "Text Topics": []
        },
        "Derived from Datasets": [],
        "Human Annotation": "No",
        "Bibtex": "@Article{Grusky2018NewsroomAD,\n author = {Max Grusky and Mor Naaman and Yoav Artzi},\n booktitle = {North American Chapter of the Association for Computational Linguistics},\n pages = {708-719},\n title = {Newsroom: A Dataset of 1.3 Million Summaries with Diverse Extractive Strategies},\n year = {2018}\n}\n"
    },
    "fc-flan-openbookqa": {
        "Unique Dataset Identifier": "fc-flan-openbookqa",
        "Dataset Name": "openbookqa",
        "Collection": "Flan Collection (Flan 2021)",
        "Collection URL": "https://github.com/google-research/FLAN/tree/main/flan/v2",
        "Dataset URL": "https://allenai.org/data/open-book-qa",
        "GitHub URL": "",
        "Hugging Face URL": "https://huggingface.co/datasets/openbookqa",
        "Paper Title": "Can a Suit of Armor Conduct Electricity? A New Dataset for Open Book Question Answering",
        "Papers with Code URL": "https://paperswithcode.com/dataset/openbookqa",
        "ArXiv URL": "https://arxiv.org/abs/1809.02789",
        "Semantic Scholar Corpus ID": 52183757,
        "Languages": [
            "English"
        ],
        "Task Categories": [
            "Inverted Multiple-Choice QA (no trivia knowledge required)",
            "Multiple Choice Question Answering",
            "Sentence Composition"
        ],
        "Format": [
            "Zero-shot",
            "Few-shot"
        ],
        "Text Metrics": {
            "Num Dialogs": 18108,
            "Mean Inputs Length": 383.7989,
            "Mean Targets Length": 33.6263,
            "Max Inputs Length": 1642,
            "Max Targets Length": 150,
            "Min Inputs Length": 22,
            "Min Targets Length": 1,
            "Min Dialog Turns": 2,
            "Max Dialog Turns": 2,
            "Mean Dialog Turns": 2.0
        },
        "Text Sources": [
            "web exams"
        ],
        "Model Generated": [],
        "Creators": [
            "AI2",
            "Research Training Group AIPHES & Heidelberg University"
        ],
        "Licenses": [
            {
                "License": "Apache License 2.0",
                "License URL": "https://github.com/allenai/OpenBookQA/blob/main/LICENSE"
            }
        ],
        "License Notes": "",
        "License Verified By": "Shayne",
        "Dataset Filter IDs": [
            "openbookqa:0.1.0"
        ],
        "Inferred Metadata": {
            "HF Dataset": "openbookqa",
            "HF Config": "main",
            "HF Config License": "",
            "HF Yaml License": "Unspecified",
            "PwC License Name": "Custom",
            "PwC License URL": "https://github.com/allenai/OpenBookQA",
            "PwC Date": "2018-01-01",
            "S2 Date": "2018-08-13",
            "GitHub License": "",
            "Github Date": "",
            "HF Date": "2022-01-25",
            "HF Downloads (September 2023)": 33711,
            "HF Likes (September 2023)": 23,
            "PwC Description": "OpenBookQA is a new kind of question-answering dataset modeled after open book exams for assessing human understanding of a subject. It consists of 5,957 multiple-choice elementary-level science questions (4,957 train, 500 dev, 500 test), which probe the understanding of a small “book” of 1,326 core science facts and the application of these facts to novel situations. For training, the dataset includes a mapping from each question to the core science fact it was designed to probe. Answering OpenBookQA questions requires additional broad common knowledge, not contained in the book. The questions, by design, are answered incorrectly by both a retrieval-based algorithm and a word co-occurrence algorithm.\nAdditionally, the dataset includes a collection of 5,167 crowd-sourced common knowledge facts, and an expanded version of the train/dev/test questions where each question is associated with its originating core fact, a human accuracy score, a clarity score, and an anonymized crowd-worker ID.",
            "S2 Citation Count (September 2023)": 467,
            "GitHub Stars": "",
            "GitHub Topics": [],
            "Text Topics": []
        },
        "Derived from Datasets": [
            "WorldTree dataset"
        ],
        "Human Annotation": "Yes",
        "Bibtex": "@Article{Mihaylov2018CanAS,\n author = {Todor Mihaylov and Peter Clark and Tushar Khot and Ashish Sabharwal},\n booktitle = {Conference on Empirical Methods in Natural Language Processing},\n pages = {2381-2391},\n title = {Can a Suit of Armor Conduct Electricity? A New Dataset for Open Book Question Answering},\n year = {2018}\n}\n"
    },
    "fc-flan-opinion_abstracts_idebate": {
        "Unique Dataset Identifier": "fc-flan-opinion_abstracts_idebate",
        "Dataset Name": "opinion_abstracts_idebate",
        "Collection": "Flan Collection (Flan 2021)",
        "Collection URL": "https://github.com/google-research/FLAN/tree/main/flan/v2",
        "Dataset URL": "https://web.eecs.umich.edu/~wangluxy/data.html",
        "GitHub URL": "",
        "Hugging Face URL": "https://huggingface.co/datasets/ccdv/govreport-summarization",
        "Paper Title": "Efficient Attentions for Long Document Summarization",
        "Papers with Code URL": "https://paperswithcode.com/dataset/govreport",
        "ArXiv URL": "https://arxiv.org/abs/2104.02112",
        "Semantic Scholar Corpus ID": 233033613,
        "Languages": [
            "English"
        ],
        "Task Categories": [
            "Summarization",
            "Inverted Summarization"
        ],
        "Format": [
            "Zero-shot",
            "Few-shot"
        ],
        "Text Metrics": {
            "Num Dialogs": 5914,
            "Mean Inputs Length": 2232.2335,
            "Mean Targets Length": 238.1613,
            "Max Inputs Length": 8695,
            "Max Targets Length": 2546,
            "Min Inputs Length": 65,
            "Min Targets Length": 19,
            "Min Dialog Turns": 2,
            "Max Dialog Turns": 2,
            "Mean Dialog Turns": 2.0
        },
        "Text Sources": [
            "u.s. government reports"
        ],
        "Model Generated": [],
        "Creators": [
            "University of Michigan",
            "University of Illinois"
        ],
        "Licenses": [
            {
                "License": "Unspecified",
                "License URL": "Unspecified"
            }
        ],
        "License Notes": "",
        "License Verified By": "Shayne",
        "Dataset Filter IDs": [
            "opinion_abstracts_idebate"
        ],
        "Inferred Metadata": {
            "HF Dataset": "ccdv/govreport-summarization",
            "HF Config": "document",
            "HF Config License": "",
            "HF Yaml License": "",
            "PwC License Name": "Unspecified",
            "PwC License URL": "",
            "PwC Date": "2021-04-05",
            "S2 Date": "2021-04-05",
            "GitHub License": "",
            "Github Date": "",
            "HF Date": "2021-11-28",
            "HF Downloads (September 2023)": 1241,
            "HF Likes (September 2023)": 9,
            "PwC Description": "GovReport is a dataset for long document summarization, with significantly longer documents and summaries. It consists of reports written by government research agencies including Congressional Research Service and U.S. Government Accountability Office.\n\nCompared with other long document summarization datasets, government report dataset has longer summaries and documents and requires reading in more context to cover salient words to be summarized.",
            "S2 Citation Count (September 2023)": 99,
            "GitHub Stars": "",
            "GitHub Topics": [],
            "Text Topics": [
                "Politics",
                "Religion",
                "Education",
                "International relations and diplomacy",
                "Economics",
                "Freedom of speech and expression",
                "International relations",
                "Ethics and morality",
                "Sports",
                "Education policy"
            ]
        },
        "Derived from Datasets": [],
        "Human Annotation": "Yes",
        "Bibtex": "@Article{Huang2021EfficientAF,\n author = {L. Huang and Shuyang Cao and Nikolaus Nova Parulian and Heng Ji and Lu Wang},\n booktitle = {North American Chapter of the Association for Computational Linguistics},\n journal = {ArXiv},\n title = {Efficient Attentions for Long Document Summarization},\n volume = {abs/2104.02112},\n year = {2021}\n}\n"
    },
    "fc-flan-opinion_abstracts_rotten_tomatoes": {
        "Unique Dataset Identifier": "fc-flan-opinion_abstracts_rotten_tomatoes",
        "Dataset Name": "opinion_abstracts_rotten_tomatoes",
        "Collection": "Flan Collection (Flan 2021)",
        "Collection URL": "https://github.com/google-research/FLAN/tree/main/flan/v2",
        "Dataset URL": "https://web.eecs.umich.edu/~wangluxy/data.html",
        "GitHub URL": "",
        "Hugging Face URL": "https://huggingface.co/datasets/ccdv/govreport-summarization",
        "Paper Title": "Efficient Attentions for Long Document Summarization",
        "Papers with Code URL": "https://paperswithcode.com/dataset/govreport",
        "ArXiv URL": "https://arxiv.org/abs/2104.02112",
        "Semantic Scholar Corpus ID": 233033613,
        "Languages": [
            "English"
        ],
        "Task Categories": [
            "Summarization",
            "Inverted Summarization"
        ],
        "Format": [
            "Zero-shot",
            "Few-shot"
        ],
        "Text Metrics": {
            "Num Dialogs": 11415,
            "Mean Inputs Length": 2893.4605,
            "Mean Targets Length": 367.9555,
            "Max Inputs Length": 7948,
            "Max Targets Length": 1978,
            "Min Inputs Length": 45,
            "Min Targets Length": 3,
            "Min Dialog Turns": 2,
            "Max Dialog Turns": 2,
            "Mean Dialog Turns": 2.0
        },
        "Text Sources": [
            "u.s. government reports"
        ],
        "Model Generated": [],
        "Creators": [
            "University of Michigan",
            "University of Illinois"
        ],
        "Licenses": [
            {
                "License": "Unspecified",
                "License URL": "Unspecified"
            }
        ],
        "License Notes": "",
        "License Verified By": "Shayne",
        "Dataset Filter IDs": [
            "opinion_abstracts_rotten_tomatoes"
        ],
        "Inferred Metadata": {
            "HF Dataset": "ccdv/govreport-summarization",
            "HF Config": "document",
            "HF Config License": "",
            "HF Yaml License": "",
            "PwC License Name": "Unspecified",
            "PwC License URL": "",
            "PwC Date": "2021-04-05",
            "S2 Date": "2021-04-05",
            "GitHub License": "",
            "Github Date": "",
            "HF Date": "2021-11-28",
            "HF Downloads (September 2023)": 1241,
            "HF Likes (September 2023)": 9,
            "PwC Description": "GovReport is a dataset for long document summarization, with significantly longer documents and summaries. It consists of reports written by government research agencies including Congressional Research Service and U.S. Government Accountability Office.\n\nCompared with other long document summarization datasets, government report dataset has longer summaries and documents and requires reading in more context to cover salient words to be summarized.",
            "S2 Citation Count (September 2023)": 99,
            "GitHub Stars": "",
            "GitHub Topics": [],
            "Text Topics": [
                "Comedy",
                "Film criticism",
                "Film analysis",
                "Acting performance",
                "Acting and performance",
                "Movie reviews",
                "Action movies",
                "Film critique",
                "Film analysis and critique"
            ]
        },
        "Derived from Datasets": [],
        "Human Annotation": "Yes",
        "Bibtex": "@Article{Huang2021EfficientAF,\n author = {L. Huang and Shuyang Cao and Nikolaus Nova Parulian and Heng Ji and Lu Wang},\n booktitle = {North American Chapter of the Association for Computational Linguistics},\n journal = {ArXiv},\n title = {Efficient Attentions for Long Document Summarization},\n volume = {abs/2104.02112},\n year = {2021}\n}\n"
    },
    "fc-flan-para_crawl_enes": {
        "Unique Dataset Identifier": "fc-flan-para_crawl_enes",
        "Dataset Name": "para_crawl_enes",
        "Collection": "Flan Collection (Flan 2021)",
        "Collection URL": "https://github.com/google-research/FLAN/tree/main/flan/v2",
        "Dataset URL": "https://www.paracrawl.eu/",
        "GitHub URL": "",
        "Hugging Face URL": "https://huggingface.co/datasets/yawnick/para_crawl_enen",
        "Paper Title": "ParaCrawl: Web-Scale Acquisition of Parallel Corpora",
        "Papers with Code URL": "https://paperswithcode.com/dataset/paracrawl",
        "ArXiv URL": "https://aclanthology.org/2020.acl-main.417/",
        "Semantic Scholar Corpus ID": 219165306,
        "Languages": [
            "English"
        ],
        "Task Categories": [
            "Translation",
            "Inverted Translation"
        ],
        "Format": [
            "Zero-shot",
            "Few-shot"
        ],
        "Text Metrics": {
            "Num Dialogs": 108176,
            "Mean Inputs Length": 607.8348,
            "Mean Targets Length": 136.6116,
            "Max Inputs Length": 4112,
            "Max Targets Length": 1020,
            "Min Inputs Length": 28,
            "Min Targets Length": 13,
            "Min Dialog Turns": 2,
            "Max Dialog Turns": 2,
            "Mean Dialog Turns": 2.0
        },
        "Text Sources": [
            "undisclosed web"
        ],
        "Model Generated": [],
        "Creators": [
            "University of Edinburgh",
            "University of Alicante",
            "Johns Hopkins University",
            "Omniscien Technologies"
        ],
        "Licenses": [
            {
                "License": "CC0 1.0",
                "License URL": "https://paracrawl.eu/index.php"
            }
        ],
        "License Notes": "",
        "License Verified By": "Shayne",
        "Dataset Filter IDs": [
            "para_crawl_enes"
        ],
        "Inferred Metadata": {
            "HF Dataset": "yawnick/para_crawl_enen",
            "HF Config": "yawnick--para_crawl_enen",
            "HF Config License": "",
            "HF Yaml License": "",
            "PwC License Name": "CC0 1.0",
            "PwC License URL": "https://creativecommons.org/share-your-work/public-domain/cc0/",
            "PwC Date": "2020-07-01",
            "S2 Date": "2020-07-01",
            "GitHub License": "",
            "Github Date": "",
            "HF Date": "2023-04-28",
            "HF Downloads (September 2023)": 20,
            "HF Likes (September 2023)": 0,
            "PwC Description": "ParaCrawl v.7.1 is a parallel dataset with 41 language pairs primarily aligned with English (39 out of 41) and mined using the parallel-data-crawling tool Bitextor which includes downloading documents, preprocessing and normalization, aligning documents and segments, and filtering noisy data via Bicleaner. ParaCrawl focuses on European languages, but also includes 9 lower-resource, non-European language pairs in v7.1.",
            "S2 Citation Count (September 2023)": 141,
            "GitHub Stars": "",
            "GitHub Topics": [],
            "Text Topics": [
                "Religion",
                "Cultural differences",
                "Translation",
                "Philosophy",
                "Travel",
                "Education",
                "History",
                "Technology"
            ]
        },
        "Derived from Datasets": [],
        "Human Annotation": "No",
        "Bibtex": "@Article{Bañón2020ParaCrawlWA,\n author = {Marta Bañón and Pinzhen Chen and B. Haddow and Kenneth Heafield and Hieu D. Hoang and Miquel Esplà-GomisF and Mikel ForcadaF and Amir Kamran and Faheem Kirefu and Philipp Koehn and Sergio Ortiz-Rojas and Leopoldo PlaF and Gema Ramírez-Sánchez and Elsa Sarrı́asF and Marek Střelec and Brian Thompson and W. Waites and Dion WigginsN and Jaume Zaragoza},\n booktitle = {Annual Meeting of the Association for Computational Linguistics},\n pages = {4555-4567},\n title = {ParaCrawl: Web-Scale Acquisition of Parallel Corpora},\n year = {2020}\n}\n"
    },
    "fc-flan-paws_wiki": {
        "Unique Dataset Identifier": "fc-flan-paws_wiki",
        "Dataset Name": "paws_wiki",
        "Collection": "Flan Collection (Flan 2021)",
        "Collection URL": "https://github.com/google-research/FLAN/tree/main/flan/v2",
        "Dataset URL": "https://github.com/google-research-datasets/paws",
        "GitHub URL": "https://github.com/google-research-datasets/paws",
        "Hugging Face URL": "https://huggingface.co/datasets/paws",
        "Paper Title": "PAWS: Paraphrase Adversaries from Word Scrambling",
        "Papers with Code URL": "https://paperswithcode.com/dataset/paws",
        "ArXiv URL": "https://arxiv.org/abs/1904.01130",
        "Semantic Scholar Corpus ID": 91184042,
        "Languages": [
            "English"
        ],
        "Task Categories": [
            "Inverted Coreference Resolution",
            "Paraphrase Detection"
        ],
        "Format": [
            "Zero-shot",
            "Few-shot"
        ],
        "Text Metrics": {
            "Num Dialogs": 109019,
            "Mean Inputs Length": 751.2153,
            "Mean Targets Length": 2.7706,
            "Max Inputs Length": 2317,
            "Max Targets Length": 5,
            "Min Inputs Length": 73,
            "Min Targets Length": 2,
            "Min Dialog Turns": 2,
            "Max Dialog Turns": 2,
            "Mean Dialog Turns": 2.0
        },
        "Text Sources": [
            "quora",
            "wikipedia.org"
        ],
        "Model Generated": [
            "Other"
        ],
        "Creators": [
            "Google Research"
        ],
        "Licenses": [
            {
                "License": "Custom",
                "License URL": "https://github.com/google-research-datasets/paws"
            }
        ],
        "License Notes": "",
        "License Verified By": "Shayne",
        "Dataset Filter IDs": [
            "paws_wiki:1.1.0"
        ],
        "Inferred Metadata": {
            "HF Dataset": "paws",
            "HF Config": "labeled_final",
            "HF Config License": "All Uses-Attribution",
            "HF Yaml License": "",
            "PwC License Name": "Custom",
            "PwC License URL": "https://github.com/google-research-datasets/paws",
            "PwC Date": "2019-04-01",
            "S2 Date": "2019-04-01",
            "GitHub License": "",
            "Github Date": "",
            "HF Date": "2022-01-25",
            "HF Downloads (September 2023)": 9253,
            "HF Likes (September 2023)": 16,
            "PwC Description": "Paraphrase Adversaries from Word Scrambling (PAWS) is a dataset contains 108,463 human-labeled and 656k noisily labeled pairs that feature the importance of modeling structure, context, and word order information for the problem of paraphrase identification. The dataset has two subsets, one based on Wikipedia and the other one based on the Quora Question Pairs (QQP) dataset.",
            "S2 Citation Count (September 2023)": 367,
            "GitHub Stars": 498,
            "GitHub Topics": [],
            "Text Topics": []
        },
        "Derived from Datasets": [],
        "Human Annotation": "Yes",
        "Bibtex": "@Article{Zhang2019PAWSPA,\n author = {Yuan Zhang and Jason Baldridge and Luheng He},\n booktitle = {North American Chapter of the Association for Computational Linguistics},\n journal = {ArXiv},\n title = {PAWS: Paraphrase Adversaries from Word Scrambling},\n volume = {abs/1904.01130},\n year = {2019}\n}\n"
    },
    "fc-flan-piqa": {
        "Unique Dataset Identifier": "fc-flan-piqa",
        "Dataset Name": "piqa",
        "Collection": "Flan Collection (Flan 2021)",
        "Collection URL": "https://github.com/google-research/FLAN/tree/main/flan/v2",
        "Dataset URL": "https://yonatanbisk.com/piqa/",
        "GitHub URL": "",
        "Hugging Face URL": "https://huggingface.co/datasets/piqa",
        "Paper Title": "PIQA: Reasoning about Physical Commonsense in Natural Language",
        "Papers with Code URL": "https://paperswithcode.com/dataset/piqa",
        "ArXiv URL": "https://arxiv.org/abs/1911.11641",
        "Semantic Scholar Corpus ID": 208290939,
        "Languages": [
            "English"
        ],
        "Task Categories": [
            "Inverted Multiple-Choice QA (no trivia knowledge required)",
            "Multiple Choice Question Answering",
            "Wrong Candidate Generation"
        ],
        "Format": [
            "Zero-shot",
            "Few-shot"
        ],
        "Text Metrics": {
            "Num Dialogs": 57751,
            "Mean Inputs Length": 659.7451,
            "Mean Targets Length": 71.7193,
            "Max Inputs Length": 7645,
            "Max Targets Length": 2183,
            "Min Inputs Length": 23,
            "Min Targets Length": 2,
            "Min Dialog Turns": 2,
            "Max Dialog Turns": 2,
            "Mean Dialog Turns": 2.0
        },
        "Text Sources": [
            "instructables.com",
            "crowdsourced"
        ],
        "Model Generated": [],
        "Creators": [
            "AI2",
            "Microsoft Research",
            "Carnegie Mellon University"
        ],
        "Licenses": [
            {
                "License": "Academic Free License v3.0",
                "License URL": "https://opensource.org/licenses/AFL-3.0"
            }
        ],
        "License Notes": "",
        "License Verified By": "Shayne",
        "Dataset Filter IDs": [
            "piqa:1.0.0"
        ],
        "Inferred Metadata": {
            "HF Dataset": "piqa",
            "HF Config": "plain_text",
            "HF Config License": "",
            "HF Yaml License": "Unspecified",
            "PwC License Name": "Unspecified",
            "PwC License URL": "",
            "PwC Date": "2019-11-26",
            "S2 Date": "2019-11-26",
            "GitHub License": "",
            "Github Date": "",
            "HF Date": "2022-01-25",
            "HF Downloads (September 2023)": 186395,
            "HF Likes (September 2023)": 41,
            "PwC Description": "PIQA is a dataset for commonsense reasoning, and was created to investigate the physical knowledge of existing models in NLP.",
            "S2 Citation Count (September 2023)": 384,
            "GitHub Stars": "",
            "GitHub Topics": [],
            "Text Topics": []
        },
        "Derived from Datasets": [],
        "Human Annotation": "Yes",
        "Bibtex": "@Article{Bisk2019PIQARA,\n author = {Yonatan Bisk and Rowan Zellers and Ronan Le Bras and Jianfeng Gao and Yejin Choi},\n booktitle = {AAAI Conference on Artificial Intelligence},\n journal = {ArXiv},\n title = {PIQA: Reasoning about Physical Commonsense in Natural Language},\n volume = {abs/1911.11641},\n year = {2019}\n}\n"
    },
    "fc-flan-qnli": {
        "Unique Dataset Identifier": "fc-flan-qnli",
        "Dataset Name": "qnli",
        "Collection": "Flan Collection (Flan 2021)",
        "Collection URL": "https://github.com/google-research/FLAN/tree/main/flan/v2",
        "Dataset URL": "https://rajpurkar.github.io/SQuAD-explorer/",
        "GitHub URL": "https://rajpurkar.github.io/SQuAD-explorer/",
        "Hugging Face URL": "https://huggingface.co/datasets/SetFit/qnli",
        "Paper Title": "GLUE: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding",
        "Papers with Code URL": "https://paperswithcode.com/dataset/qnli",
        "ArXiv URL": "https://arxiv.org/abs/1804.07461",
        "Semantic Scholar Corpus ID": 5034059,
        "Languages": [
            "English"
        ],
        "Task Categories": [
            "Inverted Multiple-Choice QA (no trivia knowledge required)",
            "Multiple Choice Question Answering"
        ],
        "Format": [
            "Zero-shot",
            "Few-shot"
        ],
        "Text Metrics": {
            "Num Dialogs": 108853,
            "Mean Inputs Length": 693.3264,
            "Mean Targets Length": 10.0916,
            "Max Inputs Length": 4383,
            "Max Targets Length": 256,
            "Min Inputs Length": 50,
            "Min Targets Length": 2,
            "Min Dialog Turns": 2,
            "Max Dialog Turns": 2,
            "Mean Dialog Turns": 2.0
        },
        "Text Sources": [
            "videos",
            "glosses",
            "forum posts",
            "twitter",
            "student answers",
            "wikipedia.org",
            "image descriptions"
        ],
        "Model Generated": [],
        "Creators": [
            "New York University",
            "University of Washington",
            "DeepMind"
        ],
        "Licenses": [
            {
                "License": "CC BY-SA 4.0",
                "License URL": "https://creativecommons.org/licenses/by-sa/4.0/legalcode"
            }
        ],
        "License Notes": "",
        "License Verified By": "Shayne",
        "Dataset Filter IDs": [
            "glue/qnli:2.0.0"
        ],
        "Inferred Metadata": {
            "HF Dataset": "SetFit/qnli",
            "HF Config": "SetFit--qnli",
            "HF Config License": "",
            "HF Yaml License": "",
            "PwC License Name": "CC BY-SA 4.0",
            "PwC License URL": "",
            "PwC Date": "2019-01-01",
            "S2 Date": "",
            "GitHub License": "",
            "Github Date": "",
            "HF Date": "2022-02-28",
            "HF Downloads (September 2023)": 612,
            "HF Likes (September 2023)": 0,
            "PwC Description": "The QNLI (Question-answering NLI) dataset is a Natural Language Inference dataset automatically derived from the Stanford Question Answering Dataset v1.1 (SQuAD). SQuAD v1.1 consists of question-paragraph pairs, where one of the sentences in the paragraph (drawn from Wikipedia) contains the answer to the corresponding question (written by an annotator). The dataset was converted into sentence pair classification by forming a pair between each question and each sentence in the corresponding context, and filtering out pairs with low lexical overlap between the question and the context sentence. The task is to determine whether the context sentence contains the answer to the question. This modified version of the original task removes the requirement that the model select the exact answer, but also removes the simplifying assumptions that the answer is always present in the input and that lexical overlap is a reliable cue. The QNLI dataset is part of GLEU benchmark.",
            "S2 Citation Count (September 2023)": "",
            "GitHub Stars": "",
            "GitHub Topics": [],
            "Text Topics": []
        },
        "Derived from Datasets": [],
        "Human Annotation": "Yes",
        "Bibtex": "@Article{Wang2018GLUEAM,\n author = {Alex Wang and Amanpreet Singh and Julian Michael and Felix Hill and Omer Levy and Samuel R. Bowman},\n booktitle = {BlackboxNLP@EMNLP},\n pages = {353-355},\n title = {GLUE: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding},\n year = {2018}\n}\n"
    },
    "fc-flan-quac": {
        "Unique Dataset Identifier": "fc-flan-quac",
        "Dataset Name": "quac",
        "Collection": "Flan Collection (Flan 2021)",
        "Collection URL": "https://github.com/google-research/FLAN/tree/main/flan/v2",
        "Dataset URL": "https://quac.ai/",
        "GitHub URL": "",
        "Hugging Face URL": "https://huggingface.co/datasets/quac",
        "Paper Title": "QuAC: Question Answering in Context",
        "Papers with Code URL": "https://paperswithcode.com/dataset/quac",
        "ArXiv URL": "https://arxiv.org/abs/1808.07036",
        "Semantic Scholar Corpus ID": 52057510,
        "Languages": [
            "English"
        ],
        "Task Categories": [
            "Inverted Multiple-Choice QA (no trivia knowledge required)",
            "Multiple Choice Question Answering"
        ],
        "Format": [
            "Zero-shot",
            "Few-shot"
        ],
        "Text Metrics": {
            "Num Dialogs": 108711,
            "Mean Inputs Length": 4641.699,
            "Mean Targets Length": 68.0846,
            "Max Inputs Length": 10700,
            "Max Targets Length": 238,
            "Min Inputs Length": 1487,
            "Min Targets Length": 0,
            "Min Dialog Turns": 2,
            "Max Dialog Turns": 2,
            "Mean Dialog Turns": 2.0
        },
        "Text Sources": [
            "crowdsourced"
        ],
        "Model Generated": [],
        "Creators": [
            "AI2",
            "University of Washington",
            "Stanford University",
            "UMass Amherst"
        ],
        "Licenses": [
            {
                "License": "CC BY-SA 4.0",
                "License URL": "https://quac.ai/"
            }
        ],
        "License Notes": "",
        "License Verified By": "Shayne",
        "Dataset Filter IDs": [
            "quac:1.0.0"
        ],
        "Inferred Metadata": {
            "HF Dataset": "quac",
            "HF Config": "plain_text",
            "HF Config License": "MIT License",
            "HF Yaml License": "MIT License",
            "PwC License Name": "CC BY-SA 4.0",
            "PwC License URL": "http://creativecommons.org/licenses/by-sa/4.0/legalcode",
            "PwC Date": "2018-01-01",
            "S2 Date": "2018-08-21",
            "GitHub License": "",
            "Github Date": "",
            "HF Date": "2022-01-25",
            "HF Downloads (September 2023)": 1313,
            "HF Likes (September 2023)": 11,
            "PwC Description": "Question Answering in Context is a large-scale dataset that consists of around 14K crowdsourced Question Answering dialogs with 98K question-answer pairs in total. Data instances consist of an interactive dialog between two crowd workers: (1) a student who poses a sequence of freeform questions to learn as much as possible about a hidden Wikipedia text, and (2) a teacher who answers the questions by providing short excerpts (spans) from the text.",
            "S2 Citation Count (September 2023)": 596,
            "GitHub Stars": "",
            "GitHub Topics": [],
            "Text Topics": []
        },
        "Derived from Datasets": [],
        "Human Annotation": "Yes",
        "Bibtex": "@Article{Choi2018QuACQA,\n author = {Eunsol Choi and He He and Mohit Iyyer and Mark Yatskar and Wen-tau Yih and Yejin Choi and Percy Liang and Luke Zettlemoyer},\n booktitle = {Conference on Empirical Methods in Natural Language Processing},\n pages = {2174-2184},\n title = {QuAC: Question Answering in Context},\n year = {2018}\n}\n"
    },
    "fc-flan-record": {
        "Unique Dataset Identifier": "fc-flan-record",
        "Dataset Name": "record",
        "Collection": "Flan Collection (Flan 2021)",
        "Collection URL": "https://github.com/google-research/FLAN/tree/main/flan/v2",
        "Dataset URL": "https://sheng-z.github.io/ReCoRD-explorer/",
        "GitHub URL": "https://sheng-z.github.io/ReCoRD-explorer/",
        "Hugging Face URL": "https://huggingface.co/datasets/super_glue",
        "Paper Title": "ReCoRD: Bridging the Gap between Human and Machine Commonsense Reading Comprehension",
        "Papers with Code URL": "https://paperswithcode.com/dataset/record",
        "ArXiv URL": "https://arxiv.org/abs/1810.12885",
        "Semantic Scholar Corpus ID": 53116244,
        "Languages": [
            "English"
        ],
        "Task Categories": [
            "Span Selection Question Answering",
            "Inverted Extractive QA",
            "Question Answering"
        ],
        "Format": [
            "Zero-shot",
            "Few-shot"
        ],
        "Text Metrics": {
            "Num Dialogs": 108413,
            "Mean Inputs Length": 2967.3341,
            "Mean Targets Length": 75.6686,
            "Max Inputs Length": 10209,
            "Max Targets Length": 561,
            "Min Inputs Length": 543,
            "Min Targets Length": 2,
            "Min Dialog Turns": 2,
            "Max Dialog Turns": 2,
            "Mean Dialog Turns": 2.0
        },
        "Text Sources": [
            "cnn.com",
            "dailymail.co.uk"
        ],
        "Model Generated": [],
        "Creators": [
            "Johns Hopkins University",
            "Microsoft Research"
        ],
        "Licenses": [
            {
                "License": "Custom",
                "License URL": "https://sheng-z.github.io/ReCoRD-explorer/"
            }
        ],
        "License Notes": "Dataset 1 under Apache 2.0. Dataset 2 under Internet Archive ToS. IA ToS includes this requirement: \"Access to the Archive’s Collections is provided at no cost to you and is granted for scholarship and research purposes only.\"",
        "License Verified By": "Shayne",
        "Dataset Filter IDs": [
            "super_glue/record:1.0.2"
        ],
        "Inferred Metadata": {
            "HF Dataset": "super_glue",
            "HF Config": "record",
            "HF Config License": "",
            "HF Yaml License": "",
            "PwC License Name": "Custom",
            "PwC License URL": "https://sheng-z.github.io/ReCoRD-explorer/",
            "PwC Date": "2018-10-30",
            "S2 Date": "2018-10-30",
            "GitHub License": "",
            "Github Date": "",
            "HF Date": "2022-01-25",
            "HF Downloads (September 2023)": 199405,
            "HF Likes (September 2023)": 111,
            "PwC Description": "Reading Comprehension with Commonsense Reasoning Dataset (ReCoRD) is a large-scale reading comprehension dataset which requires commonsense reasoning. ReCoRD consists of queries automatically generated from CNN/Daily Mail news articles; the answer to each query is a text span from a summarizing passage of the corresponding news. The goal of ReCoRD is to evaluate a machine's ability of commonsense reasoning in reading comprehension. ReCoRD is pronounced as [ˈrɛkərd].",
            "S2 Citation Count (September 2023)": 227,
            "GitHub Stars": "",
            "GitHub Topics": [],
            "Text Topics": []
        },
        "Derived from Datasets": [],
        "Human Annotation": "Yes",
        "Bibtex": "@Article{Zhang2018ReCoRDBT,\n author = {Sheng Zhang and Xiaodong Liu and Jingjing Liu and Jianfeng Gao and Kevin Duh and Benjamin Van Durme},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {ReCoRD: Bridging the Gap between Human and Machine Commonsense Reading Comprehension},\n volume = {abs/1810.12885},\n year = {2018}\n}\n"
    },
    "fc-flan-rte": {
        "Unique Dataset Identifier": "fc-flan-rte",
        "Dataset Name": "rte",
        "Collection": "Flan Collection (Flan 2021)",
        "Collection URL": "https://github.com/google-research/FLAN/tree/main/flan/v2",
        "Dataset URL": "https://aclweb.org/aclwiki/Recognizing_Textual_Entailment",
        "GitHub URL": "",
        "Hugging Face URL": "https://huggingface.co/datasets/SetFit/rte",
        "Paper Title": "",
        "Papers with Code URL": "https://paperswithcode.com/dataset/rte",
        "ArXiv URL": "https://arxiv.org/abs/1804.07461",
        "Semantic Scholar Corpus ID": 5034059,
        "Languages": [
            "English"
        ],
        "Task Categories": [
            "Inverted Natural Language Inference",
            "Natural Language Inference",
            "Textual Entailment"
        ],
        "Format": [
            "Zero-shot",
            "Few-shot"
        ],
        "Text Metrics": {
            "Num Dialogs": 8280,
            "Mean Inputs Length": 970.3819,
            "Mean Targets Length": 21.2481,
            "Max Inputs Length": 4540,
            "Max Targets Length": 1010,
            "Min Inputs Length": 36,
            "Min Targets Length": 2,
            "Min Dialog Turns": 2,
            "Max Dialog Turns": 2,
            "Mean Dialog Turns": 2.0
        },
        "Text Sources": [
            "videos",
            "glosses",
            "forum posts",
            "twitter",
            "student answers",
            "wikipedia.org",
            "image descriptions"
        ],
        "Model Generated": [],
        "Creators": [
            "New York University",
            "University of Washington",
            "DeepMind"
        ],
        "Licenses": [
            {
                "License": "Custom",
                "License URL": "https://tac.nist.gov/data/forms/index.html"
            }
        ],
        "License Notes": "Non-commercial, research only",
        "License Verified By": "Shayne",
        "Dataset Filter IDs": [
            "super_glue/rte:1.0.2"
        ],
        "Inferred Metadata": {
            "HF Dataset": "SetFit/rte",
            "HF Config": "SetFit--rte",
            "HF Config License": "",
            "HF Yaml License": "",
            "PwC License Name": "",
            "PwC License URL": "",
            "PwC Date": "",
            "S2 Date": "2018-04-20",
            "GitHub License": "",
            "Github Date": "",
            "HF Date": "2022-02-28",
            "HF Downloads (September 2023)": 737,
            "HF Likes (September 2023)": 0,
            "PwC Description": "The Recognizing Textual Entailment (RTE) datasets come from a series of textual entailment challenges. Data from RTE1, RTE2, RTE3 and RTE5 is combined. Examples are constructed based on news and Wikipedia text.",
            "S2 Citation Count (September 2023)": 4367,
            "GitHub Stars": "",
            "GitHub Topics": [],
            "Text Topics": []
        },
        "Derived from Datasets": [],
        "Human Annotation": "Yes",
        "Bibtex": "@Article{Wang2018GLUEAM,\n author = {Alex Wang and Amanpreet Singh and Julian Michael and Felix Hill and Omer Levy and Samuel R. Bowman},\n booktitle = {BlackboxNLP@EMNLP},\n pages = {353-355},\n title = {GLUE: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding},\n year = {2018}\n}\n"
    },
    "fc-flan-samsum": {
        "Unique Dataset Identifier": "fc-flan-samsum",
        "Dataset Name": "samsum",
        "Collection": "Flan Collection (Flan 2021)",
        "Collection URL": "https://github.com/google-research/FLAN/tree/main/flan/v2",
        "Dataset URL": "https://arxiv.org/pdf/1911.12237v2.pdf",
        "GitHub URL": "",
        "Hugging Face URL": "https://huggingface.co/datasets/samsum",
        "Paper Title": "SAMSum Corpus: A Human-annotated Dialogue Dataset for Abstractive Summarization",
        "Papers with Code URL": "https://paperswithcode.com/dataset/samsum-corpus",
        "ArXiv URL": "https://arxiv.org/abs/1911.12237",
        "Semantic Scholar Corpus ID": 208010268,
        "Languages": [
            "English"
        ],
        "Task Categories": [
            "Summarization",
            "Inverted Summarization",
            "Coherence Classification"
        ],
        "Format": [
            "Zero-shot",
            "Few-shot"
        ],
        "Text Metrics": {
            "Num Dialogs": 53537,
            "Mean Inputs Length": 1471.7292,
            "Mean Targets Length": 212.6876,
            "Max Inputs Length": 9487,
            "Max Targets Length": 3575,
            "Min Inputs Length": 38,
            "Min Targets Length": 0,
            "Min Dialog Turns": 2,
            "Max Dialog Turns": 2,
            "Mean Dialog Turns": 2.0
        },
        "Text Sources": [
            "original"
        ],
        "Model Generated": [],
        "Creators": [
            "Samsung R&D Institute"
        ],
        "Licenses": [
            {
                "License": "CC BY-NC-ND 4.0",
                "License URL": "https://arxiv.org/src/1911.12237v2/anc/corpus.7z"
            }
        ],
        "License Notes": "",
        "License Verified By": "Shayne",
        "Dataset Filter IDs": [
            "samsum:1.0.0"
        ],
        "Inferred Metadata": {
            "HF Dataset": "",
            "HF Config": "",
            "HF Config License": "",
            "HF Yaml License": "CC BY-NC-ND 4.0",
            "PwC License Name": "",
            "PwC License URL": "",
            "PwC Date": "",
            "S2 Date": "2019-11-27",
            "GitHub License": "",
            "Github Date": "",
            "HF Date": "2022-01-25",
            "HF Downloads (September 2023)": 75087,
            "HF Likes (September 2023)": 126,
            "PwC Description": "A new dataset with abstractive dialogue summaries.",
            "S2 Citation Count (September 2023)": 289,
            "GitHub Stars": "",
            "GitHub Topics": [],
            "Text Topics": []
        },
        "Derived from Datasets": [],
        "Human Annotation": "Yes",
        "Bibtex": "@Article{Gliwa2019SAMSumCA,\n author = {Bogdan Gliwa and Iwona Mochol and M. Biesek and A. Wawer},\n booktitle = {Conference on Empirical Methods in Natural Language Processing},\n journal = {ArXiv},\n title = {SAMSum Corpus: A Human-annotated Dialogue Dataset for Abstractive Summarization},\n volume = {abs/1911.12237},\n year = {2019}\n}\n"
    },
    "fc-flan-sentiment140": {
        "Unique Dataset Identifier": "fc-flan-sentiment140",
        "Dataset Name": "sentiment140",
        "Collection": "Flan Collection (Flan 2021)",
        "Collection URL": "https://github.com/google-research/FLAN/tree/main/flan/v2",
        "Dataset URL": "http://help.sentiment140.com/home",
        "GitHub URL": "",
        "Hugging Face URL": "https://huggingface.co/datasets/sentiment140",
        "Paper Title": "Twitter Sentiment Classification using Distant Supervision",
        "Papers with Code URL": "https://paperswithcode.com/dataset/sentiment140",
        "ArXiv URL": "",
        "Semantic Scholar Corpus ID": 18635269,
        "Languages": [
            "English"
        ],
        "Task Categories": [
            "Inverted Sentiment Analysis",
            "Sentiment Analysis"
        ],
        "Format": [
            "Zero-shot",
            "Few-shot"
        ],
        "Text Metrics": {
            "Num Dialogs": 108401,
            "Mean Inputs Length": 307.2439,
            "Mean Targets Length": 48.6667,
            "Max Inputs Length": 1090,
            "Max Targets Length": 214,
            "Min Inputs Length": 23,
            "Min Targets Length": 2,
            "Min Dialog Turns": 2,
            "Max Dialog Turns": 2,
            "Mean Dialog Turns": 2.0
        },
        "Text Sources": [],
        "Model Generated": [],
        "Creators": [
            "Stanford University"
        ],
        "Licenses": [
            {
                "License": "Custom",
                "License URL": "http://help.sentiment140.com/for-students"
            }
        ],
        "License Notes": "",
        "License Verified By": "Shayne",
        "Dataset Filter IDs": [
            "sentiment140:1.0.0"
        ],
        "Inferred Metadata": {
            "HF Dataset": "sentiment140",
            "HF Config": "sentiment140",
            "HF Config License": "",
            "HF Yaml License": "",
            "PwC License Name": "",
            "PwC License URL": "",
            "PwC Date": "",
            "S2 Date": "",
            "GitHub License": "",
            "Github Date": "",
            "HF Date": "2022-01-25",
            "HF Downloads (September 2023)": 819,
            "HF Likes (September 2023)": 8,
            "PwC Description": "Sentiment140 is a dataset that allows you to discover the sentiment of a brand, product, or topic on Twitter.",
            "S2 Citation Count (September 2023)": 961,
            "GitHub Stars": "",
            "GitHub Topics": [],
            "Text Topics": []
        },
        "Human Annotation": "No",
        "Derived from Datasets": [],
        "Bibtex": "@Inproceedings{Go2009TwitterSC,\n author = {Alec Go},\n title = {Twitter Sentiment Classiﬁcation using Distant Supervision},\n year = {2009}\n}\n"
    },
    "fc-flan-snli": {
        "Unique Dataset Identifier": "fc-flan-snli",
        "Dataset Name": "snli",
        "Collection": "Flan Collection (Flan 2021)",
        "Collection URL": "https://github.com/google-research/FLAN/tree/main/flan/v2",
        "Dataset URL": "https://nlp.stanford.edu/projects/snli/",
        "GitHub URL": "",
        "Hugging Face URL": "https://huggingface.co/datasets/snli",
        "Paper Title": "A large annotated corpus for learning natural language inference",
        "Papers with Code URL": "https://paperswithcode.com/dataset/snli",
        "ArXiv URL": "https://arxiv.org/abs/1508.05326",
        "Semantic Scholar Corpus ID": 14604520,
        "Languages": [
            "English"
        ],
        "Task Categories": [
            "Inverted Natural Language Inference",
            "Natural Language Inference",
            "Textual Entailment"
        ],
        "Format": [
            "Zero-shot",
            "Few-shot"
        ],
        "Text Metrics": {
            "Num Dialogs": 108351,
            "Mean Inputs Length": 463.1776,
            "Mean Targets Length": 12.5886,
            "Max Inputs Length": 1809,
            "Max Targets Length": 177,
            "Min Inputs Length": 23,
            "Min Targets Length": 2,
            "Min Dialog Turns": 2,
            "Max Dialog Turns": 2,
            "Mean Dialog Turns": 2.0
        },
        "Text Sources": [
            "flickr",
            "crowdsourced"
        ],
        "Model Generated": [],
        "Creators": [
            "Stanford University"
        ],
        "Licenses": [
            {
                "License": "CC BY-SA 4.0",
                "License URL": "https://creativecommons.org/licenses/by-sa/4.0/"
            }
        ],
        "License Notes": "",
        "License Verified By": "Shayne",
        "Dataset Filter IDs": [
            "snli:1.1.0"
        ],
        "Inferred Metadata": {
            "HF Dataset": "snli",
            "HF Config": "plain_text",
            "HF Config License": "",
            "HF Yaml License": "CC BY 4.0",
            "PwC License Name": "CC BY-SA 4.0",
            "PwC License URL": "https://creativecommons.org/licenses/by-sa/4.0/",
            "PwC Date": "2015-01-01",
            "S2 Date": "2015-08-21",
            "GitHub License": "",
            "Github Date": "",
            "HF Date": "2022-01-25",
            "HF Downloads (September 2023)": 9891,
            "HF Likes (September 2023)": 29,
            "PwC Description": "The SNLI dataset (Stanford Natural Language Inference) consists of 570k sentence-pairs manually labeled as entailment, contradiction, and neutral. Premises are image captions from Flickr30k, while hypotheses were generated by crowd-sourced annotators who were shown a premise and asked to generate entailing, contradicting, and neutral sentences. Annotators were instructed to judge the relation between sentences given that they describe the same event. Each pair is labeled as “entailment”, “neutral”, “contradiction” or “-”, where “-” indicates that an agreement could not be reached.",
            "S2 Citation Count (September 2023)": 3334,
            "GitHub Stars": "",
            "GitHub Topics": [],
            "Text Topics": []
        },
        "Derived from Datasets": [
            "SNLI"
        ],
        "Human Annotation": "No",
        "Bibtex": "@Article{Bowman2015ALA,\n author = {Samuel R. Bowman and Gabor Angeli and Christopher Potts and Christopher D. Manning},\n booktitle = {Conference on Empirical Methods in Natural Language Processing},\n pages = {632-642},\n title = {A large annotated corpus for learning natural language inference},\n year = {2015}\n}\n"
    },
    "fc-flan-squad_v1": {
        "Unique Dataset Identifier": "fc-flan-squad_v1",
        "Dataset Name": "squad_v1",
        "Collection": "Flan Collection (Flan 2021)",
        "Collection URL": "https://github.com/google-research/FLAN/tree/main/flan/v2",
        "Dataset URL": "https://rajpurkar.github.io/SQuAD-explorer/",
        "GitHub URL": "https://rajpurkar.github.io/SQuAD-explorer/",
        "Hugging Face URL": "https://huggingface.co/datasets/squad",
        "Paper Title": "Know What You Don't Know: Unanswerable Questions for SQuAD",
        "Papers with Code URL": "https://paperswithcode.com/dataset/squad",
        "ArXiv URL": "https://arxiv.org/abs/1806.03822",
        "Semantic Scholar Corpus ID": 47018994,
        "Languages": [
            "English"
        ],
        "Task Categories": [
            "Span Selection Question Answering",
            "Inverted Extractive QA"
        ],
        "Format": [
            "Zero-shot",
            "Few-shot"
        ],
        "Text Metrics": {
            "Num Dialogs": 108010,
            "Mean Inputs Length": 2129.679,
            "Mean Targets Length": 23.4694,
            "Max Inputs Length": 31644,
            "Max Targets Length": 193,
            "Min Inputs Length": 25,
            "Min Targets Length": 1,
            "Min Dialog Turns": 2,
            "Max Dialog Turns": 2,
            "Mean Dialog Turns": 2.0
        },
        "Text Sources": [
            "crowdsourced (daemo)"
        ],
        "Model Generated": [],
        "Creators": [
            "Stanford University"
        ],
        "Licenses": [
            {
                "License": "CC BY-SA 4.0",
                "License URL": "https://creativecommons.org/licenses/by-sa/4.0/legalcode"
            }
        ],
        "License Notes": "",
        "License Verified By": "Shayne",
        "Dataset Filter IDs": [
            "squad/v1.1:3.0.0"
        ],
        "Inferred Metadata": {
            "HF Dataset": "squad",
            "HF Config": "plain_text",
            "HF Config License": "",
            "HF Yaml License": "",
            "PwC License Name": "CC BY-SA 4.0",
            "PwC License URL": "https://creativecommons.org/licenses/by-sa/4.0/",
            "PwC Date": "2016-01-01",
            "S2 Date": "2018-06-11",
            "GitHub License": "",
            "Github Date": "",
            "HF Date": "2022-01-25",
            "HF Downloads (September 2023)": 153870,
            "HF Likes (September 2023)": 128,
            "PwC Description": "The Stanford Question Answering Dataset (SQuAD) is a collection of question-answer pairs derived from Wikipedia articles. In SQuAD, the correct answers of questions can be any sequence of tokens in the given text. Because the questions and answers are produced by humans through crowdsourcing, it is more diverse than some other question-answering datasets. SQuAD 1.1 contains 107,785 question-answer pairs on 536 articles. SQuAD2.0 (open-domain SQuAD, SQuAD-Open), the latest version, combines the 100,000 questions in SQuAD1.1 with over 50,000 un-answerable questions written adversarially by crowdworkers in forms that are similar to the answerable ones.",
            "S2 Citation Count (September 2023)": 1967,
            "GitHub Stars": "",
            "GitHub Topics": [],
            "Text Topics": []
        },
        "Derived from Datasets": [],
        "Human Annotation": "Yes",
        "Bibtex": "@Article{Rajpurkar2018KnowWY,\n author = {Pranav Rajpurkar and Robin Jia and Percy Liang},\n booktitle = {Annual Meeting of the Association for Computational Linguistics},\n journal = {ArXiv},\n title = {Know What You Don’t Know: Unanswerable Questions for SQuAD},\n volume = {abs/1806.03822},\n year = {2018}\n}\n"
    },
    "fc-flan-squad_v2": {
        "Unique Dataset Identifier": "fc-flan-squad_v2",
        "Dataset Name": "squad_v2",
        "Collection": "Flan Collection (Flan 2021)",
        "Collection URL": "https://github.com/google-research/FLAN/tree/main/flan/v2",
        "Dataset URL": "https://rajpurkar.github.io/SQuAD-explorer/",
        "GitHub URL": "https://rajpurkar.github.io/SQuAD-explorer/",
        "Hugging Face URL": "https://huggingface.co/datasets/squad_v2",
        "Paper Title": "Know What You Don't Know: Unanswerable Questions for SQuAD",
        "Papers with Code URL": "https://paperswithcode.com/dataset/squad",
        "ArXiv URL": "https://arxiv.org/abs/1806.03822",
        "Semantic Scholar Corpus ID": 47018994,
        "Languages": [
            "English"
        ],
        "Task Categories": [
            "Span Selection Question Answering",
            "Inverted Extractive QA"
        ],
        "Format": [
            "Zero-shot",
            "Few-shot"
        ],
        "Text Metrics": {
            "Num Dialogs": 108495,
            "Mean Inputs Length": 2309.7636,
            "Mean Targets Length": 17.4244,
            "Max Inputs Length": 29947,
            "Max Targets Length": 160,
            "Min Inputs Length": 242,
            "Min Targets Length": 1,
            "Min Dialog Turns": 2,
            "Max Dialog Turns": 2,
            "Mean Dialog Turns": 2.0
        },
        "Text Sources": [
            "crowdsourced (daemo)"
        ],
        "Model Generated": [],
        "Creators": [
            "Stanford University"
        ],
        "Licenses": [
            {
                "License": "CC BY-SA 4.0",
                "License URL": "https://creativecommons.org/licenses/by-sa/4.0/legalcode"
            }
        ],
        "License Notes": "",
        "License Verified By": "Shayne",
        "Dataset Filter IDs": [
            "squad/v2.0:3.0.0"
        ],
        "Inferred Metadata": {
            "HF Dataset": "squad_v2",
            "HF Config": "squad_v2",
            "HF Config License": "",
            "HF Yaml License": "CC BY-SA 4.0",
            "PwC License Name": "CC BY-SA 4.0",
            "PwC License URL": "https://creativecommons.org/licenses/by-sa/4.0/",
            "PwC Date": "2016-01-01",
            "S2 Date": "2018-06-11",
            "GitHub License": "",
            "Github Date": "",
            "HF Date": "2022-01-25",
            "HF Downloads (September 2023)": 8690810,
            "HF Likes (September 2023)": 65,
            "PwC Description": "The Stanford Question Answering Dataset (SQuAD) is a collection of question-answer pairs derived from Wikipedia articles. In SQuAD, the correct answers of questions can be any sequence of tokens in the given text. Because the questions and answers are produced by humans through crowdsourcing, it is more diverse than some other question-answering datasets. SQuAD 1.1 contains 107,785 question-answer pairs on 536 articles. SQuAD2.0 (open-domain SQuAD, SQuAD-Open), the latest version, combines the 100,000 questions in SQuAD1.1 with over 50,000 un-answerable questions written adversarially by crowdworkers in forms that are similar to the answerable ones.",
            "S2 Citation Count (September 2023)": 1967,
            "GitHub Stars": "",
            "GitHub Topics": [],
            "Text Topics": []
        },
        "Derived from Datasets": [],
        "Human Annotation": "Yes",
        "Bibtex": "@Article{Rajpurkar2018KnowWY,\n author = {Pranav Rajpurkar and Robin Jia and Percy Liang},\n booktitle = {Annual Meeting of the Association for Computational Linguistics},\n journal = {ArXiv},\n title = {Know What You Don’t Know: Unanswerable Questions for SQuAD},\n volume = {abs/1806.03822},\n year = {2018}\n}\n"
    },
    "fc-flan-sst2": {
        "Unique Dataset Identifier": "fc-flan-sst2",
        "Dataset Name": "sst2",
        "Collection": "Flan Collection (Flan 2021)",
        "Collection URL": "https://github.com/google-research/FLAN/tree/main/flan/v2",
        "Dataset URL": "https://nlp.stanford.edu/sentiment/",
        "GitHub URL": "",
        "Hugging Face URL": "https://huggingface.co/datasets/sst2",
        "Paper Title": "Recursive Deep Models for Semantic Compositionality Over a Sentiment Treebank",
        "Papers with Code URL": "https://paperswithcode.com/dataset/sst-2",
        "ArXiv URL": "https://aclanthology.org/D13-1170/",
        "Semantic Scholar Corpus ID": 990233,
        "Languages": [
            "English"
        ],
        "Task Categories": [
            "Inverted Sentiment Analysis",
            "Sentiment Analysis"
        ],
        "Format": [
            "Zero-shot",
            "Few-shot"
        ],
        "Text Metrics": {
            "Num Dialogs": 108023,
            "Mean Inputs Length": 320.0962,
            "Mean Targets Length": 24.4253,
            "Max Inputs Length": 1223,
            "Max Targets Length": 267,
            "Min Inputs Length": 30,
            "Min Targets Length": 1,
            "Min Dialog Turns": 2,
            "Max Dialog Turns": 2,
            "Mean Dialog Turns": 2.0
        },
        "Text Sources": [
            "rottentomatoes.com"
        ],
        "Model Generated": [],
        "Creators": [
            "Stanford University"
        ],
        "Licenses": [
            {
                "License": "CC0 1.0",
                "License URL": "https://creativecommons.org/publicdomain/zero/1.0/"
            }
        ],
        "License Notes": "",
        "License Verified By": "Shayne",
        "Dataset Filter IDs": [
            "glue/sst2:2.0.0"
        ],
        "Inferred Metadata": {
            "HF Dataset": "sst2",
            "HF Config": "default",
            "HF Config License": "Unspecified",
            "HF Yaml License": "Unspecified",
            "PwC License Name": "",
            "PwC License URL": "",
            "PwC Date": "",
            "S2 Date": "2013-10-01",
            "GitHub License": "",
            "Github Date": "",
            "HF Date": "2022-06-13",
            "HF Downloads (September 2023)": 63152,
            "HF Likes (September 2023)": 24,
            "PwC Description": "Click to add a brief description of the dataset (Markdown and LaTeX enabled).\n\nProvide:\n\n\na high-level explanation of the dataset characteristics\nexplain motivations and summary of its content\npotential use cases of the dataset",
            "S2 Citation Count (September 2023)": 6668,
            "GitHub Stars": "",
            "GitHub Topics": [],
            "Text Topics": []
        },
        "Derived from Datasets": [
            "Movie Review Dataset (Pang & Lee 2005)"
        ],
        "Human Annotation": "Yes",
        "Bibtex": "@Article{Socher2013RecursiveDM,\n author = {R. Socher and Alex Perelygin and Jean Wu and Jason Chuang and Christopher D. Manning and A. Ng and Christopher Potts},\n booktitle = {Conference on Empirical Methods in Natural Language Processing},\n pages = {1631-1642},\n title = {Recursive Deep Models for Semantic Compositionality Over a Sentiment Treebank},\n year = {2013}\n}\n"
    },
    "fc-flan-story_cloze": {
        "Unique Dataset Identifier": "fc-flan-story_cloze",
        "Dataset Name": "story_cloze",
        "Collection": "Flan Collection (Flan 2021)",
        "Collection URL": "https://github.com/google-research/FLAN/tree/main/flan/v2",
        "Dataset URL": "https://www.cs.rochester.edu/nlp/rocstories/",
        "GitHub URL": "",
        "Hugging Face URL": "https://huggingface.co/datasets/story_cloze",
        "Paper Title": "A Corpus and Evaluation Framework for Deeper Understanding of Commonsense Stories",
        "Papers with Code URL": "https://paperswithcode.com/dataset/storycloze",
        "ArXiv URL": "https://arxiv.org/abs/1604.01696",
        "Semantic Scholar Corpus ID": 15337246,
        "Languages": [
            "English"
        ],
        "Task Categories": [
            "Inverted Next Sentence Prediction",
            "Next Sentence Prediction"
        ],
        "Format": [
            "Zero-shot",
            "Few-shot"
        ],
        "Text Metrics": {
            "Num Dialogs": 5973,
            "Mean Inputs Length": 744.6392,
            "Mean Targets Length": 97.257,
            "Max Inputs Length": 2547,
            "Max Targets Length": 331,
            "Min Inputs Length": 58,
            "Min Targets Length": 2,
            "Min Dialog Turns": 2,
            "Max Dialog Turns": 2,
            "Mean Dialog Turns": 2.0
        },
        "Text Sources": [
            "crowdsourced"
        ],
        "Model Generated": [],
        "Creators": [
            "University of Rochester",
            "United States Naval Academy",
            "Microsoft Research",
            "Virginia Tech",
            "The Institute for Human & Machine Cognition"
        ],
        "Licenses": [
            {
                "License": "Unspecified",
                "License URL": "https://www.cs.rochester.edu/nlp/rocstories/"
            }
        ],
        "License Notes": "Says its free and open to use",
        "License Verified By": "Shayne",
        "Dataset Filter IDs": [
            "story_cloze/2016:1.0.0"
        ],
        "Inferred Metadata": {
            "HF Dataset": "story_cloze",
            "HF Config": "2016",
            "HF Config License": "",
            "HF Yaml License": "Unspecified",
            "PwC License Name": "",
            "PwC License URL": "",
            "PwC Date": "2016-04-06",
            "S2 Date": "2016-04-06",
            "GitHub License": "",
            "Github Date": "",
            "HF Date": "2022-01-25",
            "HF Downloads (September 2023)": 3206,
            "HF Likes (September 2023)": 5,
            "PwC Description": "Representation and learning of commonsense knowledge is one of the foundational problems in the quest to enable deep language understanding. This issue is particularly challenging for understanding casual and correlational relationships between events. While this topic has received a lot of interest in the NLP community, research has been hindered by the lack of a proper evaluation framework. This paper attempts to address this problem with a new framework for evaluating story understanding and script learning: the 'Story Cloze Test'. This test requires a system to choose the correct ending to a four-sentence story. We created a new corpus of ~50k five-sentence commonsense stories, ROCStories, to enable this evaluation. This corpus is unique in two ways: (1) it captures a rich set of causal and temporal commonsense relations between daily events, and (2) it is a high quality collection of everyday life stories that can also be used for story generation. Experimental evaluation shows that a host of baselines and state-of-the-art models based on shallow language understanding struggle to achieve a high score on the Story Cloze Test. We discuss these implications for script and story learning, and offer suggestions for deeper language understanding.",
            "S2 Citation Count (September 2023)": 158,
            "GitHub Stars": "",
            "GitHub Topics": [],
            "Text Topics": []
        },
        "Derived from Datasets": [],
        "Human Annotation": "Yes",
        "Bibtex": "@Article{Mostafazadeh2016ACA,\n author = {N. Mostafazadeh and Nathanael Chambers and Xiaodong He and Devi Parikh and Dhruv Batra and Lucy Vanderwende and Pushmeet Kohli and James F. Allen},\n booktitle = {North American Chapter of the Association for Computational Linguistics},\n journal = {ArXiv},\n title = {A Corpus and Cloze Evaluation for Deeper Understanding of Commonsense Stories},\n volume = {abs/1604.01696},\n year = {2016}\n}\n"
    },
    "fc-flan-stsb": {
        "Unique Dataset Identifier": "fc-flan-stsb",
        "Dataset Name": "stsb",
        "Collection": "Flan Collection (Flan 2021)",
        "Collection URL": "https://github.com/google-research/FLAN/tree/main/flan/v2",
        "Dataset URL": "http://ixa2.si.ehu.eus/stswiki/index.php/STSbenchmark",
        "GitHub URL": "",
        "Hugging Face URL": "https://huggingface.co/datasets/SetFit/stsb",
        "Paper Title": "SemEval-2017 Task 1: Semantic Textual Similarity Multilingual and Cross-lingual Focused Evaluation",
        "Papers with Code URL": "https://paperswithcode.com/dataset/snli",
        "ArXiv URL": "https://arxiv.org/abs/1708.00055",
        "Semantic Scholar Corpus ID": 4421747,
        "Languages": [
            "English"
        ],
        "Task Categories": [
            "Inverted Sentiment Analysis",
            "Sentiment Analysis"
        ],
        "Format": [
            "Zero-shot",
            "Few-shot"
        ],
        "Text Metrics": {
            "Num Dialogs": 20456,
            "Mean Inputs Length": 578.0957,
            "Mean Targets Length": 15.5148,
            "Max Inputs Length": 2215,
            "Max Targets Length": 315,
            "Min Inputs Length": 123,
            "Min Targets Length": 1,
            "Min Dialog Turns": 2,
            "Max Dialog Turns": 2,
            "Mean Dialog Turns": 2.0
        },
        "Text Sources": [
            "flickr"
        ],
        "Model Generated": [
            "Google Translate"
        ],
        "Creators": [
            "Google Research",
            "George Washington University",
            "University of the Basque Country",
            "University of Sheffield"
        ],
        "Licenses": [
            {
                "License": "Unspecified",
                "License URL": "unknown"
            }
        ],
        "License Notes": "",
        "License Verified By": "Shayne",
        "Dataset Filter IDs": [
            "glue/stsb:2.0.0"
        ],
        "Inferred Metadata": {
            "HF Dataset": "SetFit/stsb",
            "HF Config": "SetFit--stsb",
            "HF Config License": "",
            "HF Yaml License": "",
            "PwC License Name": "CC BY-SA 4.0",
            "PwC License URL": "https://creativecommons.org/licenses/by-sa/4.0/",
            "PwC Date": "2015-01-01",
            "S2 Date": "2017-07-31",
            "GitHub License": "",
            "Github Date": "",
            "HF Date": "2022-02-28",
            "HF Downloads (September 2023)": 702,
            "HF Likes (September 2023)": 0,
            "PwC Description": "The SNLI dataset (Stanford Natural Language Inference) consists of 570k sentence-pairs manually labeled as entailment, contradiction, and neutral. Premises are image captions from Flickr30k, while hypotheses were generated by crowd-sourced annotators who were shown a premise and asked to generate entailing, contradicting, and neutral sentences. Annotators were instructed to judge the relation between sentences given that they describe the same event. Each pair is labeled as “entailment”, “neutral”, “contradiction” or “-”, where “-” indicates that an agreement could not be reached.",
            "S2 Citation Count (September 2023)": 1346,
            "GitHub Stars": "",
            "GitHub Topics": [],
            "Text Topics": []
        },
        "Derived from Datasets": [
            "SNLI"
        ],
        "Human Annotation": "Yes",
        "Bibtex": "@Article{Cer2017SemEval2017T1,\n author = {Daniel Matthew Cer and Mona T. Diab and Eneko Agirre and I. Lopez-Gazpio and Lucia Specia},\n booktitle = {International Workshop on Semantic Evaluation},\n pages = {1-14},\n title = {SemEval-2017 Task 1: Semantic Textual Similarity Multilingual and Crosslingual Focused Evaluation},\n year = {2017}\n}\n"
    },
    "fc-flan-trec": {
        "Unique Dataset Identifier": "fc-flan-trec",
        "Dataset Name": "trec",
        "Collection": "Flan Collection (Flan 2021)",
        "Collection URL": "https://github.com/google-research/FLAN/tree/main/flan/v2",
        "Dataset URL": "https://cogcomp.seas.upenn.edu/Data/QA/QC/",
        "GitHub URL": "",
        "Hugging Face URL": "https://huggingface.co/datasets/trec",
        "Paper Title": "Learning Question Classifiers",
        "Papers with Code URL": "https://paperswithcode.com/dataset/trec-10",
        "ArXiv URL": "",
        "Semantic Scholar Corpus ID": 11039301,
        "Languages": [
            "English"
        ],
        "Task Categories": [
            "Inverted Topic Classification",
            "Text Classification",
            "Question Understanding"
        ],
        "Format": [
            "Zero-shot",
            "Few-shot"
        ],
        "Text Metrics": {
            "Num Dialogs": 19000,
            "Mean Inputs Length": 435.1429,
            "Mean Targets Length": 12.2991,
            "Max Inputs Length": 1507,
            "Max Targets Length": 160,
            "Min Inputs Length": 25,
            "Min Targets Length": 2,
            "Min Dialog Turns": 2,
            "Max Dialog Turns": 2,
            "Mean Dialog Turns": 2.0
        },
        "Text Sources": [],
        "Model Generated": [],
        "Creators": [
            "University of Illinois"
        ],
        "Licenses": [
            {
                "License": "CC0 1.0",
                "License URL": "https://www.kaggle.com/datasets/thedevastator/the-trec-question-classification-dataset-a-longi"
            }
        ],
        "License Notes": "",
        "License Verified By": "Shayne",
        "Dataset Filter IDs": [
            "trec:1.0.0"
        ],
        "Inferred Metadata": {
            "HF Dataset": "trec",
            "HF Config": "default",
            "HF Config License": "",
            "HF Yaml License": "Unspecified",
            "PwC License Name": "",
            "PwC License URL": "",
            "PwC Date": "2002-08-01",
            "S2 Date": "2002-08-24",
            "GitHub License": "",
            "Github Date": "",
            "HF Date": "2022-01-25",
            "HF Downloads (September 2023)": 16100,
            "HF Likes (September 2023)": 31,
            "PwC Description": "A question type classification dataset with 6 classes for questions about a person, location, numeric information, etc. The test split has 500 questions, and the training split has 5452 questions.\n\nPaper: Learning Question Classifiers",
            "S2 Citation Count (September 2023)": 1359,
            "GitHub Stars": "",
            "GitHub Topics": [],
            "Text Topics": []
        },
        "Human Annotation": "No",
        "Derived from Datasets": [],
        "Bibtex": "@Article{Li2002LearningQC,\n author = {Xin Li and D. Roth},\n booktitle = {International Conference on Computational Linguistics},\n pages = {1-7},\n title = {Learning Question Classifiers},\n year = {2002}\n}\n"
    },
    "fc-flan-trivia_qa": {
        "Unique Dataset Identifier": "fc-flan-trivia_qa",
        "Dataset Name": "trivia_qa",
        "Collection": "Flan Collection (Flan 2021)",
        "Collection URL": "https://github.com/google-research/FLAN/tree/main/flan/v2",
        "Dataset URL": "https://github.com/mandarjoshi90/triviaqa",
        "GitHub URL": "https://github.com/mandarjoshi90/triviaqa",
        "Hugging Face URL": "https://huggingface.co/datasets/trivia_qa",
        "Paper Title": "TriviaQA: A Large Scale Distantly Supervised Challenge Dataset for Reading Comprehension",
        "Papers with Code URL": "https://paperswithcode.com/dataset/triviaqa",
        "ArXiv URL": "https://arxiv.org/abs/1705.03551",
        "Semantic Scholar Corpus ID": 26501419,
        "Languages": [
            "English"
        ],
        "Task Categories": [
            "Span Selection Question Answering",
            "Inverted Extractive QA"
        ],
        "Format": [
            "Zero-shot",
            "Few-shot"
        ],
        "Text Metrics": {
            "Num Dialogs": 109120,
            "Mean Inputs Length": 291.6294,
            "Mean Targets Length": 9.9382,
            "Max Inputs Length": 1931,
            "Max Targets Length": 134,
            "Min Inputs Length": 22,
            "Min Targets Length": 0,
            "Min Dialog Turns": 2,
            "Max Dialog Turns": 2,
            "Mean Dialog Turns": 2.0
        },
        "Text Sources": [
            "wikipedia.org",
            "undisclosed web"
        ],
        "Model Generated": [],
        "Creators": [
            "University of Washington",
            "AI2"
        ],
        "Licenses": [
            {
                "License": "Apache License 2.0",
                "License URL": "https://github.com/mandarjoshi90/triviaqa/blob/master/LICENSE"
            }
        ],
        "License Notes": "No commercial use. Scraped from multiple trivia/quizz websites.",
        "License Verified By": "Shayne",
        "Dataset Filter IDs": [
            "trivia_qa/rc:1.1.0"
        ],
        "Inferred Metadata": {
            "HF Dataset": "trivia_qa",
            "HF Config": "rc",
            "HF Config License": "",
            "HF Yaml License": "Unspecified",
            "PwC License Name": "Unspecified",
            "PwC License URL": "http://nlp.cs.washington.edu/triviaqa/#:~:text=copyright",
            "PwC Date": "2017-01-01",
            "S2 Date": "2017-05-01",
            "GitHub License": "Apache License 2.0",
            "Github Date": "",
            "HF Date": "2022-01-25",
            "HF Downloads (September 2023)": 93735,
            "HF Likes (September 2023)": 23,
            "PwC Description": "TriviaQA is a realistic text-based question answering dataset which includes 950K question-answer pairs from 662K documents collected from Wikipedia and the web. This dataset is more challenging than standard QA benchmark datasets such as Stanford Question Answering Dataset (SQuAD), as the answers for a question may not be directly obtained by span prediction and the context is very long. TriviaQA dataset consists of both human-verified and machine-generated QA subsets.",
            "S2 Citation Count (September 2023)": 1045,
            "GitHub Stars": 220,
            "GitHub Topics": [
                "acl2017",
                "machine-reading",
                "nlp",
                "question-answering",
                "reading-comprehension",
                "triviaqa"
            ],
            "Text Topics": []
        },
        "Derived from Datasets": [],
        "Human Annotation": "Yes",
        "Bibtex": "@Article{Joshi2017TriviaQAAL,\n author = {Mandar Joshi and Eunsol Choi and Daniel S. Weld and Luke Zettlemoyer},\n booktitle = {Annual Meeting of the Association for Computational Linguistics},\n journal = {ArXiv},\n title = {TriviaQA: A Large Scale Distantly Supervised Challenge Dataset for Reading Comprehension},\n volume = {abs/1705.03551},\n year = {2017}\n}\n"
    },
    "fc-flan-true_case": {
        "Unique Dataset Identifier": "fc-flan-true_case",
        "Dataset Name": "true_case",
        "Collection": "Flan Collection (Flan 2021)",
        "Collection URL": "https://github.com/google-research/FLAN/tree/main/flan/v2",
        "Dataset URL": "https://www.paracrawl.eu/",
        "GitHub URL": "",
        "Hugging Face URL": "",
        "Paper Title": "",
        "Papers with Code URL": "",
        "ArXiv URL": "",
        "Semantic Scholar Corpus ID": "",
        "Languages": [
            "English"
        ],
        "Task Categories": [
            "Casing Selection",
            "Inverted Casing Selection"
        ],
        "Format": [
            "Zero-shot",
            "Few-shot"
        ],
        "Text Metrics": {
            "Num Dialogs": 106135,
            "Mean Inputs Length": 607.2719,
            "Mean Targets Length": 121.3865,
            "Max Inputs Length": 3654,
            "Max Targets Length": 976,
            "Min Inputs Length": 38,
            "Min Targets Length": 15,
            "Min Dialog Turns": 2,
            "Max Dialog Turns": 2,
            "Mean Dialog Turns": 2.0
        },
        "Text Sources": [],
        "Model Generated": [],
        "Creators": [],
        "Licenses": [
            {
                "License": "CC0 1.0",
                "License URL": "https://creativecommons.org/share-your-work/public-domain/cc0/"
            }
        ],
        "License Notes": "",
        "License Verified By": "Shayne",
        "Dataset Filter IDs": [
            "true_case"
        ],
        "Inferred Metadata": {
            "HF Dataset": "",
            "HF Config": "",
            "HF Config License": "",
            "HF Yaml License": "",
            "PwC License Name": "",
            "PwC License URL": "",
            "PwC Date": "",
            "S2 Date": "",
            "GitHub License": "",
            "Github Date": "",
            "HF Date": "",
            "HF Downloads (September 2023)": "",
            "HF Likes (September 2023)": "",
            "PwC Description": "",
            "S2 Citation Count (September 2023)": "",
            "GitHub Stars": "",
            "GitHub Topics": [],
            "Text Topics": [
                "Religion",
                "Technology",
                "General knowledge",
                "Travel",
                "Transportation",
                "Mathematics",
                "Translation",
                "Geography"
            ]
        },
        "Human Annotation": "No",
        "Derived from Datasets": [],
        "Bibtex": ""
    },
    "fc-flan-unified_qa_science_inst": {
        "Unique Dataset Identifier": "fc-flan-unified_qa_science_inst",
        "Dataset Name": "unified_qa_science_inst",
        "Collection": "Flan Collection (Flan 2021)",
        "Collection URL": "https://github.com/google-research/FLAN/tree/main/flan/v2",
        "Dataset URL": "http://data.allenai.org/ai2-science-questions",
        "GitHub URL": "",
        "Hugging Face URL": "",
        "Paper Title": "",
        "Papers with Code URL": "",
        "ArXiv URL": "",
        "Semantic Scholar Corpus ID": "",
        "Languages": [
            "English"
        ],
        "Task Categories": [
            "Question Answering",
            "Inverted Question Answering"
        ],
        "Format": [
            "Zero-shot",
            "Few-shot"
        ],
        "Text Metrics": {
            "Num Dialogs": 2135,
            "Mean Inputs Length": 635.3902,
            "Mean Targets Length": 25.4445,
            "Max Inputs Length": 4076,
            "Max Targets Length": 153,
            "Min Inputs Length": 25,
            "Min Targets Length": 2,
            "Min Dialog Turns": 2,
            "Max Dialog Turns": 2,
            "Mean Dialog Turns": 2.0
        },
        "Text Sources": [],
        "Model Generated": [],
        "Creators": [],
        "Licenses": [
            {
                "License": "Unspecified",
                "License URL": "Unspecified"
            }
        ],
        "License Notes": "",
        "License Verified By": "Shayne",
        "Dataset Filter IDs": [
            "unified_qa_science_inst"
        ],
        "Inferred Metadata": {
            "HF Dataset": "",
            "HF Config": "",
            "HF Config License": "",
            "HF Yaml License": "",
            "PwC License Name": "",
            "PwC License URL": "",
            "PwC Date": "",
            "S2 Date": "",
            "GitHub License": "",
            "Github Date": "",
            "HF Date": "",
            "HF Downloads (September 2023)": "",
            "HF Likes (September 2023)": "",
            "PwC Description": "",
            "S2 Citation Count (September 2023)": "",
            "GitHub Stars": "",
            "GitHub Topics": [],
            "Text Topics": [
                "Chemistry",
                "Earth Science",
                "Physics",
                "Astronomy",
                "Geology",
                "Environmental science",
                "Science",
                "Ecology",
                "Paleontology"
            ]
        },
        "Human Annotation": "No",
        "Derived from Datasets": [],
        "Bibtex": ""
    },
    "fc-flan-web_nlg_en": {
        "Unique Dataset Identifier": "fc-flan-web_nlg_en",
        "Dataset Name": "web_nlg_en",
        "Collection": "Flan Collection (Flan 2021)",
        "Collection URL": "https://github.com/google-research/FLAN/tree/main/flan/v2",
        "Dataset URL": "https://gem-benchmark.com/data_cards/web_nlg",
        "GitHub URL": "",
        "Hugging Face URL": "https://huggingface.co/datasets/web_nlg",
        "Paper Title": "Creating Training Corpora for NLG Micro-Planners",
        "Papers with Code URL": "https://paperswithcode.com/dataset/webnlg",
        "ArXiv URL": "",
        "Semantic Scholar Corpus ID": 6702871,
        "Languages": [
            "English"
        ],
        "Task Categories": [
            "Structured Data to Text",
            "Inverted Structured Data to Text"
        ],
        "Format": [
            "Zero-shot",
            "Few-shot"
        ],
        "Text Metrics": {
            "Num Dialogs": 108072,
            "Mean Inputs Length": 688.2444,
            "Mean Targets Length": 123.6053,
            "Max Inputs Length": 3115,
            "Max Targets Length": 649,
            "Min Inputs Length": 54,
            "Min Targets Length": 17,
            "Min Dialog Turns": 2,
            "Max Dialog Turns": 2,
            "Mean Dialog Turns": 2.0
        },
        "Text Sources": [],
        "Model Generated": [],
        "Creators": [
            "University of Edinburgh"
        ],
        "Licenses": [
            {
                "License": "CC BY-NC 4.0",
                "License URL": "https://gem-benchmark.com/data_cards/web_nlg"
            }
        ],
        "License Notes": "",
        "License Verified By": "Shayne",
        "Dataset Filter IDs": [
            "gem/web_nlg_en:1.1.0"
        ],
        "Inferred Metadata": {
            "HF Dataset": "web_nlg",
            "HF Config": "webnlg_challenge_2017",
            "HF Config License": "",
            "HF Yaml License": "",
            "PwC License Name": "CC BY-NC-SA 4.0",
            "PwC License URL": "https://gitlab.com/shimorina/webnlg-dataset",
            "PwC Date": "2017-01-01",
            "S2 Date": "2017-08-04",
            "GitHub License": "",
            "Github Date": "",
            "HF Date": "2022-01-25",
            "HF Downloads (September 2023)": 4897,
            "HF Likes (September 2023)": 10,
            "PwC Description": "The WebNLG corpus comprises of sets of triplets describing facts (entities and relations between them) and the corresponding facts in form of natural language text. The corpus contains sets with up to 7 triplets each along with one or more reference texts for each set. The test set is split into two parts: seen, containing inputs created for entities and relations belonging to DBpedia categories that were seen in the training data, and unseen, containing inputs extracted for entities and relations belonging to 5 unseen categories.\n\nInitially, the dataset was used for the WebNLG natural language generation challenge which consists of mapping the sets of triplets to text, including referring expression generation, aggregation, lexicalization, surface realization, and sentence segmentation.\nThe corpus is also used for a reverse task of triplets extraction.\n\nVersioning history of the dataset can be found here.",
            "S2 Citation Count (September 2023)": 285,
            "GitHub Stars": "",
            "GitHub Topics": [],
            "Text Topics": []
        },
        "Human Annotation": "No",
        "Derived from Datasets": [],
        "Bibtex": "@Article{Gardent2017CreatingTC,\n author = {Claire Gardent and Anastasia Shimorina and Shashi Narayan and Laura Perez-Beltrachini},\n booktitle = {Annual Meeting of the Association for Computational Linguistics},\n pages = {179-188},\n title = {Creating Training Corpora for NLG Micro-Planners},\n year = {2017}\n}\n"
    },
    "fc-flan-wic": {
        "Unique Dataset Identifier": "fc-flan-wic",
        "Dataset Name": "wic",
        "Collection": "Flan Collection (Flan 2021)",
        "Collection URL": "https://github.com/google-research/FLAN/tree/main/flan/v2",
        "Dataset URL": "https://pilehvar.github.io/wic/",
        "GitHub URL": "https://pilehvar.github.io/wic/",
        "Hugging Face URL": "",
        "Paper Title": "WiC: the Word-in-Context Dataset for Evaluating Context-Sensitive Meaning Representations",
        "Papers with Code URL": "https://paperswithcode.com/dataset/wic",
        "ArXiv URL": "https://arxiv.org/abs/1808.09121",
        "Semantic Scholar Corpus ID": 102353817,
        "Languages": [
            "English"
        ],
        "Task Categories": [
            "Inverted Coreference Resolution",
            "Word Sense Disambiguation"
        ],
        "Format": [
            "Zero-shot",
            "Few-shot"
        ],
        "Text Metrics": {
            "Num Dialogs": 18856,
            "Mean Inputs Length": 521.3943,
            "Mean Targets Length": 14.5406,
            "Max Inputs Length": 1728,
            "Max Targets Length": 18,
            "Min Inputs Length": 74,
            "Min Targets Length": 2,
            "Min Dialog Turns": 2,
            "Max Dialog Turns": 2,
            "Mean Dialog Turns": 2.0
        },
        "Text Sources": [
            "wordnet",
            "verbnet",
            "wiktionary.org"
        ],
        "Model Generated": [],
        "Creators": [
            "University of Cambridge",
            "Tehran Institute for Advanced Studies",
            "Cardiff University"
        ],
        "Licenses": [
            {
                "License": "CC BY-NC 4.0",
                "License URL": "https://pilehvar.github.io/wic/"
            }
        ],
        "License Notes": "",
        "License Verified By": "Shayne",
        "Dataset Filter IDs": [
            "super_glue/wic:1.0.2"
        ],
        "Inferred Metadata": {
            "HF Dataset": "",
            "HF Config": "",
            "HF Config License": "",
            "HF Yaml License": "",
            "PwC License Name": "CC BY-NC 4.0",
            "PwC License URL": "https://creativecommons.org/licenses/by-nc/4.0/legalcode",
            "PwC Date": "",
            "S2 Date": "2018-08-28",
            "GitHub License": "",
            "Github Date": "",
            "HF Date": "",
            "HF Downloads (September 2023)": "",
            "HF Likes (September 2023)": "",
            "PwC Description": "WiC is a benchmark for the evaluation of context-sensitive word embeddings. WiC is framed as a binary classification task. Each instance in WiC has a target word w, either a verb or a noun, for which two contexts are provided. Each of these contexts triggers a specific meaning of w. The task is to identify if the occurrences of w in the two contexts correspond to the same meaning or not. In fact, the dataset can also be viewed as an application of Word Sense Disambiguation in practise.",
            "S2 Citation Count (September 2023)": 285,
            "GitHub Stars": "",
            "GitHub Topics": [],
            "Text Topics": []
        },
        "Derived from Datasets": [],
        "Human Annotation": "Yes",
        "Bibtex": "@Article{Pilehvar2018WiCTW,\n author = {Mohammad Taher Pilehvar and José Camacho-Collados},\n booktitle = {North American Chapter of the Association for Computational Linguistics},\n pages = {1267-1273},\n title = {WiC: the Word-in-Context Dataset for Evaluating Context-Sensitive Meaning Representations},\n year = {2018}\n}\n"
    },
    "fc-flan-wiki_lingua_english_en": {
        "Unique Dataset Identifier": "fc-flan-wiki_lingua_english_en",
        "Dataset Name": "wiki_lingua_english_en",
        "Collection": "Flan Collection (Flan 2021)",
        "Collection URL": "https://github.com/google-research/FLAN/tree/main/flan/v2",
        "Dataset URL": "https://gem-benchmark.com/data_cards/wiki_lingua",
        "GitHub URL": "",
        "Hugging Face URL": "https://huggingface.co/datasets/wiki_lingua",
        "Paper Title": "WikiLingua: A New Benchmark Dataset for Cross-Lingual Abstractive Summarization",
        "Papers with Code URL": "https://paperswithcode.com/dataset/wikilingua",
        "ArXiv URL": "https://arxiv.org/abs/2010.03093",
        "Semantic Scholar Corpus ID": 222177239,
        "Languages": [
            "English"
        ],
        "Task Categories": [
            "Summarization",
            "Inverted Summarization"
        ],
        "Format": [
            "Zero-shot",
            "Few-shot"
        ],
        "Text Metrics": {
            "Num Dialogs": 108903,
            "Mean Inputs Length": 3993.6574,
            "Mean Targets Length": 529.9124,
            "Max Inputs Length": 15706,
            "Max Targets Length": 16395,
            "Min Inputs Length": 8,
            "Min Targets Length": 0,
            "Min Dialog Turns": 2,
            "Max Dialog Turns": 2,
            "Mean Dialog Turns": 2.0
        },
        "Text Sources": [
            "wikihow.com"
        ],
        "Model Generated": [],
        "Creators": [
            "Columbia University",
            "Cornell University"
        ],
        "Licenses": [
            {
                "License": "CC BY 3.0",
                "License URL": "https://creativecommons.org/licenses/by/3.0/"
            }
        ],
        "License Notes": "",
        "License Verified By": "Shayne",
        "Dataset Filter IDs": [
            "gem/wiki_lingua_english_en:1.1.0"
        ],
        "Inferred Metadata": {
            "HF Dataset": "wiki_lingua",
            "HF Config": "arabic",
            "HF Config License": "CC BY-NC-SA 3.0",
            "HF Yaml License": "",
            "PwC License Name": "",
            "PwC License URL": "",
            "PwC Date": "",
            "S2 Date": "2020-10-07",
            "GitHub License": "",
            "Github Date": "",
            "HF Date": "2022-01-25",
            "HF Downloads (September 2023)": 5621,
            "HF Likes (September 2023)": 20,
            "PwC Description": "WikiLingua includes ~770k article and summary pairs in 18 languages from WikiHow. Gold-standard article-summary alignments across languages are extracted by aligning the images that are used to describe each how-to step in an article.",
            "S2 Citation Count (September 2023)": 125,
            "GitHub Stars": "",
            "GitHub Topics": [],
            "Text Topics": []
        },
        "Derived from Datasets": [],
        "Human Annotation": "No",
        "Bibtex": "@Article{Ladhak2020WikiLinguaAN,\n author = {Faisal Ladhak and Esin Durmus and Claire Cardie and K. McKeown},\n booktitle = {Findings},\n journal = {ArXiv},\n title = {WikiLingua: A New Benchmark Dataset for Multilingual Abstractive Summarization},\n volume = {abs/2010.03093},\n year = {2020}\n}\n"
    },
    "fc-flan-winogrande": {
        "Unique Dataset Identifier": "fc-flan-winogrande",
        "Dataset Name": "winogrande",
        "Collection": "Flan Collection (Flan 2021)",
        "Collection URL": "https://github.com/google-research/FLAN/tree/main/flan/v2",
        "Dataset URL": "https://github.com/allenai/winogrande",
        "GitHub URL": "https://github.com/allenai/winogrande",
        "Hugging Face URL": "https://huggingface.co/datasets/winogrande/",
        "Paper Title": "WinoGrande: An Adversarial Winograd Schema Challenge at Scale",
        "Papers with Code URL": "https://paperswithcode.com/dataset/winogrande",
        "ArXiv URL": "https://arxiv.org/abs/1907.10641",
        "Semantic Scholar Corpus ID": 198893658,
        "Languages": [
            "English"
        ],
        "Task Categories": [
            "Coreference Resolution",
            "Inverted Coreference Resolution"
        ],
        "Format": [
            "Zero-shot",
            "Few-shot"
        ],
        "Text Metrics": {
            "Num Dialogs": 108678,
            "Mean Inputs Length": 452.0497,
            "Mean Targets Length": 26.3727,
            "Max Inputs Length": 1664,
            "Max Targets Length": 154,
            "Min Inputs Length": 26,
            "Min Targets Length": 2,
            "Min Dialog Turns": 2,
            "Max Dialog Turns": 2,
            "Mean Dialog Turns": 2.0
        },
        "Text Sources": [
            "crowdsourced"
        ],
        "Model Generated": [],
        "Creators": [
            "AI2",
            "University of Washington"
        ],
        "Licenses": [
            {
                "License": "CC BY 4.0",
                "License URL": "https://github.com/allenai/winogrande"
            }
        ],
        "License Notes": "",
        "License Verified By": "Shayne",
        "Dataset Filter IDs": [
            "winogrande:1.1.0"
        ],
        "Inferred Metadata": {
            "HF Dataset": "winogrande",
            "HF Config": "winogrande_xs",
            "HF Config License": "",
            "HF Yaml License": "",
            "PwC License Name": "CC BY 4.0",
            "PwC License URL": "https://github.com/allenai/winogrande",
            "PwC Date": "",
            "S2 Date": "2019-07-24",
            "GitHub License": "Apache License 2.0",
            "Github Date": "",
            "HF Date": "",
            "HF Downloads (September 2023)": 97059,
            "HF Likes (September 2023)": 21,
            "PwC Description": "WinoGrande is a large-scale dataset of 44k problems, inspired by the original WSC design, but adjusted to improve both the scale and the hardness of the dataset. The key steps of the dataset construction consist of (1) a carefully designed crowdsourcing procedure, followed by (2) systematic bias reduction using a novel AfLite algorithm that generalizes human-detectable word associations to machine-detectable embedding associations.",
            "S2 Citation Count (September 2023)": 495,
            "GitHub Stars": 67,
            "GitHub Topics": [],
            "Text Topics": []
        },
        "Derived from Datasets": [],
        "Human Annotation": "Yes",
        "Bibtex": "@Article{Sakaguchi2019WinoGrande,\n author = {Keisuke Sakaguchi and Ronan Le Bras and Chandra Bhagavatula and Yejin Choi},\n booktitle = {AAAI Conference on Artificial Intelligence},\n journal = {Communications of the ACM},\n pages = {99 - 106},\n title = {WinoGrande},\n volume = {64},\n year = {2019}\n}\n"
    },
    "fc-flan-wmt14_enfr": {
        "Unique Dataset Identifier": "fc-flan-wmt14_enfr",
        "Dataset Name": "wmt14_enfr",
        "Collection": "Flan Collection (Flan 2021)",
        "Collection URL": "https://github.com/google-research/FLAN/tree/main/flan/v2",
        "Dataset URL": "https://www.statmt.org/wmt14/translation-task.html",
        "GitHub URL": "",
        "Hugging Face URL": "https://huggingface.co/datasets/wmt14",
        "Paper Title": "Findings of the 2014 Workshop on Statistical Machine Translation",
        "Papers with Code URL": "https://paperswithcode.com/dataset/wmt-2014",
        "ArXiv URL": "https://aclanthology.org/W14-3302/",
        "Semantic Scholar Corpus ID": 15535376,
        "Languages": [
            "English",
            "French"
        ],
        "Task Categories": [
            "Translation",
            "Inverted Translation"
        ],
        "Format": [
            "Zero-shot",
            "Few-shot"
        ],
        "Text Metrics": {
            "Num Dialogs": 109197,
            "Mean Inputs Length": 754.8683,
            "Mean Targets Length": 162.3348,
            "Max Inputs Length": 7426,
            "Max Targets Length": 4858,
            "Min Inputs Length": 23,
            "Min Targets Length": 1,
            "Min Dialog Turns": 2,
            "Max Dialog Turns": 2,
            "Mean Dialog Turns": 2.0
        },
        "Text Sources": [
            "commoncrawl.org",
            "wikipedia.org",
            "united nations",
            "commoncrawl.org",
            "yandex",
            "news commentary",
            "european parliament proceedings"
        ],
        "Model Generated": [
            "Other"
        ],
        "Creators": [
            "Charles University",
            "Christian Buck University of Edinburgh",
            "Microsoft Research",
            "Dublin City University",
            "University of Amsterdam",
            "Johns Hopkins University",
            "Google",
            "University of Sheffield"
        ],
        "Licenses": [
            {
                "License": "Unspecified",
                "License URL": "Unspecified"
            }
        ],
        "License Notes": "",
        "License Verified By": "Shayne",
        "Dataset Filter IDs": [
            "wmt14_translate/fr-en:1.0.0"
        ],
        "Inferred Metadata": {
            "HF Dataset": "wmt14",
            "HF Config": "cs-en",
            "HF Config License": "",
            "HF Yaml License": "",
            "PwC License Name": "Unspecified",
            "PwC License URL": "",
            "PwC Date": "2014-01-01",
            "S2 Date": "2014-06-01",
            "GitHub License": "",
            "Github Date": "",
            "HF Date": "2022-01-25",
            "HF Downloads (September 2023)": 47479,
            "HF Likes (September 2023)": 4,
            "PwC Description": "WMT 2014 is a collection of datasets used in shared tasks of the Ninth Workshop on Statistical Machine Translation. The workshop featured four tasks:\n\n\na news translation task,\na quality estimation task,\na metrics task,\na medical text translation task.",
            "S2 Citation Count (September 2023)": 580,
            "GitHub Stars": "",
            "GitHub Topics": [],
            "Text Topics": []
        },
        "Derived from Datasets": [
            "Europarl Parallel Corpus",
            "News Commentary Parallel Corpus",
            "Common Crawl Parallel Corpus",
            "United Nations Parallel Corpus",
            "10^9 Word Parallel Corpus",
            "CzEng Parallel Corpus",
            "Hindi-English Parallel Corpus",
            "Yandex 1M Parallel Corpus",
            "Wiki Headlines Parallel Corpus",
            "Europarl Language Model Data",
            "News Language Model data"
        ],
        "Human Annotation": "Yes",
        "Bibtex": "@Article{Bojar2014FindingsOT,\n author = {Ondrej Bojar and C. Buck and C. Federmann and B. Haddow and Philipp Koehn and Johannes Leveling and Christof Monz and Pavel Pecina and Matt Post and Herve Saint-Amand and Radu Soricut and Lucia Specia and A. Tamchyna},\n booktitle = {WMT@ACL},\n pages = {12-58},\n title = {Findings of the 2014 Workshop on Statistical Machine Translation},\n year = {2014}\n}\n"
    },
    "fc-flan-wmt16_translate_csen": {
        "Unique Dataset Identifier": "fc-flan-wmt16_translate_csen",
        "Dataset Name": "wmt16_translate_csen",
        "Collection": "Flan Collection (Flan 2021)",
        "Collection URL": "https://github.com/google-research/FLAN/tree/main/flan/v2",
        "Dataset URL": "https://www.statmt.org/wmt16/translation-task.html",
        "GitHub URL": "",
        "Hugging Face URL": "https://huggingface.co/datasets/wmt16",
        "Paper Title": "Findings of the 2016 Conference on Machine Translation",
        "Papers with Code URL": "https://paperswithcode.com/dataset/wmt-2016",
        "ArXiv URL": "https://aclanthology.org/W16-2301/",
        "Semantic Scholar Corpus ID": 14421595,
        "Languages": [
            "English",
            "Czech"
        ],
        "Task Categories": [
            "Translation",
            "Inverted Translation"
        ],
        "Format": [
            "Zero-shot",
            "Few-shot"
        ],
        "Text Metrics": {
            "Num Dialogs": 108185,
            "Mean Inputs Length": 326.262,
            "Mean Targets Length": 55.5622,
            "Max Inputs Length": 7553,
            "Max Targets Length": 7519,
            "Min Inputs Length": 23,
            "Min Targets Length": 1,
            "Min Dialog Turns": 2,
            "Max Dialog Turns": 2,
            "Mean Dialog Turns": 2.0
        },
        "Text Sources": [
            "united nations",
            "commoncrawl.org",
            "yandex",
            "wikipedia headlines",
            "czeng v1.6pre",
            "opus news-commentary",
            "opensubtitles.org",
            "european parliament proceedings"
        ],
        "Model Generated": [],
        "Creators": [
            "Charles University",
            "Microsoft Research",
            "Dublin City University",
            "University of Edinburgh",
            "IBM",
            "Johns Hopkins University",
            "University of Sheffield",
            "University of Amsterdam",
            "Hasso Plattner Institute - Potsdam",
            "Charles University",
            "Johns Hopkins University",
            "Saarland University",
            "University of Melbourne"
        ],
        "Licenses": [
            {
                "License": "Unspecified",
                "License URL": "Unspecified"
            }
        ],
        "License Notes": "",
        "License Verified By": "Shayne",
        "Dataset Filter IDs": [
            "wmt16_translate/cs-en:1.0.0"
        ],
        "Inferred Metadata": {
            "HF Dataset": "wmt16",
            "HF Config": "cs-en",
            "HF Config License": "",
            "HF Yaml License": "",
            "PwC License Name": "",
            "PwC License URL": "",
            "PwC Date": "2016-01-01",
            "S2 Date": "2016-08-12",
            "GitHub License": "",
            "Github Date": "",
            "HF Date": "2022-01-25",
            "HF Downloads (September 2023)": 35204,
            "HF Likes (September 2023)": 12,
            "PwC Description": "WMT 2016 is a collection of datasets used in shared tasks of the First Conference on Machine Translation. The conference builds on ten previous Workshops on statistical Machine Translation.\n\nThe conference featured ten shared tasks:\n\n\na news translation task,\nan IT domain translation task,\na biomedical translation task,\nan automatic post-editing task,\na metrics task (assess MT quality given reference translation).\na quality estimation task (assess MT quality without access to any reference),\na tuning task (optimize a given MT system),\na pronoun translation task,\na bilingual document alignment task,\na multimodal translation task.",
            "S2 Citation Count (September 2023)": 627,
            "GitHub Stars": "",
            "GitHub Topics": [],
            "Text Topics": []
        },
        "Derived from Datasets": [
            "Europarl",
            "Europarl3"
        ],
        "Human Annotation": "Yes",
        "Bibtex": "@Article{Bojar2016FindingsOT,\n author = {Ondrej Bojar and Rajen Chatterjee and C. Federmann and Yvette Graham and B. Haddow and Matthias Huck and Antonio Jimeno-Yepes and Philipp Koehn and V. Logacheva and Christof Monz and Matteo Negri and Aurélie Névéol and Mariana Neves and M. Popel and Matt Post and Raphaël Rubino and Carolina Scarton and Lucia Specia and Marco Turchi and Karin M. Verspoor and Marcos Zampieri},\n booktitle = {Conference on Machine Translation},\n pages = {131-198},\n title = {Findings of the 2016 Conference on Machine Translation},\n year = {2016}\n}\n"
    },
    "fc-flan-wmt16_translate_deen": {
        "Unique Dataset Identifier": "fc-flan-wmt16_translate_deen",
        "Dataset Name": "wmt16_translate_deen",
        "Collection": "Flan Collection (Flan 2021)",
        "Collection URL": "https://github.com/google-research/FLAN/tree/main/flan/v2",
        "Dataset URL": "https://www.statmt.org/wmt16/translation-task.html",
        "GitHub URL": "",
        "Hugging Face URL": "https://huggingface.co/datasets/wmt16",
        "Paper Title": "Findings of the 2016 Conference on Machine Translation",
        "Papers with Code URL": "https://paperswithcode.com/dataset/wmt-2016",
        "ArXiv URL": "https://aclanthology.org/W16-2301/",
        "Semantic Scholar Corpus ID": 14421595,
        "Languages": [
            "English",
            "German"
        ],
        "Task Categories": [
            "Translation",
            "Inverted Translation"
        ],
        "Format": [
            "Zero-shot",
            "Few-shot"
        ],
        "Text Metrics": {
            "Num Dialogs": 108628,
            "Mean Inputs Length": 655.2876,
            "Mean Targets Length": 134.6521,
            "Max Inputs Length": 5213,
            "Max Targets Length": 2365,
            "Min Inputs Length": 22,
            "Min Targets Length": 1,
            "Min Dialog Turns": 2,
            "Max Dialog Turns": 2,
            "Mean Dialog Turns": 2.0
        },
        "Text Sources": [
            "united nations",
            "commoncrawl.org",
            "yandex",
            "wikipedia headlines",
            "czeng v1.6pre",
            "opus news-commentary",
            "opensubtitles.org",
            "european parliament proceedings"
        ],
        "Model Generated": [],
        "Creators": [
            "Charles University",
            "Microsoft Research",
            "Dublin City University",
            "University of Edinburgh",
            "IBM",
            "Johns Hopkins University",
            "University of Sheffield",
            "University of Amsterdam",
            "Hasso Plattner Institute - Potsdam",
            "Charles University",
            "Johns Hopkins University",
            "Saarland University",
            "University of Melbourne"
        ],
        "Licenses": [
            {
                "License": "Unspecified",
                "License URL": "Unspecified"
            }
        ],
        "License Notes": "",
        "License Verified By": "Shayne",
        "Dataset Filter IDs": [
            "wmt16_translate/de-en:1.0.0"
        ],
        "Inferred Metadata": {
            "HF Dataset": "wmt16",
            "HF Config": "cs-en",
            "HF Config License": "",
            "HF Yaml License": "",
            "PwC License Name": "",
            "PwC License URL": "",
            "PwC Date": "2016-01-01",
            "S2 Date": "2016-08-12",
            "GitHub License": "",
            "Github Date": "",
            "HF Date": "2022-01-25",
            "HF Downloads (September 2023)": 35204,
            "HF Likes (September 2023)": 12,
            "PwC Description": "WMT 2016 is a collection of datasets used in shared tasks of the First Conference on Machine Translation. The conference builds on ten previous Workshops on statistical Machine Translation.\n\nThe conference featured ten shared tasks:\n\n\na news translation task,\nan IT domain translation task,\na biomedical translation task,\nan automatic post-editing task,\na metrics task (assess MT quality given reference translation).\na quality estimation task (assess MT quality without access to any reference),\na tuning task (optimize a given MT system),\na pronoun translation task,\na bilingual document alignment task,\na multimodal translation task.",
            "S2 Citation Count (September 2023)": 627,
            "GitHub Stars": "",
            "GitHub Topics": [],
            "Text Topics": []
        },
        "Derived from Datasets": [
            "Europarl",
            "Europarl3"
        ],
        "Human Annotation": "Yes",
        "Bibtex": "@Article{Bojar2016FindingsOT,\n author = {Ondrej Bojar and Rajen Chatterjee and C. Federmann and Yvette Graham and B. Haddow and Matthias Huck and Antonio Jimeno-Yepes and Philipp Koehn and V. Logacheva and Christof Monz and Matteo Negri and Aurélie Névéol and Mariana Neves and M. Popel and Matt Post and Raphaël Rubino and Carolina Scarton and Lucia Specia and Marco Turchi and Karin M. Verspoor and Marcos Zampieri},\n booktitle = {Conference on Machine Translation},\n pages = {131-198},\n title = {Findings of the 2016 Conference on Machine Translation},\n year = {2016}\n}\n"
    },
    "fc-flan-wmt16_translate_fien": {
        "Unique Dataset Identifier": "fc-flan-wmt16_translate_fien",
        "Dataset Name": "wmt16_translate_fien",
        "Collection": "Flan Collection (Flan 2021)",
        "Collection URL": "https://github.com/google-research/FLAN/tree/main/flan/v2",
        "Dataset URL": "https://www.statmt.org/wmt16/translation-task.html",
        "GitHub URL": "",
        "Hugging Face URL": "https://huggingface.co/datasets/wmt16",
        "Paper Title": "Findings of the 2016 Conference on Machine Translation",
        "Papers with Code URL": "https://paperswithcode.com/dataset/wmt-2016",
        "ArXiv URL": "https://aclanthology.org/W16-2301/",
        "Semantic Scholar Corpus ID": 14421595,
        "Languages": [
            "English",
            "Finnish"
        ],
        "Task Categories": [
            "Translation",
            "Inverted Translation"
        ],
        "Format": [
            "Zero-shot",
            "Few-shot"
        ],
        "Text Metrics": {
            "Num Dialogs": 108923,
            "Mean Inputs Length": 628.5495,
            "Mean Targets Length": 127.6058,
            "Max Inputs Length": 4623,
            "Max Targets Length": 1125,
            "Min Inputs Length": 23,
            "Min Targets Length": 1,
            "Min Dialog Turns": 2,
            "Max Dialog Turns": 2,
            "Mean Dialog Turns": 2.0
        },
        "Text Sources": [
            "united nations",
            "commoncrawl.org",
            "yandex",
            "wikipedia headlines",
            "czeng v1.6pre",
            "opus news-commentary",
            "opensubtitles.org",
            "european parliament proceedings"
        ],
        "Model Generated": [],
        "Creators": [
            "Charles University",
            "Microsoft Research",
            "Dublin City University",
            "University of Edinburgh",
            "IBM",
            "Johns Hopkins University",
            "University of Sheffield",
            "University of Amsterdam",
            "Hasso Plattner Institute - Potsdam",
            "Saarland University",
            "University of Melbourne"
        ],
        "Licenses": [
            {
                "License": "Unspecified",
                "License URL": "Unspecified"
            }
        ],
        "License Notes": "",
        "License Verified By": "Shayne",
        "Dataset Filter IDs": [
            "wmt16_translate/fi-en:1.0.0"
        ],
        "Inferred Metadata": {
            "HF Dataset": "wmt16",
            "HF Config": "cs-en",
            "HF Config License": "",
            "HF Yaml License": "",
            "PwC License Name": "",
            "PwC License URL": "",
            "PwC Date": "2016-01-01",
            "S2 Date": "2016-08-12",
            "GitHub License": "",
            "Github Date": "",
            "HF Date": "2022-01-25",
            "HF Downloads (September 2023)": 35204,
            "HF Likes (September 2023)": 12,
            "PwC Description": "WMT 2016 is a collection of datasets used in shared tasks of the First Conference on Machine Translation. The conference builds on ten previous Workshops on statistical Machine Translation.\n\nThe conference featured ten shared tasks:\n\n\na news translation task,\nan IT domain translation task,\na biomedical translation task,\nan automatic post-editing task,\na metrics task (assess MT quality given reference translation).\na quality estimation task (assess MT quality without access to any reference),\na tuning task (optimize a given MT system),\na pronoun translation task,\na bilingual document alignment task,\na multimodal translation task.",
            "S2 Citation Count (September 2023)": 627,
            "GitHub Stars": "",
            "GitHub Topics": [],
            "Text Topics": []
        },
        "Derived from Datasets": [
            "Europarl",
            "Europarl3"
        ],
        "Human Annotation": "Yes",
        "Bibtex": "@Article{Bojar2016FindingsOT,\n author = {Ondrej Bojar and Rajen Chatterjee and C. Federmann and Yvette Graham and B. Haddow and Matthias Huck and Antonio Jimeno-Yepes and Philipp Koehn and V. Logacheva and Christof Monz and Matteo Negri and Aurélie Névéol and Mariana Neves and M. Popel and Matt Post and Raphaël Rubino and Carolina Scarton and Lucia Specia and Marco Turchi and Karin M. Verspoor and Marcos Zampieri},\n booktitle = {Conference on Machine Translation},\n pages = {131-198},\n title = {Findings of the 2016 Conference on Machine Translation},\n year = {2016}\n}\n"
    },
    "fc-flan-wmt16_translate_roen": {
        "Unique Dataset Identifier": "fc-flan-wmt16_translate_roen",
        "Dataset Name": "wmt16_translate_roen",
        "Collection": "Flan Collection (Flan 2021)",
        "Collection URL": "https://github.com/google-research/FLAN/tree/main/flan/v2",
        "Dataset URL": "https://www.statmt.org/wmt16/translation-task.html",
        "GitHub URL": "",
        "Hugging Face URL": "https://huggingface.co/datasets/wmt16",
        "Paper Title": "Findings of the 2016 Conference on Machine Translation",
        "Papers with Code URL": "https://paperswithcode.com/dataset/wmt-2016",
        "ArXiv URL": "https://aclanthology.org/W16-2301/",
        "Semantic Scholar Corpus ID": 14421595,
        "Languages": [
            "English",
            "Romanian"
        ],
        "Task Categories": [
            "Translation",
            "Inverted Translation"
        ],
        "Format": [
            "Zero-shot",
            "Few-shot"
        ],
        "Text Metrics": {
            "Num Dialogs": 108611,
            "Mean Inputs Length": 664.8995,
            "Mean Targets Length": 136.3313,
            "Max Inputs Length": 4512,
            "Max Targets Length": 1110,
            "Min Inputs Length": 24,
            "Min Targets Length": 1,
            "Min Dialog Turns": 2,
            "Max Dialog Turns": 2,
            "Mean Dialog Turns": 2.0
        },
        "Text Sources": [
            "united nations",
            "commoncrawl.org",
            "yandex",
            "wikipedia headlines",
            "czeng v1.6pre",
            "opus news-commentary",
            "opensubtitles.org",
            "european parliament proceedings"
        ],
        "Model Generated": [],
        "Creators": [
            "Charles University",
            "Microsoft Research",
            "Dublin City University",
            "University of Edinburgh",
            "IBM",
            "Johns Hopkins University",
            "University of Sheffield",
            "University of Amsterdam",
            "Hasso Plattner Institute - Potsdam",
            "Charles University",
            "Johns Hopkins University",
            "Saarland University",
            "University of Melbourne"
        ],
        "Licenses": [
            {
                "License": "Unspecified",
                "License URL": "Unspecified"
            }
        ],
        "License Notes": "",
        "License Verified By": "Shayne",
        "Dataset Filter IDs": [
            "wmt16_translate/ro-en:1.0.0"
        ],
        "Inferred Metadata": {
            "HF Dataset": "wmt16",
            "HF Config": "cs-en",
            "HF Config License": "",
            "HF Yaml License": "",
            "PwC License Name": "",
            "PwC License URL": "",
            "PwC Date": "2016-01-01",
            "S2 Date": "2016-08-12",
            "GitHub License": "",
            "Github Date": "",
            "HF Date": "2022-01-25",
            "HF Downloads (September 2023)": 35204,
            "HF Likes (September 2023)": 12,
            "PwC Description": "WMT 2016 is a collection of datasets used in shared tasks of the First Conference on Machine Translation. The conference builds on ten previous Workshops on statistical Machine Translation.\n\nThe conference featured ten shared tasks:\n\n\na news translation task,\nan IT domain translation task,\na biomedical translation task,\nan automatic post-editing task,\na metrics task (assess MT quality given reference translation).\na quality estimation task (assess MT quality without access to any reference),\na tuning task (optimize a given MT system),\na pronoun translation task,\na bilingual document alignment task,\na multimodal translation task.",
            "S2 Citation Count (September 2023)": 627,
            "GitHub Stars": "",
            "GitHub Topics": [],
            "Text Topics": []
        },
        "Derived from Datasets": [
            "Europarl",
            "Europarl3"
        ],
        "Human Annotation": "Yes",
        "Bibtex": "@Article{Bojar2016FindingsOT,\n author = {Ondrej Bojar and Rajen Chatterjee and C. Federmann and Yvette Graham and B. Haddow and Matthias Huck and Antonio Jimeno-Yepes and Philipp Koehn and V. Logacheva and Christof Monz and Matteo Negri and Aurélie Névéol and Mariana Neves and M. Popel and Matt Post and Raphaël Rubino and Carolina Scarton and Lucia Specia and Marco Turchi and Karin M. Verspoor and Marcos Zampieri},\n booktitle = {Conference on Machine Translation},\n pages = {131-198},\n title = {Findings of the 2016 Conference on Machine Translation},\n year = {2016}\n}\n"
    },
    "fc-flan-wmt16_translate_ruen": {
        "Unique Dataset Identifier": "fc-flan-wmt16_translate_ruen",
        "Dataset Name": "wmt16_translate_ruen",
        "Collection": "Flan Collection (Flan 2021)",
        "Collection URL": "https://github.com/google-research/FLAN/tree/main/flan/v2",
        "Dataset URL": "https://www.statmt.org/wmt16/translation-task.html",
        "GitHub URL": "",
        "Hugging Face URL": "https://huggingface.co/datasets/wmt16",
        "Paper Title": "Findings of the 2016 Conference on Machine Translation",
        "Papers with Code URL": "https://paperswithcode.com/dataset/wmt-2016",
        "ArXiv URL": "https://aclanthology.org/W16-2301/",
        "Semantic Scholar Corpus ID": 14421595,
        "Languages": [
            "English",
            "Russian"
        ],
        "Task Categories": [
            "Translation",
            "Inverted Translation"
        ],
        "Format": [
            "Zero-shot",
            "Few-shot"
        ],
        "Text Metrics": {
            "Num Dialogs": 108682,
            "Mean Inputs Length": 543.9277,
            "Mean Targets Length": 107.3493,
            "Max Inputs Length": 3896,
            "Max Targets Length": 1749,
            "Min Inputs Length": 23,
            "Min Targets Length": 1,
            "Min Dialog Turns": 2,
            "Max Dialog Turns": 2,
            "Mean Dialog Turns": 2.0
        },
        "Text Sources": [
            "united nations",
            "commoncrawl.org",
            "yandex",
            "wikipedia headlines",
            "czeng v1.6pre",
            "opus news-commentary",
            "opensubtitles.org",
            "european parliament proceedings"
        ],
        "Model Generated": [],
        "Creators": [
            "Charles University",
            "Microsoft Research",
            "Dublin City University",
            "University of Edinburgh",
            "IBM",
            "Johns Hopkins University",
            "University of Sheffield",
            "University of Amsterdam",
            "Hasso Plattner Institute - Potsdam",
            "Charles University",
            "Johns Hopkins University",
            "Saarland University",
            "University of Melbourne"
        ],
        "Licenses": [
            {
                "License": "Unspecified",
                "License URL": "Unspecified"
            }
        ],
        "License Notes": "",
        "License Verified By": "Shayne",
        "Dataset Filter IDs": [
            "wmt16_translate/ru-en:1.0.0"
        ],
        "Inferred Metadata": {
            "HF Dataset": "wmt16",
            "HF Config": "cs-en",
            "HF Config License": "",
            "HF Yaml License": "",
            "PwC License Name": "",
            "PwC License URL": "",
            "PwC Date": "2016-01-01",
            "S2 Date": "2016-08-12",
            "GitHub License": "",
            "Github Date": "",
            "HF Date": "2022-01-25",
            "HF Downloads (September 2023)": 35204,
            "HF Likes (September 2023)": 12,
            "PwC Description": "WMT 2016 is a collection of datasets used in shared tasks of the First Conference on Machine Translation. The conference builds on ten previous Workshops on statistical Machine Translation.\n\nThe conference featured ten shared tasks:\n\n\na news translation task,\nan IT domain translation task,\na biomedical translation task,\nan automatic post-editing task,\na metrics task (assess MT quality given reference translation).\na quality estimation task (assess MT quality without access to any reference),\na tuning task (optimize a given MT system),\na pronoun translation task,\na bilingual document alignment task,\na multimodal translation task.",
            "S2 Citation Count (September 2023)": 627,
            "GitHub Stars": "",
            "GitHub Topics": [],
            "Text Topics": []
        },
        "Derived from Datasets": [
            "Europarl",
            "Europarl3"
        ],
        "Human Annotation": "Yes",
        "Bibtex": "@Article{Bojar2016FindingsOT,\n author = {Ondrej Bojar and Rajen Chatterjee and C. Federmann and Yvette Graham and B. Haddow and Matthias Huck and Antonio Jimeno-Yepes and Philipp Koehn and V. Logacheva and Christof Monz and Matteo Negri and Aurélie Névéol and Mariana Neves and M. Popel and Matt Post and Raphaël Rubino and Carolina Scarton and Lucia Specia and Marco Turchi and Karin M. Verspoor and Marcos Zampieri},\n booktitle = {Conference on Machine Translation},\n pages = {131-198},\n title = {Findings of the 2016 Conference on Machine Translation},\n year = {2016}\n}\n"
    },
    "fc-flan-wmt16_translate_tren": {
        "Unique Dataset Identifier": "fc-flan-wmt16_translate_tren",
        "Dataset Name": "wmt16_translate_tren",
        "Collection": "Flan Collection (Flan 2021)",
        "Collection URL": "https://github.com/google-research/FLAN/tree/main/flan/v2",
        "Dataset URL": "https://www.statmt.org/wmt16/translation-task.html",
        "GitHub URL": "",
        "Hugging Face URL": "https://huggingface.co/datasets/wmt16",
        "Paper Title": "Findings of the 2016 Conference on Machine Translation",
        "Papers with Code URL": "https://paperswithcode.com/dataset/wmt-2016",
        "ArXiv URL": "https://aclanthology.org/W16-2301/",
        "Semantic Scholar Corpus ID": 14421595,
        "Languages": [
            "English",
            "Turkish"
        ],
        "Task Categories": [
            "Translation",
            "Inverted Translation"
        ],
        "Format": [
            "Zero-shot",
            "Few-shot"
        ],
        "Text Metrics": {
            "Num Dialogs": 108442,
            "Mean Inputs Length": 620.1376,
            "Mean Targets Length": 124.9894,
            "Max Inputs Length": 3005,
            "Max Targets Length": 607,
            "Min Inputs Length": 25,
            "Min Targets Length": 2,
            "Min Dialog Turns": 2,
            "Max Dialog Turns": 2,
            "Mean Dialog Turns": 2.0
        },
        "Text Sources": [
            "united nations",
            "commoncrawl.org",
            "yandex",
            "wikipedia headlines",
            "czeng v1.6pre",
            "opus news-commentary",
            "opensubtitles.org",
            "european parliament proceedings"
        ],
        "Model Generated": [],
        "Creators": [
            "Charles University",
            "Microsoft Research",
            "Dublin City University",
            "University of Edinburgh",
            "IBM",
            "Johns Hopkins University",
            "University of Sheffield",
            "University of Amsterdam",
            "Hasso Plattner Institute - Potsdam",
            "Charles University",
            "Johns Hopkins University",
            "Saarland University",
            "University of Melbourne"
        ],
        "Licenses": [
            {
                "License": "Unspecified",
                "License URL": "Unspecified"
            }
        ],
        "License Notes": "",
        "License Verified By": "Shayne",
        "Dataset Filter IDs": [
            "wmt16_translate/tr-en:1.0.0"
        ],
        "Inferred Metadata": {
            "HF Dataset": "wmt16",
            "HF Config": "cs-en",
            "HF Config License": "",
            "HF Yaml License": "",
            "PwC License Name": "",
            "PwC License URL": "",
            "PwC Date": "2016-01-01",
            "S2 Date": "2016-08-12",
            "GitHub License": "",
            "Github Date": "",
            "HF Date": "2022-01-25",
            "HF Downloads (September 2023)": 35204,
            "HF Likes (September 2023)": 12,
            "PwC Description": "WMT 2016 is a collection of datasets used in shared tasks of the First Conference on Machine Translation. The conference builds on ten previous Workshops on statistical Machine Translation.\n\nThe conference featured ten shared tasks:\n\n\na news translation task,\nan IT domain translation task,\na biomedical translation task,\nan automatic post-editing task,\na metrics task (assess MT quality given reference translation).\na quality estimation task (assess MT quality without access to any reference),\na tuning task (optimize a given MT system),\na pronoun translation task,\na bilingual document alignment task,\na multimodal translation task.",
            "S2 Citation Count (September 2023)": 627,
            "GitHub Stars": "",
            "GitHub Topics": [],
            "Text Topics": []
        },
        "Derived from Datasets": [
            "Europarl",
            "Europarl3"
        ],
        "Human Annotation": "Yes",
        "Bibtex": "@Article{Bojar2016FindingsOT,\n author = {Ondrej Bojar and Rajen Chatterjee and C. Federmann and Yvette Graham and B. Haddow and Matthias Huck and Antonio Jimeno-Yepes and Philipp Koehn and V. Logacheva and Christof Monz and Matteo Negri and Aurélie Névéol and Mariana Neves and M. Popel and Matt Post and Raphaël Rubino and Carolina Scarton and Lucia Specia and Marco Turchi and Karin M. Verspoor and Marcos Zampieri},\n booktitle = {Conference on Machine Translation},\n pages = {131-198},\n title = {Findings of the 2016 Conference on Machine Translation},\n year = {2016}\n}\n"
    },
    "fc-flan-wnli": {
        "Unique Dataset Identifier": "fc-flan-wnli",
        "Dataset Name": "wnli",
        "Collection": "Flan Collection (Flan 2021)",
        "Collection URL": "https://github.com/google-research/FLAN/tree/main/flan/v2",
        "Dataset URL": "https://cs.nyu.edu/~davise/papers/WinogradSchemas/WS.html",
        "GitHub URL": "",
        "Hugging Face URL": "https://huggingface.co/datasets/SetFit/wnli",
        "Paper Title": "The Winograd Schema Challenge",
        "Papers with Code URL": "https://paperswithcode.com/dataset/wsc",
        "ArXiv URL": "",
        "Semantic Scholar Corpus ID": 15710851,
        "Languages": [
            "English"
        ],
        "Task Categories": [
            "Coreference Resolution",
            "Inverted Coreference Resolution"
        ],
        "Format": [
            "Zero-shot",
            "Few-shot"
        ],
        "Text Metrics": {
            "Num Dialogs": 2111,
            "Mean Inputs Length": 567.7494,
            "Mean Targets Length": 2.8626,
            "Max Inputs Length": 2024,
            "Max Targets Length": 8,
            "Min Inputs Length": 80,
            "Min Targets Length": 2,
            "Min Dialog Turns": 2,
            "Max Dialog Turns": 2,
            "Mean Dialog Turns": 2.0
        },
        "Text Sources": [],
        "Model Generated": [],
        "Creators": [
            "University of Toronto",
            "New York University",
            "S.A.I.C"
        ],
        "Licenses": [
            {
                "License": "CC BY 4.0",
                "License URL": "https://creativecommons.org/licenses/by/4.0/"
            }
        ],
        "License Notes": "",
        "License Verified By": "Shayne",
        "Dataset Filter IDs": [
            "glue/wnli:2.0.0"
        ],
        "Inferred Metadata": {
            "HF Dataset": "SetFit/wnli",
            "HF Config": "SetFit--wnli",
            "HF Config License": "",
            "HF Yaml License": "",
            "PwC License Name": "CC BY 4.0",
            "PwC License URL": "https://creativecommons.org/licenses/by/4.0/",
            "PwC Date": "2012-01-01",
            "S2 Date": "2011-03-20",
            "GitHub License": "",
            "Github Date": "",
            "HF Date": "2022-02-28",
            "HF Downloads (September 2023)": 567,
            "HF Likes (September 2023)": 0,
            "PwC Description": "The Winograd Schema Challenge was introduced both as an alternative to the Turing Test and as a test of a system’s ability to do commonsense reasoning. A Winograd schema is a pair of sentences differing in one or two words with a highly ambiguous pronoun, resolved differently in the two sentences, that appears to require commonsense knowledge to be resolved correctly. The examples were designed to be easily solvable by humans but difficult for machines, in principle requiring a deep understanding of the content of the text and the situation it describes.\n\nThe original Winograd Schema Challenge dataset consisted of 100 Winograd schemas constructed manually by AI experts. As of 2020 there are 285 examples available; however, the last 12 examples were only added recently. To ensure consistency with earlier models, several authors often prefer to report the performance on the first 273 examples only. These datasets are usually referred to as WSC285 and WSC273, respectively.",
            "S2 Citation Count (September 2023)": 1000,
            "GitHub Stars": "",
            "GitHub Topics": [],
            "Text Topics": []
        },
        "Human Annotation": "No",
        "Derived from Datasets": [],
        "Bibtex": "@Article{Levesque2011TheWS,\n author = {H. Levesque and E. Davis and L. Morgenstern},\n booktitle = {AAAI Spring Symposium: Logical Formalizations of Commonsense Reasoning},\n title = {The Winograd Schema Challenge},\n year = {2011}\n}\n"
    },
    "fc-flan-word_segment": {
        "Unique Dataset Identifier": "fc-flan-word_segment",
        "Dataset Name": "word_segment",
        "Collection": "Flan Collection (Flan 2021)",
        "Collection URL": "https://github.com/google-research/FLAN/tree/main/flan/v2",
        "Dataset URL": "https://www.paracrawl.eu/",
        "GitHub URL": "",
        "Hugging Face URL": "",
        "Paper Title": "",
        "Papers with Code URL": "",
        "ArXiv URL": "",
        "Semantic Scholar Corpus ID": "",
        "Languages": [
            "English"
        ],
        "Task Categories": [
            "Word Segmentation",
            "Inverted Word Segmentation"
        ],
        "Format": [
            "Zero-shot",
            "Few-shot"
        ],
        "Text Metrics": {
            "Num Dialogs": 108728,
            "Mean Inputs Length": 562.4387,
            "Mean Targets Length": 116.4661,
            "Max Inputs Length": 3821,
            "Max Targets Length": 1016,
            "Min Inputs Length": 21,
            "Min Targets Length": 12,
            "Min Dialog Turns": 2,
            "Max Dialog Turns": 2,
            "Mean Dialog Turns": 2.0
        },
        "Text Sources": [],
        "Model Generated": [],
        "Creators": [],
        "Licenses": [
            {
                "License": "CC0 1.0",
                "License URL": "https://creativecommons.org/share-your-work/public-domain/cc0/"
            }
        ],
        "License Notes": "",
        "License Verified By": "Shayne",
        "Dataset Filter IDs": [
            "word_segment"
        ],
        "Inferred Metadata": {
            "HF Dataset": "",
            "HF Config": "",
            "HF Config License": "",
            "HF Yaml License": "",
            "PwC License Name": "",
            "PwC License URL": "",
            "PwC Date": "",
            "S2 Date": "",
            "GitHub License": "",
            "Github Date": "",
            "HF Date": "",
            "HF Downloads (September 2023)": "",
            "HF Likes (September 2023)": "",
            "PwC Description": "",
            "S2 Citation Count (September 2023)": "",
            "GitHub Stars": "",
            "GitHub Topics": [],
            "Text Topics": [
                "Communication",
                "Text manipulation",
                "Text editing",
                "Transportation",
                "Translation",
                "Text formatting",
                "Religion",
                "Language processing",
                "Travel"
            ]
        },
        "Human Annotation": "No",
        "Derived from Datasets": [],
        "Bibtex": ""
    },
    "fc-flan-wsc": {
        "Unique Dataset Identifier": "fc-flan-wsc",
        "Dataset Name": "wsc",
        "Collection": "Flan Collection (Flan 2021)",
        "Collection URL": "https://github.com/google-research/FLAN/tree/main/flan/v2",
        "Dataset URL": "https://cs.nyu.edu/~davise/papers/WinogradSchemas/WS.html",
        "GitHub URL": "",
        "Hugging Face URL": "https://huggingface.co/datasets/winograd_wsc",
        "Paper Title": "The Winograd Schema Challenge",
        "Papers with Code URL": "https://paperswithcode.com/dataset/wsc",
        "ArXiv URL": "",
        "Semantic Scholar Corpus ID": 15710851,
        "Languages": [
            "English"
        ],
        "Task Categories": [
            "Coreference Resolution",
            "Inverted Coreference Resolution"
        ],
        "Format": [
            "Zero-shot",
            "Few-shot"
        ],
        "Text Metrics": {
            "Num Dialogs": 1832,
            "Mean Inputs Length": 518.5437,
            "Mean Targets Length": 2.6572,
            "Max Inputs Length": 2005,
            "Max Targets Length": 5,
            "Min Inputs Length": 78,
            "Min Targets Length": 2,
            "Min Dialog Turns": 2,
            "Max Dialog Turns": 2,
            "Mean Dialog Turns": 2.0
        },
        "Text Sources": [],
        "Model Generated": [],
        "Creators": [
            "University of Toronto",
            "New York University",
            "S.A.I.C"
        ],
        "Licenses": [
            {
                "License": "CC BY 4.0",
                "License URL": "https://cs.nyu.edu/~davise/papers/WinogradSchemas/WS.html"
            }
        ],
        "License Notes": "",
        "License Verified By": "Shayne",
        "Dataset Filter IDs": [
            "super_glue/wsc.fixed:1.0.2"
        ],
        "Inferred Metadata": {
            "HF Dataset": "winograd_wsc",
            "HF Config": "wsc285",
            "HF Config License": "",
            "HF Yaml License": "",
            "PwC License Name": "CC BY 4.0",
            "PwC License URL": "https://creativecommons.org/licenses/by/4.0/",
            "PwC Date": "2012-01-01",
            "S2 Date": "2011-03-20",
            "GitHub License": "",
            "Github Date": "",
            "HF Date": "2022-01-25",
            "HF Downloads (September 2023)": 3264,
            "HF Likes (September 2023)": 5,
            "PwC Description": "The Winograd Schema Challenge was introduced both as an alternative to the Turing Test and as a test of a system’s ability to do commonsense reasoning. A Winograd schema is a pair of sentences differing in one or two words with a highly ambiguous pronoun, resolved differently in the two sentences, that appears to require commonsense knowledge to be resolved correctly. The examples were designed to be easily solvable by humans but difficult for machines, in principle requiring a deep understanding of the content of the text and the situation it describes.\n\nThe original Winograd Schema Challenge dataset consisted of 100 Winograd schemas constructed manually by AI experts. As of 2020 there are 285 examples available; however, the last 12 examples were only added recently. To ensure consistency with earlier models, several authors often prefer to report the performance on the first 273 examples only. These datasets are usually referred to as WSC285 and WSC273, respectively.",
            "S2 Citation Count (September 2023)": 1000,
            "GitHub Stars": "",
            "GitHub Topics": [],
            "Text Topics": []
        },
        "Human Annotation": "No",
        "Derived from Datasets": [],
        "Bibtex": "@Article{Levesque2011TheWS,\n author = {H. Levesque and E. Davis and L. Morgenstern},\n booktitle = {AAAI Spring Symposium: Logical Formalizations of Commonsense Reasoning},\n title = {The Winograd Schema Challenge},\n year = {2011}\n}\n"
    },
    "fc-flan-xsum": {
        "Unique Dataset Identifier": "fc-flan-xsum",
        "Dataset Name": "xsum",
        "Collection": "Flan Collection (Flan 2021)",
        "Collection URL": "https://github.com/google-research/FLAN/tree/main/flan/v2",
        "Dataset URL": "https://github.com/EdinburghNLP/XSum/tree/master/XSum-Dataset",
        "GitHub URL": "https://github.com/EdinburghNLP/XSum/tree/master/XSum-Dataset",
        "Hugging Face URL": "https://huggingface.co/datasets/xsum",
        "Paper Title": "Don't Give Me the Details, Just the Summary! Topic-Aware Convolutional Neural Networks for Extreme Summarization",
        "Papers with Code URL": "https://paperswithcode.com/dataset/xsum",
        "ArXiv URL": "https://arxiv.org/abs/1808.08745",
        "Semantic Scholar Corpus ID": 215768182,
        "Languages": [
            "English"
        ],
        "Task Categories": [
            "Summarization",
            "Inverted Summarization"
        ],
        "Format": [
            "Zero-shot",
            "Few-shot"
        ],
        "Text Metrics": {
            "Num Dialogs": 108628,
            "Mean Inputs Length": 3919.1134,
            "Mean Targets Length": 499.6831,
            "Max Inputs Length": 41434,
            "Max Targets Length": 30927,
            "Min Inputs Length": 10,
            "Min Targets Length": 0,
            "Min Dialog Turns": 2,
            "Max Dialog Turns": 2,
            "Mean Dialog Turns": 2.0
        },
        "Text Sources": [
            "bbc"
        ],
        "Model Generated": [],
        "Creators": [
            "University of Edinburgh"
        ],
        "Licenses": [
            {
                "License": "Unspecified",
                "License URL": "Unspecified"
            }
        ],
        "License Notes": "Data derived from BBC articles",
        "License Verified By": "Shayne",
        "Dataset Filter IDs": [
            "huggingface:xsum"
        ],
        "Inferred Metadata": {
            "HF Dataset": "xsum",
            "HF Config": "default",
            "HF Config License": "",
            "HF Yaml License": "Unspecified",
            "PwC License Name": "",
            "PwC License URL": "",
            "PwC Date": "2018-01-01",
            "S2 Date": "2018-08-27",
            "GitHub License": "MIT License",
            "Github Date": "",
            "HF Date": "2022-01-25",
            "HF Downloads (September 2023)": 22832,
            "HF Likes (September 2023)": 38,
            "PwC Description": "The Extreme Summarization (XSum) dataset is a dataset for evaluation of abstractive single-document summarization systems. The goal is to create a short, one-sentence new summary answering the question “What is the article about?”. The dataset consists of 226,711 news articles accompanied with a one-sentence summary. The articles are collected from BBC articles (2010 to 2017) and cover a wide variety of domains (e.g., News, Politics, Sports, Weather, Business, Technology, Science, Health, Family, Education, Entertainment and Arts). The official random split contains 204,045 (90%), 11,332 (5%) and 11,334 (5) documents in training, validation and test sets, respectively.",
            "S2 Citation Count (September 2023)": 892,
            "GitHub Stars": 316,
            "GitHub Topics": [
                "abstractive-summarization",
                "convolutional-neural-networks",
                "extreme-summarization",
                "topic-aware"
            ],
            "Text Topics": []
        },
        "Derived from Datasets": [],
        "Human Annotation": "No",
        "Bibtex": "@Article{Narayan2018DontGM,\n author = {Shashi Narayan and Shay B. Cohen and Mirella Lapata},\n booktitle = {Conference on Empirical Methods in Natural Language Processing},\n journal = {ArXiv},\n title = {Don’t Give Me the Details, Just the Summary! Topic-Aware Convolutional Neural Networks for Extreme Summarization},\n volume = {abs/1808.08745},\n year = {2018}\n}\n"
    },
    "fc-flan-yelp_polarity_reviews": {
        "Unique Dataset Identifier": "fc-flan-yelp_polarity_reviews",
        "Dataset Name": "yelp_polarity_reviews",
        "Collection": "Flan Collection (Flan 2021)",
        "Collection URL": "https://github.com/google-research/FLAN/tree/main/flan/v2",
        "Dataset URL": "https://www.yelp.com/dataset",
        "GitHub URL": "",
        "Hugging Face URL": "https://huggingface.co/datasets/yelp_review_full",
        "Paper Title": "Character-level Convolutional Networks for Text Classification",
        "Papers with Code URL": "https://paperswithcode.com/dataset/yahoo-answers",
        "ArXiv URL": "https://arxiv.org/abs/1509.01626",
        "Semantic Scholar Corpus ID": 368182,
        "Languages": [
            "English"
        ],
        "Task Categories": [
            "Inverted Sentiment Analysis",
            "Sentiment Analysis"
        ],
        "Format": [
            "Zero-shot",
            "Few-shot"
        ],
        "Text Metrics": {
            "Num Dialogs": 107871,
            "Mean Inputs Length": 1665.2251,
            "Mean Targets Length": 365.5927,
            "Max Inputs Length": 8354,
            "Max Targets Length": 5033,
            "Min Inputs Length": 29,
            "Min Targets Length": 1,
            "Min Dialog Turns": 2,
            "Max Dialog Turns": 2,
            "Mean Dialog Turns": 2.0
        },
        "Text Sources": [
            "undisclosed web",
            "amazon.com",
            "yelp",
            "sogou news",
            "dbpedia",
            "yahoo! answers",
            "ag news"
        ],
        "Model Generated": [],
        "Creators": [
            "New York University"
        ],
        "Licenses": [
            {
                "License": "Custom",
                "License URL": "https://s3-media0.fl.yelpcdn.com/assets/srv0/engineering_pages/dc1cabe7cb95/assets/vendor/Dataset_User_Agreement.pdf"
            }
        ],
        "License Notes": "",
        "License Verified By": "Shayne",
        "Dataset Filter IDs": [
            "yelp_polarity_reviews:0.2.0"
        ],
        "Inferred Metadata": {
            "HF Dataset": "yelp_review_full",
            "HF Config": "yelp_review_full",
            "HF Config License": "Custom",
            "HF Yaml License": "",
            "PwC License Name": "",
            "PwC License URL": "",
            "PwC Date": "2015-09-04",
            "S2 Date": "2015-09-04",
            "GitHub License": "",
            "Github Date": "",
            "HF Date": "2022-01-25",
            "HF Downloads (September 2023)": 24020,
            "HF Likes (September 2023)": 27,
            "PwC Description": "The Yahoo! Answers topic classification dataset is constructed using 10 largest main categories. Each class contains 140,000 training samples and 6,000 testing samples. Therefore, the total number of training samples is 1,400,000 and testing samples 60,000 in this dataset. From all the answers and other meta-information, we only used the best answer content and the main category information.",
            "S2 Citation Count (September 2023)": 4622,
            "GitHub Stars": "",
            "GitHub Topics": [],
            "Text Topics": []
        },
        "Derived from Datasets": [],
        "Human Annotation": "No",
        "Bibtex": "@Article{Zhang2015CharacterlevelCN,\n author = {Xiang Zhang and J. Zhao and Yann LeCun},\n booktitle = {Neural Information Processing Systems},\n pages = {649-657},\n title = {Character-level Convolutional Networks for Text Classification},\n year = {2015}\n}\n"
    }
}
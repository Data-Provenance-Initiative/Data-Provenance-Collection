{
    "toxicchat": {
        "Unique Dataset Identifier": "toxicchat",
        "Dataset Name": "ToxicChat",
        "Collection": "ToxicChat",
        "Collection URL": "https://lmsys.org/blog/2023-10-30-toxicchat/",
        "Paper Title": "ToxicChat: Unveiling Hidden Challenges of Toxicity Detection in Real-World User-AI Conversation",
        "Dataset URL": "https://huggingface.co/datasets/lmsys/toxic-chat",
        "GitHub URL": "",
        "Hugging Face URL": "https://huggingface.co/datasets/lmsys/toxic-chat",
        "Papers with Code URL": "",
        "ArXiv URL": "https://arxiv.org/abs/2310.17389",
        "Semantic Scholar Corpus ID": "264491114",
        "Languages": [
            "English"
        ],
        "Task Categories": [
            "Binary Classification of Hate Speech",
            "Binary Classification of Racial Bias",
            "Offensive Language Detection",
            "Stereotype Detection",
            "Toxicity Detection"
        ],
        "Text Sources": [
            "crowdsourced",
            "human"
        ],
        "Model Generated": [
            "Vicuna"
        ],
        "Format": [
            "Zero-shot"
        ],
        "Human Annotation": "Yes",
        "Derived from Datasets": [
            "LMSYS-Chat-1M",
            "ShareGPT"
        ],
        "Creators": [
            "UC San Diego"
        ],
        "Licenses": [
            {
                "License": "OpenAI",
                "License URL": ""
            },
            {
                "License": "CC BY-NC 4.0",
                "License URL": "https://huggingface.co/datasets/lmsys/toxic-chat"
            }
        ],
        "License Notes": "Partially generated with Vicuna model, which itself is trained off of ShareGPT, which contains OpenAI generated outputs.",
        "License Verified By": "Zi Lin & Shayne",
        "Dataset Filter IDs": [
            "toxicchat0124"
        ],
        "Text Metrics": {
            "Num Dialogs": 5082,
            "Mean Inputs Length": 174.2794,
            "Mean Targets Length": 864.7033,
            "Max Inputs Length": 1536,
            "Max Targets Length": 4117,
            "Min Inputs Length": 12,
            "Min Targets Length": 1,
            "Min Dialog Turns": 2,
            "Max Dialog Turns": 2,
            "Mean Dialog Turns": 2.0
        },
        "Bibtex": "@Article{Lin2023ToxicChatUH,\n author = {Zi Lin and Zihan Wang and Yongqi Tong and Yangkun Wang and Yuxin Guo and Yujia Wang and Jingbo Shang},\n booktitle = {Conference on Empirical Methods in Natural Language Processing},\n pages = {4694-4702},\n title = {ToxicChat: Unveiling Hidden Challenges of Toxicity Detection in Real-World User-AI Conversation},\n year = {2023}\n}\n"
    }
}
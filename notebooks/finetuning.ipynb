{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8095d4af-c903-456e-8780-dc39917c963d",
   "metadata": {},
   "source": [
    "## TODO\n",
    "\n",
    "* Turn initial dat cell into CSV shareable across notebooks\n",
    "* Split this notebook up -- instruction table and refs/licenses table\n",
    "* Add the audio and video datasets to the refs/licenses table\n",
    "\n",
    "* Mock up audio table\n",
    "* Mock up video table\n",
    "\n",
    "## After paste into doc\n",
    "Formatting tweaks still needed after pasting these tables into the LaTeX doc:\n",
    "* Edit the subsequent-pages captions. Pandas defaults to the full caption, which is too long, and doesn't let you change it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1937b851-b7a2-4b52-adae-a4f32552376b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib as mp\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "import matplotlib.colors as mcolors\n",
    "\n",
    "import utils as ut"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "af21738b-828d-4f9c-94fd-712f092dc766",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(os.path.expanduser('~/github/Data-Provenance-Collection'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c2c05ed-0cfd-42d8-82f4-79461080f91c",
   "metadata": {},
   "source": [
    "# Prepare data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0784d801-73b6-4b53-8188-1e9fd496574f",
   "metadata": {},
   "source": [
    "## List of datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1b0aa587-e76a-4b65-ae75-b1f8bb0196a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "dat = pd.DataFrame([\n",
    "    ('Aya', 'Aya Dataset', 'singhAyaDatasetOpenAccess2024'),\n",
    "    ('ChatbotArena', 'ChatbotArena', 'zhengJudgingLLMasaJudgeMTBench2023'),\n",
    "    ('Open Asst. v2', 'Open Assistant v2', 'kopfOpenAssistantConversationsDemocratizing2023'),\n",
    "    ('UltraChat-200k', 'UltraChat_200k', 'dingEnhancingChatLanguage2023'),\n",
    "    ('AgentInstr.', 'AgentInstruct', 'shridharALFWorldAligningText2021|yaoWebShopScalableRealWorld2023|liuAgentBenchEvaluatingLLMs2023|zengAgentTuningEnablingGeneralized2023|dengMind2WebGeneralistAgent2023'),\n",
    "    ('Bactrian-X', 'Bactrian-X', 'liBactrianXMultilingualReplicable2023'),\n",
    "    ('COIG', 'COIG|COIG-CQIA', 'zhangChineseOpenInstruction2023|baiCOIGCQIAQualityAll2024'),\n",
    "    ('ChatDoctor', 'ChatDoctor', 'liChatDoctorMedicalChat2023'),\n",
    "    ('Cidar', 'Cidar', 'alyafeaiCIDARCulturallyRelevant2024'),\n",
    "    ('COBRA Frames', 'Cobra Frames', 'zhouCOBRAFramesContextual2023'),\n",
    "    ('Conifer', 'Conifer', 'sunConiferImprovingComplex2024'),\n",
    "    ('Deita 10K', 'Deita 10K', 'liuWhatMakesGood2024'),\n",
    "    ('ExpertQA', 'ExpertQA', 'malaviyaExpertQAExpertCuratedQuestions2024'),\n",
    "    ('Feedback Coll.', 'Feedback Collection', 'kimPrometheusInducingFinegrained2024'),\n",
    "    ('HelpSteer', 'HelpSteer', 'wangHelpSteerMultiattributeHelpfulness2023'),\n",
    "    ('Indic-Instr.', 'Indic-Instruct', 'galaAiravataIntroducingHindi2024'),\n",
    "    ('KIWI', 'KIWI', 'xuKIWIDatasetKnowledgeIntensive2024'),\n",
    "    ('Llama2-MedTuned-Instr.', 'Llama2-MedTuned-Instructions', 'rohanianExploringEffectivenessInstruction2023'),\n",
    "    ('LongAlign-10k', 'LongAlign-10k', 'baiLongAlignRecipeLong2024'),\n",
    "    ('MathDial', 'MathDial', 'macinaMathDialDialogueTutoring2023'),\n",
    "    ('MathInstr.', 'MathInstruct', 'yueMAmmoTHBuildingMath2023'),\n",
    "    ('MedInstr.', 'MedInstruct', 'zhangAlpaCareInstructiontunedLarge2024'),\n",
    "    ('Medical Meadow', 'Medical Meadow', 'hanMedAlpacaOpenSourceCollection2023|wangCORD19COVID19Open2020|jinWhatDiseaseDoes2020|saveryQuestionDrivenSummarizationAnswers2020'),\n",
    "    ('MegaWika', 'MegaWika', 'barhamMegaWikaMillionsReports2023'),\n",
    "    ('MetaMathQA', 'MetaMathQA', 'yuMetaMathBootstrapYour2023'),\n",
    "    ('OpenMathInstr.-1', 'OpenMathInstruct-1', 'toshniwalOpenMathInstruct1MillionMath2024'),\n",
    "    ('Orca-Math', 'Orca-Math', 'mitraOrcaMathUnlockingPotential2024'),\n",
    "    ('PMC-LLaMA Instr.', 'PMC-LLaMA Instructions', 'wuPMCLLaMABuildingOpensource2023|jinPubMedQADatasetBiomedical2019'),\n",
    "    ('PygmalionAI-PIPPA', 'PygmalionAI-PIPPA', 'goslingPIPPAPartiallySynthetic2023'),\n",
    "    ('RiddleSense', 'RiddleSense', 'linRiddleSenseReasoningRiddle2021'),\n",
    "    ('SeaBench', 'SeaBench', 'nguyenSeaLLMsLargeLanguage2023'),\n",
    "    ('SelFee', 'SelFee', 'yeSelFeeIterativeSelfRevising2023'),\n",
    "    ('WildChat', 'WildChat', 'zhaoWildChat1MChatGPT2023'),\n",
    "    ('LMSYS-Chat-1M', 'lmsys_chat_1m', 'zhengLMSYSChat1MLargeScaleRealWorld2024'),\n",
    "    ('Open-Platypus', 'Open-Platypus', 'sawadaARBAdvancedReasoning2023|dettmersQLoRAEfficientFinetuning2023|lightmanLetVerifyStep2023a|yuReClorReadingComprehension2020|wangSciBenchEvaluatingCollegeLevel2024|luLearnExplainMultimodal2022|chenTheoremQATheoremdrivenQuestion2023'),\n",
    "    ('DialogStudio', 'DialogStudio', 'chenActionBasedConversationsDataset2021|weiAirDialogueEnvironmentGoalOriented2018|linBiToDBilingualMultiDomain2021|chawlaCaSiNoCorpusCampsite2021|heDecouplingStrategyGeneration2018|mrksicNeuralBeliefTracker2017|qianDatabaseSearchResults2022|liuDuRecDialBilingualParallel2021|elasriFramesCorpusAdding2017|quanGECOREndtoEndGenerative2019|chenSemanticallyConditionedDialog2019a|chenKETODKnowledgeEnrichedTaskOriented2022|ericKeyValueRetrievalNetworks2017|zangMultiWOZDialogueDataset2020|shalyminovFewShotDialogueGeneration2019|martinMuDoCoCorpusMultidomain2020|peskovMultiDomainGoalOrientedDialogues2019|ericMultiWOZConsolidatedMultiDomain2019|moonOpenDialKGExplainableConversational2019|rastogiScalableMultidomainConversational2020|mosigSTARSchemaGuidedDialog2020|chiuSalesBotTransitioningChitChat2022|shahBuildingConversationalAgent2018|byrneTaskmaster1RealisticDiverse2019|mrksicFullyStatisticalNeural2018|shangUnsupervisedAbstractiveMeeting2018|rameshkumarStorytellingDialogueCritical2020|fabbriConvoSummConversationSummarization2021|chenDialogSumRealLifeScenario2021|mukherjeeECTSumNewBenchmark2022|shangUnsupervisedAbstractiveMeeting2018|zhuMediaSumLargescaleMedia2021|zhongQMSumNewBenchmark2021|gliwaSAMSumCorpusHumanannotated2019|chenSummScreenDatasetAbstractive2022|feigenblatTWEETSUMMDialogSummarization2021|liEndtoEndTrainableNonCollaborative2019|dinanSecondConversationalIntelligence2019|rashkinEmpatheticOpendomainConversation2019|baiTrainingHelpfulHarmless2022|chenPLACESPromptingLanguage2023|kimProsocialDialogProsocialBackbone2022|myersConversationalScaffoldingAnalogybased2020|reddyCoQAConversationalQuestion2019|yuCoSQLConversationalTexttoSQL2019|talmorWebKnowledgeBaseAnswering2018|nanDARTOpenDomainStructured2021|nanFeTaQAFreeformTable2022|guThreeLevelsGeneralization2021|chenHybridQADatasetMultiHop2020|guptaMMQAMultidomainMultilingual2018|liMTOPComprehensiveMultilingual2021|talmorMultiModalQAComplexQuestion2021|yuSParCCrossDomainSemantic2019|iyyerSearchbasedNeuralStructured2017|yuSpiderLargeScaleHumanLabeled2019|parikhToTToControlledTableToText2020|yihValueSemanticParse2016|zhongSeq2SQLGeneratingStructured2017|pasupatCompositionalSemanticParsing2015|komeiliInternetAugmentedDialogueGeneration2022|dinanWizardWikipediaKnowledgePowered2019|hemphillATISSpokenLanguage1990|casanuevaEfficientIntentDetection2020|zhangArePretrainedTransformers2022|larsonEvaluationDatasetIntent2019|rastogiScalableMultidomainConversational2020|liuBenchmarkingNaturalLanguage2019|liuAsgardPortableArchitecture2013|coopeSpanConveRTFewshotSpan2020|couckeSnipsVoicePlatform2018|guptaSemanticParsingTask2018'),\n",
    "    \n",
    "    # no paper - we could make @misc entries but have not\n",
    "    ('10k Prompt Ranked', '10k Prompt Ranked', ''),\n",
    "    ('Capybara', 'Capybara', ''),\n",
    "    ('CollectiveCognition', 'CollectiveCognition', ''),\n",
    "    ('EverythingLM', 'EverythingLM', ''),\n",
    "    ('Glaive Code Asst.', 'Glaive Code Assistant', ''),\n",
    "    ('Gretel Text-to-SQL', 'Gretel Text-to-SQL', ''),\n",
    "    ('Nectar', 'Nectar', ''),\n",
    "    ('No Robots', 'No Robots', ''),\n",
    "    ('OpenGPT Healthcare', 'OpenGPT Healthcare', ''),\n",
    "    ('PII-Masking-200k', 'PII-masking-200k', ''),\n",
    "    ('Pure-Dove', 'Pure-Dove', ''),\n",
    "    ('Thai Gen AI', 'Thai Gen AI (Alpaca)|Thai Gen AI (Dolly)|Thai Gen AI (GPTeacher)', ''),\n",
    "    ('UltraFeedback Argilla', 'UltraFeedback Argilla', ''),    \n",
    "], columns=['Collection', 'summary_keys', 'Cite']).set_index('Collection')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7508391f-367d-4a26-ae00-fbe06fd5f699",
   "metadata": {},
   "outputs": [],
   "source": [
    "files = [y for x in dat['summary_keys'].str.split('|').tolist() for y in x]\n",
    "\n",
    "assert set(files) <= set([\n",
    "    os.path.splitext(f)[0]\n",
    "    for f in os.listdir('data_summaries')\n",
    "    if not f.startswith('_template')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "949fe910-4d04-47ff-b215-a5a7f32d76ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = []\n",
    "short_names = pd.DataFrame(dat['summary_keys'].str.split('|')).to_records().tolist()\n",
    "for short, keys in short_names:\n",
    "    for key in keys:\n",
    "        tmp += [(short, key)]\n",
    "short_names = pd.DataFrame(tmp, columns=['short_name', 'summary_key']).set_index('summary_key')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b9f63d4-8588-43c1-90af-dbe5bc65f2e8",
   "metadata": {},
   "source": [
    "## Dimension data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "eb01af1d-e511-4056-a451-73b900988468",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('constants/domain_groups.json', 'rt') as f:\n",
    "    domain_groups = json.load(f)\n",
    "    domain_groups = {\n",
    "        v: k\n",
    "        for k, vs in domain_groups.items()\n",
    "        for v in vs\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0ef36a96-8993-4b89-9342-efedc7aaca6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('constants/task_groups.json', 'rt') as f:\n",
    "    task_groups = json.load(f)\n",
    "    task_groups = {\n",
    "        v: k\n",
    "        for k, vs in task_groups.items()\n",
    "        for v in vs\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "87523fb2-3c74-4f78-ab1e-972b0332c3a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('constants/license_classes.json', 'rt') as f:\n",
    "    license_classes = json.load(f)\n",
    "    license_classes = {k : v[0] for k, v in license_classes.items()}\n",
    "    license_classes['Custom'] = 'Unspecified'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b70abbe3-3456-4815-b66b-8e1420a211b6",
   "metadata": {},
   "source": [
    "## Load summary files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ddabc229-c3cf-4e19-8257-9c7642c6f1c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "summaries = {}\n",
    "for file in os.listdir('data_summaries'):\n",
    "    if file.startswith('_template'):\n",
    "        continue\n",
    "    \n",
    "    with open(os.path.join('data_summaries', file), 'rt') as f:\n",
    "        summaries[file.split('.')[0]] = json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42d2bce0-f790-42bd-a420-ed3fde6efd1d",
   "metadata": {},
   "source": [
    "# Make tables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b26662c7-5d8a-450c-81d7-72e1b7b4784e",
   "metadata": {},
   "source": [
    "## Licenses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "34659407-729b-4c53-87d3-efdce68c6e1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "licenses = {}\n",
    "for k in summaries.keys():\n",
    "    for ds in summaries[k].keys():\n",
    "        for lic in summaries[k][ds]['Licenses']:\n",
    "            licenses[k] = licenses.get(k, []) + [lic['License']]\n",
    "licenses = pd.Series({k : list(set(v)) for k, v in licenses.items()})\n",
    "\n",
    "tmp = short_names.copy()\n",
    "tmp['licenses'] = licenses\n",
    "dat['License'] = tmp.groupby('short_name')['licenses'].apply(lambda s: list(set([y for x in s for y in x])))\n",
    "\n",
    "dat['OAI'] = dat['License'].apply(lambda s: 'OpenAI' in s or 'OANC' in s)\n",
    "dat['OAI'] = dat['OAI'].replace({True: r'\\greencheck', False: '\\emojiblank'})\n",
    "dat['License'] = dat['License'].apply(lambda s: [v for v in s if v != 'OpenAI' and v != 'OANC'])\n",
    "\n",
    "license_table = dat[['License', 'Cite']].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ca1141ce-cfc0-4452-8879-8c8833ca54a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "dat['Use'] = dat['License'].apply(lambda s: list(set([license_classes[v] for v in s])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9c3a7601-7b39-4981-a7c5-25ab21d86969",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def color_license_classes(s):\n",
    "    ret = []\n",
    "\n",
    "    if 'All' in s:\n",
    "        ret += [r'\\CommercialDataCircle']\n",
    "    else:\n",
    "        ret += [r'\\TransparentCircle']\n",
    "    \n",
    "    if 'Unspecified' in s or len(s) == 0:\n",
    "        ret += [r'\\UnspecifiedDataCircle']\n",
    "    else:\n",
    "        ret += [r'\\TransparentCircle']\n",
    "    \n",
    "    if 'Acad' in s or 'NC' in s:\n",
    "        ret += [r'\\NCDataCircle']\n",
    "    else:\n",
    "        ret += [r'\\TransparentCircle']\n",
    "    \n",
    "    return ' '.join(ret)\n",
    "\n",
    "dat['Use'] = dat['License'].apply(lambda s: list(set([license_classes[v] for v in s])))\n",
    "dat['Use'] = dat['Use'].apply(color_license_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f3409805-d5df-4e1a-becd-1d8dc75ee360",
   "metadata": {},
   "outputs": [],
   "source": [
    "dat.drop(['License', 'Cite'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc195158-48fe-4775-825b-275a4649f958",
   "metadata": {},
   "source": [
    "## Property counts and text lens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2e0be802-6b39-4368-8ada-dd4aa237c784",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw = []\n",
    "for collection in dat.index:\n",
    "    for file in dat.loc[collection, 'summary_keys'].split('|'):\n",
    "        for k in summaries[file].keys():\n",
    "            if 'Languages' in summaries[file][k].keys():\n",
    "                langs = summaries[file][k]['Languages']\n",
    "            else:\n",
    "                langs = None\n",
    "            \n",
    "            metrics = summaries[file][k].get('Text Metrics', None)\n",
    "            if metrics is None or metrics == '' or metrics == {}:\n",
    "                num_dialogs = np.nan\n",
    "                mean_inputs_length = np.nan\n",
    "                mean_targets_length = np.nan\n",
    "            else:\n",
    "                num_dialogs = metrics['Num Dialogs']\n",
    "                mean_inputs_length = metrics['Mean Inputs Length']\n",
    "                mean_targets_length = metrics['Mean Targets Length']\n",
    "                \n",
    "            if 'Text Sources' not in summaries[file][k].keys():\n",
    "                domains = None\n",
    "            elif not isinstance(summaries[file][k]['Text Sources'], (list, tuple)):\n",
    "                domains = None\n",
    "            else:\n",
    "                domains = summaries[file][k]['Text Sources']\n",
    "                domains = [domain_groups[d] for d in domains]\n",
    "\n",
    "            if 'Task Categories' not in summaries[file][k].keys():\n",
    "                tasks = None\n",
    "            elif not isinstance(summaries[file][k]['Task Categories'], (list, tuple)):\n",
    "                tasks = None\n",
    "            else:\n",
    "                tasks = summaries[file][k]['Task Categories']\n",
    "                tasks = [task_groups[d] for d in tasks]\n",
    "\n",
    "            inf_metadata = summaries[file][k].get('Inferred Metadata', None)\n",
    "            if inf_metadata is None or inf_metadata == '' or inf_metadata == {}:\n",
    "                topics = None\n",
    "            else:\n",
    "                if 'Text Topics' not in inf_metadata.keys():\n",
    "                    topics = None\n",
    "                elif not isinstance(inf_metadata['Text Topics'], (list, tuple)):\n",
    "                    topics = None\n",
    "                else:\n",
    "                    topics = inf_metadata['Text Topics']\n",
    "\n",
    "            raw += [{\n",
    "                'collection': collection,\n",
    "                'summary_key': file,\n",
    "                'sub': k,\n",
    "\n",
    "                'num_dialogs': num_dialogs,\n",
    "                'mean_inputs_length': mean_inputs_length,\n",
    "                'mean_targets_length': mean_targets_length,\n",
    "\n",
    "                'langs': langs,\n",
    "                'topics': topics,\n",
    "                'domains': domains,\n",
    "                'tasks': tasks,\n",
    "                'datasets': 1,\n",
    "            }]\n",
    "raw = pd.DataFrame(raw)\n",
    "\n",
    "total_input_length = raw['num_dialogs'] * raw['mean_inputs_length']\n",
    "total_targets_length = raw['num_dialogs'] * raw['mean_targets_length']\n",
    "\n",
    "num_dialogs = raw.groupby('collection')['num_dialogs'].sum()\n",
    "mean_inputs_length = total_input_length.groupby(raw['collection']).sum() / num_dialogs\n",
    "mean_targets_length = total_targets_length.groupby(raw['collection']).sum() / num_dialogs\n",
    "\n",
    "num_langs = raw.groupby('collection')['langs'].apply(ut.count_unique_with_none)\n",
    "num_topics = raw.groupby('collection')['topics'].apply(ut.count_unique_with_none)\n",
    "num_domains = raw.groupby('collection')['domains'].apply(ut.count_unique_with_none)\n",
    "num_tasks = raw.groupby('collection')['tasks'].apply(ut.count_unique_with_none)\n",
    "num_datasets = raw.groupby('collection')['datasets'].sum()\n",
    "\n",
    "dat['Num Langs'] = num_langs.fillna(0).astype(int)\n",
    "dat['Num Dialogs'] = num_dialogs.fillna(0).astype(int)\n",
    "dat['Mean Inputs Length'] = mean_inputs_length.fillna(0).astype(int)\n",
    "dat['Mean Targets Length'] = mean_targets_length.fillna(0).astype(int)\n",
    "dat['Num Topics'] = num_topics.fillna(0).astype(int)\n",
    "dat['Num Datasets'] = num_datasets.fillna(0).astype(int)\n",
    "dat['Num Tasks'] = num_tasks.fillna(0).astype(int)\n",
    "dat['Num Domains'] = num_domains.fillna(0).astype(int).apply(lambda s: max(s, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3735c5f2-c5f7-4837-aedd-ead6cc21453d",
   "metadata": {},
   "source": [
    "## Source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fb479332-8bd6-412f-b841-87e10c790663",
   "metadata": {},
   "outputs": [],
   "source": [
    "mgen = {}\n",
    "for k in summaries.keys():\n",
    "    for ds in summaries[k].keys():\n",
    "        models = summaries[k][ds]['Model Generated']\n",
    "        models = [m for m in models if m != '']\n",
    "        mgen[k] = mgen.get(k, []) + [len(models) > 0]\n",
    "mgen = pd.Series({k : list(set(v)) for k, v in mgen.items()})\n",
    "tmp = short_names.copy()\n",
    "tmp['mgen'] = mgen\n",
    "\n",
    "sources = tmp.groupby('short_name')['mgen'] \\\n",
    "    .agg(lambda x: [item for sublist in x for item in sublist]) \\\n",
    "    .apply(lambda s: 1 - sum(s) / len(s))\n",
    "\n",
    "dat.loc[(sources > 0) & (sources < 1), 'Source'] = r'\\emoji{globe-with-meridians}\\emoji{robot}'\n",
    "dat.loc[sources == 1, 'Source'] = r'\\emoji{globe-with-meridians}\\emojiblank'\n",
    "dat.loc[sources == 0, 'Source'] = r'\\emojiblank\\emoji{robot}'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5838c91-b252-4be9-be85-3db3412cf581",
   "metadata": {},
   "source": [
    "## Format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "eb184790-c5c2-4611-b049-3a67529c346f",
   "metadata": {},
   "outputs": [],
   "source": [
    "formats_map = {\n",
    "    'Zero-shot': 'ZS',\n",
    "    'zero-shot': 'ZS',\n",
    "    'Few-shot': 'FS',\n",
    "\n",
    "    'Single-turn Dialog': 'SD',\n",
    "    'Multi-turn Dialog': 'MD',\n",
    "    \n",
    "    'Chain-of-Thought': 'CT',\n",
    "    'Program of Thoughts': 'PT',\n",
    "    'Program-of-Thought': 'PT',\n",
    "    \n",
    "    'Response Ranking': 'RR',\n",
    "    'Evaluation': 'EV',\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b6111a88-2c0c-462a-bdcc-21a296c22ae5",
   "metadata": {},
   "outputs": [],
   "source": [
    "found_formats = [\n",
    "    summaries[file][k]['Format']\n",
    "    for file in summaries.keys()\n",
    "    for k in summaries[file].keys()\n",
    "]\n",
    "found_formats = set([y for x in found_formats for y in x])\n",
    "assert found_formats <= set(formats_map.keys())\n",
    "\n",
    "fmts = []\n",
    "for collection in dat.index:\n",
    "    for file in dat.loc[collection, 'summary_keys'].split('|'):\n",
    "        for k in summaries[file].keys():\n",
    "            tmp_fmts = summaries[file][k].get('Format', [])\n",
    "            tmp_fmts = [formats_map[f] for f in tmp_fmts]\n",
    "            \n",
    "            fmts += [{\n",
    "                'collection': collection,\n",
    "                'summary_key': file,\n",
    "                'sub': k,\n",
    "                \n",
    "                'formats': tmp_fmts,\n",
    "            }]\n",
    "fmts = pd.DataFrame(fmts)\n",
    "fmts = fmts.groupby('collection')['formats'] \\\n",
    "           .apply(lambda s: list(set([y for x in s for y in x]))) \\\n",
    "           .rename('formats')\n",
    "\n",
    "for fmt in formats_map.values():\n",
    "    dat[fmt] = fmts.apply(lambda s: fmt in s) \\\n",
    "        .replace(True, r'\\greencheck') \\\n",
    "        .replace(False, r'\\emojiblank')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2acde63d-c13f-4346-bd7f-1047182717ff",
   "metadata": {},
   "source": [
    "## Format for LaTeX output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2e1a62b6-ce23-4d37-9598-21f9c3ea1c9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "dat = dat[[\n",
    "    'Num Datasets',\n",
    "    # 'Num Dialogs',\n",
    "    'Num Tasks',\n",
    "    'Num Langs',\n",
    "    # 'Num Topics',\n",
    "    'Num Domains',\n",
    "    \n",
    "    # 'Mean Inputs Length',\n",
    "    # 'Mean Targets Length',\n",
    "    \n",
    "    'Source',\n",
    "    \n",
    "    'ZS',\n",
    "    'FS',\n",
    "    'CT',\n",
    "    'RR',\n",
    "    'MD',\n",
    "    \n",
    "    'Use',\n",
    "    'OAI',\n",
    "]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6e435e1d-0fc5-47b2-b5c9-42d0f7d8784e",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'mp' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 74\u001b[0m\n\u001b[1;32m     68\u001b[0m         col_color_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(col)\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m) \\\n\u001b[1;32m     69\u001b[0m             \u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mtextsc\u001b[39m\u001b[38;5;124m{\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mthead\u001b[39m\u001b[38;5;124m{\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m}}\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m) \\\n\u001b[1;32m     70\u001b[0m             \u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mtextsc\u001b[39m\u001b[38;5;124m{\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m}\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     72\u001b[0m         color_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcolor\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrow_color_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mcol_color_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 74\u001b[0m         color_def \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mdefinecolor\u001b[39m\u001b[38;5;130;01m{{\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mcolor_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m}}\u001b[39;00m\u001b[38;5;130;01m{{\u001b[39;00m\u001b[38;5;124mRGB\u001b[39m\u001b[38;5;130;01m}}\u001b[39;00m\u001b[38;5;130;01m{{\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mcolor_map(value\u001b[38;5;250m \u001b[39m\u001b[38;5;241m/\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m4\u001b[39m,\u001b[38;5;250m \u001b[39mvmin\u001b[38;5;241m=\u001b[39mvmin,\u001b[38;5;250m \u001b[39mvmax\u001b[38;5;241m=\u001b[39mvmax)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m}}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     76\u001b[0m         tmp_val_color[col][dat\u001b[38;5;241m.\u001b[39mloc[row, col]] \u001b[38;5;241m=\u001b[39m color_name\n\u001b[1;32m     78\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDialogs\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m col[\u001b[38;5;241m1\u001b[39m]:  \u001b[38;5;66;03m# or 'Downs' in col[1]:\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[19], line 37\u001b[0m, in \u001b[0;36mcolor_map\u001b[0;34m(value, cmap, vmin, vmax)\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcolor_map\u001b[39m(value, cmap\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mBrBG\u001b[39m\u001b[38;5;124m'\u001b[39m, vmin\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, vmax\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m     36\u001b[0m     norm \u001b[38;5;241m=\u001b[39m mcolors\u001b[38;5;241m.\u001b[39mNormalize(vmin\u001b[38;5;241m=\u001b[39mvmin, vmax\u001b[38;5;241m=\u001b[39mvmax)\n\u001b[0;32m---> 37\u001b[0m     colormap \u001b[38;5;241m=\u001b[39m \u001b[43mmp\u001b[49m\u001b[38;5;241m.\u001b[39mcolormaps[cmap]\n\u001b[1;32m     38\u001b[0m     color \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mint\u001b[39m(\u001b[38;5;241m255\u001b[39m\u001b[38;5;241m*\u001b[39mx) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m colormap(norm(value))[:\u001b[38;5;241m3\u001b[39m]]\n\u001b[1;32m     39\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;28mmap\u001b[39m(\u001b[38;5;28mstr\u001b[39m, color))\n",
      "\u001b[0;31mNameError\u001b[0m: name 'mp' is not defined"
     ]
    }
   ],
   "source": [
    "column_mapping = {\n",
    "    'Num Datasets': ('Property Counts', 'Datasets'),\n",
    "    'Num Dialogs': ('Property Counts', 'Dialogs'),\n",
    "    'Num Tasks': ('Property Counts', 'Tasks'),\n",
    "    'Num Langs': ('Property Counts', 'Langs'),\n",
    "    'Num Topics': ('Property Counts', 'Topics'),\n",
    "    'Num Cites': ('Property Counts', 'Cites'),\n",
    "    'Num Domains': ('Property Counts', 'Domains'),\n",
    "    'Mean Inputs Length': ('Text Lens', 'Inpt'),\n",
    "    'Mean Targets Length': ('Text Lens', 'Tgt'),\n",
    "    'Source': ('Dataset Types', 'Source'),\n",
    "\n",
    "    'CT': ('Dataset Types', 'C'),\n",
    "    'ZS': ('Dataset Types', 'Z'),\n",
    "    'RR': ('Dataset Types', 'R'),\n",
    "    'MD': ('Dataset Types', 'M'),\n",
    "    'FS': ('Dataset Types', 'F'),\n",
    "    \n",
    "    'Use': ('Dataset Types', 'Use'),\n",
    "    'OAI': ('Dataset Types', 'O'),\n",
    "}\n",
    "\n",
    "dat.columns = pd.MultiIndex.from_arrays([\n",
    "    [column_mapping[col][0] for col in dat.columns],\n",
    "    [column_mapping[col][1] for col in dat.columns]\n",
    "])\n",
    "\n",
    "dat.columns = pd.MultiIndex.from_tuples([\n",
    "    (r'\\textsc{' + c[0] + r'}', r'\\textsc{\\thead{' + c[1] + r'}}')\n",
    "    for c in dat.columns\n",
    "])\n",
    "\n",
    "dat.index.name = r'\\textsc{' + dat.index.name + r'}'\n",
    "\n",
    "def color_map(value, cmap='BrBG', vmin=None, vmax=None):\n",
    "    norm = mcolors.Normalize(vmin=vmin, vmax=vmax)\n",
    "    colormap = mp.colormaps[cmap]\n",
    "    color = [int(255*x) for x in colormap(norm(value))[:3]]\n",
    "    return ','.join(map(str, color))  # Convert the color to a CSV string\n",
    "\n",
    "color_def = ''\n",
    "formatters = {}\n",
    "tmp_val_color = {}\n",
    "num_cols = [\n",
    "    c\n",
    "    for c in dat.columns\n",
    "    if (\n",
    "        'Property Counts' in c[0] or\n",
    "        'Text Lens' in c[0]\n",
    "    )\n",
    "]\n",
    "\n",
    "for col in num_cols:\n",
    "    tmp_val_color[col] = {}\n",
    "    \n",
    "    vmin = np.log(dat[col].min() + 1e-6)\n",
    "    vmax = np.log(dat[col].max() + 1e-6)\n",
    "    midpt = (vmax + vmin) / 2\n",
    "    vmin, vmax = vmin - midpt, vmax - midpt\n",
    "    \n",
    "    for row in dat.index:\n",
    "        value = np.log(dat.loc[row, col] + 1e-6)\n",
    "        value -= midpt\n",
    "        \n",
    "        if pd.notnull(value):\n",
    "            row_color_name = row.replace(' ', '') \\\n",
    "                .replace(r'\\textsc{', '').replace('}', '')\n",
    "            col_color_name = '_'.join(col).replace(' ', '') \\\n",
    "                .replace(r'\\textsc{\\thead{', '').replace('}}', '') \\\n",
    "                .replace(r'\\textsc{', '').replace('}', '')\n",
    "            \n",
    "            color_name = f\"color{row_color_name}{col_color_name}\"\n",
    "            \n",
    "            color_def += f\"\\\\definecolor{{{color_name}}}{{RGB}}{{{color_map(value / 4, vmin=vmin, vmax=vmax)}}}\\n\"\n",
    "            \n",
    "            tmp_val_color[col][dat.loc[row, col]] = color_name\n",
    "\n",
    "    if 'Dialogs' in col[1]:  # or 'Downs' in col[1]:\n",
    "        def func(v, col=col):\n",
    "            color_name = tmp_val_color[col][v]\n",
    "    \n",
    "            if col in num_cols and v >= 1000:\n",
    "                v /= 1000\n",
    "                return f'\\\\cellcolor{{{color_name}}}{{{v:,.0f}k}}' if pd.notnull(v) else '-'\n",
    "            elif col in num_cols and v == 0:\n",
    "                return '-'\n",
    "            else:\n",
    "                return f'\\\\cellcolor{{{color_name}}}{{<1k}}' if pd.notnull(v) else '-'\n",
    "    else:\n",
    "        def func(v, col=col):\n",
    "            color_name = tmp_val_color[col][v]\n",
    "    \n",
    "            if col in num_cols and v >= 1000:\n",
    "                v /= 1000\n",
    "                return f'\\\\cellcolor{{{color_name}}}{{{v:,.0f}k}}' if pd.notnull(v) else '-'\n",
    "            elif col in num_cols and v == 0:\n",
    "                return '-'\n",
    "            else:\n",
    "                return f'\\\\cellcolor{{{color_name}}}{{{v:,.0f}}}' if pd.notnull(v) else '-'\n",
    "    \n",
    "    formatters[col] = func"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6431fe91-5470-41fb-8207-f8aa2e740160",
   "metadata": {},
   "source": [
    "# Main table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1526d036-6a7c-482e-97d3-b4dfc5edd48e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "kwargs = {\n",
    "    'environment': 'longtable',\n",
    "    \n",
    "    'label': 'tab:collections',\n",
    "    'column_format': 'l|cccc|c|p{0.35cm}p{0.35cm}p{0.35cm}p{0.35cm}p{0.35cm}cp{0.35cm}',\n",
    "    'multicol_align': 'c',\n",
    "    \n",
    "    'caption': (r'''\n",
    "    \\textbf{Alignment tuning collections and properties}. Collection properties include numbers of datasets, tasks, languages, and text domains. The \\textsc{Source} column indicates whether a collection contains human-generated web text (\\emoji{globe-with-meridians}), language model outputs (\\emoji{robot}) or both (\\emoji{globe-with-meridians}\\emoji{robot}). Several columns indicate the type of dialogs, with some collections having more than one: zero-shot (Z), few-shot (F), response ranking (R), chain-of-thought (C), program-of-thought (PT), single-turn dialog (SD), multi-turn dialog (M), and evaluation (EV). Finally, the \\textsc{Use} column indicates whether a collection includes data freely usable even for commercial purposes (\\protect\\CommercialDataCircle), data usable only for noncommercial purposes or academic research (\\protect\\NCDataCircle) and data whose license status is not specified precisely enough to allow us to determine commercial use permissions (\\protect\\UnspecifiedDataCircle). Note that each collection may have different datasets with one, two, or all three of these statuses. The O column indicates collections which include OpenAI model generations.\n",
    "    '''.strip(), r'\\textbf{Alignment tuning collections and properties}'),\n",
    "    \n",
    "    'hrules': True,\n",
    "    'convert_css': True,\n",
    "}\n",
    "\n",
    "latex = dat \\\n",
    "    .sort_index() \\\n",
    "    .reset_index() \\\n",
    "    .style \\\n",
    "    .hide() \\\n",
    "    .format(formatter=formatters) \\\n",
    "    .to_latex(**kwargs)\n",
    "\n",
    "print('\\n'.join([\n",
    "    r'\\setlength{\\tabcolsep}{1.9pt}',\n",
    "    color_def,\n",
    "    latex,\n",
    "]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1335a6b5-9fb1-4c83-9954-80320d9257b7",
   "metadata": {},
   "source": [
    "# Appendix license/cite table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f6ec891-8345-4649-9d1a-b375c818a72b",
   "metadata": {},
   "outputs": [],
   "source": [
    "license_table['OpenAI NC'] = license_table['License'] \\\n",
    "    .apply(lambda s: 'OpenAI' in s) \\\n",
    "    .replace(True, r'\\redcross') \\\n",
    "    .replace(False, r'\\emojiblank')\n",
    "\n",
    "license_table['License'] = license_table['License'].apply(lambda s: [v for v in s if v != 'OpenAI'])\n",
    "license_table['License'] = license_table['License'].apply(lambda s: [v if v != 'Academic Research Purposes Only' else 'Academic Only' for v in s])\n",
    "license_table['License'] = license_table['License'].apply(lambda s: s if 'Various' not in s else ['Various'])\n",
    "license_table['License'] = license_table['License'].apply(lambda s: s if len(s) > 0 else ['Unspecified'])\n",
    "license_table['License'] = license_table['License'].apply(lambda s: s if len(s) <= 3 else ['Various'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3acd044b-a57c-445d-a3c5-83ca52edb804",
   "metadata": {},
   "outputs": [],
   "source": [
    "license_table = license_table[[\n",
    "#    'OpenAI NC',\n",
    "    'License',\n",
    "    'Cite',\n",
    "]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49f58c9d-c813-4e2e-a301-57c89a4e0fc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "license_table['Cite'] = license_table['Cite'].apply(lambda s: r'\\cite{' + s.replace('|', ',') + '}')\n",
    "license_table.loc[license_table['Cite'] == r'\\cite{}', 'Cite'] = '--'\n",
    "\n",
    "license_table['License'] = license_table['License'].str.join(', ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5a5568e-ffb2-45e3-81a1-55a3680b149a",
   "metadata": {},
   "outputs": [],
   "source": [
    "license_table.rename({'License': 'Licenses'}, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3abd2b9d-b83f-4c68-a24b-3787f13caec6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "kwargs = {\n",
    "    'environment': 'longtable',\n",
    "    \n",
    "    'label': 'tab:licenses',\n",
    "    'column_format': 'l|p{5cm}|p{5cm}',\n",
    "    \n",
    "    'caption': (r'''\n",
    "    \\textbf{References and licenses} for the dataset collections presented in this paper. Collections containing material under more than three distinct licenses are marked as having ''Various`` licenses, and we refer readers to our raw data for the full details.\n",
    "    '''.strip(), r'\\textbf{References and licenses}'),\n",
    "    \n",
    "    'hrules': True,\n",
    "    'convert_css': True,\n",
    "}\n",
    "\n",
    "latex = license_table \\\n",
    "    .sort_index() \\\n",
    "    .reset_index() \\\n",
    "    .style \\\n",
    "    .hide() \\\n",
    "    .to_latex(**kwargs)\n",
    "\n",
    "print(latex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa945fa4-75d7-4b50-aeb5-52e126e219e7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DPC",
   "language": "python",
   "name": "pdc"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
